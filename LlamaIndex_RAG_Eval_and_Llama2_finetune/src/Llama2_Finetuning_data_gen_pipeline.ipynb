{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16dd09d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jimta\\anaconda3\\envs\\NLP_env\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\jimta\\anaconda3\\envs\\NLP_env\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "C:\\Users\\jimta\\anaconda3\\envs\\NLP_env\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import chromadb\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from llama_index.core import VectorStoreIndex, StorageContext, Settings\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.response.notebook_utils import display_response\n",
    "from llama_index.readers.file import FlatReader\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f76dc7",
   "metadata": {},
   "source": [
    "### Intialize environment/global variables, load course material data from documents, transform into embedding vectors and store in vectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4eb7bc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "r_hf_token=os.getenv(\"HUGGINGFACE_READ_API\")\n",
    "w_hf_token=os.getenv(\"HUGGINGFACE_WRITE_API\")\n",
    "os.environ['OPENAI_API_KEY']=os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "INPUT_DATA_PATH = '../data/course_materials_markdown' # Path to markdown file that contains all the course materials text\n",
    "EMBED_MODLE_ID = \"BAAI/bge-small-en-v1.5\" # Embedding model ID\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are an AI teaching Assistant for the course SEP 775. \n",
    "You will provide an interactive platform for students to ask questions and receive guidance on course materials.\n",
    "Your goal is to answer questions as accurately as possible based on the instructions and context provided.\n",
    "If you do not know the answer, response with \"I don't know.\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "081652e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "md_doc = FlatReader().load_data(Path('../data/materials_formatted.md'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f8fcce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up text chunk settings and embedding model\n",
    "Settings.chunk_size = 512\n",
    "Settings.chunk_overlap = 20\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=EMBED_MODLE_ID, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fad0270a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a Chroma VectorDB\n",
    "db = chromadb.PersistentClient(path=\"../course_materials_db\")\n",
    "chroma_collection = db.get_or_create_collection(\"NLP_course_materials\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5221f6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and store embedding vectors from course material docs, or load embeddings from an existing vectorDB\n",
    "#index = VectorStoreIndex.from_documents(md_doc, storage_context=storage_context)\n",
    "index_from_vec_store = VectorStoreIndex.from_vector_store(vector_store, embed_model=Settings.embed_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b7c671",
   "metadata": {},
   "source": [
    "### Generate training data for finetuning Llama2 model, with ChatGPT generated questions and GPT-4 as the RAG system's LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f55db304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 230 sample questions generated by ChatGPT with GPT-4\n",
    "questions_context_df = pd.read_csv('../data/question_context.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ce5a27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 230 entries, 0 to 229\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Question  230 non-null    object\n",
      " 1   Content   230 non-null    object\n",
      " 2   Type      230 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 5.5+ KB\n"
     ]
    }
   ],
   "source": [
    "questions_context_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04b0b45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up LLM and query engine\n",
    "Settings.llm = OpenAI(model=\"gpt-4\", temperature=0.3, system_prompt=SYSTEM_PROMPT)\n",
    "Settings.context_window = 2048\n",
    "query_engine = index_from_vec_store.as_query_engine(similarity_top_k=3,response_mode=\"compact\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53970d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_QCR_data(questions, save_to_file):\n",
    "    \"\"\"\n",
    "    Function for generating question-context-response dataset for finetuning Llama2 model and testing finetuned Llama2 model\n",
    "    \n",
    "    \"\"\"\n",
    "    out_path = save_to_file\n",
    "\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    no_ans_Q = 0\n",
    "    for Q in tqdm(questions):\n",
    "        response = query_engine.query(Q)\n",
    "        context = [x.node.get_content() for x in response.source_nodes]\n",
    "        answer = str(response)\n",
    "        if \"not provide\" in answer:\n",
    "            no_ans_Q += 1\n",
    "            continue\n",
    "        with open(out_path, \"a\") as f:\n",
    "            newitem = {\n",
    "                \"question\": Q,\n",
    "                \"context\": context,\n",
    "                \"response\": answer,\n",
    "            }\n",
    "            f.write(json.dumps(newitem) + \"\\n\")\n",
    "    \n",
    "    print(\"QCR data generated, roughly %d questions can't find answer from the context.\"%no_ans_Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31560646",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 230/230 [17:47<00:00,  4.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QCR data generated, roughly 7 questions can't find answer from the context.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gen_QCR_data(questions_context_df['Question'].values,Path(\"../llm_finetune_data/QCR_data.jsonl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e040748b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_QCR_data = load_dataset(\"json\", data_files=Path(\"../llm_finetune_data/QCR_data.jsonl\").as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf7f12f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_prompt_dict(prompts_dict):\n",
    "    for k, p in prompts_dict.items():\n",
    "        text_md = f\"**Prompt Key**: {k}<br>\" f\"**Text:** <br>\"\n",
    "        display(Markdown(text_md))\n",
    "        print(p.get_template())\n",
    "        display(Markdown(\"<br><br>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a971970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Prompt Key**: response_synthesizer:text_qa_template<br>**Text:** <br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context information is below.\n",
      "---------------------\n",
      "{context_str}\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: {query_str}\n",
      "Answer: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Prompt Key**: response_synthesizer:refine_template<br>**Text:** <br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original query is as follows: {query_str}\n",
      "We have provided an existing answer: {existing_answer}\n",
      "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
      "------------\n",
      "{context_msg}\n",
      "------------\n",
      "Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\n",
      "Refined Answer: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_prompt_dict(query_engine.get_prompts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bca4477e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11cd57904dd641fd83f8305462a87dd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc2e31453c6d4127ae8120a1ca145ff0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "270472"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the QCR_data into train and test sets, save the test set\n",
    "ref_QCR_data_split = ref_QCR_data['train'].train_test_split(test_size=40, shuffle=True, seed=97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c7a9b1e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'context', 'response'],\n",
       "    num_rows: 40\n",
       "})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_QCR_data_split['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0505a96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_QCR_test_data_df = pd.DataFrame({\"questions\":ref_QCR_data_split['test']['question'],\n",
    "                                     \"responses\":ref_QCR_data_split['test']['response'],})\n",
    "ref_QCR_test_data_df.to_json(Path(\"../llm_finetune_data/ref_QCR_test_data.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a60ac8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_prompt(qcr_item):\n",
    "    \"\"\"\n",
    "    Function to inject a sample question-contexts-response data into \n",
    "    the LlamaIndex default prompt template with the predefined system prompt and response\n",
    "    \"\"\"\n",
    "    context_str = (\"filename: materials_formatted.md\\nextension: .md\\n\\n\"+\n",
    "                   \"\\n\\nfilename: materials_formatted.md\\nextension: .md\\n\\n\".join(qcr_item['context']))\n",
    "    query_str = \"[INST]<<SYS>>\\n\" + SYSTEM_PROMPT + \"<</SYS>>\\n\\n\"+qcr_item['question']+\"[/INST] \"\n",
    "    \n",
    "    return f\"\"\"Context information is below.\n",
    "---------------------\n",
    "{context_str}\n",
    "---------------------\n",
    "Given the context information and not prior knowledge, answer the query.\n",
    "Query: {query_str}\n",
    "Answer: {qcr_item['response']}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4e337502",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context information is below.\n",
      "---------------------\n",
      "filename: materials_formatted.md\n",
      "extension: .md\n",
      "\n",
      "Our experiments show that using triplet loss does not affect the results much. More details can be found in Appendix B.  Cross-dataset generalization One interesting question regarding DPR’s discriminative training is how much performance degradation it may suf- fer from a non-iid setting. In other words, can it still generalize well when directly applied to a different dataset without additional ﬁne-tuning? To test the cross-dataset generalization, we train DPR on Natural Questions only and test it directly on the smaller WebQuestions and CuratedTREC datasets. We ﬁnd that DPR generalizes well, with 3-5 points loss from the best performing ﬁne-tuned model in top-20 retrieval accuracy (69.9/86.3 vs. 75.0/89.1 for WebQuestions and TREC, respec- tively), while still greatly outperforming the BM25 baseline (55.0/70.9).  5.3 Qualitative Analysis  Although DPR performs better than BM25 in gen- eral, passages retrieved by these two methods dif- fer qualitatively. Term-matching methods like BM25 are sensitive to highly selective keywords and phrases, while DPR captures lexical variations or semantic relationships better. See Appendix C for examples and more discussion.  5.4 Run-time Efﬁciency  The main reason that we require a retrieval compo- nent for open-domain QA is to reduce the number of candidate passages that the reader needs to con- sider, which is crucial for answering user’s ques- tions in real-time. We proﬁled the passage retrieval speed on a server with Intel Xeon CPU E5-2698 v4 @ 2.20GHz and 512GB memory. With the help of FAISS in-memory index for real-valued vectors10, DPR can be made incredibly efﬁcient, processing 995.0 questions per second, returning top 100 pas- sages per question. In contrast, BM25/Lucene (im- plemented in Java, using ﬁle index) processes 23.7 questions per second per CPU thread.  On the other hand, the time required for building an index for dense vectors is much longer.\n",
      "\n",
      "filename: materials_formatted.md\n",
      "extension: .md\n",
      "\n",
      "For a ﬁxed k, a retriever can be evaluated in isolation on top-k retrieval accuracy, which is the fraction of ques- tions for which CF contains a span that answers the question.  3 Dense Passage Retriever (DPR)  We focus our research in this work on improv- ing the retrieval component in open-domain QA. Given a collection of M text passages, the goal of our dense passage retriever (DPR) is to index all the passages in a low-dimensional and continuous space, such that it can retrieve efﬁciently the top k passages relevant to the input question for the reader at run-time. Note that M can be very large (e.g., 21 million passages in our experiments, de- scribed in Section 4.1) and k is usually small, such as 20–100.  3.1 Overview  Our dense passage retriever (DPR) uses a dense encoder EP (·) which maps any text passage to a d- dimensional real-valued vectors and builds an index for all the M passages that we will use for retrieval.  3The ideal size and boundary of a text passage are func- tions of both the retriever and reader. We also experimented with natural paragraphs in our preliminary trials and found that using ﬁxed-length passages performs better in both retrieval and ﬁnal QA accuracy, as observed by Wang et al. (2019).  4Exceptions include (Seo et al., 2019) and (Roberts et al., 2020), which retrieves and generates the answers, respectively.  At run-time, DPR applies a different encoder EQ(·) that maps the input question to a d-dimensional vector, and retrieves k passages of which vectors are the closest to the question vector. We deﬁne the similarity between the question and the passage using the dot product of their vectors:  sim(q, p) = EQ(q)(cid:124)EP (p).  (1)  Although more expressive model forms for measur- ing the similarity between a question and a passage do exist, such as networks consisting of multiple layers of cross attentions, the similarity function needs to be decomposable so that the represen- tations of the collection of passages can be pre- computed. Most decomposable similarity functions are some transformations of Euclidean distance (L2).\n",
      "\n",
      "filename: materials_formatted.md\n",
      "extension: .md\n",
      "\n",
      "Starting from our trained DPR model, they show that the retrieval performance can be further improved. Recent work (Izacard and Grave, 2020; Lewis et al., 2020b) have also shown that DPR can be combined with generation models such as BART (Lewis et al., 2020a) and T5 (Raf- fel et al., 2019), achieving good performance on open-domain QA and other knowledge-intensive tasks.  8 Conclusion  In this work, we demonstrated that dense retrieval can outperform and potentially replace the tradi- tional sparse retrieval component in open-domain question answering. While a simple dual-encoder approach can be made to work surprisingly well, we showed that there are some critical ingredients to training a dense retriever successfully. Moreover, our empirical analysis and ablation studies indicate that more complex model frameworks or similarity functions do not necessarily provide additional val- ues. As a result of improved retrieval performance, we obtained new state-of-the-art results on multiple open-domain question answering benchmarks.  Acknowledgments  We thank the anonymous reviewers for their helpful comments and suggestions.  References  Akari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi, Richard Socher, and Caiming Xiong. 2020. Learn- ing to retrieve reasoning paths over Wikipedia graph for question answering. In International Conference on Learning Representations (ICLR).  Petr Baudiˇs and Jan ˇSediv`y. 2015. Modeling of the question answering task in the yodaqa system. In In- ternational Conference of the Cross-Language Eval- uation Forum for European Languages, pages 222– 228. Springer.  Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on Freebase from  question-answer pairs. In Empirical Methods in Nat- ural Language Processing (EMNLP).  Jane Bromley, Isabelle Guyon, Yann LeCun, Eduard S¨ackinger, and Roopak Shah. 1994. Signature veriﬁ- cation using a “Siamese” time delay neural network. In NIPS, pages 737–744.\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: [INST]<<SYS>>\n",
      "\n",
      "You are an AI teaching Assistant for the course SEP 775. \n",
      "You will provide an interactive platform for students to ask questions and receive guidance on course materials.\n",
      "Your goal is to answer questions as accurately as possible based on the instructions and context provided.\n",
      "If you do not know the answer, response with \"I don't know.\"\n",
      "<</SYS>>\n",
      "\n",
      "How does DPR achieve efficient retrieval at run-time?[/INST] \n",
      "Answer: DPR achieves efficient retrieval at run-time by using a different encoder that maps the input question to a d-dimensional vector. It then retrieves k passages whose vectors are the closest to the question vector. The similarity between the question and the passage is defined by the dot product of their vectors. This allows DPR to index all the passages in a low-dimensional and continuous space, enabling efficient retrieval of the top k passages relevant to the input question.\n"
     ]
    }
   ],
   "source": [
    "print(gen_prompt(ref_QCR_data_split['train'][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5b4335cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_prompt_response_data(qcr_dataset, save_to_file):\n",
    "    \"\"\"\n",
    "    Function to transfer question-context-response dataset into dataset of concatenated prompts and responses\n",
    "    \"\"\"\n",
    "    dataset_splits = {\"train\": qcr_dataset[\"train\"]}\n",
    "    out_path = save_to_file\n",
    "\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for key, ds in dataset_splits.items():\n",
    "        with open(out_path, \"a\") as f:\n",
    "            for item in ds:\n",
    "                prompt = gen_prompt(item)\n",
    "                newitem = {\n",
    "                    \"prompt_response_text\": prompt\n",
    "                }\n",
    "                f.write(json.dumps(newitem) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e7184771",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_prompt_response_data(ref_QCR_data_split, Path(\"../llm_finetune_data/prompt_response_data.jsonl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "abca5f28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73c511b3f7914b6ab59b9fb90fea7b09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt_response_data = load_dataset(\"json\", data_files=Path(\"../llm_finetune_data/prompt_response_data.jsonl\").as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8f557e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_data_split = prompt_response_data['train'].train_test_split(test_size=0.3, shuffle=True, seed=97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "738ab52b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt_response_text'],\n",
       "        num_rows: 128\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['prompt_response_text'],\n",
       "        num_rows: 55\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr_data_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "32f22167",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context information is below.\n",
      "---------------------\n",
      "filename: materials_formatted.md\n",
      "extension: .md\n",
      "\n",
      "Our experiments show that using triplet loss does not affect the results much. More details can be found in Appendix B.  Cross-dataset generalization One interesting question regarding DPR’s discriminative training is how much performance degradation it may suf- fer from a non-iid setting. In other words, can it still generalize well when directly applied to a different dataset without additional ﬁne-tuning? To test the cross-dataset generalization, we train DPR on Natural Questions only and test it directly on the smaller WebQuestions and CuratedTREC datasets. We ﬁnd that DPR generalizes well, with 3-5 points loss from the best performing ﬁne-tuned model in top-20 retrieval accuracy (69.9/86.3 vs. 75.0/89.1 for WebQuestions and TREC, respec- tively), while still greatly outperforming the BM25 baseline (55.0/70.9).  5.3 Qualitative Analysis  Although DPR performs better than BM25 in gen- eral, passages retrieved by these two methods dif- fer qualitatively. Term-matching methods like BM25 are sensitive to highly selective keywords and phrases, while DPR captures lexical variations or semantic relationships better. See Appendix C for examples and more discussion.  5.4 Run-time Efﬁciency  The main reason that we require a retrieval compo- nent for open-domain QA is to reduce the number of candidate passages that the reader needs to con- sider, which is crucial for answering user’s ques- tions in real-time. We proﬁled the passage retrieval speed on a server with Intel Xeon CPU E5-2698 v4 @ 2.20GHz and 512GB memory. With the help of FAISS in-memory index for real-valued vectors10, DPR can be made incredibly efﬁcient, processing 995.0 questions per second, returning top 100 pas- sages per question. In contrast, BM25/Lucene (im- plemented in Java, using ﬁle index) processes 23.7 questions per second per CPU thread.  On the other hand, the time required for building an index for dense vectors is much longer.\n",
      "\n",
      "filename: materials_formatted.md\n",
      "extension: .md\n",
      "\n",
      "For a ﬁxed k, a retriever can be evaluated in isolation on top-k retrieval accuracy, which is the fraction of ques- tions for which CF contains a span that answers the question.  3 Dense Passage Retriever (DPR)  We focus our research in this work on improv- ing the retrieval component in open-domain QA. Given a collection of M text passages, the goal of our dense passage retriever (DPR) is to index all the passages in a low-dimensional and continuous space, such that it can retrieve efﬁciently the top k passages relevant to the input question for the reader at run-time. Note that M can be very large (e.g., 21 million passages in our experiments, de- scribed in Section 4.1) and k is usually small, such as 20–100.  3.1 Overview  Our dense passage retriever (DPR) uses a dense encoder EP (·) which maps any text passage to a d- dimensional real-valued vectors and builds an index for all the M passages that we will use for retrieval.  3The ideal size and boundary of a text passage are func- tions of both the retriever and reader. We also experimented with natural paragraphs in our preliminary trials and found that using ﬁxed-length passages performs better in both retrieval and ﬁnal QA accuracy, as observed by Wang et al. (2019).  4Exceptions include (Seo et al., 2019) and (Roberts et al., 2020), which retrieves and generates the answers, respectively.  At run-time, DPR applies a different encoder EQ(·) that maps the input question to a d-dimensional vector, and retrieves k passages of which vectors are the closest to the question vector. We deﬁne the similarity between the question and the passage using the dot product of their vectors:  sim(q, p) = EQ(q)(cid:124)EP (p).  (1)  Although more expressive model forms for measur- ing the similarity between a question and a passage do exist, such as networks consisting of multiple layers of cross attentions, the similarity function needs to be decomposable so that the represen- tations of the collection of passages can be pre- computed. Most decomposable similarity functions are some transformations of Euclidean distance (L2).\n",
      "\n",
      "filename: materials_formatted.md\n",
      "extension: .md\n",
      "\n",
      "Starting from our trained DPR model, they show that the retrieval performance can be further improved. Recent work (Izacard and Grave, 2020; Lewis et al., 2020b) have also shown that DPR can be combined with generation models such as BART (Lewis et al., 2020a) and T5 (Raf- fel et al., 2019), achieving good performance on open-domain QA and other knowledge-intensive tasks.  8 Conclusion  In this work, we demonstrated that dense retrieval can outperform and potentially replace the tradi- tional sparse retrieval component in open-domain question answering. While a simple dual-encoder approach can be made to work surprisingly well, we showed that there are some critical ingredients to training a dense retriever successfully. Moreover, our empirical analysis and ablation studies indicate that more complex model frameworks or similarity functions do not necessarily provide additional val- ues. As a result of improved retrieval performance, we obtained new state-of-the-art results on multiple open-domain question answering benchmarks.  Acknowledgments  We thank the anonymous reviewers for their helpful comments and suggestions.  References  Akari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi, Richard Socher, and Caiming Xiong. 2020. Learn- ing to retrieve reasoning paths over Wikipedia graph for question answering. In International Conference on Learning Representations (ICLR).  Petr Baudiˇs and Jan ˇSediv`y. 2015. Modeling of the question answering task in the yodaqa system. In In- ternational Conference of the Cross-Language Eval- uation Forum for European Languages, pages 222– 228. Springer.  Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on Freebase from  question-answer pairs. In Empirical Methods in Nat- ural Language Processing (EMNLP).  Jane Bromley, Isabelle Guyon, Yann LeCun, Eduard S¨ackinger, and Roopak Shah. 1994. Signature veriﬁ- cation using a “Siamese” time delay neural network. In NIPS, pages 737–744.\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: [INST]<<SYS>>\n",
      "\n",
      "You are an AI teaching Assistant for the course SEP 775. \n",
      "You will provide an interactive platform for students to ask questions and receive guidance on course materials.\n",
      "Your goal is to answer questions as accurately as possible based on the instructions and context provided.\n",
      "If you do not know the answer, response with \"I don't know.\"\n",
      "<</SYS>>\n",
      "\n",
      "How does DPR achieve efficient retrieval at run-time?[/INST] \n",
      "Answer: DPR achieves efficient retrieval at run-time by using a different encoder that maps the input question to a d-dimensional vector. It then retrieves k passages whose vectors are the closest to the question vector. The similarity between the question and the passage is defined by the dot product of their vectors. This allows DPR to index all the passages in a low-dimensional and continuous space, enabling efficient retrieval of the top k passages relevant to the input question.\n"
     ]
    }
   ],
   "source": [
    "print(prompt_response_data['train'][1]['prompt_response_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f83ffc",
   "metadata": {},
   "source": [
    "## After running all the code cells above, upload the course_materials_db directory, the llm_finetune_data direcotry and the Llama2_Finetune_Eval_pipeline_colab notebook onto Google Drive and run the notebook with T4 GPU and High RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019735b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
