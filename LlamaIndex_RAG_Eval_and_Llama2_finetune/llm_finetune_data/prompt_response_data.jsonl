{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nThe word 2 vec model is a probabilistic model specified as follows, where uwrefers to the row ofUcorresponding to wordw\u2208V(and likewise forV):   pU,V(o|c) = expu\u22a4ovc \u2211w\u2208Vexpu\u22a4wvc   ( 4 )  This may be familiar to you as the softmax function, which takes arbitrary scores (here, one for each word in the vocabulary, resulting from dot products) and produces a probability distribution where larger-scored things get higher probability. Note that the vector of probabilities over all words given a center wordpU,V(\u00b7|c)\u2208R|V|is a lot like a row of our old co-occurrence matrix X c. The story isn\u2019t over yet; this is just a model. How do we estimate the values of the parametersU,V? We learn to minimize the cross- entropy loss objective with the true distributionP\u2217(O|C):   min U,V E o,c  ####    \u2212logpU,V(o|c)  ####   #### . ( 5 )  This equation should be read as \u201cminimize with respect to param- etersUandVthe expectation over values ofoandcdrawn from the distributions ofOandVthe negative-log probability under the (U,V)-model of that value ofogiven that value ofc\u201d. There is so much rich detail to get into here. How do we perform theminoperation? How do we \u201cget\u201d the random variablesOandC? Why the negative-log of the probability? Why is this so much better than co-occurrence counting? Can you tell whynot all distributions overogivenccan be represented by this model?(Should this be good? Bad? Surprising? Obvious?) For now, let\u2019s go through a few details about how to implement this in practice.  ### 3. 3 Estimating a word 2 vec model from a corpus  How do we train word 2 vec in practice? Specifying the word 2 vec model is relatively transparent from the math we\u2019ve given above: one constructs matricesUandVand can write out the math of the probability.\n\nfilename: materials_formatted.md\nextension: .md\n\nThis ends up being a big mistake. We\u2019ve already stated that high-dimensional vectors tend to be un- wieldy in today\u2019s neural systems. But another issue is that raw counts of words end up over-emphasizing the importance of very common words likethe. Taking the log token frequency ends up being much more useful. A very influential paper on word repre- sentation taught us much more about what is wrong with the raw co-occurrence method by introducing GloVe (Pennington et al., 2014 ) a co-occurence-based word representation algorithm that works as well as word 2 vec, the method we\u2019ll introduce in the next section. However, many of the details of word 2 vec will hold true in methods that we\u2019ll proceed to further in the course, so we\u2019ll focus our time on that.  ### 3. 2 Word 2 vec model and objective  The word 2 vec model represents each word in a fixed vocabulary as a low-dimensional (much smaller than vocabulary size) vector. It learns the value of each word\u2019s vector to be predictive via a simple function of the distribution of words in a (usually short; 2 - 4 words) context. The model we\u2019ll describe here is called theskipgramword 2 vec algo- rithm.  Skipgram word _2_ vec. As usual, we have a finite vocabularyV. LetC,O be random variables representing an (unknown) pair ofC\u2208 V(a    deep learning 8  center word), andO\u2208 V(anoutside word, appearing in the context of the center word). We\u2019ll usec,oto refer to specific values of the random variables. LetU \u2208 R |V|\u00d7dandV \u2208 R |V|\u00d7d. Note that each word inVis associated with a single row ofUand one ofV; we think of this as resulting from an arbitrary ordering ofV.\n\nfilename: materials_formatted.md\nextension: .md\n\nSo, thevcvector is updated to be \u201cmore like\u201d the word vector that was actually observed than the word vector it expected. If you didn\u2019t follow the math above, don\u2019t fear; I\u2019d suggest going through it a few times and not rushing yourself. And if this was all dreadfully boring to you because you understood it quickly, use your newfound free time to help teach others!  ### 3. 5 Skipgram-negative-sampling  Now that we\u2019re doing stochastic estimates of our gradients, one re- maining bottleneck of efficiency in estimating our word 2 vec model is computing the exact model probability\u2212logp(U,V)(o|c)when computingL\u02c6(U,V). For a given word, it is cheap to compute the un- normalized score exp(uc\u22a4vo). However, it is expensive to compute the partition function (sum of scores for all words) in the denomina- tor, since it requries a term for each word in the vocabulary.    deep learning 12  Intuitively, what is the partition function doing, such that we might understand how to remove it? Let\u2019s repeat the softmax here:   pU,V(o|c) =   expu\u22a4ovc \u2211w\u2208Vexpu\u22a4wvc   ( 14 )   Affinity of wordcfor contexto   Partition function, or, normalization  From a probabilistic perspective, the partition function guarantees a probability by normalizing the scores to sum to 1. (The exponen- tial guarantees that the scores are non-negative.) From a learning perspective, the partition function \u201cpushes down\u201d on all the words other than the observed words. Put another way, the numerator of this equation encourages the model to makeuomore likevc; the denominator encourages all the otheruwforw\u0338=oless likevc. The intuition of negative sampling is that we don\u2019t need to push down on all the uw all the time , since that\u2019s where most of the cost comes from.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat kind of model does the 'word 2 vec' algorithm represent each word as?[/INST] \nAnswer: The 'word 2 vec' algorithm represents each word in a fixed vocabulary as a low-dimensional vector."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nOur experiments show that using triplet loss does not affect the results much. More details can be found in Appendix B.  Cross-dataset generalization One interesting question regarding DPR\u2019s discriminative training is how much performance degradation it may suf- fer from a non-iid setting. In other words, can it still generalize well when directly applied to a different dataset without additional \ufb01ne-tuning? To test the cross-dataset generalization, we train DPR on Natural Questions only and test it directly on the smaller WebQuestions and CuratedTREC datasets. We \ufb01nd that DPR generalizes well, with 3-5 points loss from the best performing \ufb01ne-tuned model in top-20 retrieval accuracy (69.9/86.3 vs. 75.0/89.1 for WebQuestions and TREC, respec- tively), while still greatly outperforming the BM25 baseline (55.0/70.9).  5.3 Qualitative Analysis  Although DPR performs better than BM25 in gen- eral, passages retrieved by these two methods dif- fer qualitatively. Term-matching methods like BM25 are sensitive to highly selective keywords and phrases, while DPR captures lexical variations or semantic relationships better. See Appendix C for examples and more discussion.  5.4 Run-time Ef\ufb01ciency  The main reason that we require a retrieval compo- nent for open-domain QA is to reduce the number of candidate passages that the reader needs to con- sider, which is crucial for answering user\u2019s ques- tions in real-time. We pro\ufb01led the passage retrieval speed on a server with Intel Xeon CPU E5-2698 v4 @ 2.20GHz and 512GB memory. With the help of FAISS in-memory index for real-valued vectors10, DPR can be made incredibly ef\ufb01cient, processing 995.0 questions per second, returning top 100 pas- sages per question. In contrast, BM25/Lucene (im- plemented in Java, using \ufb01le index) processes 23.7 questions per second per CPU thread.  On the other hand, the time required for building an index for dense vectors is much longer.\n\nfilename: materials_formatted.md\nextension: .md\n\nFor a \ufb01xed k, a retriever can be evaluated in isolation on top-k retrieval accuracy, which is the fraction of ques- tions for which CF contains a span that answers the question.  3 Dense Passage Retriever (DPR)  We focus our research in this work on improv- ing the retrieval component in open-domain QA. Given a collection of M text passages, the goal of our dense passage retriever (DPR) is to index all the passages in a low-dimensional and continuous space, such that it can retrieve ef\ufb01ciently the top k passages relevant to the input question for the reader at run-time. Note that M can be very large (e.g., 21 million passages in our experiments, de- scribed in Section 4.1) and k is usually small, such as 20\u2013100.  3.1 Overview  Our dense passage retriever (DPR) uses a dense encoder EP (\u00b7) which maps any text passage to a d- dimensional real-valued vectors and builds an index for all the M passages that we will use for retrieval.  3The ideal size and boundary of a text passage are func- tions of both the retriever and reader. We also experimented with natural paragraphs in our preliminary trials and found that using \ufb01xed-length passages performs better in both retrieval and \ufb01nal QA accuracy, as observed by Wang et al. (2019).  4Exceptions include (Seo et al., 2019) and (Roberts et al., 2020), which retrieves and generates the answers, respectively.  At run-time, DPR applies a different encoder EQ(\u00b7) that maps the input question to a d-dimensional vector, and retrieves k passages of which vectors are the closest to the question vector. We de\ufb01ne the similarity between the question and the passage using the dot product of their vectors:  sim(q, p) = EQ(q)(cid:124)EP (p).  (1)  Although more expressive model forms for measur- ing the similarity between a question and a passage do exist, such as networks consisting of multiple layers of cross attentions, the similarity function needs to be decomposable so that the represen- tations of the collection of passages can be pre- computed. Most decomposable similarity functions are some transformations of Euclidean distance (L2).\n\nfilename: materials_formatted.md\nextension: .md\n\nStarting from our trained DPR model, they show that the retrieval performance can be further improved. Recent work (Izacard and Grave, 2020; Lewis et al., 2020b) have also shown that DPR can be combined with generation models such as BART (Lewis et al., 2020a) and T5 (Raf- fel et al., 2019), achieving good performance on open-domain QA and other knowledge-intensive tasks.  8 Conclusion  In this work, we demonstrated that dense retrieval can outperform and potentially replace the tradi- tional sparse retrieval component in open-domain question answering. While a simple dual-encoder approach can be made to work surprisingly well, we showed that there are some critical ingredients to training a dense retriever successfully. Moreover, our empirical analysis and ablation studies indicate that more complex model frameworks or similarity functions do not necessarily provide additional val- ues. As a result of improved retrieval performance, we obtained new state-of-the-art results on multiple open-domain question answering benchmarks.  Acknowledgments  We thank the anonymous reviewers for their helpful comments and suggestions.  References  Akari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi, Richard Socher, and Caiming Xiong. 2020. Learn- ing to retrieve reasoning paths over Wikipedia graph for question answering. In International Conference on Learning Representations (ICLR).  Petr Baudi\u02c7s and Jan \u02c7Sediv`y. 2015. Modeling of the question answering task in the yodaqa system. In In- ternational Conference of the Cross-Language Eval- uation Forum for European Languages, pages 222\u2013 228. Springer.  Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on Freebase from  question-answer pairs. In Empirical Methods in Nat- ural Language Processing (EMNLP).  Jane Bromley, Isabelle Guyon, Yann LeCun, Eduard S\u00a8ackinger, and Roopak Shah. 1994. Signature veri\ufb01- cation using a \u201cSiamese\u201d time delay neural network. In NIPS, pages 737\u2013744.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nHow does DPR achieve efficient retrieval at run-time?[/INST] \nAnswer: DPR achieves efficient retrieval at run-time by using a different encoder that maps the input question to a d-dimensional vector. It then retrieves k passages whose vectors are the closest to the question vector. The similarity between the question and the passage is defined by the dot product of their vectors. This allows DPR to index all the passages in a low-dimensional and continuous space, enabling efficient retrieval of the top k passages relevant to the input question."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\n### 1. 3 A few uses of NLP  Natural language processing algorithms are increasingly useful and deployed, but their failures and limitations are still largely opaque and sometimes hard to detect. Here are a few of the major applications; this list is intended to pique your interest, not to be exhaustive:  Machine translation. Perhaps one of the earliest and most successful applications and driving uses of natural language processing, MT systems learn to translate between languages and are ubiquitous in the digital world. Still, failures of these systems for most of the world\u2019s 7000 languages, difficulties in translating long text, and ensuring contextual correctness of translations make this still a fruitful field of research.  Question answering and information retrieval. The concept of \u201cquestion answering\u201d should seem overly broad\u2014can\u2019t we express any problem as question answering?\u2014but in NLP, question answering has tended to be related to information-seeking questions (\u201cWho is the emir of Abu Dhabi?\u201d, \u201cWhat is the process by which I can get an intern visa for the United Kingdom?\u201d). Continually broadening the scope of answerable questions, providing provenance for answers, answering questions in an interactive dialogue\u2014this is one of the fastest-evolving research directions.  Summarization and analysis of text. There are myriad reasons to want to understand ( 1 ) what people are talking about and ( 2 ) what they think about those things. Companies want to do market research, politicians want to know peoples\u2019 opinions, individuals want summaries of complex topics in digestible form. NLP tools can be powerful for both the increase of access to information to the public, as well as surveillance, corporate or governmental. Bear this aspect of \u201cdual use\u201d in mind as you progress and decide what you are building.  Note: speech(or sign)-to-text. The process of automatic transcription of spoken or signed language (audio or video) to textual represen- tations is a massive and useful application, but one we\u2019ll largely    deep learning 3   avoid in this course. Partly, this is historical and methodological; the raw signal processing methods and expertise are generally covered in other courses ( 224 s!) and other research communities, though there has been some convergence of techniques of late.\n\nfilename: materials_formatted.md\nextension: .md\n\nUnfortunately, no consensus has emerged about the form or the existence of such a data structure. Until such fundamental Articial Intelligence problems are resolved, computer scientists must settle forthe reduced objective of extracting simpler representations that describe limited aspects of the textual information. These simpler representations are often motivated by specific applications (for instance, bag- of-words variants for information retrieval), or by our belief that they capture something more gen- eral about natural language. They can describe syntactic information (e.g., part-of-speech tagging, chunking, and parsing) or semantic information (e.g., word-sense disambiguation, semantic role labeling, named entity extraction, and anaphora resolution). Text corpora have been manually an- notated with such data structures in order to compare the performance of various systems. The availability of standard benchmarks has stimulated research in Natural Language Processing (NLP)   \u2217. Ronan Collobert is now with the Idiap Research Institute, Switzerland. \u2020. Jason Weston is now with Google, New York, NY. \u2021. Leon Bottou is now with Microsoft, Redmond, WA. \u0301 \u00a7. Koray Kavukcuoglu is also with New York University, New York, NY. \u00b6. Pavel Kuksa is also with Rutgers University, New Brunswick, NJ.  \u00a9c2011 Ronan Collobert, Jason Weston, Leon Bottou, Michael Karlen, Koray Kavukcuoglu and Pavel Kuk \u0301 sa.    COLLOBERT, WESTON, BOTTOU, KARLEN, KAVUKCUOGLU ANDKUKSA  and effective systems have been designed for all these tasks. Such systems are often viewed as software components for constructing real-world NLP solutions. The overwhelming majority of these state-of-the-art systems address their single benchmark task by applying linear statistical models to ad-hoc features. In other words, the researchers them- selves discover intermediate representations by engineering task-specific features. These features are often derived from the output of preexisting systems, leading to complex runtime dependencies. This approach is effective because researchers leverage a large body of linguistic knowledge. On the other hand, there is a great temptation to optimize the performance of a system for a specific benchmark.\n\nfilename: materials_formatted.md\nextension: .md\n\nAlthough such performance improvements can be very useful in practice, they teach us little about the means to progress toward the broader goals of natural language understanding and the elusive goals of Artificial Intelligence. In this contribution, we try to excel onmultiple benchmarkswhileavoiding task-specific engi- neering. Instead we use asingle learning systemable to discover adequate internal representations. In fact we view the benchmarks as indirect measurements of the relevanceof the internal represen- tations discovered by the learning procedure, and we posit that these intermediate representations are more general than any of the benchmarks. Our desire to avoid task-specific engineered features prevented us from using a large body of linguistic knowledge. Instead wereach good performance levels in most of the tasks by transferring intermediate representations discovered on large unlabeled data sets. We call this approach \u201calmost from scratch\u201d to emphasize the reduced (but still important) reliance on a priori NLP knowledge. The paper is organized as follows. Section 2 describes the benchmark tasks of interest. Sec- tion 3 describes the unified model and reports benchmark results obtained with supervised training. Section 4 leverages large unlabeled data sets (\u223c852 million words) to train the model on a language modeling task. Performance improvements are then demonstrated by transferring the unsupervised internal representations into the supervised benchmark models. Section 5 investigates multitask supervised training. Section 6 then evaluates how much further improvementcan be achieved by incorporating standard NLP task-specific engineering into our systems. Drifting away from our ini- tial goals gives us the opportunity to construct an all-purpose tagger thatis simultaneously accurate, practical, and fast. We then conclude with a short discussion section.  #### 2. The Benchmark Tasks  In this section, we briefly introduce four standard NLP tasks on which we will benchmark our architectures within this paper: Part-Of-Speech tagging (POS), chunking (CHUNK), Named Entity Recognition (NER) and Semantic Role Labeling (SRL). For each of them, we consider a standard experimental setup and give an overview of state-of-the-art systems onthis setup. The experimental setups are summarized in Table 1, while state-of-the-art systems are reported in Table 2.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat is one of the most successful applications of Natural Language Processing?[/INST] \nAnswer: One of the most successful applications of Natural Language Processing is machine translation."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nProbably the most successful concept is to use distributed representations of words [10]. For example, neural network based language models significantly outperform N-gram models [1, 27, 17].   1.1 Goals of the Paper   The main goal of this paper is to introduce techniques that can be used for learning high-quality word vectors from huge data sets with billions of words, and with millions of words in the vocabulary. As far as we know, none of the previously proposed architectures has been successfully trained on more  # arXiv:1301.3781v3 [cs.CL] 7 Sep 2013   than a few hundred of millions of words, with a modest dimensionality of the word vectors between 50 - 100.  We use recently proposed techniques for measuring the quality of the resulting vector representa- tions, with the expectation that not only will similar words tend to be close to each other, but that words can havemultiple degrees of similarity[20]. This has been observed earlier in the context of inflectional languages - for example, nouns can have multiple word endings, and if we search for similar words in a subspace of the original vector space, it is possible to find words that have similar endings [13, 14].  Somewhat surprisingly, it was found that similarity of word representations goes beyond simple syntactic regularities. Using a word offset technique where simple algebraic operations are per- formed on the word vectors, it was shown for example thatvector(\u201dKing\u201d) - vector(\u201dMan\u201d) + vec- tor(\u201dWoman\u201d)results in a vector that is closest to the vector representation of the wordQueen[20].  In this paper, we try to maximize accuracy of these vector operations by developing new model architectures that preserve the linear regularities among words. We design a new comprehensive test set for measuring both syntactic and semantic regularities^1 , and show that many such regularities can be learned with high accuracy. Moreover, we discuss how training time and accuracy depends on the dimensionality of the word vectors and on the amount of the training data.  1.2 Previous Work  Representation of words as continuous vectors has a long history [10, 26, 8].\n\nfilename: materials_formatted.md\nextension: .md\n\nAnother way to improve accuracy is to provide more than one example of the relationship. By using ten examples instead of one to form the relationship vector (we average the individual vectors together), we have observed improvement of accuracy of our best models by about 10% absolutely on the semantic-syntactic test.  It is also possible to apply the vector operations to solve different tasks. For example, we have observed good accuracy for selecting out-of-the-list words, by computing average vector for a list of words, and finding the most distant word vector. This is a popular type of problems in certain human intelligence tests. Clearly, there is still a lot of discoveries to be made using these techniques.  ## 6 Conclusion  In this paper we studied the quality of vector representations of words derived by various models on a collection of syntactic and semantic language tasks. We observed that it is possible to train high quality word vectors using very simple model architectures, compared to the popular neural network models (both feedforward and recurrent). Because of the much lower computational complexity, it is possible to compute very accurate high dimensional word vectors from a much larger data set. Using the DistBelief distributed framework, it should be possible to train the CBOW and Skip-gram models even on corpora with one trillion words, for basically unlimited size of the vocabulary. That is several orders of magnitude larger than the best previously published results for similar models.  An interesting task where the word vectors have recently been shown to significantly outperform the previous state of the art is the SemEval-2012 Task 2 [11]. The publicly available RNN vectors were used together with other techniques to achieve over 50% increase in Spearman\u2019s rank correlation over the previous best result [31]. The neural network based word vectors were previously applied to many other NLP tasks, for example sentiment analysis [12] and paraphrase detection [28]. It can be expected that these applications can benefit from the model architectures described in this paper.  Our ongoing work shows that the word vectors can be successfully applied to automatic extension of facts in Knowledge Bases, and also for verification of correctness of existing facts. Results from machine translation experiments also look very promising. In the future, it would be also interesting to compare our techniques to Latent Relational Analysis [30] and others.\n\nfilename: materials_formatted.md\nextension: .md\n\nWe successfully trained models on several orders of magnitude more data than the previously pub- lished models, thanks to the computationally efficient model architecture. This results in a great improvement in the quality of the learned word and phrase representations, especially for the rare entities. We also found that the subsampling of the frequentwords results in both faster training and significantly better representations of uncommon words. Another contribution of our paper is the Negative sampling algorithm, which is an extremely simple training method that learns accurate representations especially for frequent words.  The choice of the training algorithm and the hyper-parameter selection is a task specific decision, as we found that different problems have different optimal hyperparameter configurations. In our experiments, the most crucial decisions that affect the performance are the choice of the model architecture, the size of the vectors, the subsampling rate, and the size of the training window.  A very interesting result of this work is that the word vectors can be somewhat meaningfully com- bined using just simple vector addition. Another approach for learning representations of phrases presented in this paper is to simply represent the phrases with a single token. Combination of these two approaches gives a powerful yet simple way how to represent longer pieces of text, while hav- ing minimal computational complexity. Our work can thus be seen as complementary to the existing approach that attempts to represent phrases using recursive matrix-vector operations [16].  We made the code for training the word and phrase vectors based on the techniques described in this paper available as an open-source project^4.  (^4) code.google.com/p/word2vec   ## References   [1] Yoshua Bengio, R \u0301ejean Ducharme, Pascal Vincent, and Christian Janvin. A neural probabilistic language model.The Journal of Machine Learning Research, 3:1137\u20131155, 2003. [2] Ronan Collobert and Jason Weston. A unified architecturefor natural language processing: deep neu- ral networks with multitask learning. InProceedings of the 25th international conference on Machine learning, pages 160\u2013167. ACM, 2008. [3] Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Domain adaptation for large-scale sentiment classi- fication: A deep learning approach. InICML, 513\u2013520, 2011.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat model does the paper introduce for learning high-quality vector representations of words?[/INST] \nAnswer: The paper introduces techniques for learning high-quality word vectors from large data sets. It focuses on developing new model architectures that preserve the linear regularities among words. These models are trained on significantly more data than previously published models, resulting in improved quality of learned word and phrase representations. The paper also introduces the Negative sampling algorithm, a simple training method that learns accurate representations especially for frequent words."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nA very popular model architecture for estimating neural network language model (NNLM) was proposed in [1], where a feedforward neural network with a linear projection layer and a non-linear hidden layer was used to learn jointly the word vector representation and a statistical language model. This work has been followed by many others.  Another interesting architecture of NNLM was presented in [13, 14], where the word vectors are first learned using neural network with a single hidden layer. The word vectors are then used to train the NNLM. Thus, the word vectors are learned even without constructing the full NNLM. In this work, we directly extend this architecture, and focus just on the first step where the word vectors are learned using a simple model.  It was later shown that the word vectors can be used to significantly improve and simplify many NLP applications [4, 5, 29]. Estimation of the word vectors itself was performed using different model architectures and trained on various corpora [4, 29, 23, 19, 9], and some of the resulting word vectors were made available for future research and comparison^2. However, as far as we know, these architectures were significantly more computationally expensive for training than the one proposed in [13], with the exception of certain version of log-bilinear model where diagonal weight matrices are used [23].  ## 2 Model Architectures  Many different types of models were proposed for estimating continuous representations of words, including the well-known Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA). In this paper, we focus on distributed representations of words learned by neural networks, as it was previously shown that they perform significantly better than LSA for preserving linear regularities among words [20, 31]; LDA moreover becomes computationally very expensive on large data sets.  Similar to [18], to compare different model architectures we define first the computational complex- ity of a model as the number of parameters that need to be accessed to fully train the model. Next, we will try to maximize the accuracy, while minimizing the computational complexity.\n\nfilename: materials_formatted.md\nextension: .md\n\nThe x-axis is the value of k.     \n\n# Efficient Estimation of Word Representations in Vector Space:   ## Efficient Estimation of Word Representations in  ## Vector Space   Tomas Mikolov Google Inc., Mountain View, CA tmikolov@google.com   Kai Chen Google Inc., Mountain View, CA kaichen@google.com   Greg Corrado Google Inc., Mountain View, CA gcorrado@google.com   Jeffrey Dean Google Inc., Mountain View, CA jeff@google.com  ## Abstract   We propose two novel model architectures for computing continuous vector repre- sentations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previ- ously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art perfor- mance on our test set for measuring syntactic and semantic word similarities.  ## 1 Introduction   Many current NLP systems and techniques treat words as atomic units - there is no notion of similar- ity between words, as these are represented as indices in a vocabulary. This choice has several good reasons - simplicity, robustness and the observation that simple models trained on huge amounts of data outperform complex systems trained on less data. An example is the popular N-gram model used for statistical language modeling - today, it is possible to train N-grams on virtually all available data (trillions of words [3]).   However, the simple techniques are at their limits in many tasks. For example, the amount of relevant in-domain data for automatic speech recognition is limited - the performance is usually dominated by the size of high quality transcribed speech data (often just millions of words). In machine translation, the existing corpora for many languages contain only a few billions of words or less. Thus, there are situations where simple scaling up of the basic techniques will not result in any significant progress, and we have to focus on more advanced techniques.   With progress of machine learning techniques in recent years, it has become possible to train more complex models on much larger data set, and they typically outperform the simple models.\n\nfilename: materials_formatted.md\nextension: .md\n\nProbably the most successful concept is to use distributed representations of words [10]. For example, neural network based language models significantly outperform N-gram models [1, 27, 17].   1.1 Goals of the Paper   The main goal of this paper is to introduce techniques that can be used for learning high-quality word vectors from huge data sets with billions of words, and with millions of words in the vocabulary. As far as we know, none of the previously proposed architectures has been successfully trained on more  # arXiv:1301.3781v3 [cs.CL] 7 Sep 2013   than a few hundred of millions of words, with a modest dimensionality of the word vectors between 50 - 100.  We use recently proposed techniques for measuring the quality of the resulting vector representa- tions, with the expectation that not only will similar words tend to be close to each other, but that words can havemultiple degrees of similarity[20]. This has been observed earlier in the context of inflectional languages - for example, nouns can have multiple word endings, and if we search for similar words in a subspace of the original vector space, it is possible to find words that have similar endings [13, 14].  Somewhat surprisingly, it was found that similarity of word representations goes beyond simple syntactic regularities. Using a word offset technique where simple algebraic operations are per- formed on the word vectors, it was shown for example thatvector(\u201dKing\u201d) - vector(\u201dMan\u201d) + vec- tor(\u201dWoman\u201d)results in a vector that is closest to the vector representation of the wordQueen[20].  In this paper, we try to maximize accuracy of these vector operations by developing new model architectures that preserve the linear regularities among words. We design a new comprehensive test set for measuring both syntactic and semantic regularities^1 , and show that many such regularities can be learned with high accuracy. Moreover, we discuss how training time and accuracy depends on the dimensionality of the word vectors and on the amount of the training data.  1.2 Previous Work  Representation of words as continuous vectors has a long history [10, 26, 8].\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat novel model architectures does the paper propose for computing continuous vector representations of words?[/INST] \nAnswer: The paper proposes two novel model architectures for computing continuous vector representations of words from very large data sets."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\n6 DISCUSSION  Our paper has explored a simple question in zero-shot prompting: does \ufb01netuning a model on a collection of tasks phrased as instructions improve its performance on unseen tasks? We operationalize this question via instruction tuning, a simple method that combines appealing aspects of both the pretrain\u2013\ufb01netune and prompting paradigms. Our instruction-tuned model, FLAN, improves performance against an untuned model and surpasses zero-shot GPT-3 on the majority of tasks that we evaluate on. Ablation studies reveal that performance on unseen tasks improves with the number of instruction tuning task clusters, and, interestingly, that performance improvements from instruction tuning emerge only with suf\ufb01cient model scale. Moreover, instruction tuning can be combined with other prompting methods such as few-shot prompting and prompt tuning.  The diverse capabilities of language models at scale have drawn attention to the tradeoffs between specialist models (one model per task) and generalist models (one model for many tasks; Arivazhagan et al., 2019; Pratap et al., 2020), for which our study has potential implications. Although one might expect labeled data to have the most natural role in improving specialist models, instruction tuning demonstrates how labeled data can be used to help large language models perform many, unseen tasks. In other words, the positive effect of instruction tuning on cross-task generalization shows that task-speci\ufb01c training is complementary to general language modeling and motivates further research on generalist models.  As for limitations of our study, there is a degree of subjectivity in assigning tasks to clusters (though we try to use accepted categorizations in the literature), and we only explore the use of relatively short instructions of typically a single sentence (c.f. detailed instructions given to crowd-workers). A limitation for our evaluation is that individual examples might have appeared in the models\u2019 pretraining data, which includes web documents, though in post-hoc analysis (Appendix C) we do not \ufb01nd any evidence that data overlap substantially impacted the results. Finally, the scale of FLAN 137B makes it costly to serve.\n\nfilename: materials_formatted.md\nextension: .md\n\nQA\u2013 Commonsense NLI  Closed-book QA49.955.059.359.260.861.963.5Base LM: CommonsenseBase LM: NLIBase LM: Closed-book QAPublished as a conference paper at ICLR 2022  4.2 SCALING LAWS  As Brown et al. (2020) shows that zero and few-shot capabilities of language models sub- stantially improve for larger models, we next explore how the bene\ufb01ts of instruction tuning are affected by model scale. Using the same cluster split as in the previous ablation study, we evaluate the effect of instruction tuning on models of size 422M, 2B, 8B, 68B, and 137B parameters.  Figure 7 shows these results. We see that for the two models on the order of 100B pa- rameters, instruction tuning substantially im- proves performance on held-out tasks, as is expected given the prior results in our pa- per. The behavior on held-out tasks for the 8B and smaller models, however, is thought- provoking\u2014instruction tuning actually hurts performance on held-out tasks. One potential explana- tion for this result could be that for small-scale models, learning the \u223c40 tasks used during instruction tuning \ufb01lls the entire model capacity, causing these models to perform worse on new tasks. Under this potential explanation, for the larger scale models, instruction tuning \ufb01lls up some model capacity but also teaches these models how to follow instructions, allowing them to generalize to new tasks with the remaining capacity.  Figure 7: Whereas instruction tuning helps large models generalize to new tasks, for small models it actually hurts generalization to unseen tasks, poten- tially because all model capacity is used to learn the mixture of instruction tuning tasks.  4.3 ROLE OF INSTRUCTIONS  In a \ufb01nal ablation study, we explore the role of in- structions during \ufb01netuning, as one possibility is that performance gains come entirely from multi-task \ufb01ne- tuning and the model could perform just as well without instructions. We hence consider two \ufb01netuning setups without instructions.\n\nfilename: materials_formatted.md\nextension: .md\n\nFLAN\u2019s zero-shot also outperforms 175B-parameter GPT-3\u2019s zero-shot on 20 of 25 datasets that we evaluate, and even outperforms GPT-3\u2019s few-shot by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. In ablation studies, we \ufb01nd that increasing the number of task clusters in instruction tuning improves performance on unseen tasks and that the bene\ufb01ts of instruction tuning emerge only with suf\ufb01cient model scale.  Instruction tuning is a simple method that, as depicted in Figure 2, combines appealing aspects of both the pretrain\u2013\ufb01netune and prompting paradigms by using supervision via \ufb01netuning to improve language model\u2019s responses to inference-time text interactions. Our empirical results demonstrate promising abilities of language models to perform tasks described purely via instructions. Source code for loading the instruction tuning dataset used for FLAN is publicly available at https://github.com/google-research/flan.  Figure 2: Comparing instruction tuning with pretrain\u2013\ufb01netune and prompting.  2 FLAN: INSTRUCTION TUNING IMPROVES ZERO-SHOT LEARNING  The motivation of instruction tuning is to improve the ability of language models to respond to NLP instructions. The idea is that by using supervision to teach an LM to perform tasks described via instructions, the LM will learn to follow instructions and do so even for unseen tasks. To evaluate performance on unseen tasks, we group datasets into clusters by task type and hold out each task cluster for evaluation while instruction tuning on all remaining clusters.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat is the primary objective of instruction tuning in language models?[/INST] \nAnswer: The primary objective of instruction tuning in language models is to enhance the ability of these models to respond to natural language processing instructions. The idea is to use supervision to teach a language model to perform tasks described via instructions, enabling the model to follow instructions and perform even unseen tasks effectively."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nBy contrast pre-training at large enough scale appears to offer a \u201cnatural\u201d broad distribution of tasks implicitly contained in predicting the text itself. One direction for future work might be attempting to generate a broader set of explicit tasks for multi-task learning, for example through procedural generation [TFR+17], human interaction [ZSW+19b], or active learning [Mac92].  Algorithmic innovation in language models over the last two years has been enormous, including denoising-based bidirectionality [DCLT18], pre\ufb01xLM [DL15] and encoder-decoder architectures [LLG+19, RSR+19], random permu- tations during training [YDY+19], architectures that improve the ef\ufb01ciency of sampling [DYY+19], improvements in data and training procedures [LOG+19], and ef\ufb01ciency increases in the embedding parameters [LCG+19]. Many of these techniques provide signi\ufb01cant gains on downstream tasks. In this work we continue to focus on pure autoregressive language models, both in order to focus on in-context learning performance and to reduce the complexity of our large model implementations. However, it is very likely that incorporating these algorithmic advances could improve GPT-3\u2019s performance on downstream tasks, especially in the \ufb01ne-tuning setting, and combining GPT-3\u2019s scale with these algorithmic techniques is a promising direction for future work.  8 Conclusion  We presented a 175 billion parameter language model which shows strong performance on many NLP tasks and benchmarks in the zero-shot, one-shot, and few-shot settings, in some cases nearly matching the performance of  40  state-of-the-art \ufb01ne-tuned systems, as well as generating high-quality samples and strong qualitative performance at tasks de\ufb01ned on-the-\ufb02y. We documented roughly predictable trends of scaling in performance without using \ufb01ne-tuning. We also discussed the social impacts of this class of model. Despite many limitations and weaknesses, these results suggest that very large language models may be an important ingredient in the development of adaptable, general language systems.  Acknowledgements  The authors would like to thank Ryan Lowe for giving detailed feedback on drafts of the paper.\n\nfilename: materials_formatted.md\nextension: .md\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.3 Energy Usage .  .  .  .  .  .  .  .  .  .  .  .  .  .  7 Related Work  8 Conclusion  A Details of Common Crawl Filtering  B Details of Model Training  C Details of Test Set Contamination Studies  D Total Compute Used to Train Language Models  E Human Quality Assessment of Synthetic News Articles  F Additional Samples from GPT-3  G Details of Task Phrasing and Speci\ufb01cations  H Results on All Tasks for All Model Sizes  2  3  6 8 8 9 10  10 11 13 14 16 17 18 18 20 21  29  33  34 35 36 39  39  40  43  43  43  46  46  48  50  63  1  Introduction  Recent years have featured a trend towards pre-trained language representations in NLP systems, applied in increasingly \ufb02exible and task-agnostic ways for downstream transfer.\n\nfilename: materials_formatted.md\nextension: .md\n\nAdvanced language models may also lead to the automation of various jobs in the coming decades [ 16 ]. In order to mitigate these risks, AI systems could be employed to fight against misleading content and automated spam/phishing.  ### Acknowledgments  The authors would like to thank the reviewers for their thoughtful and constructive feedback on this paper, as well as HuggingFace for their help in open-sourcing code to run RAG models. The authors would also like to thank Kyunghyun Cho and Sewon Min for productive discussions and advice. EP thanks supports from the NSF Graduate Research Fellowship. PL is supported by the FAIR PhD program.  ### References   [1]Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Tiwary, and Tong Wang. MS MARCO: A Human Generated MAchine Reading COmprehension Dataset. arXiv:1611.09268 [cs], November 2016. URLhttp: //arxiv.org/abs/1611.09268. arXiv: 1611.09268.   [2]Petr Baudi\u0161 and Jan \u0160edivy. Modeling of the question answering task in the yodaqa system. In` International Conference of the Cross-Language Evaluation Forum for European Languages, pages 222\u2013228. Springer, 2015. URLhttps://link.springer.com/chapter/10.1007% 2F978-3-319-24027-5_20.   [3]Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic Parsing on Freebase from Question-Answer Pairs. InProceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1533\u20131544, Seattle, Washington, USA, October 2013. Association for Computational Linguistics. URLhttp://www.aclweb.org/anthology/ D13-1160.   [4]Bin Bi, Chenliang Li, Chen Wu, Ming Yan, and Wei Wang.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nAccording to the note, what major challenge do current NLP tools face?[/INST] \nAnswer: The major challenge current NLP tools face, as per the note, is the need for algorithmic innovation and the incorporation of these advances to improve performance on downstream tasks. The note also suggests that while large language models like GPT-3 show strong performance on many tasks, there is still room for improvement, especially in the fine-tuning setting."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nTo facilitate systematic evaluation and compar- ison of new embedding models, we release a new frequency-calibrated query inventory along with all user judgments athttp://www.cs. cornell.edu/ \u0303schnabts/eval/. Finally, we observe that word embeddings en- code a surprising degree of information about word frequency. We found this was true even in models that explicitly reserve parameters to com- pensate for frequency effects. This finding may explain some of the variability across embeddings and across evaluation methods. It also casts doubt on the common practice of using the vanilla co- sine similarity as a similarity measure in the em- bedding space. It is important to note that this work is a survey ofevaluationmethods not a survey ofembedding methods. The specific example embeddings pre- sented here were chosen as representative samples only, and may not be optimal.  ## 2 Word embeddings  We refer to a word embedding as a mappingV\u2192 RD:w7\u2192~wthat maps a wordwfrom a vocabu- laryV to a real-valued vector~win an embedding space of dimensionalityD. Following previous work (Collobert et al., 2011; Mikolov et al., 2013a) we use the com- monly employed cosine similarity, defined as similarity(w 1 ,w 2 ) = \u2016~w~w 11 \u2016\u2016\u00b7~w~w^22 \u2016, for all similar- ity computations in the embedding space. The list of nearest neighbors of a wordware all words v \u2208 V \\ {w}, sorted in descending order by similarity(w,v). We will denotewas thequery wordin the remainder of this paper. All experiments in this paper are carried out on six popular unsupervised embedding meth- ods. These embeddings form a representative but incomplete subset; and since we are study-   ing evaluation methods and not embeddings them- selves, no attempt has been made to optimize these embeddings. The first two embedding models, the CBOW model of word2vec (Mikolov et al., 2013a) and C&W embeddings (Collobert et al., 2011) both are motivated by a probabilistic predic- tion approach.\n\nfilename: materials_formatted.md\nextension: .md\n\n# Evaluation methods for unsupervised word embeddings  ## Tobias Schnabel  ## Cornell University  ## Ithaca, NY, 14853  ## tbs49@cornell.edu  ## Igor Labutov  ## Cornell University  ## Ithaca, NY, 14853  ## iil4@cornell.edu  ## David Mimno,  ## Thorsten Joachims  ## Cornell University  ## mimno@cornell.edu,  ## tj@cs.cornell.edu  ## Abstract   We present a comprehensive study of eval- uation methods for unsupervised embed- ding techniques that obtain meaningful representations of words from text. Differ- ent evaluations result in different orderings of embedding methods, calling into ques- tion the common assumption that there is one single optimal vector representation. We present new evaluation techniques that directly compare embeddings with respect to specific queries. These methods re- duce bias, provide greater insight, and allow us to solicit data-driven relevance judgments rapidly and accurately through crowdsourcing.  ## 1 Introduction  Neural word embeddings represent meaning via geometry. A good embedding provides vector rep- resentations of words such that the relationship be- tween two vectors mirrors the linguistic relation- ship between the two words. Despite the growing interest in vector representations of semantic in- formation, there has been relatively little work on direct evaluations of these models. In this work, we explore several approaches to measuring the quality of neural word embeddings. In particu- lar, we perform a comprehensive analysis of eval- uation methods and introduce novel methods that can be implemented through crowdsourcing, pro- viding better insights into the relative strengths of different embeddings. Existing schemes fall into two major categories: extrinsic and intrinsic evaluation. In extrinsic eval- uation, we use word embeddings as input features to a downstream task and measure changes in per- formance metrics specific to that task. Examples include part-of-speech tagging and named-entity recognition (Pennington et al., 2014). Extrinsic   evaluation only provides one way to specify the goodness of an embedding, and it is not clear how it connects to other measures. Intrinsic evaluations directly test for syntactic or semantic relationships between words (Mikolov et al., 2013a; Baroni et al., 2014).\n\nfilename: materials_formatted.md\nextension: .md\n\nThese embeddings improve the generalization performance on all tasks (Section 4.) The literature describes other ways to induce word representations. Mnih and Hinton (2007) proposed a related language model ap- proach inspired from Restricted Boltzmann Machines. However, word representations are perhaps more commonly inferred fromn-gram language modeling rather than purely continuous language models. One popular approach is the Brown clustering algorithm (Brown etal., 1992a), which builds hierarchical word clusters by maximizing the bigram\u2019s mutual information. The induced word representation has been used with success in a wide variety of NLP tasks, including POS (Schutze, 1995), NER (Miller et al., 2004; Ratinov and Roth, 2009), or parsin \u0308 g (Koo et al., 2008). Other related approaches exist, like phrase clustering (Lin and Wu, 2009) which has been shown to work well for NER. Finally, Huang and Yates (2009) have recently proposed a smoothed language modeling approach based on a Hidden Markov Model, with success on POSand Chunking tasks. While a comparison of all these word representations is beyond the scope of this paper, it is rather fair to question the quality of our word embeddings compared to a popular NLP approach. In this section, we report a comparison of our word embeddings against Brown clusters, when used as features into our neural network architecture. We report as baseline previous results where our word embeddings arefine-tunedfor each task. We also report performance when our embeddings are kept fixed during task-specific training. Sinceconvexmachine learning algorithms are common practice in NLP, we finally report performances for the convex versionof our architecture. For the convex experiments, we considered the linear version of our neural networks (instead of having several linear layers interleaved with a non-linearity). While we always picked the sentence approach for SRL, we had to consider the window approach in this particular convex setup, as the sentence approach network (see Figure 2) includes a Max layer. Having only one linear layer in our neural network is not enough to make our architecture convex: all lookup-tables (for each discrete feature) must also befixed. The word-lookup table is simply fixed to the embeddings obtained from our language model LM2.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat are the two main classes of methods for finding word embeddings mentioned?[/INST] \nAnswer: The two main classes of methods for finding word embeddings mentioned are unsupervised embedding methods and n-gram language modeling."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nWhile the focus of the presentation in the word- embedding literature is on the mathematical model and the objective being optimized, other factors af- fect the results as well. In particular, embedding algorithms suggest some naturalhyperparameters that can be tuned; many of which were already tuned to some extent by the algorithms\u2019 design- ers. Some hyperparameters, such as the number of negative samples to use, are clearly marked as tunable. Other modifications, such as smoothing the negative-sampling distribution, are reported in passing and considered thereafter as part of the al- gorithm. Others still, such as dynamically-sized context windows, are not even mentioned in some of the papers, but are part of the canonical imple- mentation. All of these modifications and system design choices, which we collectively denote as hyperparameters, are part of the final algorithm, and, as we show, have a substantial impact on per- formance.  ### 211   Transactions of the Association for Computational Linguistics, vol. 3, pp. 211\u2013225, 2015. Action Editor: Patrick Pantel. Submission batch: 1/2015; Revision batch 3/2015; Published 5/2015. \u00a9c2015 Association for Computational Linguistics. Distributed under a CC-BY-NC-SA 4.0 license.   In this work, we make these hyperparameters explicit, and show how they can beadapted and transferredinto the traditional count-based ap- proach. To asses how each hyperparameter con- tributes to the algorithms\u2019 performance, we con- duct a comprehensive set of experiments and com- pare four different representation methods, while controlling for the various hyperparameters. Once adapted across methods, hyperparameter tuning significantly improves performance in ev- ery task. In many cases, changing the setting of a single hyperparameter yields a greater increase in performance than switching to a better algorithm or training on a larger corpus. In particular,word2vec\u2019s smoothing of the negative sampling distribution can be adapted to PPMI-based methods by introducing a novel, smoothed variant of the PMI association measure (see Section 3.2). Using this variant increases per- formance by over 3 points per task, on average.\n\nfilename: materials_formatted.md\nextension: .md\n\n50 dimensional feature vectors in the lookup table. Ideally, we would like semantically similar words to be close in the embedding space represented by the word lookup table: by continuity of the neural network function, tags produced on semantically similar sentences would be similar. We show in Table 6 that it is not the case: neighboring words in the embedding space do not seem to be semantically related. We will focus in the next section on improving these word embeddings by leveraging unlabeled data. We will see our approach results in a performance boost for all tasks.  Remark 8 (Architectures)In all our experiments in this paper, we tuned the hyper-parameters by trying only a few different architectures by validation. In practice, the choice of hyperparameters such as the number of hidden units, provided they are large enough, has a limited impact on the generalization performance. In Figure 4, we report the F1 score for each task on the validation set, with respect to the number of hidden units. Considering the variance related to the network initial- ization, we chose the smallest network achieving \u201creasonable\u201d performance, rather than picking the network achieving the top performance obtained on a single run.  Remark 9 (Training Time)Training our network is quite computationally expensive. Chunking and NER take about one hour to train, POS takes few hours, and SRL takesabout three days. Training could be faster with a larger learning rate, but we preferred to stickto a small one which works, rather than finding the optimal one for speed. Second order methods (LeCun et al., 1998) could be another speedup technique.  #### 4. Lots of Unlabeled Data  We would like to obtain word embeddings carrying more syntactic and semantic information than shown in Table 6. Since most of the trainable parameters of our system are associated with the word embeddings, these poor results suggest that we should use considerably more training data.    NATURALLANGUAGEPROCESSING(ALMOST)FROMSCRATCH   95.   96   96.   100 300 500 700 900 (a) POS   90   90.   91   91.   100 300 500 700 900 (b) CHUNK   85   85.   86   86.\n\nfilename: materials_formatted.md\nextension: .md\n\n# Evaluation methods for unsupervised word embeddings  ## Tobias Schnabel  ## Cornell University  ## Ithaca, NY, 14853  ## tbs49@cornell.edu  ## Igor Labutov  ## Cornell University  ## Ithaca, NY, 14853  ## iil4@cornell.edu  ## David Mimno,  ## Thorsten Joachims  ## Cornell University  ## mimno@cornell.edu,  ## tj@cs.cornell.edu  ## Abstract   We present a comprehensive study of eval- uation methods for unsupervised embed- ding techniques that obtain meaningful representations of words from text. Differ- ent evaluations result in different orderings of embedding methods, calling into ques- tion the common assumption that there is one single optimal vector representation. We present new evaluation techniques that directly compare embeddings with respect to specific queries. These methods re- duce bias, provide greater insight, and allow us to solicit data-driven relevance judgments rapidly and accurately through crowdsourcing.  ## 1 Introduction  Neural word embeddings represent meaning via geometry. A good embedding provides vector rep- resentations of words such that the relationship be- tween two vectors mirrors the linguistic relation- ship between the two words. Despite the growing interest in vector representations of semantic in- formation, there has been relatively little work on direct evaluations of these models. In this work, we explore several approaches to measuring the quality of neural word embeddings. In particu- lar, we perform a comprehensive analysis of eval- uation methods and introduce novel methods that can be implemented through crowdsourcing, pro- viding better insights into the relative strengths of different embeddings. Existing schemes fall into two major categories: extrinsic and intrinsic evaluation. In extrinsic eval- uation, we use word embeddings as input features to a downstream task and measure changes in per- formance metrics specific to that task. Examples include part-of-speech tagging and named-entity recognition (Pennington et al., 2014). Extrinsic   evaluation only provides one way to specify the goodness of an embedding, and it is not clear how it connects to other measures. Intrinsic evaluations directly test for syntactic or semantic relationships between words (Mikolov et al., 2013a; Baroni et al., 2014).\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat are some hyperparameters that might be tuned for word embedding techniques?[/INST] \nAnswer: Some hyperparameters that might be tuned for word embedding techniques include the number of negative samples to use, smoothing the negative-sampling distribution, and the size of context windows. These hyperparameters can significantly impact the performance of the algorithm."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nHere, we discuss the use of a gated activation function thereby modifying the RNN architecture. What motivates this? Well, although RNNs can theoretically capture long-term dependencies, they are very hard to actually train to do this. Gated recurrent units are designed in a manner to have more persistent memory thereby making it easier for RNNs to capture long-term dependencies. Let us see mathematically how a GRU usesht\u2212 1 andxtto generate the next hidden stateht. We will then dive into the intuition of this architecture.   zt= \u03c3 (W(z)xt+U(z)ht\u2212 1 ) (Update gate) rt= \u03c3 (W(r)xt+U(r)ht\u2212 1 ) (Reset gate) \u0303ht=tanh(rt\u25e6Uht\u2212 1 +Wxt) (New memory) ht= ( 1 \u2212zt)\u25e6h \u0303t+zt\u25e6ht\u2212 1 (Hidden state)   ### language models,rnn,gru and lstm 12  The above equations can be thought of a GRU\u2019s four fundamental op- erational stages and they have intuitive interpretations that make this model much more intellectually satisfying (see Figure 12 ):  1. New memory generation: A new memory \u0303htis the consolidation of a new input wordxtwith the past hidden stateht\u2212 1. Anthropomor- phically, this stage is the one who knows the recipe of combining a newly observed word with the past hidden stateht\u2212 1 to summarize this new word in light of the contextual past as the vectorh \u0303t.  2. Reset Gate: The reset signalrtis responsible for determining how importantht\u2212 1 is to the summarizationh \u0303t. The reset gate has the ability to completely diminish past hidden state if it finds thatht\u2212 1 is irrelevant to the computation of the new memory.  3. Update Gate: The update signalztis responsible for determining how much ofht\u2212 1 should be carried forward to the next state. For instance, ifzt \u22481, thenht\u2212 1 is almost entirely copied out toht. Conversely, ifzt\u22480, then mostly the new memoryh \u0303tis forwarded to the next hidden state.  4.\n\nfilename: materials_formatted.md\nextension: .md\n\nFigure 1: The Transformer - model architecture.  The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.  3.1 Encoder and Decoder Stacks  Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position- wise fully connected feed-forward network. We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512.  Decoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.  3.2 Attention  An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum  Scaled Dot-Product Attention  Multi-Head Attention  Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.  of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.  3.2.1 Scaled Dot-Product Attention  We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2).\n\nfilename: materials_formatted.md\nextension: .md\n\nso the connection allows for learning much deeper networks, and ( 2 ) it is easier to learn the difference of a function from the identity function than it is to learn the function from scratch. As simple as these seem, they\u2019re massively useful in deep learning, not just in Transformers!  Add & Norm. In the Transformer diagrams you\u2019ll see, including Fig- ure 4 , the application of layer normalization and residual connection are often combined in a single visual block labeledAdd & Norm. Such a layer might look like:   h pre-norm=f(LN( h )) + h , ( 30 )  wherefis either a feed-forward operation or a self-attention opera- tion, (this is known aspre-normalization), or like:   h post-norm=LN(f( h ) + h ), ( 31 )  which is known aspost-normalization. It turns out that the gradients of pre-normalization are much better at initialization, leading to much faster training [Xiong et al., 2020 ].  ##### 3. 4 Attention logit scaling  Another trick introduced in [Vaswani et al., 2017 ] they dubscaled dot product attention. The dot product part comes from the fact that we\u2019re computing dot products q \u22a4ikj. The intuition of scaling is that, when the dimensionalitydof the vectors we\u2019re dotting grows large, the dot product of even random vectors (e.g., at initialization) grows roughly as   \u221a   d. So, we normalize the dot products by   \u221a   dto stop this scaling:   \u03b1 =softmax( x 1:nQK\u22a4 x \u22a41:n \u221a d   )\u2208 R n\u00d7n ( 32 )    with deep learning 13  ##### 3. 5 Transformer Encoder  A Transformer Encoder takes a single sequence w 1:n, and performs no future masking. It embeds the sequence withEto make x 1:n, adds the position representation, and then applies a stack of independently parameterizedEncoder Blocks, each of which consisting of ( 1 ) multi- head attention and Add & Norm, and ( 2 ) feed-forward and Add & Norm. So, the output of each Block is the input to the next. Figure 5 presents this.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nHow do residual connections contribute to the Transformer's architecture?[/INST] \nAnswer: Residual connections contribute to the Transformer's architecture in two significant ways. Firstly, they allow for the learning of much deeper networks. Secondly, they make it easier to learn the difference of a function from the identity function than it is to learn the function from scratch. These connections are often combined with layer normalization in a single visual block labeled Add & Norm. The application of layer normalization and residual connection can be represented as either pre-normalization or post-normalization. The gradients of pre-normalization are much better at initialization, leading to much faster training."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nThe resulting human performance score on the test set is 77.0% for the exact match metric, and 86.8% for F1. Mismatch occurs mostly due to inclusion/exclusion of non-essential phrases (e.g., monsoon trough ver- sus movement of the monsoon trough) rather than fundamental disagreements about the answer.  6.3 Model Performance  Table 5 shows the performance of our models along- side human performance on the v1.0 of development and test sets. The logistic regression model signi\ufb01- cantly outperforms the baselines, but underperforms  Exact Match  F1  Dev  Test  Dev  Test  Logistic Regression Human Dev F1  Dev F1  1.3%  1.1% 4.3% Random Guess Sliding Window 13.2% 12.5% 20.2% 19.7% Sliding Win. + Dist. 13.3% 13.0% 20.2% 20.0% Logistic Regression 40.0% 40.4% 51.0% 51.0% 80.3% 77.0% 90.5% 86.8% Human  4.1%  Table 5: Performance of various methods and humans. Logis-  tic regression outperforms the baselines, while there is still a  signi\ufb01cant gap between humans.  F1  Train  Dev  91.7% 51.0% Logistic Regression 33.9% 35.8% \u2013 Lex., \u2013 Dep. Paths 53.5% 45.4% \u2013 Lexicalized 91.4% 46.4% \u2013 Dep. Paths 91.7% 48.1% \u2013 Match. Word Freq. \u2013 Span POS Tags 91.7% 49.7% \u2013 Match. Bigram Freq. 91.7% 50.3% 91.7% 50.4% \u2013 Constituent Label 91.8% 50.5% \u2013 Lengths 91.7% 50.5% \u2013 Span Word Freq. 91.7% 50.6% \u2013 Root Match  Table 6: Performance with feature ablations.\n\nfilename: materials_formatted.md\nextension: .md\n\nOur best model achieves an F1 score of 51.0%,1 which is much better than the sliding window base- line (20%). Over the last four months (since June 2016), we have witnessed signi\ufb01cant improvements from more sophisticated neural network-based mod- els. For example, Wang and Jiang (2016) obtained 70.3% F1 on SQuAD v1.1 (results on v1.0 are sim- ilar). These results are still well behind human performance, which is 86.8% F1 based on inter- annotator agreement. This suggests that there is plenty of room for advancement in modeling and learning on the SQuAD dataset.  1All experimental results in this paper are on SQuAD v1.0.  Dataset  SQuAD  Question source  Formulation  Size  crowdsourced RC,  spans  100K  MCTest (Richardson et al., 2013) Algebra (Kushman et al., 2014) Science (Clark and Etzioni, 2016)  crowdsourced  standardized tests standardized tests  in passage  RC, multiple choice computation  reasoning, multiple choice  2640  514  855  WikiQA (Yang et al., 2015) TREC-QA (Voorhees and Tice, 2000)  query logs  query logs + human editor  3047  IR, sentence selection IR, free form 1479  CNN/Daily Mail (Hermann et al., 2015) CBT (Hill et al., 2015)  summary cloze cloze  +  RC, \ufb01ll in single entity RC, \ufb01ll in single word  1.4M  688K  Table 1: A survey of several reading comprehension and ques-  tion answering datasets. SQuAD is much larger than all datasets  except the semi-synthetic cloze-style datasets, and it is similar  to TREC-QA in the open-endedness of the answers.  2 Existing Datasets  We begin with a survey of existing reading com- prehension and question answering (QA) datasets, highlighting a variety of task formulation and cre- ation strategies (see Table 1 for an overview).  Reading comprehension.\n\nfilename: materials_formatted.md\nextension: .md\n\n2% 54.5% \u00b12.1%  46.1% \u00b12.2% 71.4% \u00b12.0%  44.7% \u00b12.1% \u2014  Table 22: Agreement rates between humans and various automated metrics on TL;DR 6.7B human feedback optimized model at T=0.7. Standard errors estimated via bootstrapping. Note: in the entry for labeler vs. labeler ensemble, the ensembles are slightly smaller than for other comparisons because we need to exclude the labeler being predicted. All ensembles have at least 3 workers.  37  H Samples  H.1 Random samples  Here we provide non-cherry-picked samples and human evaluations for various models. In Tables 25- 26 we show samples on the TL;DR dataset, and in Tables 27-28 we show samples on the CNN/DM dataset (where we truncate the article for brevity). See our website for more uncurated policy samples.  H.2 Overoptimized samples  We show examples of samples from a policy overoptimized to rm3. The summaries, while clearly long, low quality, and full of idiosyncrasies, do still re\ufb02ect the rough gist of the post.  38  CNN/DM 6.7B sup. T=0.3  labeler  labeler  66.9% \u00b14.3%  length  labeler ensemble  74.5% \u00b16.8% 62.4% \u00b11.4% 49.6% \u00b11.4% ROUGE 55.2% \u00b11.4%  copying  1.3B sup. logprob  45.7% \u00b11.4% 1.3B RM 64.8% \u00b11.4%  6.7B sup. logprob  47.6% \u00b11.4% 6.7B RM 66.5% \u00b11.3%  labeler ensem- ble 74.5% \u00b16.8%  \u2014  57.5% \u00b17.7% 52.5% \u00b17.6% 75.0% \u00b16.7%  57.5% \u00b17.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nHow does human performance compare to the logistic regression model's performance on the SQuAD dataset?[/INST] \nAnswer: Human performance on the SQuAD dataset is significantly better than the logistic regression model's performance. The human performance score on the test set is 77.0% for the exact match metric, and 86.8% for F1. On the other hand, the logistic regression model's performance is 40.4% for exact match and 51.0% for F1 on the test set."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nArXiv, abs/1909.08593, 2019.  75     \n\n# Dense Passage Retrieval for Open-Domain Question Answering:  Dense Passage Retrieval for Open-Domain Question Answering  Vladimir Karpukhin\u2217, Barlas O\u02d8guz\u2217, Sewon Min\u2020, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen\u2021, Wen-tau Yih  Facebook AI  \u2020University of Washington  \u2021Princeton University  {vladk, barlaso, plewis, ledell, edunov, scottyih}@fb.com sewon@cs.washington.edu danqic@cs.princeton.edu  Abstract  Open-domain question answering relies on ef- \ufb01cient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented us- ing dense representations alone, where em- beddings are learned from a small number of questions and passages by a simple dual- encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene- BM25 system greatly by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.1  1  Introduction  Open-domain question answering (QA) (Voorhees, 1999) is a task that answers factoid questions us- ing a large collection of documents. While early QA systems are often complicated and consist of multiple components (Ferrucci (2012); Moldovan et al. (2003), inter alia), the advances of reading comprehension models suggest a much simpli\ufb01ed two-stage framework: (1) a context retriever \ufb01rst selects a small subset of passages where some of them contain the answer to the question, and then (2) a machine reader can thoroughly exam- ine the retrieved contexts and identify the correct answer (Chen et al., 2017). Although reducing open-domain QA to machine reading is a very rea- sonable strategy, a huge performance degradation is often observed in practice2, indicating the needs of improving retrieval.\n\nfilename: materials_formatted.md\nextension: .md\n\n[26], we use the December 2018 dump. Each Wikipedia article is split into disjoint 100-word chunks, to make a total of 21M documents. We use the document encoder to compute an embedding for each document, and build a single MIPS index using FAISS [ 23 ] with a Hierarchical Navigable Small World approximation for fast retrieval [ 37 ]. During training, we retrieve the top kdocuments for each query. We considerk\u2208{ 5 , 10 }for training and setkfor test time using dev data. We now discuss experimental details for each task.   3.1 Open-domain Question Answering   Open-domain question answering (QA) is an important real-world application and common testbed for knowledge-intensive tasks [ 20 ]. We treat questions and answers as input-output text pairs(x,y) and train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to the popular extractive QA paradigm [ 5 , 7 , 31 , 26 ], where answers are extracted spans from retrieved documents, relying primarily on non-parametric knowledge. We also compare to \u201cClosed-Book QA\u201d approaches [ 52 ], which, like RAG, generate answers, but which do not exploit retrieval, instead relying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural Questions (NQ) [ 29 ], TriviaQA (TQA) [ 24 ]. WebQuestions (WQ) [ 3 ] and CuratedTrec (CT) [ 2 ]. As CT and WQ are small, we follow DPR [ 26 ] by initializing CT and WQ models with our NQ RAG model. We use the same train/dev/test splits as prior work [ 31 , 26 ] and report Exact Match (EM) scores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set.   3.2 Abstractive Question Answering   RAG models can go beyond simple extractive QA and answer questions with free-form, abstractive text generation. To test RAG\u2019s natural language generation (NLG) in a knowledge-intensive setting, we use the MSMARCO NLG task v2.1 [ 43 ].\n\nfilename: materials_formatted.md\nextension: .md\n\nThis presents a more realistic scenario for practical applications.  Current approaches require a blackbox informa- tion retrieval (IR) system to do much of the heavy lifting, even though it cannot be \ufb01ne-tuned on the downstream task. In the strongly supervised set- ting popularized by DrQA (Chen et al., 2017), they also assume a reading comprehension model trained on question-answer-evidence triples, such as SQuAD (Rajpurkar et al., 2016). The IR sys- tem is used at test time to generate evidence candi- dates in place of the gold evidence. In the weakly supervised setting, proposed by TriviaQA (Joshi  et al., 2017), SearchQA (Dunn et al., 2017), and Quasar (Dhingra et al., 2017), the dependency on strong supervision is removed by assuming that the IR system provides noisy gold evidence.  These approaches rely on the IR system to mas- sively reduce the search space and/or reduce spu- rious ambiguity. However, QA is fundamentally different from IR (Singh, 2012). Whereas IR is concerned with lexical and semantic matching, questions are by de\ufb01nition under-speci\ufb01ed and re- quire more language understanding, since users are explicitly looking for unknown information. Instead of being subject to the recall ceiling from blackbox IR systems, we should directly learn to retrieve using question-answering data.  In this work, we introduce the \ufb01rst Open- Retrieval Question Answering system (ORQA). ORQA learns to retrieve evidence from an open corpus, and is supervised only by question- answer string pairs. While recent work on im- proving evidence retrieval has made signi\ufb01cant progress (Wang et al., 2018; Kratzwald and Feuer- riegel, 2018; Lee et al., 2018; Das et al., 2019), they still only rerank a closed evidence set. The main challenge to fully end-to-end learning is that retrieval over the open corpus must be considered a latent variable that would be impractical to train from scratch. IR systems offer a reasonable but potentially suboptimal starting point.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat challenges does ORQA address in open-domain question answering?[/INST] \nAnswer: ORQA, or Open-Retrieval Question Answering system, addresses several challenges in open-domain question answering. It aims to overcome the limitations of traditional information retrieval (IR) systems that are often used in question answering but cannot be fine-tuned for the specific task. These IR systems are used to generate evidence candidates or reduce the search space, but they may not be optimal for question answering, which requires more language understanding. ORQA introduces a new approach where it learns to retrieve evidence from an open corpus, supervised only by question-answer string pairs. This differs from previous methods that only rerank a closed evidence set. The main challenge to this end-to-end learning approach is that retrieval over the open corpus must be considered a latent variable that would be impractical to train from scratch."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nAt this writing, there are many open questions about the relative performance of the different methods. A full explanation of the differences in the learning algorithms, particularly the neural network architectures, is out of scope for this introduction, but it\u2019s fair to say that the space of possible learners for contextual word vectors has not yet been fully explored; see Peters et al. (2018b) for some exploration. Some of the \ufb01ndings on BERT suggest that the role of \ufb01netuning may be critical; indeed, earlier work that used pretrained language models to improve text classi\ufb01cation assumed \ufb01netuning was necessary (Dai and Le, 2015). While ELMo is derived from language modeling, the modeling problem solved by BERT (i.e., the objective function minimized during estimation) is rather different.11 The effects of the dataset used to learn the language model have not been fully assessed, except for the unsurprising pattern that larger datasets tend to offer more bene\ufb01t.  6 Cautionary Notes  Word vectors are biased. Like any engineered artifact, a computer program is likely to re\ufb02ect the per- spective of its builders. Computer programs that are built from data will re\ufb02ect what\u2019s in the data\u2014in this case, a text corpus. If the text corpus signals associations between concepts that re\ufb02ect cultural biases, these associations should be expected to persist in the word vectors and any system that uses them. Hence it is not surprising that NLP programs that use corpus-derived word vectors associate, for example, doctor with  11BERT pretraining focuses on two tasks: (i) prediction of words given contexts on both sides (rather than one or the other) and  (ii) predicting the words in a sentence given its preceding sentence.  11  male pronouns and nurse with female ones. Methods for detecting, avoiding, and correcting unwanted asso- ciations is an active area of research (Bolukbasi et al., 2016; Caliskan et al., 2017). The advent of contextual word vectors offers some possibility of new ways to avoid unwanted generalization from distributional pat- terns.  Language is a lot more than words.\n\nfilename: materials_formatted.md\nextension: .md\n\nMany of the word (type) vector algorithms already in use were based on a notion \ufb01xed-size contexts, collected across all instances of the word type in a corpus. ELMo went farther, using arbitrary-length histories and directly incorporating the language models known at the time to be most effective (based on recurrent neural networks; Sundermeyer et al., 2012). Although recurrent networks were already widely used in NLP (see Goldberg (2017) for a thorough introduc- tion), training them as language models, then using the context vectors they provide for each word token as pretrained word (token) vectors was novel.  It\u2019s interesting to see how the ideas around getting words into computers have come full circle. The powerful idea that text data can shed light on a word\u2019s meaning, by observing the contexts in which a word appears, has led us to try to capture a word token\u2019s meaning primarily through the speci\ufb01c context it appears in. This means that every instance of plant will have a different word vector; those with a context that look like a context for references to vegetation are expected to be close to each other, while those that are likely contexts for references to manufacturing centers will cluster elsewhere in vector space. Returning to the example in Figure 2, while instances of bodies in this article will likely remain closer to objects than to astronomers, in a medical news story, we might expect instances of bodies to be closer to humans (and by extension astronomers).  9Though such examples abound in linguistics, this one is due to Chris Dyer. 10The author does not endorse this substitution in actual cocktails.  10  Why is this advance so exciting? Whether the development of contextual word vectors completely solves the challenge of ambiguous words remains to be seen. New ideas in NLP are often tested on benchmark tasks with objectively measurable performance scores.\n\nfilename: materials_formatted.md\nextension: .md\n\n4 Words as Distributional Vectors: Context as Meaning  An important idea in linguistics is that words (or expressions) that can be used in similar ways are likely to have related meanings (Firth, 1957; consider our day of the week example above). In a large corpus, we can collect information about the ways a word type w is used, for example, by counting the number of times it appears near every other word type. When we begin looking at the full distribution of contexts (nearby words or sequences of words) in a corpus where w is found, we are taking a distributional view of word meaning.  One highly successful approach to automatically deriving features based on this idea is clustering; for example, the Brown et al. (1992) clustering algorithm automatically organized words into clusters based on the contexts they appear in, in a corpus. Words that tended to occur in the same neighboring contexts (other words) were grouped together into a cluster. These clusters could then be merged into larger clusters. The resulting hierarchy, while by no means identical to the expert-crafted data structure in WordNet, was surprisingly interpretable and useful (an example is shown in Figure 1). It also had the advantage that it could be rebuilt using any given corpus, and every word observed would be included. Hence suitable word clusters could be built separately for news text, or biomedical articles, or tweets.  Another line of approaches started by creating word vectors in which each dimension corresponded to the frequency the word type occurred in some context (Deerwester et al., 1990). For instance, one dimension might correspond to the and contain the number of times the word occurred within a small window of a token the. Contextual patterns on the left or the right, and of varying distances and lengths, might be included. The result is a vector perhaps many times longer than the size of the vocabulary, in which each dimension contains a tiny bit of information that may or may not be useful. An example is shown in Figure 2. Using methods from linear algebra, aptly named dimensionality reduction, these vectors could be compressed into shorter vectors in which redundancies across dimensions were collapsed.  These reduced-dimensionality vectors had several advantages. First, the dimensionality could be chosen by the NLP programmer to suit the needs of the program.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat are contextual word vectors, and why are they significant?[/INST] \nAnswer: Contextual word vectors are a method of representing words in natural language processing (NLP). Unlike traditional word vectors that assign a fixed vector to each word type, contextual word vectors assign different vectors to each instance of a word, depending on the specific context in which it appears. This means that the same word can have different vectors in different contexts. For example, the word \"plant\" would have different vectors when used in a context referring to vegetation versus a context referring to manufacturing centers. This approach is significant because it allows for a more nuanced understanding of word meanings, potentially improving the performance of NLP tasks. It also offers new ways to avoid unwanted generalization from distributional patterns, such as cultural biases reflected in the data. However, the full potential of this approach is still being explored."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nThis note introduces the field of Natural Language Pro- cessing (NLP) briefly, and then discusses word 2 vec and the funda- mental, beautiful idea of representing words as low-dimensional real-valued vectors learned from distributional signal.  ## 1 Introduction to Natural Language Processing   Natural language processing is a field of science and engineering focused on the development and study of automatic systems that understand and generate natural (that is, human,) languages.  ### 1. 1 Humans and language   Human languages are communicative devices enabling the effi- cient sharing and storage of complex ideas, facts, and intents. As [Manning, 2022 ] argues, the complexity of communication enabled by language is a uniquely human intelligence among species. As scientists and engineers interested in the creation and study of intel- ligent systems, human language is to us both a fascinating object of study\u2014after all, it hasevolved to be learnable and useful\u2014and a great enabler for interacting with humans even in contexts where other modalities (e.g., vision) are also of interest.  ### 1. 2 Language and machines   Human children, interacting with a rich multi-modality world and various forms of feedback, acquire language with exceptional sample efficiency (not observing that much language) and compute efficiency (brains are efficient computing machines!) With all the (impressive!) advances in NLP in the last decades, we are still nowhere close to developing learning machines that have a fraction of acquisition ability of children. One fundamental (and still quite open) problem in building language-learning machines is the question of representa- tion ; how should we represent language in a computer such that the computer can robustly process and/or generate it? This is where this course focuses on the tools provided by deep learning , a highly effec- tive toolkit for representing both the wild variety of natural language    deep learning 2  and some of the rules and structures it sometimes adheres to. Much of this course will be dedicated to this question of representation, and the rest of this note will talk about a basic subquestion: how do we represent words? Before that, though, let\u2019s briefly discuss some of the applications you can hope to build after learning modern NLP techniques.  ### 1.\n\nfilename: materials_formatted.md\nextension: .md\n\nFrom frequency to meaning: Vector space models of semantics. Journal  of Arti\ufb01cial Intelligence Research, 37(1):141\u2013188, 2010.  Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. XLNet:  Generalized autoregressive pretraining for language understanding. In Proc. of NeurIPS, 2019.  Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. SWAG: A large-scale adversarial dataset for  grounded commonsense inference. In Proc. of EMNLP, 2018.  15     \n\n# Natural Language Processing (Almost) from Scratch:  Journal of Machine Learning Research 12 (2011) 2493-2537 Submitted 1/10; Revised 11/10; Published 8/  ### Natural Language Processing (Almost) from Scratch  Ronan Collobert\u2217 RONAN@COLLOBERT.COM Jason Weston\u2020 JWESTON@GOOGLE.COM Leon Bottou \u0301 \u2021 LEON@BOTTOU.ORG Michael Karlen MICHAEL.KARLEN@GMAIL.COM Koray Kavukcuoglu\u00a7 KORAY@CS.NYU.EDU Pavel Kuksa\u00b6 PKUKSA@CS.RUTGERS.EDU NEC Laboratories America 4 Independence Way Princeton, NJ 08540  Editor:Michael Collins  #### Abstract   We propose a unified neural network architecture and learning algorithm that can be applied to var- ious natural language processing tasks including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatilityis achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational re- quirements. Keywords: natural language processing, neural networks  #### 1. Introduction  Will a computer program ever be able to convert a piece of English text into aprogrammer friendly data structure that describes the meaning of the natural language text?\n\nfilename: materials_formatted.md\nextension: .md\n\nRTE Recognizing Textual Entailment is a bi- nary entailment task similar to MNLI, but with much less training data (Bentivogli et al., 2009).14  WNLI Winograd NLI is a small natural lan- guage inference dataset (Levesque et al., 2011). The GLUE webpage notes that there are issues with the construction of this dataset, 15 and every trained system that\u2019s been submitted to GLUE has performed worse than the 65.1 baseline accuracy of predicting the majority class. We therefore ex- clude this set to be fair to OpenAI GPT. For our GLUE submission, we always predicted the ma-  14Note that we only report single-task \ufb01ne-tuning results in this paper. A multitask \ufb01ne-tuning approach could poten- tially push the performance even further. For example, we did observe substantial improvements on RTE from multi- task training with MNLI.  15https://gluebenchmark.com/faq  BERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphBERTE[CLS]E1 E2 ENCT1 T2 TNSingle Sentence ......BERTTok 1 Tok 2 Tok N...[CLS]E[CLS]E1 E2 ENCT1 T2 TNSingle Sentence B-PEROO......E[CLS]E1 E[SEP]Class Label...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019Start/End SpanClass LabelBERTTok 1 Tok 2 Tok N...[CLS]Tok 1[CLS][CLS]Tok 1 [SEP]...Tok NTok 1...TokMSentence 1...Sentence 2Note that the purpose of the masking strategies is to reduce the mismatch between pre-training and \ufb01ne-tuning, as the [MASK] symbol never ap- pears during the \ufb01ne-tuning stage. We report the Dev results for both MNLI and NER.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat is the main contribution of the 'Natural Language Processing (Almost) from Scratch' paper?[/INST] \nAnswer: The main contribution of the 'Natural Language Processing (Almost) from Scratch' paper is the proposal of a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and disregarding a lot of prior knowledge. Instead of exploiting man-made input features optimized for each task, the system learns internal representations based on vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nBut this is unnecessary for the logit outputs where the prediction con\ufb01dence is determined by the scale of the logits. We only apply layer normalization to the fully-connected hidden layers that excludes the last softmax layer.  All the models were trained using 55000 training data points and the Adam [Kingma and Ba, 2014] optimizer. For the smaller batch-size, the variance term for batch normalization is computed using the unbiased estimator. The experimental results from Figure 6 highlight that layer normalization is robust to the batch-sizes and exhibits a faster training convergence comparing to batch normalization that is applied to all layers.  6.7 Convolutional Networks  We have also experimented with convolutional neural networks. In our preliminary experiments, we observed that layer normalization offers a speedup over the baseline model without normalization, but batch normalization outperforms the other methods. With fully connected layers, all the hidden units in a layer tend to make similar contributions to the \ufb01nal prediction and re-centering and re- scaling the summed inputs to a layer works well. However, the assumption of similar contributions is no longer true for convolutional neural networks. The large number of the hidden units whose receptive \ufb01elds lie near the boundary of the image are rarely turned on and thus have very different statistics from the rest of the hidden units within the same layer. We think further research is needed to make layer normalization work well in ConvNets.  7 Conclusion  In this paper, we introduced layer normalization to speed-up the training of neural networks. We provided a theoretical analysis that compared the invariance properties of layer normalization with batch normalization and weight normalization. We showed that layer normalization is invariant to per training-case feature shifting and scaling.  Empirically, we showed that recurrent neural networks bene\ufb01t the most from the proposed method especially for long sequences and small mini-batches.  Acknowledgments  This research was funded by grants from NSERC, CFI, and Google.\n\nfilename: materials_formatted.md\nextension: .md\n\nThe collapse could be due to a less-explicit requirement for factual knowledge in some tasks, or the longer target sequences, which could result in less informative gradients for the retriever. Perez et al.[46]also found spurious retrieval results when optimizing a retrieval component in order to improve performance on downstream tasks.  ### I Number of instances per dataset  The number of training, development and test datapoints in each of our datasets is shown in Table 7.      \n\n# Layer Normalization:  Layer Normalization  Jimmy Lei Ba University of Toronto jimmy@psi.toronto.edu  Jamie Ryan Kiros University of Toronto rkiros@cs.toronto.edu  Geoffrey E. Hinton University of Toronto and Google Inc. hinton@cs.toronto.edu  Abstract  Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This signi\ufb01cantly reduces the training time in feed- forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural net- works. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empiri- cally, we show that layer normalization can substantially reduce the training time compared with previously published techniques.\n\nfilename: materials_formatted.md\nextension: .md\n\n1  Introduction  Deep neural networks trained with some version of Stochastic Gradient Descent have been shown to substantially outperform previous approaches on various supervised learning tasks in computer vision [Krizhevsky et al., 2012] and speech processing [Hinton et al., 2012]. But state-of-the-art deep neural networks often require many days of training. It is possible to speed-up the learning by computing gradients for different subsets of the training cases on different machines or splitting the neural network itself over many machines [Dean et al., 2012], but this can require a lot of com- munication and complex software. It also tends to lead to rapidly diminishing returns as the degree of parallelization increases. An orthogonal approach is to modify the computations performed in the forward pass of the neural net to make learning easier. Recently, batch normalization [Ioffe and Szegedy, 2015] has been proposed to reduce training time by including additional normalization stages in deep neural networks. The normalization standardizes each summed input using its mean and its standard deviation across the training data. Feedforward neural networks trained using batch normalization converge faster even with simple SGD. In addition to training time improvement, the stochasticity from the batch statistics serves as a regularizer during training.  Despite its simplicity, batch normalization requires running averages of the summed input statis- tics. In feed-forward networks with \ufb01xed depth, it is straightforward to store the statistics separately for each hidden layer. However, the summed inputs to the recurrent neurons in a recurrent neu- ral network (RNN) often vary with the length of the sequence so applying batch normalization to RNNs appears to require different statistics for different time-steps. Furthermore, batch normaliza-              tion cannot be applied to online learning tasks or to extremely large distributed models where the minibatches have to be small.  This paper introduces layer normalization, a simple normalization method to improve the training speed for various neural network models. Unlike batch normalization, the proposed method directly estimates the normalization statistics from the summed inputs to the neurons within a hidden layer so the normalization does not introduce any new dependencies between training cases. We show that layer normalization works well for RNNs and improves both the training time and the generalization performance of several existing RNN models.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nHow does layer normalization differ from batch normalization in its application?[/INST] \nAnswer: Layer normalization differs from batch normalization in several ways. First, while batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance, layer normalization computes the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Second, layer normalization performs exactly the same computation at training and test times, unlike batch normalization. Lastly, layer normalization can be applied straightforwardly to recurrent neural networks by computing the normalization statistics separately at each time step, whereas applying batch normalization to RNNs appears to require different statistics for different time-steps."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nOur experiments on multiple existing QA datasets indicate that (1) both modules to are highly competitive with respect existing counterparts and (2) multitask learning using distant supervision on their combination is an effective complete system on this challenging task.  1  Introduction  This paper considers the problem of answering factoid questions in an open-domain setting us- ing Wikipedia as the unique knowledge source, such as one does when looking for answers in an encyclopedia. Wikipedia is a constantly evolv- ing source of detailed information that could fa- cilitate intelligent machines \u2014 if they are able to leverage its power. Unlike knowledge bases (KBs) such as Freebase (Bollacker et al., 2008) or DB- Pedia (Auer et al., 2007), which are easier for computers to process but too sparsely populated for open-domain question answering (Miller et al.,  \u2217 Most of this work was done while DC was with Face-  book AI Research.  2016), Wikipedia contains up-to-date knowledge that humans are interested in. It is designed, how- ever, for humans \u2013 not machines \u2013 to read.  Using Wikipedia articles as the knowledge source causes the task of question answering (QA) to combine the challenges of both large-scale open-domain QA and of machine comprehension of text. In order to answer any question, one must \ufb01rst retrieve the few relevant articles among more than 5 million items, and then scan them care- fully to identify the answer. We term this setting, machine reading at scale (MRS). Our work treats Wikipedia as a collection of articles and does not rely on its internal graph structure. As a result, our approach is generic and could be switched to other collections of documents, books, or even daily up- dated newspapers.  Large-scale QA systems like IBM\u2019s DeepQA (Ferrucci et al., 2010) rely on multiple sources to answer: besides Wikipedia, it is also paired with KBs, dictionaries, and even news articles, books, etc. As a result, such systems heavily rely on information redundancy among the sources to answer correctly. Having a single knowledge source forces the model to be very precise while searching for an answer as the evidence might appear only once.\n\nfilename: materials_formatted.md\nextension: .md\n\n[26], we use the December 2018 dump. Each Wikipedia article is split into disjoint 100-word chunks, to make a total of 21M documents. We use the document encoder to compute an embedding for each document, and build a single MIPS index using FAISS [ 23 ] with a Hierarchical Navigable Small World approximation for fast retrieval [ 37 ]. During training, we retrieve the top kdocuments for each query. We considerk\u2208{ 5 , 10 }for training and setkfor test time using dev data. We now discuss experimental details for each task.   3.1 Open-domain Question Answering   Open-domain question answering (QA) is an important real-world application and common testbed for knowledge-intensive tasks [ 20 ]. We treat questions and answers as input-output text pairs(x,y) and train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to the popular extractive QA paradigm [ 5 , 7 , 31 , 26 ], where answers are extracted spans from retrieved documents, relying primarily on non-parametric knowledge. We also compare to \u201cClosed-Book QA\u201d approaches [ 52 ], which, like RAG, generate answers, but which do not exploit retrieval, instead relying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural Questions (NQ) [ 29 ], TriviaQA (TQA) [ 24 ]. WebQuestions (WQ) [ 3 ] and CuratedTrec (CT) [ 2 ]. As CT and WQ are small, we follow DPR [ 26 ] by initializing CT and WQ models with our NQ RAG model. We use the same train/dev/test splits as prior work [ 31 , 26 ] and report Exact Match (EM) scores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set.   3.2 Abstractive Question Answering   RAG models can go beyond simple extractive QA and answer questions with free-form, abstractive text generation. To test RAG\u2019s natural language generation (NLG) in a knowledge-intensive setting, we use the MSMARCO NLG task v2.1 [ 43 ].\n\nfilename: materials_formatted.md\nextension: .md\n\nThe dataset contains 87k examples for training and 10k for development, with a large hidden test set which can only be accessed by the SQuAD creators. Each example is composed of a paragraph extracted from a Wikipedia article and an associated human-generated question. The answer is always a span from this paragraph and a model is given credit if its predicted answer matches it. Two evaluation metrics are used: exact string match (EM) and F1 score, which measures the weighted average of precision and recall at the token level.  In the following, we use SQuAD for training and evaluating our Document Reader for the stan- dard machine comprehension task given the rel-  2https://dumps.wikimedia.org/enwiki/  latest  evant paragraph as de\ufb01ned in (Rajpurkar et al., 2016). For the task of evaluating open-domain question answering over Wikipedia, we use the SQuAD development set QA pairs only, and we ask systems to uncover the correct answer spans without having access to the associated para- graphs. That is, a model is required to answer a question given the whole of Wikipedia as a re- source; it is not given the relevant paragraph as in the standard SQuAD setting.  4.3 Open-domain QA Evaluation Resources  SQuAD is one of the largest general purpose QA datasets currently available. SQuAD questions have been collected via a process involving show- ing a paragraph to each human annotator and ask- ing them to write a question. As a result, their distribution is quite speci\ufb01c. We hence propose to train and evaluate our system on other datasets de- veloped for open-domain QA that have been con- structed in different ways (not necessarily in the context of answering from Wikipedia).  CuratedTREC This dataset is based on the benchmarks from the TREC QA tasks that have been curated by Baudi\u02c7s and \u02c7Sediv`y (2015). We use the large version, which contains a total of 2,180 questions extracted from the datasets from TREC 1999, 2000, 2001 and 2002.4  WebQuestions Introduced in (Berant et al., 2013), this dataset is built to answer questions from the Freebase KB.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWho are the authors of 'Reading Wikipedia to Answer Open-Domain Questions'?[/INST] \nAnswer: I don't know."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nBy applying a modern reader model to the top retrieved passages, we achieve compara- ble or better results on multiple QA datasets in the open-retrieval setting, compared to several, much complicated systems.  2 Background  The problem of open-domain QA studied in this paper can be described as follows. Given a factoid question, such as \u201cWho \ufb01rst voiced Meg on Family Guy?\u201d or \u201cWhere was the 8th Dalai Lama born?\u201d, a system is required to answer it using a large corpus of diversi\ufb01ed topics. More speci\ufb01cally, we assume  e  the extractive QA setting, in which the answer is restricted to a span appearing in one or more pas- sages in the corpus. Assume that our collection contains D documents, d1, d2, \u00b7 \u00b7 \u00b7 , dD. We \ufb01rst split each of the documents into text passages of equal lengths as the basic retrieval units3 and get M total passages in our corpus C = {p1, p2, . . . , pM }, where each passage pi can be viewed as a sequence 2 , \u00b7 \u00b7 \u00b7 , w(i) 1 , w(i) of tokens w(i) |pi|. Given a question q, the task is to \ufb01nd a span w(i) s+1, \u00b7 \u00b7 \u00b7 , w(i) s , w(i) from one of the passages pi that can answer the question. Notice that to cover a wide variety of domains, the corpus size can easily range from millions of docu- ments (e.g., Wikipedia) to billions (e.g., the Web). As a result, any open-domain QA system needs to include an ef\ufb01cient retriever component that can se- lect a small set of relevant texts, before applying the reader to extract the answer (Chen et al., 2017).4 Formally speaking, a retriever R : (q, C) \u2192 CF is a function that takes as input a question q and a corpus C and returns a much smaller \ufb01lter set of texts CF \u2282 C, where |CF | = k (cid:28) |C|.\n\nfilename: materials_formatted.md\nextension: .md\n\n- Retriever: \ud835\udc53(\ud835\udc9f, \ud835\udc44) \u27f6 \ud835\udc43 1 , ..., \ud835\udc43\ud835\udc3e - Reader: \ud835\udc54(\ud835\udc44, {\ud835\udc43 1 ,... , \ud835\udc43\ud835\udc3e}) \u27f6 \ud835\udc34   InDrQA,  - Retriever = A standard TF-IDF information-retrieval sparse model (a fixed module) - Reader = a neural reading comprehension model that we just learned     - Trained on SQuAD and other distantly-supervised QA datasets  _Distantly-supervised examples:_ (Q, A) \u27f6 (P, Q, A)   #### We can train the retriever too   Leeet al., 2019. LatentRetrieval for Weakly SupervisedOpen Domain Question Answering^46  - Joint training of retriever and reader     - Each text passage can beencoded as a vector using BERT and the retriever score can be measured as        the dot product between the question representation and passage representation.     - However, it is not easy to model as there are a huge number of passages (e.g., 21M in English Wikipedia)   #### We can train the retriever too  - Dense passage retrieval (DPR) - We can also just train the retriever using question-answer pairs!     - Trainable retriever (using BERT) largely outperforms traditional IR retrieval models   Karpukhin et al., 2020. Dense Passage Retrieval for Open-DomainQuestionAnswering^47   #### We can train the retriever too   http://qa.cs.washington.edu:2020/   Karpukhin et al., 2020. Dense Passage Retrieval for Open-DomainQuestionAnswering^48   #### Dense retrieval + generative models   49   Recent work shows that it is beneficial to generate answers instead of to extract answers.   Izacard andGrave2020.LeveragingPassageRetrievalwithGenerativeModelsfor OpenDomainQuestionAnswering   Fusion-in-decoder (FID) = DPR + T5   Large language models can do open-domain QA well  - ... without an explicit retriever stage   Roberts etal., 2020. HowMuchKnowledge CanYou Pack Into the Parameters ofaLanguageModel?^50   Maybe the reader model is not necessary too!\n\nfilename: materials_formatted.md\nextension: .md\n\nArXiv, abs/1909.08593, 2019.  75     \n\n# Dense Passage Retrieval for Open-Domain Question Answering:  Dense Passage Retrieval for Open-Domain Question Answering  Vladimir Karpukhin\u2217, Barlas O\u02d8guz\u2217, Sewon Min\u2020, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen\u2021, Wen-tau Yih  Facebook AI  \u2020University of Washington  \u2021Princeton University  {vladk, barlaso, plewis, ledell, edunov, scottyih}@fb.com sewon@cs.washington.edu danqic@cs.princeton.edu  Abstract  Open-domain question answering relies on ef- \ufb01cient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented us- ing dense representations alone, where em- beddings are learned from a small number of questions and passages by a simple dual- encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene- BM25 system greatly by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.1  1  Introduction  Open-domain question answering (QA) (Voorhees, 1999) is a task that answers factoid questions us- ing a large collection of documents. While early QA systems are often complicated and consist of multiple components (Ferrucci (2012); Moldovan et al. (2003), inter alia), the advances of reading comprehension models suggest a much simpli\ufb01ed two-stage framework: (1) a context retriever \ufb01rst selects a small subset of passages where some of them contain the answer to the question, and then (2) a machine reader can thoroughly exam- ine the retrieved contexts and identify the correct answer (Chen et al., 2017). Although reducing open-domain QA to machine reading is a very rea- sonable strategy, a huge performance degradation is often observed in practice2, indicating the needs of improving retrieval.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat is the function of a retriever in the retriever-reader framework for open-domain QA?[/INST] \nAnswer: In the retriever-reader framework for open-domain question answering, the retriever's function is to select a small subset of relevant text passages from a large corpus of documents. This is done in response to a given factoid question. The retriever essentially filters down the vast amount of information in the corpus to a manageable set of texts that are likely to contain the answer to the question."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nThus turning the problem into a sequence modeling problem, the state of the art approaches apply recurrent or convolutional neural networks to predict each next pixel given all previously generated pixels (van den Oord et al., 2016a). Training recurrent neural networks to sequentially predict each pixel of even a small image is computationally very challenging. Thus, parallelizable models that use convolutional neural networks such as the PixelCNN have recently received much more attention, and have now surpassed the PixelRNN in quality (van den Oord et al., 2016b).  One disadvantage of CNNs compared to RNNs is their typically fairly limited receptive \ufb01eld. This can adversely affect their ability to model long-range phenomena common in images, such as symmetry and occlusion, especially with a small number of layers. Growing the receptive \ufb01eld has been shown to improve quality signi\ufb01cantly (Salimans et al.). Doing so, however, comes at a signi\ufb01cant cost in number              Image Transformer  of parameters and consequently computational performance and can make training such models more challenging.  In this work we show that self-attention (Cheng et al., 2016; Parikh et al., 2016; Vaswani et al., 2017) can achieve a better balance in the trade-off between the virtually unlimited receptive \ufb01eld of the necessarily sequential PixelRNN and the limited receptive \ufb01eld of the much more parallelizable PixelCNN and its various extensions.  We adopt similar factorizations of the joint pixel distribu- tion as previous work. Following recent work on model- ing text (Vaswani et al., 2017), however, we propose es- chewing recurrent and convolutional networks in favor of the Image Transformer, a model based entirely on a self- attention mechanism. The speci\ufb01c, locally restricted form of multi-head self-attention we propose can be interpreted as a sparsely parameterized form of gated convolution. By decoupling the size of the receptive \ufb01eld from the num- ber of parameters, this allows us to use signi\ufb01cantly larger receptive \ufb01elds than the PixelCNN.  Despite comparatively low resource requirements for train- ing, the Image Transformer attains a new state of the art in modeling images from the standard ImageNet data set, as measured by log-likelihood.\n\nfilename: materials_formatted.md\nextension: .md\n\nOn average, we get a MS-SSIM score of 44.3, on 50 samples from the validation set, which shows that our models don\u2019t merely learn to copy training images but generate high-quality images by adding synthesized details on the low resolution input image.  Image Transformer  Input  1D Local Attention \u03c4 = 0.9  \u03c4 = 0.8  \u03c4 = 1.0  \u03c4 = 0.8  2D Local Attention \u03c4 = 0.9  \u03c4 = 1.0  Original  Table 6. Images from our 1D and 2D local attention super-resolution models trained on CelebA, sampled with different temperatures. 2D local attention with \u03c4 = 0.9 scored highest in our human evaluation study.  CIFAR-10 We also trained a super-resolution model on the CIFAR-10 data set. Our model reached a negative log- likelihood of 2.76 using 1D local attention and 2.78 using 2D local attention on the test set. As seen in Figure 2, our model commonly generates plausible looking objects even though the input images seem to barely show any discernible structure beyond coarse shapes.  previously proposed (Mansimov et al., 2015), and tasks combining modalities such as language-driven editing of images.  Fundamentally, we aim to move beyond still images to video (Kalchbrenner et al., 2016) and towards applications in model-based reinforcement learning.  6. Conclusion  In this work we demonstrate that models based on self- attention can operate effectively on modalities other than text, and through local self-attention scale to signi\ufb01cantly larger structures than sentences. With fewer layers, its larger receptive \ufb01elds allow the Image Transformer to signi\ufb01cantly improve over the state of the art in unconditional, probabilis- tic image modeling of comparatively complex images from ImageNet as well as super-resolution.  We further hope to have provided additional evidence that even in the light of generative adversarial networks, likelihood-based models of images is very much a promising area for further research - as is using network architectures such as the Image Transformer in GANs.  References  Ba, Jimmy Lei, Kiros, Jamie Ryan, and Hinton, Geoffrey E. Layer normalization.\n\nfilename: materials_formatted.md\nextension: .md\n\nModel Type ResNet srez GAN PixelRecursive (Dahl et al., 2017)  1D local Image Transformer  2D local Image Transformer  \u03c4 n/a n/a 1.0 0.9 0.8 1.0 0.9 0.8 1.0 0.9 0.8  %Fooled 4.0 8.5 11.0 10.4 10.2 29.6 \u00b1 4.0 33.5 \u00b1 3.5 35.94 \u00b1 3.0 30.64 \u00b1 4 34 \u00b1 3.5 36.11 \u00b1 2.5  Turk where each worker is required to make a binary choice when shown one generated and one real image. Following the same procedure for the evaluation study as (Dahl et al., 2017), we show 50 pairs of images, selected randomly from the validation set, to 50 workers each. Each generated and original image is upscaled to 128 \u00d7 128 pixels using the Bilinear interpolation method. Each worker then has 1-2 seconds to make a choice between these two images. In our method, workers choose images from our model up to 36.1% of the time, a signi\ufb01cant improvement over pre- vious models. Sampling temperature of 0.8 and 2D local attention maximized perceptual quality as measured by this evaluation.  To measure how well the high resolution samples correspond to the low resolution input, we calculate Consistency, the L2 distance between the low resolution input and a bicubic downsampled version of the high resolution sample. We observe a Consistency score of 0.01 which is on par with the models in (Dahl et al., 2017).  We quantify that our models are more effective than exem- plar based Super Resolution techniques like Nearest Neigh- bors, which perform a naive look-up of the training data to \ufb01nd the high resolution output. We take a bicubic down- sampled version of our high resolution sample, \ufb01nd the nearest low resolution input image in the training data for that sample, and calculate the MS-SSIM score between the high resolution sample and the corresponding high reso- lution image in the training data.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nHow does the Image Transformer model differ in its approach from traditional convolutional neural networks?[/INST] \nAnswer: The Image Transformer model differs from traditional convolutional neural networks (CNNs) in that it is based entirely on a self-attention mechanism, rather than relying on recurrent or convolutional networks. This allows it to achieve a better balance in the trade-off between the virtually unlimited receptive field of the necessarily sequential PixelRNN and the limited receptive field of the much more parallelizable PixelCNN and its various extensions. The specific, locally restricted form of multi-head self-attention proposed can be interpreted as a sparsely parameterized form of gated convolution. By decoupling the size of the receptive field from the number of parameters, the Image Transformer can use significantly larger receptive fields than the PixelCNN. Despite comparatively low resource requirements for training, the Image Transformer attains a new state of the art in modeling images from the standard ImageNet data set, as measured by log-likelihood."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nThe first technique is that instead of initializingW(hh)ran- domly, start off from an identity matrix initialization. The second technique is to use the Rectified Linear Units (ReLU) in- stead of the sigmoid function. The derivative for the ReLU is either 0 or   ## language models,rnn,gru and lstm 9  1. This way, gradients would flow through the neurons whose deriva- tive is 1 without getting attenuated while propagating back through time-steps.  #### 2. 5 Deep Bidirectional RNNs  So far, we have focused on RNNs that condition on past words to predict the next word in the sequence. It is possible to make predic- tions based on future words by having the RNN model read through the corpus backwards. Irsoy et al. shows a bi-directional deep neu- ral network; at each time-step,t, this network maintains two hidden layers, one for the left-to-right propagation and another for the right- to-left propagation. To maintain two hidden layers at any time, this network consumes twice as much memory space for its weight and bias parameters. The final classification result,y\u02c6t, is generated through combining the score results produced by both RNN hidden layers. Fig- ure 8 shows the bi-directional network architecture, and Equations 17 and 18 show the mathematical formulation behind setting up the bi- directional RNN hidden layer. The only difference between these two relationships is in the direction of recursing through the corpus. Equa- tion 19 shows the classification relationship used for predicting the next word via summarizing past and future word representations.  ## Bidirectionality   RNNs can also be multi-layered. Figure 9 shows a multi-layer bi- directional RNN where each lower layer feeds the next layer. As shown in this figure, in this network architecture, at time-stepteach interme- diate neuron receives one set of parameters from the previous time- step (in the same RNN layer), and two sets of parameters from the previous RNN hidden layer; one input comes from the left-to-right RNN and the other from the right-to-left RNN.\n\nfilename: materials_formatted.md\nextension: .md\n\nHere, we discuss the use of a gated activation function thereby modifying the RNN architecture. What motivates this? Well, although RNNs can theoretically capture long-term dependencies, they are very hard to actually train to do this. Gated recurrent units are designed in a manner to have more persistent memory thereby making it easier for RNNs to capture long-term dependencies. Let us see mathematically how a GRU usesht\u2212 1 andxtto generate the next hidden stateht. We will then dive into the intuition of this architecture.   zt= \u03c3 (W(z)xt+U(z)ht\u2212 1 ) (Update gate) rt= \u03c3 (W(r)xt+U(r)ht\u2212 1 ) (Reset gate) \u0303ht=tanh(rt\u25e6Uht\u2212 1 +Wxt) (New memory) ht= ( 1 \u2212zt)\u25e6h \u0303t+zt\u25e6ht\u2212 1 (Hidden state)   ### language models,rnn,gru and lstm 12  The above equations can be thought of a GRU\u2019s four fundamental op- erational stages and they have intuitive interpretations that make this model much more intellectually satisfying (see Figure 12 ):  1. New memory generation: A new memory \u0303htis the consolidation of a new input wordxtwith the past hidden stateht\u2212 1. Anthropomor- phically, this stage is the one who knows the recipe of combining a newly observed word with the past hidden stateht\u2212 1 to summarize this new word in light of the contextual past as the vectorh \u0303t.  2. Reset Gate: The reset signalrtis responsible for determining how importantht\u2212 1 is to the summarizationh \u0303t. The reset gate has the ability to completely diminish past hidden state if it finds thatht\u2212 1 is irrelevant to the computation of the new memory.  3. Update Gate: The update signalztis responsible for determining how much ofht\u2212 1 should be carried forward to the next state. For instance, ifzt \u22481, thenht\u2212 1 is almost entirely copied out toht. Conversely, ifzt\u22480, then mostly the new memoryh \u0303tis forwarded to the next hidden state.  4.\n\nfilename: materials_formatted.md\nextension: .md\n\nmax _\u03b8_  ##### 1  ##### N   N   \u2211   n= 1   logp \u03b8 (y(n)|x(n)) ( 26 )  Extension I: train different RNN weights for encoding and decoding. This decouples the two units and allows for more accuracy prediction of each of the two RNN modules. This means the _\u03c6_ ()functions in Equations 23 and 24 would have differentW(hh)matrices.   Figure 11 : Language model with three inputs to each decoder neuron: (ht\u2212 1 ,c,yt\u2212 1 )  Extension II: compute every hidden state in the decoder using three different inputs:  - The previous hidden state (standard) - Last hidden layer of the encoder (c=hTin Figure 11 )   ### language models,rnn,gru and lstm 11  - Previous predicted output word,y\u02c6t\u2212 1  Combining the above three inputs transforms the _\u03c6_ function in the decoder function of Equation 24 to the one in Equation 27. Figure 11 illustrates this model.   ht= \u03c6 (ht\u2212 1 ,c,yt\u2212 1 ) ( 27 )  Extension III: train deep recurrent neural networks using multiple RNN layers as discussed earlier in this chapter. Deeper layers often improve prediction accuracy due to their higher learning capacity. Of course, this implies a large training corpus must be used to train the model.  Extension IV: train bi-directional encoders to improve accuracy simi- lar to what was discussed earlier in this chapter.  Extension V: given a word sequenceA B Cin German whose transla- tion isX Yin English, instead of training the RNN usingA B C\u2192X Y, train it usingC B A\u2192X Y. The intutition behind this technique is thatAis more likely to be translated toX. Thus, given the vanishing gradient problem discussed earlier, reversing the order of the input words can help reduce the error rate in generating the output phrase.  ### 3 Gated Recurrent Units  Beyond the extensions discussed so far, RNNs have been found to per- form better with the use of more complex units for activation. So far, we have discussed methods that transition from hidden stateht\u2212 1 to htusing an affine transformation and a point-wise nonlinearity.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nHow do Deep Bidirectional RNNs enhance the capability of standard RNNs?[/INST] \nAnswer: Deep Bidirectional RNNs enhance the capability of standard RNNs by conditioning not only on past words but also on future words to make predictions. This is achieved by having the RNN model read through the corpus backwards. At each time-step, this network maintains two hidden layers, one for the left-to-right propagation and another for the right-to-left propagation. This dual-layer approach allows the network to consume twice as much memory space for its weight and bias parameters. The final classification result is generated by combining the score results produced by both RNN hidden layers. This bidirectional approach allows the model to have a more comprehensive understanding of the sequence, enhancing its predictive capabilities."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\n#### CoCa Contrastive Captioner (Yu et al., 2022)  Best of both (contrastive and generative) worlds.   #### Frozen (Tsimpoukelli, Menick, Cabi, et al., 2021)  Kind of like MMBT but with a better LLM (T5) and a better vision encoder (NF-ResNet).  Multi-Modal Few-Shot Learners!   #### Flamingo (Alayrac et al., 2022)  80b param model based on Chinchilla. Multi-image.   #### Perceiver Resampler   #### Gated XATTN  Inject visual info directly into a frozen LM via cross-attention (remember FiLM?).   #### Why is this funny?  Original image from Karpathy as a \u201cvisual Turing test\u201d \u2192   #### BLIP/BLIP2 (Li et al., 2023)  Freeze it all (CLIP-ViT / OPT decoder / FlanT5 encoder-decoder)    #### Multimodal \u201cChain of Thought\u201d (Zhang et al., 2023)  Providing a rationale helps give the right answer.   #### KOSMOS- 1 (Huang et al., 2023)  LLMs => MLLMs == FMs   #### Outline  1. Early models 2. Features and fusion 3. Contrastive models 4. Multimodal foundation models 5. Evaluation 6. Beyond images: Other modalities 7. Where to next?   #### COCO - Common Objects in Context  Super impactful datasets (Lin et al. 2014; Chen et al. 2015)  Main multimodal tasks:   \u25cf Image captioning \u25cf Image-caption retrieval  Similar datasets:   \u25cf Flickr30k, ConceptualCaptions, VisualGenome, SBU, RedCaps, LAION   cocodataset.org   #### VQA - Visual Question Answering (Antol et al., 2015)   \u25cf The dominant task in vision and language.\n\nfilename: materials_formatted.md\nextension: .md\n\n#### DALL-E2 on Winoground I   #### DALL-E2 on Winoground II   STILL NOT SOLVED   #### Outline  1. Early models 2. Features and fusion 3. Contrastive models 4. Multimodal foundation models 5. Evaluation 6. Beyond images: Other modalities 7. Where to next?   #### Speech / audio  Can EASILY do another full lecture just on this topic.  Recent cool example: Whisper, trained on 680,000 hours of multilingual multitask data.  We can also just treat audio as vision ;) Radford et al., 2022  Kiela et al., (^2017) Bapna et al., 2022   #### Video and text and audio  MERLOT (Zellers et al., 2021) MERLOT Reserve (idem, 2022)   #### Grounded language learning in simulated environments   Hermann, Hill, et al. 2017 Das et al., 2018   #### Text to 3D  POINT-E (Nichol, Jun, et al., 2022)   #### Outline  1. Early models 2. Multimodal fusion 3. Contrastive models 4. Multimodal foundation models 5. Other modalities 6. Evaluation 7. Where to next?   #### One foundation model to rule them all  There will modality-agnostic foundation models that can read and generate many modalities.  These models can be bigger and be trained on vastly more data. Parameters will be shared in interesting ways.  Automatic alignment from unpaired unimodal data will become a big topic.   #### Multimodal scaling laws  We are just beginning to understand multimodal scaling laws, lots of interesting work to do here in understanding trade-offs.   #### Retrieval augmented generative multimodal models   Lewis et al., 2021   These can be multimodal   This can be multimodal This^ can^ be^ multimodal   #### Better evaluation and benchmarking   Barbosa-Silva et al. 2022   Kiela et al. 2020  We need better measurement.   # Thank you!\n\nfilename: materials_formatted.md\nextension: .md\n\nMicrosoft coco: Common objects in context. ECCV, 2014.  Kyunghyun Cho, Bart Van Merri\u00a8enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. EMNLP, 2014.  Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition.  ICLR, 2015.  Ryan Kiros, Ruslan Salakhutdinov, and Richard S Zemel. Unifying visual-semantic embeddings with multi-  modal neural language models. arXiv preprint arXiv:1411.2539, 2014.  D. Kingma and J. L. Ba. Adam: a method for stochastic optimization. ICLR, 2014. arXiv:1412.6980.  Liwei Wang, Yin Li, and Svetlana Lazebnik. Learning deep structure-preserving image-text embeddings.  CVPR, 2016.  Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman,  and Phil Blunsom. Teaching machines to read and comprehend. In NIPS, 2015.  Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Torralba, and Sanja  Fidler. Skip-thought vectors. In NIPS, 2015.  Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Ef\ufb01cient estimation of word representations in  vector space. arXiv preprint arXiv:1301.3781, 2013.  Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In ICCV, 2015.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat are the main tasks associated with the COCO dataset in multimodal research?[/INST] \nAnswer: The main tasks associated with the COCO dataset in multimodal research are image captioning and image-caption retrieval."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nInstead, we introduce a more general weighting function, which we are free to take to depend on the context word as well:  #### J\u02c6=   W   \u2211   i= 1   W   \u2211   j= 1   f(Xij)(~uTj~vi\u2212logXij)^2  ### 1. 4 Conclusion  In conclusion, the GloVe model efficiently leverages global statistical information by training only on the nonzero elements in a word- word co-occurrence matrix, and produces a vector space with mean- ingful sub-structure. It consistently outperformsword 2 vecon the word analogy task, given the same corpus, vocabulary, window size, and training time. It achieves better results faster, and also obtains the best results irrespective of speed.  ## 2 Evaluation of Word Vectors  So far, we have discussed methods such as theWord _2_ VecandGloVe methods to train and discover latent vector representations of natural language words in a semantic space. In this section, we discuss how we can quantitatively evaluate the quality of word vectors produced by such techniques.  ### 2. 1 Intrinsic Evaluation  Intrinsic evaluation of word vectors is the evaluation of a set of word vectors generated by an embedding technique (such as Word 2 Vec or GloVe) on specific intermediate subtasks (such as analogy comple- tion). These subtasks are typically simple and fast to compute and thereby allow us to help understand the system used to generate the word vectors. An intrinsic evaluation should typically return to us a number that indicates the performance of those word vectors on the evaluation subtask.   Figure 1 : The left subsystem (red) being expensive to train is modified by substituting with a simpler subsystem (green) for intrinsic evaluation.  Motivation: Let us consider an example where our final goal is to create a question answering system which uses word vectors   ## word vectors ii:glove,evaluation and training 4  as inputs. One approach of doing so would be to train a machine learning system that:  1. Takes words as inputs  2. Converts them to word vectors  3. Uses word vectors as inputs for an elaborate machine learning system  4. Maps the output word vectors by this system back to natural language words  5.\n\nfilename: materials_formatted.md\nextension: .md\n\nAnother way to improve accuracy is to provide more than one example of the relationship. By using ten examples instead of one to form the relationship vector (we average the individual vectors together), we have observed improvement of accuracy of our best models by about 10% absolutely on the semantic-syntactic test.  It is also possible to apply the vector operations to solve different tasks. For example, we have observed good accuracy for selecting out-of-the-list words, by computing average vector for a list of words, and finding the most distant word vector. This is a popular type of problems in certain human intelligence tests. Clearly, there is still a lot of discoveries to be made using these techniques.  ## 6 Conclusion  In this paper we studied the quality of vector representations of words derived by various models on a collection of syntactic and semantic language tasks. We observed that it is possible to train high quality word vectors using very simple model architectures, compared to the popular neural network models (both feedforward and recurrent). Because of the much lower computational complexity, it is possible to compute very accurate high dimensional word vectors from a much larger data set. Using the DistBelief distributed framework, it should be possible to train the CBOW and Skip-gram models even on corpora with one trillion words, for basically unlimited size of the vocabulary. That is several orders of magnitude larger than the best previously published results for similar models.  An interesting task where the word vectors have recently been shown to significantly outperform the previous state of the art is the SemEval-2012 Task 2 [11]. The publicly available RNN vectors were used together with other techniques to achieve over 50% increase in Spearman\u2019s rank correlation over the previous best result [31]. The neural network based word vectors were previously applied to many other NLP tasks, for example sentiment analysis [12] and paraphrase detection [28]. It can be expected that these applications can benefit from the model architectures described in this paper.  Our ongoing work shows that the word vectors can be successfully applied to automatic extension of facts in Knowledge Bases, and also for verification of correctness of existing facts. Results from machine translation experiments also look very promising. In the future, it would be also interesting to compare our techniques to Latent Relational Analysis [30] and others.\n\nfilename: materials_formatted.md\nextension: .md\n\n# Evaluation methods for unsupervised word embeddings  ## Tobias Schnabel  ## Cornell University  ## Ithaca, NY, 14853  ## tbs49@cornell.edu  ## Igor Labutov  ## Cornell University  ## Ithaca, NY, 14853  ## iil4@cornell.edu  ## David Mimno,  ## Thorsten Joachims  ## Cornell University  ## mimno@cornell.edu,  ## tj@cs.cornell.edu  ## Abstract   We present a comprehensive study of eval- uation methods for unsupervised embed- ding techniques that obtain meaningful representations of words from text. Differ- ent evaluations result in different orderings of embedding methods, calling into ques- tion the common assumption that there is one single optimal vector representation. We present new evaluation techniques that directly compare embeddings with respect to specific queries. These methods re- duce bias, provide greater insight, and allow us to solicit data-driven relevance judgments rapidly and accurately through crowdsourcing.  ## 1 Introduction  Neural word embeddings represent meaning via geometry. A good embedding provides vector rep- resentations of words such that the relationship be- tween two vectors mirrors the linguistic relation- ship between the two words. Despite the growing interest in vector representations of semantic in- formation, there has been relatively little work on direct evaluations of these models. In this work, we explore several approaches to measuring the quality of neural word embeddings. In particu- lar, we perform a comprehensive analysis of eval- uation methods and introduce novel methods that can be implemented through crowdsourcing, pro- viding better insights into the relative strengths of different embeddings. Existing schemes fall into two major categories: extrinsic and intrinsic evaluation. In extrinsic eval- uation, we use word embeddings as input features to a downstream task and measure changes in per- formance metrics specific to that task. Examples include part-of-speech tagging and named-entity recognition (Pennington et al., 2014). Extrinsic   evaluation only provides one way to specify the goodness of an embedding, and it is not clear how it connects to other measures. Intrinsic evaluations directly test for syntactic or semantic relationships between words (Mikolov et al., 2013a; Baroni et al., 2014).\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat kind of evaluation methods are discussed for word vectors?[/INST] \nAnswer: The evaluation methods discussed for word vectors include intrinsic and extrinsic evaluation. Intrinsic evaluation involves evaluating a set of word vectors generated by an embedding technique on specific intermediate subtasks, such as analogy completion. These subtasks are typically simple and fast to compute and help understand the system used to generate the word vectors. Extrinsic evaluation, on the other hand, uses word embeddings as input features to a downstream task and measures changes in performance metrics specific to that task. Examples of such tasks include part-of-speech tagging and named-entity recognition."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nOn average, we get a MS-SSIM score of 44.3, on 50 samples from the validation set, which shows that our models don\u2019t merely learn to copy training images but generate high-quality images by adding synthesized details on the low resolution input image.  Image Transformer  Input  1D Local Attention \u03c4 = 0.9  \u03c4 = 0.8  \u03c4 = 1.0  \u03c4 = 0.8  2D Local Attention \u03c4 = 0.9  \u03c4 = 1.0  Original  Table 6. Images from our 1D and 2D local attention super-resolution models trained on CelebA, sampled with different temperatures. 2D local attention with \u03c4 = 0.9 scored highest in our human evaluation study.  CIFAR-10 We also trained a super-resolution model on the CIFAR-10 data set. Our model reached a negative log- likelihood of 2.76 using 1D local attention and 2.78 using 2D local attention on the test set. As seen in Figure 2, our model commonly generates plausible looking objects even though the input images seem to barely show any discernible structure beyond coarse shapes.  previously proposed (Mansimov et al., 2015), and tasks combining modalities such as language-driven editing of images.  Fundamentally, we aim to move beyond still images to video (Kalchbrenner et al., 2016) and towards applications in model-based reinforcement learning.  6. Conclusion  In this work we demonstrate that models based on self- attention can operate effectively on modalities other than text, and through local self-attention scale to signi\ufb01cantly larger structures than sentences. With fewer layers, its larger receptive \ufb01elds allow the Image Transformer to signi\ufb01cantly improve over the state of the art in unconditional, probabilis- tic image modeling of comparatively complex images from ImageNet as well as super-resolution.  We further hope to have provided additional evidence that even in the light of generative adversarial networks, likelihood-based models of images is very much a promising area for further research - as is using network architectures such as the Image Transformer in GANs.  References  Ba, Jimmy Lei, Kiros, Jamie Ryan, and Hinton, Geoffrey E. Layer normalization.\n\nfilename: materials_formatted.md\nextension: .md\n\nModel Type ResNet srez GAN PixelRecursive (Dahl et al., 2017)  1D local Image Transformer  2D local Image Transformer  \u03c4 n/a n/a 1.0 0.9 0.8 1.0 0.9 0.8 1.0 0.9 0.8  %Fooled 4.0 8.5 11.0 10.4 10.2 29.6 \u00b1 4.0 33.5 \u00b1 3.5 35.94 \u00b1 3.0 30.64 \u00b1 4 34 \u00b1 3.5 36.11 \u00b1 2.5  Turk where each worker is required to make a binary choice when shown one generated and one real image. Following the same procedure for the evaluation study as (Dahl et al., 2017), we show 50 pairs of images, selected randomly from the validation set, to 50 workers each. Each generated and original image is upscaled to 128 \u00d7 128 pixels using the Bilinear interpolation method. Each worker then has 1-2 seconds to make a choice between these two images. In our method, workers choose images from our model up to 36.1% of the time, a signi\ufb01cant improvement over pre- vious models. Sampling temperature of 0.8 and 2D local attention maximized perceptual quality as measured by this evaluation.  To measure how well the high resolution samples correspond to the low resolution input, we calculate Consistency, the L2 distance between the low resolution input and a bicubic downsampled version of the high resolution sample. We observe a Consistency score of 0.01 which is on par with the models in (Dahl et al., 2017).  We quantify that our models are more effective than exem- plar based Super Resolution techniques like Nearest Neigh- bors, which perform a naive look-up of the training data to \ufb01nd the high resolution output. We take a bicubic down- sampled version of our high resolution sample, \ufb01nd the nearest low resolution input image in the training data for that sample, and calculate the MS-SSIM score between the high resolution sample and the corresponding high reso- lution image in the training data.\n\nfilename: materials_formatted.md\nextension: .md\n\nThus turning the problem into a sequence modeling problem, the state of the art approaches apply recurrent or convolutional neural networks to predict each next pixel given all previously generated pixels (van den Oord et al., 2016a). Training recurrent neural networks to sequentially predict each pixel of even a small image is computationally very challenging. Thus, parallelizable models that use convolutional neural networks such as the PixelCNN have recently received much more attention, and have now surpassed the PixelRNN in quality (van den Oord et al., 2016b).  One disadvantage of CNNs compared to RNNs is their typically fairly limited receptive \ufb01eld. This can adversely affect their ability to model long-range phenomena common in images, such as symmetry and occlusion, especially with a small number of layers. Growing the receptive \ufb01eld has been shown to improve quality signi\ufb01cantly (Salimans et al.). Doing so, however, comes at a signi\ufb01cant cost in number              Image Transformer  of parameters and consequently computational performance and can make training such models more challenging.  In this work we show that self-attention (Cheng et al., 2016; Parikh et al., 2016; Vaswani et al., 2017) can achieve a better balance in the trade-off between the virtually unlimited receptive \ufb01eld of the necessarily sequential PixelRNN and the limited receptive \ufb01eld of the much more parallelizable PixelCNN and its various extensions.  We adopt similar factorizations of the joint pixel distribu- tion as previous work. Following recent work on model- ing text (Vaswani et al., 2017), however, we propose es- chewing recurrent and convolutional networks in favor of the Image Transformer, a model based entirely on a self- attention mechanism. The speci\ufb01c, locally restricted form of multi-head self-attention we propose can be interpreted as a sparsely parameterized form of gated convolution. By decoupling the size of the receptive \ufb01eld from the num- ber of parameters, this allows us to use signi\ufb01cantly larger receptive \ufb01elds than the PixelCNN.  Despite comparatively low resource requirements for train- ing, the Image Transformer attains a new state of the art in modeling images from the standard ImageNet data set, as measured by log-likelihood.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat is the primary goal of the Image Transformer model?[/INST] \nAnswer: The primary goal of the Image Transformer model is to operate effectively on modalities other than text, and through local self-attention, scale to significantly larger structures than sentences. It aims to improve over the state of the art in unconditional, probabilistic image modeling of complex images from ImageNet as well as super-resolution. The model also seeks to provide additional evidence that likelihood-based models of images are a promising area for further research."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\n# Learning Dense Representations of Phrases at Scale:  Learning Dense Representations of Phrases at Scale  Jinhyuk Lee1,2\u2217 Mujeen Sung1  Jaewoo Kang1 Danqi Chen2  Korea University1 Princeton University2 {jinhyuk_lee,mujeensung,kangj}@korea.ac.kr danqic@cs.princeton.edu  Abstract  Open-domain question answering can be refor- mulated as a phrase retrieval problem, without the need for processing documents on-demand during inference (Seo et al., 2019). However, current phrase retrieval models heavily depend on sparse representations and still underper- form retriever-reader approaches. In this work, we show for the \ufb01rst time that we can learn dense representations of phrases alone that achieve much stronger performance in open- domain QA. We present an effective method to learn phrase representations from the super- vision of reading comprehension tasks, cou- pled with novel negative sampling methods. We also propose a query-side \ufb01ne-tuning strat- egy, which can support transfer learning and reduce the discrepancy between training and inference. On \ufb01ve popular open-domain QA datasets, our model DensePhrases improves over previous phrase retrieval models by 15%\u2013 25% absolute accuracy and matches the perfor- mance of state-of-the-art retriever-reader mod- els. Our model is easy to parallelize due to pure dense representations and processes more than 10 questions per second on CPUs. Finally, we directly use our pre-indexed dense phrase representations for two slot \ufb01lling tasks, show- ing the promise of utilizing DensePhrases as a dense knowledge base for downstream tasks.1  1  Introduction  Open-domain question answering (QA) aims to provide answers to natural-language questions us- ing a large text corpus (Voorhees et al., 1999; Fer- rucci et al., 2010; Chen and Yih, 2020). While a dominating approach is a two-stage retriever-reader approach (Chen et al., 2017; Lee et al., 2019; Guu et al., 2020; Karpukhin et al., 2020), we focus on  a recent new paradigm solely based on phrase re- trieval (Seo et al., 2019; Lee et al., 2020).\n\nfilename: materials_formatted.md\nextension: .md\n\nAl- though it is still behind cross-attention models, the gap has been greatly reduced and serves as a strong starting point for the open-domain QA model.  Open-domain QA. Experimental results on open-domain QA are summarized in Table 3. With- out any sparse representations, DensePhrases out- performs previous phrase retrieval models by a large margin and achieves a 15%\u201325% absolute improvement on all datasets except SQuAD. Train- ing the model of Lee et al. (2020) on Cphrase = {NQ, SQuAD} only increases the result from 14.5% to 16.5% on NQ, demonstrating that it does not suf\ufb01ce to simply add more datasets for train- ing phrase representations. Our performance is also competitive with recent retriever-reader mod- els (Karpukhin et al., 2020), while running much faster during inference (Table 1).  Model  Retriever-reader  Cretr: (Pre-)Training  NQ WQ TREC TQA SQuAD  DrQA (Chen et al., 2017) BERT + BM25 (Lee et al., 2019) ORQA (Lee et al., 2019) REALMNews (Guu et al., 2020) DPR-multi (Karpukhin et al., 2020)  - - {Wiki.\n\nfilename: materials_formatted.md\nextension: .md\n\n10 Conclusion  In this study, we show that we can learn dense repre- sentations of phrases at the Wikipedia scale, which are readily retrievable for open-domain QA and other knowledge-intensive NLP tasks. We learn both phrase and question encoders from the supervi- sion of reading comprehension tasks and introduce two batch-negative techniques to better discrimi- nate phrases at scale. We also introduce query-side \ufb01ne-tuning that adapts our model to different types of queries. We achieve strong performance on \ufb01ve popular open-domain QA datasets, while reducing the storage footprint and improving latency signif- icantly. We also achieve strong performance on two slot \ufb01lling datasets using only a small number of training examples, showing the possibility of utilizing our DensePhrases as a knowledge base.  Acknowledgments  We thank Sewon Min, Hyunjae Kim, Gyuwan Kim, Jungsoo Park, Zexuan Zhong, Dan Fried- man, Chris Sciavolino for providing valuable com- ments and feedback. This research was supported by a grant of the Korea Health Technology R&D Project through the Korea Health Industry Develop- ment Institute (KHIDI), funded by the Ministry of Health & Welfare, Republic of Korea (grant num- ber: HR20C0021) and National Research Foun- dation of Korea (NRF-2020R1A2C3010638). It was also partly supported by the James Mi *91 Re- search Innovation Fund for Data Science and an Amazon Research Award.  Ethical Considerations  Our work builds on standard reading comprehen- sion datasets such as SQuAD to build phrase rep- resentations. SQuAD, in particular, is created from a small number of Wikipedia articles sampled from top-10,000 most popular articles (measured by PageRanks), hence some of our models trained only on SQuAD could be easily biased towards the small number of topics that SQuAD contains. We hope that excluding such datasets during training or inventing an alternative pre-training procedure for learning phrase representations could mitigate this problem. Although most of our efforts have been made to reduce the computational complexity of previous phrase retrieval models (further detailed in Appendices A and E), leveraging our phrase re- trieval model as a knowledge base will inevitably increase the minimum requirement for the addi- tional experiments.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat significant improvements do dense phrase representations provide in open-domain QA according to the experiments?[/INST] \nAnswer: Dense phrase representations provide significant improvements in open-domain question answering (QA) according to the experiments. The model, DensePhrases, outperforms previous phrase retrieval models by a large margin, achieving a 15%\u201325% absolute improvement on all datasets except SQuAD. It also matches the performance of state-of-the-art retriever-reader models while running much faster during inference. Furthermore, DensePhrases can be used as a knowledge base for downstream tasks, showing promise for further applications."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nHowever, this approach suffers from error propagation when incorrect docu- ments are retrieved and can be slow as it usually re- quires running an expensive reader model on every retrieved document or passage at inference time.  Phrase retrieval. Seo et al. (2019) introduce the phrase retrieval approach that encodes phrase and question representations independently and per- forms similarity search over the phrase representa- tions to \ufb01nd an answer. Their scoring function f is computed as follows:  \u2022 We \ufb01rst learn a high-quality phrase encoder and an (initial) question encoder from the supervision of reading comprehension tasks (\u00a74.1), as well as incorporating effective nega- tive sampling to better discriminate phrases at scale (\u00a74.2, \u00a74.3).  \u2022 Then, we \ufb01x the phrase encoder and encode all the phrases s \u2208 S(D) and store the phrase indexing of\ufb02ine to enable ef\ufb01cient search (\u00a75). \u2022 Finally, we introduce an additional strategy called query-side \ufb01ne-tuning (\u00a76) by further updating the question encoder.4 We \ufb01nd this step to be very effective, as it can reduce the discrepancy between training (the \ufb01rst step) and inference, as well as support transfer learning to new domains.  Before we present the approach in detail, we \ufb01rst  describe our base architecture below.  f (s | D, q) = Es(s, D)(cid:62)Eq(q),  (2)  3.2 Base Architecture  where Es and Eq denote the phrase encoder and the question encoder respectively. As Es(\u00b7) and Eq(\u00b7) representations are decomposable, it can support maximum inner product search (MIPS) and improve the ef\ufb01ciency of open-domain QA models. Previous approaches (Seo et al., 2019; Lee et al., 2020) leverage both dense and sparse vectors for phrase and question representations by taking their concatenation: Es(s, D) = [Esparse(s, D), Edense(s, D)].3 However, since the sparse vectors are dif\ufb01cult to parallelize with dense vectors, their method essentially conducts sparse and dense vector search separately.\n\nfilename: materials_formatted.md\nextension: .md\n\nPhrase retrieval highlights the use of phrase representa- tions and \ufb01nds answers purely based on the similar- ity search in the vector space of phrases.2 Without relying on an expensive reader model for process- ing text passages, it has demonstrated great runtime ef\ufb01ciency at inference time.  Despite great promise, it remains a formidable challenge to build vector representations for ev- ery single phrase in a large corpus. Since phrase representations are decomposed from question rep- resentations, they are inherently less expressive than cross-attention models (Devlin et al., 2019). Moreover, the approach requires retrieving answers correctly out of billions of phrases (e.g., 6 \u00d7 1010 phrases in English Wikipedia), making the scale of the learning problem dif\ufb01cult. Consequently, ex- isting approaches heavily rely on sparse represen- tations for locating relevant documents and para- graphs while still falling behind retriever-reader models (Seo et al., 2019; Lee et al., 2020).  In this work, we investigate whether we can build fully dense phrase representations at scale for open- domain QA. First, we aim to learn strong phrase representations from the supervision of reading comprehension tasks. We propose to use data aug- mentation and knowledge distillation to learn better phrase representations within a single passage. We then adopt negative sampling strategies such as in- batch negatives (Henderson et al., 2017; Karpukhin et al., 2020), to better discriminate the phrases at a larger scale. Here, we present a novel method called pre-batch negatives, which leverages preced- ing mini-batches as negative examples to compen- sate the need of large-batch training. Lastly, we present a query-side \ufb01ne-tuning strategy that dras-  \u2217Work partly done while visiting Princeton University. 1Our code is available at https://github.com/  princeton-nlp/DensePhrases.  2Following previous work (Seo et al., 2018), \u2018phrase\u2019 de- notes any contiguous segment of text up to L words (including single words), which is not necessarily a linguistic phrase.              Category  Model  Sparse?\n\nfilename: materials_formatted.md\nextension: .md\n\nCategory  Model  Sparse?  Storage (GB)  #Q/sec (GPU, CPU)  NQ SQuAD (Acc)  (Acc)  Retriever-Reader  Phrase Retrieval  DrQA (Chen et al., 2017) BERTSerini (Yang et al., 2019) ORQA (Lee et al., 2019) REALMNews (Guu et al., 2020) DPR-multi (Karpukhin et al., 2020)  DenSPI (Seo et al., 2019) DenSPI + Sparc (Lee et al., 2020) DensePhrases (Ours)  (cid:51) (cid:51) (cid:55) (cid:55) (cid:55)  (cid:51) (cid:51) (cid:55)  26 21 18 18 76  1,200 1,547 320  1.8, 0.6 2.0, 0.4 8.6, 1.2 8.4, 1.2 0.9, 0.04  2.9, 2.4 2.1, 1.7 20.6, 13.6  - - 33.3 40.4 41.5  8.1 14.5 40.9  29.8 38.6 20.2 - 24.1  36.2 40.7 38.0  Table 1: Retriever-reader and phrase retrieval approaches for open-domain QA. The retriever-reader approach retrieves a small number of relevant documents or passages from which the answers are extracted. The phrase retrieval approach retrieves an answer out of billions of phrase representations pre-indexed from the entire corpus. Appendix B provides detailed benchmark speci\ufb01cation. The accuracy is measured on the test sets in the open- domain setting. NQ: Natural Questions.  tically improves phrase retrieval performance and allows for transfer learning to new domains, with- out re-building billions of phrase representations. As a result, all these improvements lead to a much stronger phrase retrieval model, without the use of any sparse representations (Table 1).\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nHow does the document propose to overcome the limitations of current phrase retrieval models?[/INST] \nAnswer: The document proposes several strategies to overcome the limitations of current phrase retrieval models. Firstly, it suggests learning strong phrase representations from the supervision of reading comprehension tasks. This is achieved by using data augmentation and knowledge distillation to learn better phrase representations within a single passage. Secondly, the document proposes adopting negative sampling strategies, including a novel method called pre-batch negatives, which uses preceding mini-batches as negative examples to compensate for the need for large-batch training. Lastly, a query-side fine-tuning strategy is introduced that significantly improves phrase retrieval performance and enables transfer learning to new domains, without the need to rebuild billions of phrase representations."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\n4 ABLATION STUDIES & FURTHER ANALYSIS  4.1 NUMBER OF INSTRUCTION TUNING CLUSTERS  As the core question of our paper asks how instruction tuning improves a model\u2019s zero-shot perfor- mance on unseen tasks, in this \ufb01rst ablation we examine how performance is affected by the number of clusters and tasks used in instruction tuning. For this setup, we hold out NLI, closed-book QA, and commonsense reasoning as evaluation clusters, and use the seven remaining clusters for instruction tuning.3 We show results for one to seven instruction tuning clusters, where clusters are added in decreasing order of number of tasks per cluster.  Figure 6 shows these results. As expected, we observe that average performance across the three held-out clusters improves as we add additional clusters and tasks to instruction tuning (with the exception of the sentiment analysis cluster), con\ufb01rming the bene\ufb01ts of our proposed instruction tuning approach on zero-shot performance on novel tasks. It is further interesting to see that, for the seven clusters we test, the performance does not appear to saturate, implying that performance may further improve with even more clusters added to instruction tuning. Of note, this ablation does not allow us to draw conclusions about which instruction tuning cluster contributes the most to each evaluation cluster, although we see minimal added value from the sentiment analysis cluster.  Figure 6: Adding additional task clusters to instruction tuning improves zero-shot performance on held-out task clusters. The evaluation tasks are the following. Commonsense: CoPA, HellaSwag, PiQA, and StoryCloze. NLI: ANLI R1\u2013R3, QNLI, RTE, SNLI, and WNLI. Closed-book QA: ARC easy, ARC challenge, Natural Questions, and TriviaQA.  3We do not use the paraphrase or reading comprehension with commonsense clusters for instruction tuning  in this ablation because they are too similar to NLI and commmonsense reasoning, respectively.  6  Performance (%)  on held-out clusterClusters used for instruction tuning507090\u2013 AverageHeld-out clusters30# clusters: 1234567+ summarization+ translation+ read. comp.(# datasets): (11)(20)(26)(30)(34)(37)(39)+ sentiment+ data to text+ coreference+ conv.\n\nfilename: materials_formatted.md\nextension: .md\n\nQA\u2013 Commonsense NLI  Closed-book QA49.955.059.359.260.861.963.5Base LM: CommonsenseBase LM: NLIBase LM: Closed-book QAPublished as a conference paper at ICLR 2022  4.2 SCALING LAWS  As Brown et al. (2020) shows that zero and few-shot capabilities of language models sub- stantially improve for larger models, we next explore how the bene\ufb01ts of instruction tuning are affected by model scale. Using the same cluster split as in the previous ablation study, we evaluate the effect of instruction tuning on models of size 422M, 2B, 8B, 68B, and 137B parameters.  Figure 7 shows these results. We see that for the two models on the order of 100B pa- rameters, instruction tuning substantially im- proves performance on held-out tasks, as is expected given the prior results in our pa- per. The behavior on held-out tasks for the 8B and smaller models, however, is thought- provoking\u2014instruction tuning actually hurts performance on held-out tasks. One potential explana- tion for this result could be that for small-scale models, learning the \u223c40 tasks used during instruction tuning \ufb01lls the entire model capacity, causing these models to perform worse on new tasks. Under this potential explanation, for the larger scale models, instruction tuning \ufb01lls up some model capacity but also teaches these models how to follow instructions, allowing them to generalize to new tasks with the remaining capacity.  Figure 7: Whereas instruction tuning helps large models generalize to new tasks, for small models it actually hurts generalization to unseen tasks, poten- tially because all model capacity is used to learn the mixture of instruction tuning tasks.  4.3 ROLE OF INSTRUCTIONS  In a \ufb01nal ablation study, we explore the role of in- structions during \ufb01netuning, as one possibility is that performance gains come entirely from multi-task \ufb01ne- tuning and the model could perform just as well without instructions. We hence consider two \ufb01netuning setups without instructions.\n\nfilename: materials_formatted.md\nextension: .md\n\nFigure 11: Effect of datasets per task cluster and templates per dataset on performance on three held-out clusters: NLI, commonsense reasoning, and closed-book QA. Adding more datasets per task cluster substantially improves performance. Using more templates per dataset, however, only had a very small effect on performance, which disappeared when there were suf\ufb01cient dataset per task cluster.  B.2 ROLE OF INSTRUCTIONS DURING FINETUNING  The per-cluster results for the ablation study from \u00a74.3 are shown in Table 3.  B.3 FURTHER ANALYSIS: INSTRUCTION TUNING FACILITATES PROMPT TUNING  The per-dataset results for the analysis in \u00a74.5 are given in Table 4. As the above tasks are all classi\ufb01cation, further work in this direction might include tasks such as summarization or question answering, or try to \ufb01netune the model using the supervised datasets.  C DATA CONTAMINATION ANALYSIS  One reasonable concern is that since the pretraining corpus of FLAN has more than 2 trillion tokens, it is possible that examples from a given evaluation dataset may have already been seen verbatim by the model during pre-training, hence in\ufb02ating the performance of our purported zero-shot model. To this end, like GPT-3 (Brown et al., 2020), we perform post-hoc data contamination analysis to  24  Published as a conference paper at ICLR 2022  Finetuning prompt  Inference prompt  Zero-shot performance on unseen task cluster  NLI Read. Comp.  Closed- Book QA  Translation  Four-Task Average  Natural instructions (= FLAN)  Natural instructions 56.2  77.4  No template Task/dataset name Task/dataset name  Natural instructions 50.5 Natural instructions 52.8 60.2 Task/dataset name  58.2 63.0 64.9  56.6  25.5 44.8 40.8  30.7  15.0 25.9 21.9  55.2  37.3 46.6 47.0  Table 3: Ablation study result using models where instructions are removed from the \ufb01netuning process.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat key factors contribute to the success of instruction tuning according to the ablation studies?[/INST] \nAnswer: The success of instruction tuning is influenced by several key factors according to the ablation studies. One of these factors is the number of instruction tuning clusters used. The performance across held-out clusters improves as additional clusters and tasks are added to instruction tuning. However, the performance does not appear to saturate with the seven clusters tested, suggesting that performance may further improve with even more clusters added to instruction tuning. Another factor is the scale of the model. Instruction tuning substantially improves performance on held-out tasks for larger models, but it can actually hurt performance for smaller models. This is potentially because all model capacity is used to learn the mixture of instruction tuning tasks for smaller models. Lastly, the role of instructions during finetuning also contributes to the success of instruction tuning. The performance gains could come entirely from multi-task finetuning and the model could perform just as well without instructions."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nFigure 3 demonstrates how accuracy has been shown to improve with larger corpus.  Figure 3 : Here we see how performance Figure 4 demonstrates how other hyperparameters have been improves with data size. shown to affect the accuracies using GloVe.   ## word vectors ii:glove,evaluation and training 9   Figure 4 : We see how accuracies vary with vector dimension and context  ### 2. 5 Intrinsic Evaluation Example: Correlation Evaluation window size for GloVe  Another simple way to evaluate the quality of word vectors is by asking humans to assess the similarity between two words on a fixed scale (say 0 - 10 ) and then comparing this with the cosine similarity between the corresponding word vectors. This has been done on various datasets that contain human judgement survey data.   Model Size WS353 MC RG SCWS RW SVD 6B 35.3 35.1 42.5 38.3 25. SVD-S 6B 56.5 71.5 71.0 53.6 34. SVD-L 6B 65.7 72.7 75.1 56.5 37. CBOW 6B 57.2 65.6 68.2 57.0 32. SG 6B 62.8 65.2 69.7 58.1 37. GloVe 6B 65.8 72.7 77.8 53.9 38. SVD-L 42B 74.0 76.4 74.1 58.3 39. GloVe 42B 75.9 83.6 82.9 59.6 47. CBOW 100B 68.4 79.6 75.4 59.4 45.   Table 6 : Here we see the correlations be- tween of word vector similarities using different embedding techniques with different human judgment datasets  ### 2. 6 Further Reading: Dealing With Ambiguity  One might wonder how we handle the situation where we want to capture the same word with different vectors for its different uses in natural language. For instance, \"run\" is both a noun and a verb and is used and interpreted differently based on the context.\n\nfilename: materials_formatted.md\nextension: .md\n\nSVD-L 300 42B 38.4 58.2 49. GloVe 300 42B 81.9 69.3 75.   Table 5 : Here we compare the perfor- mance of different models under the use of different hyperparameters and datasets   ## word vectors ii:glove,evaluation and training 8   Inspecting the above table, we can make 3 primary observations:  - Performance is heavily dependent on the model used for word     embedding:     This is an expected result since different methods try embedding     words to vectors using fundamentally different properties (such as     co-occurrence count, singular vectors, etc.) Implementation Tip: A window size        of 8 around each center word typically        works well for GloVe embeddings   Figure 2 : Here we see how training time improves training performance and helps squeeze the last few perfor- mance.  - Performance increases with larger corpus sizes:     This happens because of the experience an embedding technique     gains with more examples it sees. For instance, an analogy com-     pletion example will produce incorrect results if it has not encoun-     tered the test words previously. - Performance is lower for extremely low dimensional word vec-     tors:     Lower dimensional word vectors are not able to capture the dif-     ferent meanings of the different words in the corpus. This can be     viewed as a high bias problem where our model complexity is too     low. For instance, let us consider the words \"king\", \"queen\", \"man\",     \"woman\". Intuitively, we would need to use two dimensions such     as \"gender\" and \"leadership\" to encode these into 2 -bit word vec-     tors. Any lower would fail to capture semantic differences between     the four words.        Extremely high dimensional vectors:        Intuitively, it would seem that these        vectors would capture noise in the        corpus that doesn\u2019t allow generaliza-        tion, i.e. resulting in high variance, but        Yin et al. show in On theDimen-        sionality ofWordEmbeddingthat        skip-gram and GloVe are robust to this        over-fitting.  Figure 3 demonstrates how accuracy has been shown to improve with larger corpus.\n\nfilename: materials_formatted.md\nextension: .md\n\nGloVe outperforms it while using a corpus less than half the size. Table 4 shows results on the NER task with the CRF-based model. The L-BFGS training termi- nates when no improvement has been achieved on the dev set for 25 iterations. Otherwise all config- urations are identical to those used by Wang and Manning (2013). The model labeledDiscreteis the baseline using a comprehensive set of discrete features that comes with the standard distribution of the Stanford NER model, but with no word vec- tor features. In addition to the HPCA and SVD models discussed previously, we also compare to the models of Huang et al. (2012) (HSMN) and Collobert and Weston (2008) (CW). We trained the CBOW model using the word2vectool^8. The GloVe model outperforms all other methods on all evaluation metrics, except for the CoNLL test set, on which the HPCA method does slightly better. We conclude that the GloVe vectors are useful in downstream NLP tasks, as was first (^8) We use the same parameters as above, except in this case we found 5 negative samples to work slightly better than 10.   Table 4: F1 score on NER task with 50d vectors. Discreteis the baseline without word vectors. We use publicly-available vectors for HPCA, HSMN, and CW. See text for details. Model Dev Test ACE MUC Discrete 91.0 85.4 77.4 73. SVD 90.8 85.7 77.3 73. SVD-S 91.0 85.5 77.6 74. SVD-L 90.5 84.8 73.6 71. HPCA 92.6 88.7 81.7 80. HSMN 90.5 85.7 78.7 74. CW 92.2 87.4 81.7 80. CBOW 93.1 88.2 82.2 81.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat are the main findings of the experiments conducted to evaluate the GloVe model's performance?[/INST] \nAnswer: The experiments conducted to evaluate the GloVe model's performance found that the accuracy of the model improves with larger corpus sizes. It was also observed that the performance is heavily dependent on the model used for word embedding, with different methods embedding words to vectors using fundamentally different properties. Furthermore, performance was found to be lower for extremely low dimensional word vectors, as they are not able to capture the different meanings of the words in the corpus. However, extremely high dimensional vectors were found to be robust to over-fitting. The GloVe model outperformed all other methods on all evaluation metrics in most cases."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nHowever, in datasets where the question askers already know the answer, i.e. SQuAD and Triv- iaQA, the retrieval problem resembles traditional IR. In this setting, a highly compressed 128- dimensional vector cannot match BM25\u2019s ability to precisely represent every word in the evidence. The notable drop between development and test accuracy for SQuAD is a re\ufb02ection of an artifact in the dataset\u2014its 100k questions are derived from only 536 documents. Therefore, good retrieval tar- gets are highly correlated between training exam- ples, violating the IID assumption, and making it unsuitable for learned retrieval. We strongly sug- gest that those who are interested in end-to-end open-domain QA models no longer train and eval- uate with SQuAD for this reason.  9 Analysis  9.1 Strongly supervised comparison  To verify that our BM25 baseline is indeed state of the art, we also provide direct comparisons with DrQA\u2019s setup, where systems have access to gold answer derivations from SQuAD (Rajpurkar et al., 2016). While many systems have been proposed following DrQA\u2019s original setting, we compare only to the original system and the best system that  35  30  25  20  s n o i t s e u Q  l a r u t a N  h c t a  M  t c a x E  ORQA BM25 + BERT  0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 ICT masking rate  Figure 3: Analysis: Performance on our open version of the Natural Questions dev set with various mask- ing rates for the ICT pre-training. Too much masking prevents the model from learning to exploit exact n- gram overlap. Too little masking makes language un- derstanding unnecessary.  we are aware of\u2014BERTserini (Yang et al., 2019). DrQA\u2019s reader is DocReader (Chen et al., 2017), and they use TF-IDF to retrieve the top k documents. They also include distant supervision based on TF-IDF retrieval.\n\nfilename: materials_formatted.md\nextension: .md\n\nThis presents a more realistic scenario for practical applications.  Current approaches require a blackbox informa- tion retrieval (IR) system to do much of the heavy lifting, even though it cannot be \ufb01ne-tuned on the downstream task. In the strongly supervised set- ting popularized by DrQA (Chen et al., 2017), they also assume a reading comprehension model trained on question-answer-evidence triples, such as SQuAD (Rajpurkar et al., 2016). The IR sys- tem is used at test time to generate evidence candi- dates in place of the gold evidence. In the weakly supervised setting, proposed by TriviaQA (Joshi  et al., 2017), SearchQA (Dunn et al., 2017), and Quasar (Dhingra et al., 2017), the dependency on strong supervision is removed by assuming that the IR system provides noisy gold evidence.  These approaches rely on the IR system to mas- sively reduce the search space and/or reduce spu- rious ambiguity. However, QA is fundamentally different from IR (Singh, 2012). Whereas IR is concerned with lexical and semantic matching, questions are by de\ufb01nition under-speci\ufb01ed and re- quire more language understanding, since users are explicitly looking for unknown information. Instead of being subject to the recall ceiling from blackbox IR systems, we should directly learn to retrieve using question-answering data.  In this work, we introduce the \ufb01rst Open- Retrieval Question Answering system (ORQA). ORQA learns to retrieve evidence from an open corpus, and is supervised only by question- answer string pairs. While recent work on im- proving evidence retrieval has made signi\ufb01cant progress (Wang et al., 2018; Kratzwald and Feuer- riegel, 2018; Lee et al., 2018; Das et al., 2019), they still only rerank a closed evidence set. The main challenge to fully end-to-end learning is that retrieval over the open corpus must be considered a latent variable that would be impractical to train from scratch. IR systems offer a reasonable but potentially suboptimal starting point.\n\nfilename: materials_formatted.md\nextension: .md\n\nAlthough ORQA successfully demonstrates that dense retrieval can outperform BM25, setting new state-of-the-art results on multiple open-domain  0 2 0 2  p e S 0 3  ] L C . s c [  3 v 6 0 9 4 0 . 4 0 0 2 : v i X r a              QA datasets, it also suffers from two weaknesses. First, ICT pretraining is computationally intensive and it is not completely clear that regular sentences are good surrogates of questions in the objective function. Second, because the context encoder is not \ufb01ne-tuned using pairs of questions and answers, the corresponding representations could be subop- timal.  In this paper, we address the question: can we train a better dense embedding model using only pairs of questions and passages (or answers), with- out additional pretraining? By leveraging the now standard BERT pretrained model (Devlin et al., 2019) and a dual-encoder architecture (Bromley et al., 1994), we focus on developing the right training scheme using a relatively small number of question and passage pairs. Through a series of careful ablation studies, our \ufb01nal solution is surprisingly simple: the embedding is optimized for maximizing inner products of the question and relevant passage vectors, with an objective compar- ing all pairs of questions and passages in a batch. Our Dense Passage Retriever (DPR) is exception- ally strong. It not only outperforms BM25 by a large margin (65.2% vs. 42.9% in Top-5 accuracy), but also results in a substantial improvement on the end-to-end QA accuracy compared to ORQA (41.5% vs. 33.3%) in the open Natural Questions setting (Lee et al., 2019; Kwiatkowski et al., 2019). Our contributions are twofold. First, we demon- strate that with the proper training setup, sim- ply \ufb01ne-tuning the question and passage encoders on existing question-passage pairs is suf\ufb01cient to greatly outperform BM25. Our empirical results also suggest that additional pretraining may not be needed. Second, we verify that, in the context of open-domain question answering, a higher retrieval precision indeed translates to a higher end-to-end QA accuracy.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nHow does the performance of ORQA compare to traditional IR systems like BM25 on datasets where questioners genuinely seek answers?[/INST] \nAnswer: ORQA has been shown to outperform traditional IR systems like BM25, setting new state-of-the-art results on multiple open-domain QA datasets. However, it's important to note that on datasets like SQuAD and TriviaQA, where the question askers already know the answer, a highly compressed 128-dimensional vector used by ORQA cannot match BM25\u2019s ability to precisely represent every word in the evidence. This is due to the fact that good retrieval targets are highly correlated between training examples, violating the IID assumption, and making it unsuitable for learned retrieval like ORQA."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nIR systems offer a reasonable but potentially suboptimal starting point.  The key insight of this work is that end-to- end learning is possible if we pre-train the re- triever with an unsupervised Inverse Cloze Task (ICT). In ICT, a sentence is treated as a pseudo- question, and its context is treated as pseudo- evidence. Given a pseudo-question, ICT requires selecting the corresponding pseudo-evidence out of the candidates in a batch. ICT pre-training provides a suf\ufb01ciently strong initialization such that ORQA, a joint retriever and reader model, can be \ufb01ne-tuned end-to-end by simply optimiz-              Task  Training  Evaluation  Evidence  Answer  Evidence  Answer  Example  Reading Comprehension Open-domain QA  Unsupervised QA Strongly Supervised QA Weakly Supervised QA Closed Retrieval QA Open Retrieval QA  given  none given  heuristic learned  span  none span  string string  given  string  SQuAD (Rajpurkar et al., 2016)  none heuristic  heuristic learned  string string  string string  GPT-2 (Radford et al., 2019) DrQA (Chen et al., 2017)  TriviaQA (Joshi et al., 2017) ORQA (this work)  Table 1: Comparison of assumptions made by related tasks, along with references to examples. Heuristic evidence refers to the typical strategy of considering only a closed set of evidence documents from a traditional IR system, which sets a strict upper-bound on task performance. In this work (ORQA), only question-answer string pairs are observed during training, and evidence retrieval is learned in a completely end-to-end manner.  ing the marginal log-likelihood of correct answers that were found.  We evaluate ORQA on open versions of \ufb01ve ex- isting QA datasets. On datasets where the question writers already know the answer\u2014SQuAD (Ra- jpurkar et al., 2016) and TriviaQA (Joshi et al., 2017)\u2014the retrieval problem resembles tradi- tional IR, and BM25 (Robertson et al., 2009) provides state-of-the-art retrieval.\n\nfilename: materials_formatted.md\nextension: .md\n\nICT instead requires predicting the inverse\u2014given a sentence, predict its context (see  Sretr (0, q)  BERTB(0)  [CLS]...Zebras have four gaits: walk, trot, canter and gallop. When chased, a zebra will zig-zag from ...[SEP] side to side...  BERTQ(q)  Sretr (1, q)  BERTB(1)  [CLS]They are generally slower than horses, but their great stamina helps them outrun predators.[SEP]  [CLS]...Gagarin was further selected for an elite training group known as the Sochi Six...[SEP]  Sretr (..., q)  BERTB(...)  ...  Figure 2: Example of the Inverse Cloze Task (ICT), used for retrieval pre-training. A random sentence (pseudo-query) and its context (pseudo evidence text) are derived from the text snippet: \u201c...Zebras have four gaits: walk, trot, canter and gallop. They are gener- ally slower than horses, but their great stamina helps them outrun predators. When chased, a zebra will zig- zag from side to side...\u201d The objective is to select the true context among candidates in the batch.  Figure 2). We use a discriminative objective that is analogous to downstream retrieval:  PICT(b|q) =  (cid:88)  exp(Sretr (b, q))  exp(Sretr (b(cid:48), q))  b(cid:48)\u2208BATCH  where q is a random sentence that is treated as a pseudo-question, b is the text surrounding q, and BATCH is the set of evidence blocks in the batch that are used as sampled negatives.  An important aspect of ICT is that it requires learning more than word matching features, since the pseudo-question is not present in the evi- dence. For example, the pseudo-question in Fig- ure 2 never explicitly mentions \u201cZebras\u201d, but the retriever must still be able to select the context that discusses Zebras. Being able to infer the seman- tics from under-speci\ufb01ed language is what sets QA apart from traditional IR.\n\nfilename: materials_formatted.md\nextension: .md\n\n1 1.94 4.80 7.59 9.87 12.6 18.9 25.6 39.7 0.11 0.28 2.19 4.18 6.61 11.0 27.3 67.2 0.00 0.05 0.00 0.17 0.24 0.30 0.42 0.44  35.6 39.0 45.2 44.1 50.0 49.2 52.7 53.7  30.5 41.2 43.1 46.5 55.1 54.3 53.5 59.1  30.5 40.4 42.8 40.6 48.4 51.9 53.5 65.2  Table H.1: Scores for every task, setting and model that we investigate in this paper.  63  Figure H.1: All results for all SuperGLUE tasks.  Figure H.2: Results for SAT task.  Figure H.3: All results for all Winograd tasks.  64  Figure H.4: All results for all Arithmetic tasks.  Figure H.5: All results for all Cloze and Completion tasks.  65  Figure H.6: All results for all Common Sense Reasoning tasks.  Figure H.7: All results for all QA tasks.  Figure H.8: All results for all Reading Comprehension tasks.  Figure H.9: All results for all ANLI rounds.  66  Figure H.10: All results for all Scramble tasks.  Figure H.11: All results for all Translation tasks.  67  References  [ADG+16] Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul, Brendan Shillingford, and Nando De Freitas. Learning to learn by gradient descent by gradient descent. In Advances in neural information processing systems, pages 3981\u20133989, 2016.  [AI19] WeChat AI. Tr-mt (ensemble), December 2019.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat is the Inverse Cloze Task (ICT) and how does it relate to ORQA?[/INST] \nAnswer: The Inverse Cloze Task (ICT) is a method used in the pre-training of the retriever in end-to-end learning systems. In ICT, a sentence is treated as a pseudo-question and its context as pseudo-evidence. The task requires selecting the corresponding pseudo-evidence from a batch of candidates given a pseudo-question. This pre-training provides a strong initialization for ORQA, a joint retriever and reader model. ORQA can then be fine-tuned end-to-end by optimizing the marginal log-likelihood of correct answers. In the context of ORQA, the retriever must be able to infer semantics from under-specified language, which is a skill that sets QA apart from traditional IR."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nAlthough such performance improvements can be very useful in practice, they teach us little about the means to progress toward the broader goals of natural language understanding and the elusive goals of Artificial Intelligence. In this contribution, we try to excel onmultiple benchmarkswhileavoiding task-specific engi- neering. Instead we use asingle learning systemable to discover adequate internal representations. In fact we view the benchmarks as indirect measurements of the relevanceof the internal represen- tations discovered by the learning procedure, and we posit that these intermediate representations are more general than any of the benchmarks. Our desire to avoid task-specific engineered features prevented us from using a large body of linguistic knowledge. Instead wereach good performance levels in most of the tasks by transferring intermediate representations discovered on large unlabeled data sets. We call this approach \u201calmost from scratch\u201d to emphasize the reduced (but still important) reliance on a priori NLP knowledge. The paper is organized as follows. Section 2 describes the benchmark tasks of interest. Sec- tion 3 describes the unified model and reports benchmark results obtained with supervised training. Section 4 leverages large unlabeled data sets (\u223c852 million words) to train the model on a language modeling task. Performance improvements are then demonstrated by transferring the unsupervised internal representations into the supervised benchmark models. Section 5 investigates multitask supervised training. Section 6 then evaluates how much further improvementcan be achieved by incorporating standard NLP task-specific engineering into our systems. Drifting away from our ini- tial goals gives us the opportunity to construct an all-purpose tagger thatis simultaneously accurate, practical, and fast. We then conclude with a short discussion section.  #### 2. The Benchmark Tasks  In this section, we briefly introduce four standard NLP tasks on which we will benchmark our architectures within this paper: Part-Of-Speech tagging (POS), chunking (CHUNK), Named Entity Recognition (NER) and Semantic Role Labeling (SRL). For each of them, we consider a standard experimental setup and give an overview of state-of-the-art systems onthis setup. The experimental setups are summarized in Table 1, while state-of-the-art systems are reported in Table 2.\n\nfilename: materials_formatted.md\nextension: .md\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.3 Energy Usage .  .  .  .  .  .  .  .  .  .  .  .  .  .  7 Related Work  8 Conclusion  A Details of Common Crawl Filtering  B Details of Model Training  C Details of Test Set Contamination Studies  D Total Compute Used to Train Language Models  E Human Quality Assessment of Synthetic News Articles  F Additional Samples from GPT-3  G Details of Task Phrasing and Speci\ufb01cations  H Results on All Tasks for All Model Sizes  2  3  6 8 8 9 10  10 11 13 14 16 17 18 18 20 21  29  33  34 35 36 39  39  40  43  43  43  46  46  48  50  63  1  Introduction  Recent years have featured a trend towards pre-trained language representations in NLP systems, applied in increasingly \ufb02exible and task-agnostic ways for downstream transfer.\n\nfilename: materials_formatted.md\nextension: .md\n\nRTE Recognizing Textual Entailment is a bi- nary entailment task similar to MNLI, but with much less training data (Bentivogli et al., 2009).14  WNLI Winograd NLI is a small natural lan- guage inference dataset (Levesque et al., 2011). The GLUE webpage notes that there are issues with the construction of this dataset, 15 and every trained system that\u2019s been submitted to GLUE has performed worse than the 65.1 baseline accuracy of predicting the majority class. We therefore ex- clude this set to be fair to OpenAI GPT. For our GLUE submission, we always predicted the ma-  14Note that we only report single-task \ufb01ne-tuning results in this paper. A multitask \ufb01ne-tuning approach could poten- tially push the performance even further. For example, we did observe substantial improvements on RTE from multi- task training with MNLI.  15https://gluebenchmark.com/faq  BERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphBERTE[CLS]E1 E2 ENCT1 T2 TNSingle Sentence ......BERTTok 1 Tok 2 Tok N...[CLS]E[CLS]E1 E2 ENCT1 T2 TNSingle Sentence B-PEROO......E[CLS]E1 E[SEP]Class Label...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019Start/End SpanClass LabelBERTTok 1 Tok 2 Tok N...[CLS]Tok 1[CLS][CLS]Tok 1 [SEP]...Tok NTok 1...TokMSentence 1...Sentence 2Note that the purpose of the masking strategies is to reduce the mismatch between pre-training and \ufb01ne-tuning, as the [MASK] symbol never ap- pears during the \ufb01ne-tuning stage. We report the Dev results for both MNLI and NER.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhich NLP tasks does the paper focus on for benchmarking the proposed architecture?[/INST] \nAnswer: The paper focuses on benchmarking the proposed architecture on four standard NLP tasks: Part-Of-Speech tagging (POS), chunking (CHUNK), Named Entity Recognition (NER), and Semantic Role Labeling (SRL)."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\n4 Words as Distributional Vectors: Context as Meaning  An important idea in linguistics is that words (or expressions) that can be used in similar ways are likely to have related meanings (Firth, 1957; consider our day of the week example above). In a large corpus, we can collect information about the ways a word type w is used, for example, by counting the number of times it appears near every other word type. When we begin looking at the full distribution of contexts (nearby words or sequences of words) in a corpus where w is found, we are taking a distributional view of word meaning.  One highly successful approach to automatically deriving features based on this idea is clustering; for example, the Brown et al. (1992) clustering algorithm automatically organized words into clusters based on the contexts they appear in, in a corpus. Words that tended to occur in the same neighboring contexts (other words) were grouped together into a cluster. These clusters could then be merged into larger clusters. The resulting hierarchy, while by no means identical to the expert-crafted data structure in WordNet, was surprisingly interpretable and useful (an example is shown in Figure 1). It also had the advantage that it could be rebuilt using any given corpus, and every word observed would be included. Hence suitable word clusters could be built separately for news text, or biomedical articles, or tweets.  Another line of approaches started by creating word vectors in which each dimension corresponded to the frequency the word type occurred in some context (Deerwester et al., 1990). For instance, one dimension might correspond to the and contain the number of times the word occurred within a small window of a token the. Contextual patterns on the left or the right, and of varying distances and lengths, might be included. The result is a vector perhaps many times longer than the size of the vocabulary, in which each dimension contains a tiny bit of information that may or may not be useful. An example is shown in Figure 2. Using methods from linear algebra, aptly named dimensionality reduction, these vectors could be compressed into shorter vectors in which redundancies across dimensions were collapsed.  These reduced-dimensionality vectors had several advantages. First, the dimensionality could be chosen by the NLP programmer to suit the needs of the program.\n\nfilename: materials_formatted.md\nextension: .md\n\nAnother failure is a tradeoff between dimension- ality and utility of the embedding\u2014it takes a very high-dimensional vector (think much larger than the vocabulary size) to represent all of these categories, and modern neural methods that tend to operate on dense vectors do not behave well with such vectors. Finally, a continual theme we\u2019ll see in this course is that human ideas of what    deep learning 6  the right representations should be for text tend to underperform methods that allow data to determine more aspects\u2014at least when one has a lot of data to learn from.  ## 3 Distributional semantics and Word 2 vec  A promise of deep learning is to learn rich representations of com- plex objects from data. Increasingly relevant in NLP is the idea that we canunsupervisedlylearn rich representations from data. Unsuper- vised (or lately, \u201cself-supervised\u201d) learning takes data and attempts to learn learn properties of the elements of that data, often by taking part of the data (maybe a word in a sentence) and attempting to pre- dict other parts of the data (other words) with it. In language, this idea was captured well years ago by Firth [Firth, 1957 ], who famously said   You shall know a word by the company it keeps.  At a high level, you can think of the distribution of words that show up around the wordteaas a way to define the meaning that word. So,teashows up arounddrank, the, pot, kettle, bag, delicious, oolong, hot, steam,.. ., It should become clear that words similar totea(like coffee) will have similar distributions of surrounding words. While simple, this is one of the most influential and successful ideas in all of modern NLP , and analogues of it have taken hold in myriad learning-related fields. The distributional hypothsis : the meaning of a word can be derived from the distribution of contexts in which it appears.  That\u2019s the high level. But as always, the details matter. What does it mean for a word to be near another word? (Right next to it? Two away? In the same document?) How does one represent this encoding, and learn it? Let\u2019s go through some options.  ### 3.\n\nfilename: materials_formatted.md\nextension: .md\n\n2012. Distributional semantics in technicolor. InProceedings of the 50th Annual Meeting of the Association for Computational Lin- guistics (Volume 1: Long Papers), pages 136\u2013145, Jeju Island, Korea, July. Association for Computa- tional Linguistics.  John A Bullinaria and Joseph P Levy. 2007. Extracting semantic representations from word co-occurrence statistics: a computational study.Behavior Research Methods, 39(3):510\u2013526.  John A Bullinaria and Joseph P Levy. 2012. Extracting semantic representations from word co-occurrence statistics: Stop-lists, stemming, and SVD.Behavior Research Methods, 44(3):890\u2013907.  John Caron. 2001. Experiments with LSA scor- ing: optimal rank and basis. InProceedings of the SIAM Computational Information Retrieval Work- shop, pages 157\u2013169.   Kenneth Ward Church and Patrick Hanks. 1990. Word association norms, mutual information, and lexicog- raphy.Computational Linguistics, 16(1):22\u201329.   Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. InPro- ceedings of the 25th International Conference on Machine Learning, pages 160\u2013167.   Scott C. Deerwester, Susan T. Dumais, Thomas K. Lan- dauer, George W. Furnas, and Richard A. Harshman.  1990. Indexing by latent semantic analysis.JASIS, 41(6):391\u2013407.   C Eckart and G Young. 1936. The approximation of one matrix by another of lower rank. Psychome- trika, 1:211\u2013218.   Roi Reichart Felix Hill and Anna Korhonen. 2014. Simlex-999: Evaluating semantic models with (genuine) similarity estimation. arXiv preprint arXiv:1408.3456.   Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and Silvia Bernardini. 2008.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat does the 'distributional hypothesis' suggest about word meanings?[/INST] \nAnswer: The 'distributional hypothesis' suggests that the meaning of a word can be derived from the distribution of contexts in which it appears. This means that words that show up in similar contexts tend to have related meanings. For instance, words that often appear near each other in a text corpus are likely to have similar meanings."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\n[Microsoft Bing]   (Also see OpenAI\u2019s ChatGPT, Google\u2019s Bard, Anthropic\u2019s Claude)  10   #### Language models as multitask assistants?  How do we get from _this_  #### McMaster University is located in  to _this_?  11   #### SEP 775 Lecture Plan: From Language Models to Assistants  12  1. Zero-Shot (ZS) and Few-Shot (FS) In-Context Learning 2. Instruction finetuning 3. Reinforcement Learning from Human Feedback (RLHF) 4. What\u2019s next?   #### SEP 775 Lecture Plan: From Language Models to Assistants  13  1. Zero-Shot (ZS) and Few-Shot (FS) In-Context Learning 2. Instruction finetuning 3. Reinforcement Learning from Human Feedback (RLHF) 4. What\u2019s next?    Let\u2019s revisit the Generative Pretrained Transformer (GPT) models from OpenAI as an example:  #### Emergent abilities of large language models: GPT (2018)   GPT (117M parameters; Radford et al., 2018 )  - Transformer decoder with 12 layers. - Trained on BooksCorpus: over 7000 unique books (4.6GB text).   Showed that language modeling at scale can be an effective pretraining technique for downstream tasks like natural language inference.\n\nfilename: materials_formatted.md\nextension: .md\n\nOur approach of stuf\ufb01ng the model\u2019s context with previous examples is most structurally similar to RL2 and also resembles [HYC01], in that an inner loop of adaptation takes place through computation in the model\u2019s activations across timesteps, without updating the weights, while an outer loop (in this case just language model pre-training) updates the weights, and implicitly learns the ability to adapt to or at least recognize tasks de\ufb01ned at inference-time. Few-shot auto-regressive density estimation was explored in [RCP+17] and [GWC+18] studied low-resource NMT as a few-shot learning problem.  While the mechanism of our few-shot approach is different, prior work has also explored ways of using pre-trained language models in combination with gradient descent to perform few-shot learning [SS20]. Another sub-\ufb01eld with similar goals is semi-supervised learning where approaches such as UDA [XDH+19] also explore methods of \ufb01ne-tuning when very little labeled data is available.  Giving multi-task models instructions in natural language was \ufb01rst formalized in a supervised setting with [MKXS18] and utilized for some tasks (such as summarizing) in a language model with [RWC+19]. The notion of presenting tasks in natural language was also explored in the text-to-text transformer [RSR+19], although there it was applied for multi-task \ufb01ne-tuning rather than for in-context learning without weight updates.  Another approach to increasing generality and transfer-learning capability in language models is multi-task learning [Car97], which \ufb01ne-tunes on a mixture of downstream tasks together, rather than separately updating the weights for each one. If successful multi-task learning could allow a single model to be used for many tasks without updating the weights (similar to our in-context learning approach), or alternatively could improve sample ef\ufb01ciency when updating the weights for a new task. Multi-task learning has shown some promising initial results [LGH+15, LSP+18] and multi-stage \ufb01ne-tuning has recently become a standardized part of SOTA results on some datasets [PFB18] and pushed the boundaries on certain tasks [KKS+20], but is still limited by the need to manually curate collections of datasets and set up training curricula.\n\nfilename: materials_formatted.md\nextension: .md\n\nBy contrast pre-training at large enough scale appears to offer a \u201cnatural\u201d broad distribution of tasks implicitly contained in predicting the text itself. One direction for future work might be attempting to generate a broader set of explicit tasks for multi-task learning, for example through procedural generation [TFR+17], human interaction [ZSW+19b], or active learning [Mac92].  Algorithmic innovation in language models over the last two years has been enormous, including denoising-based bidirectionality [DCLT18], pre\ufb01xLM [DL15] and encoder-decoder architectures [LLG+19, RSR+19], random permu- tations during training [YDY+19], architectures that improve the ef\ufb01ciency of sampling [DYY+19], improvements in data and training procedures [LOG+19], and ef\ufb01ciency increases in the embedding parameters [LCG+19]. Many of these techniques provide signi\ufb01cant gains on downstream tasks. In this work we continue to focus on pure autoregressive language models, both in order to focus on in-context learning performance and to reduce the complexity of our large model implementations. However, it is very likely that incorporating these algorithmic advances could improve GPT-3\u2019s performance on downstream tasks, especially in the \ufb01ne-tuning setting, and combining GPT-3\u2019s scale with these algorithmic techniques is a promising direction for future work.  8 Conclusion  We presented a 175 billion parameter language model which shows strong performance on many NLP tasks and benchmarks in the zero-shot, one-shot, and few-shot settings, in some cases nearly matching the performance of  40  state-of-the-art \ufb01ne-tuned systems, as well as generating high-quality samples and strong qualitative performance at tasks de\ufb01ned on-the-\ufb02y. We documented roughly predictable trends of scaling in performance without using \ufb01ne-tuning. We also discussed the social impacts of this class of model. Despite many limitations and weaknesses, these results suggest that very large language models may be an important ingredient in the development of adaptable, general language systems.  Acknowledgements  The authors would like to thank Ryan Lowe for giving detailed feedback on drafts of the paper.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nHow can language models become multitask assistants?[/INST] \nAnswer: Language models can become multitask assistants through a combination of methods. Firstly, they can use Zero-Shot (ZS) and Few-Shot (FS) In-Context Learning, which involves the model learning from a few examples without prior specific training. Secondly, they can use Instruction Finetuning, which involves adjusting the model's parameters based on specific instructions. Thirdly, they can use Reinforcement Learning from Human Feedback (RLHF), which involves the model learning from feedback provided by humans. Lastly, there's a focus on what's next, which could involve incorporating new techniques and methods to improve the model's performance."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nA trigram model is less surprised than a unigram model because it has a better idea of what words might come next, and so it assigns them a higher probability. And the higher the probability, the lower the perplexity (since as Eq. 3.15 showed, perplexity is related inversely to the likelihood of the test sequence according to the model). So a lower perplexity can tell us that a language model is a better predictor of the words in the test set. Note that in computing perplexities, the n-gram modelPmust be constructed without any knowledge of the test set or any prior knowledge of the vocabulary of the test set. Any kind of knowledge of the test set can cause the perplexity to be artificially low. The perplexity of two language models is only comparable if they use identical vocabularies. An (intrinsic) improvement in perplexity does not guarantee an (extrinsic) im- provement in the performance of a language processing task like speech recognition   10 CHAPTER 3 \u2022 N-GRAMLANGUAGEMODELS   or machine translation. Nonetheless, because perplexity often correlates with such improvements, it is commonly used as a quick check on an algorithm. But a model\u2019s improvement in perplexity should always be confirmed by an end-to-end evaluation of a real task before concluding the evaluation of the model.  #### 3.3 Sampling sentences from a language model   One important way to visualize what kind of knowledge a language model embodies sampling is to sample from it.Samplingfrom a distribution means to choose random points according to their likelihood. Thus sampling from a language model\u2014which rep- resents a distribution over sentences\u2014means to generate some sentences, choosing each sentence according to its likelihood as defined by the model. Thus we are more likely to generate sentences that the model thinks have a high probability and less likely to generate sentences that the model thinks have a low probability. This technique of visualizing a language model by sampling was first suggested very early on by Shannon (1951) and Miller and Selfridge (1950). It\u2019s simplest to visualize how this works for the unigram case. Imagine all the words of the English language covering the probability space between 0 and 1, each word covering an interval proportional to its frequency. Fig. 3.3 shows a visualization, using a unigram LM computed from the text of this book.\n\nfilename: materials_formatted.md\nextension: .md\n\nHere\u2019s the perplexity ofWwith a unigram language model (just the geometric mean of the unigram probabilities):   perplexity(W) = N   \u221a \u221a \u221a \u221a \u220fN   i= 1   1 P(wi)   (3.16)   The perplexity ofWcomputed with a bigram language model is still a geometric mean, but now of the bigram probabilities:   perplexity(W) = N   \u221a\u221a \u221a \u221a \u220fN   i= 1   1 P(wi|wi\u2212 1 )   (3.17)   Note that because of the inverse in Eq. 3.15, the higher the conditional probabil- ity of the word sequence, the lower the perplexity. Thus, minimizing perplexity is equivalent to maximizing the test set probability according to the language model. What we generally use for word sequence in Eq. 3.15 or Eq. 3.17 is the entire se- quence of words in some test set. Since this sequence will cross many sentence boundaries, we need to include the begin- and end-sentence markers<s>and</s> in the probability computation. We also need to include the end-of-sentence marker </s>(but not the beginning-of-sentence marker<s>) in the total count of word to- kensN. There is another way to think about perplexity: as theweighted average branch- ing factorof a language. The branching factor of a language is the number of possi- ble next words that can follow any word. Consider the task of recognizing the digits    3.2 \u2022 EVALUATINGLANGUAGEMODELS 9  in English (zero, one, two,..., nine), given that (both in some training set and in some test set) each of the 10 digits occurs with equal probabilityP= 101. The perplexity of this mini-language is in fact 10. To see that, imagine a test string of digits of length N, and assume that in the training set all the digits occurred with equal probability. By Eq.\n\nfilename: materials_formatted.md\nextension: .md\n\nBy Eq. 3.15, the perplexity will be   perplexity(W) =P(w 1 w 2 ...wN)\u2212 N^1   = (   1 10   N )\u2212 N^1   =   1 10   \u2212 1   = 10 (3.18)  But suppose that the number zero is really frequent and occurs far more often than other numbers. Let\u2019s say that 0 occur 91 times in the training set, and each of the other digits occurred 1 time each. Now we see the following test set: 0 0 0 0 0 3 0 0 0  0. We should expect the perplexity of this test set to be lower since most of the time the next number will be zero, which is very predictable, i.e. has a high probability. Thus, although the branching factor is still 10, the perplexity orweightedbranching factor is smaller. We leave this exact calculation as exercise 3.12.     We see in Section 3.8 that perplexity is also closely related to the information- theoretic notion of entropy.     We mentioned above that perplexity is a function of both the text and the lan- guage model: given a textW, different language models will have different perplex- ities. Because of this, perplexity can be used to compare different n-gram models. Let\u2019s look at an example, in which we trained unigram, bigram, and trigram gram- mars on 38 million words (including start-of-sentence tokens) from theWall Street Journal, using a 19,979 word vocabulary. We then computed the perplexity of each of these models on a test set of 1.5 million words, using Eq. 3.16 for unigrams, Eq. 3.17 for bigrams, and the corresponding equation for trigrams. The table below shows the perplexity of a 1.5 million word WSJ test set according to each of these grammars.  Unigram Bigram Trigram Perplexity 962 170 109 As we see above, the more information the n-gram gives us about the word sequence, the higher the probability the n-gram will assign to the string.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat does the paper mention as a simple example to explain the concept of perplexity in language models?[/INST] \nAnswer: The paper uses the task of recognizing digits in English as a simple example to explain the concept of perplexity in language models. In this example, each of the 10 digits (zero to nine) occurs with equal probability. The perplexity of this mini-language is 10. The paper further explains that if the number zero occurs far more frequently than other numbers, the perplexity of the test set would be lower since most of the time the next number will be zero, which is very predictable and has a high probability."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nAdding MLP in \u03b2 does not seem to help, yielding slightly worse result than \u03b2 without MLP.  13     \n\n# On the difficulty of training Recurrent Neural Networks:  # On the difficulty of training Recurrent Neural Networks   Razvan Pascanu pascanur@iro.umontreal.ca Universite de Montreal   Tomas Mikolov t.mikolov@gmail.com Brno University   Yoshua Bengio yoshua.bengio@umontreal.ca Universite de Montreal  ## Abstract   There are two widely known issues with prop- erly training Recurrent Neural Networks, the vanishing and theexplodinggradient prob- lems detailed in Bengio et al. (1994). In this paper we attempt to improve the under- standing of the underlying issues by explor- ing these problems from an analytical, a geo- metric and a dynamical systems perspective. Our analysis is used to justify a simple yet ef- fective solution. We propose a gradient norm clipping strategy to deal with exploding gra- dients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.  ## 1. Introduction   A recurrent neural network (RNN), e.g. Fig. 1, is a neural network model proposed in the 80\u2019s (Rumelhart et al., 1986; Elman, 1990; Werbos, 1988) for modeling time series. The structure of the network is similar to that of a standard multilayer perceptron, with the dis- tinction that we allow connections among hidden units associated with a time delay. Through these connec- tions the model can retain information about the past inputs, enabling it to discover temporal correlations between events that are possibly far away from each other in the data (a crucial property for proper learn- ing of time series). While in principle the recurrent network is a simple and powerful model, in practice, it is unfortunately hard to train properly. Among the main reasons why this model is so unwieldy are thevanishing gradient  ### ut xt Et   Figure 1.Schematic of a recurrent neural network. The recurrent connections in the hidden layer allow information to persist from one input to another.   andexploding gradientproblems described in Bengio et al.(1994).   1.1.\n\nfilename: materials_formatted.md\nextension: .md\n\n### 2 Recurrent Neural Networks (RNN)  Unlike the conventional translation models, where only a finite win- dow of previous words would be considered for conditioning the language model, Recurrent Neural Networks (RNN) are capable of conditioning the model onallprevious words in the corpus.   xt\u2212 1 xt xt +   ht\u2212 1 ht ht + W\" W\"   yt\u2212 1 yt yt+ 1   Figure 3 : A Recurrent Neural Network (RNN). Three time-steps are shown.  Figure 3 introduces the RNN architecture where each vertical rect- angular box is a hidden layer at a time-step,t. Each such layer holds a number of neurons, each of which performs a linear matrix oper- ation on its inputs followed by a non-linear operation (e.g. tanh()). At each time-step, there are two inputs to the hidden layer: the out- put of the previous layerht\u2212 1 , and the input at that timestepxt. The former input is multiplied by a weight matrixW(hh)and the latter by a weight matrixW(hx)to produce output featuresht, which are multiplied with a weight matrixW(S)and run through a softmax over the vocabulary to obtain a prediction outputy\u02c6of the next word (Equations 5 and 6 ). The inputs and outputs of each single neuron are illustrated in Figure 4.   ht= \u03c3 (W(hh)ht\u2212 1 +W(hx)x[t]) ( 5 )   y\u02c6t=so f tmax(W(S)ht) ( 6 )   ### language models,rnn,gru and lstm 4  What is interesting here is that the same weightsW(hh)andW(hx)are applied repeatedly at each timestep. Thus, the number of parameters the model has to learn is less, and most importantly, is independent of the length of the input sequence - thus defeating the curse of di- mensionality!   Figure 4 : The inputs and outputs to a neuron of a RNN  Below are the details associated with each parameter in the net- work:  - x 1 , ...,xt\u2212 1 ,xt,xt+ 1 , ...xT: the word vectors corresponding to a cor-     pus with T words.\n\nfilename: materials_formatted.md\nextension: .md\n\nThe first technique is that instead of initializingW(hh)ran- domly, start off from an identity matrix initialization. The second technique is to use the Rectified Linear Units (ReLU) in- stead of the sigmoid function. The derivative for the ReLU is either 0 or   ## language models,rnn,gru and lstm 9  1. This way, gradients would flow through the neurons whose deriva- tive is 1 without getting attenuated while propagating back through time-steps.  #### 2. 5 Deep Bidirectional RNNs  So far, we have focused on RNNs that condition on past words to predict the next word in the sequence. It is possible to make predic- tions based on future words by having the RNN model read through the corpus backwards. Irsoy et al. shows a bi-directional deep neu- ral network; at each time-step,t, this network maintains two hidden layers, one for the left-to-right propagation and another for the right- to-left propagation. To maintain two hidden layers at any time, this network consumes twice as much memory space for its weight and bias parameters. The final classification result,y\u02c6t, is generated through combining the score results produced by both RNN hidden layers. Fig- ure 8 shows the bi-directional network architecture, and Equations 17 and 18 show the mathematical formulation behind setting up the bi- directional RNN hidden layer. The only difference between these two relationships is in the direction of recursing through the corpus. Equa- tion 19 shows the classification relationship used for predicting the next word via summarizing past and future word representations.  ## Bidirectionality   RNNs can also be multi-layered. Figure 9 shows a multi-layer bi- directional RNN where each lower layer feeds the next layer. As shown in this figure, in this network architecture, at time-stepteach interme- diate neuron receives one set of parameters from the previous time- step (in the same RNN layer), and two sets of parameters from the previous RNN hidden layer; one input comes from the left-to-right RNN and the other from the right-to-left RNN.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat are the two widely known issues with training Recurrent Neural Networks (RNNs)?[/INST] \nAnswer: The two widely known issues with training Recurrent Neural Networks (RNNs) are the vanishing and the exploding gradient problems."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nBecause most of the weights are randomly sam- pled, is not clear what size of models one would need to solve complex real world tasks. We would make a final note about the approach pro- posed by Tomas Mikolov in his PhD thesis (Mikolov, 2012)(and implicitly used in the state of the art re- sults on language modelling (Mikolovet al., 2011)). It involves clipping the gradient\u2019s temporal compo- nents element-wise (clipping an entry when it exceeds in absolute value a fixed threshold). Clipping has been shown to do well in practice and it forms the backbone of our approach.   3.2. Scaling down the gradients As suggested in section 2.3, one simple mechanism to deal with a sudden increase in the norm of the gradi- ents is to rescale them whenever they go over a thresh- old (see algorithm 1).   Algorithm 1Pseudo-code for norm clipping the gra- dients whenever they explode g\u02c6\u2190\u2202\u2202\u03b8E if \u2016\u02c6g\u2016\u2265thresholdthen g\u02c6\u2190threshold\u2016\u02c6g\u2016 \u02c6g end if   This algorithm is very similar to the one proposed by Tomas Mikolov and we only diverged from the original proposal in an attempt to provide a better theoretical foundation (ensuring that we always move in a de- scent direction with respect to the current mini-batch), though in practice both variants behave similarly. The proposed clipping is simple to implement and computationally efficient, but it does however in- troduce an additional hyper-parameter, namely the threshold. One good heuristic for setting this thresh- old is to look at statistics on the average norm over a sufficiently large number of updates. In our ex- periments we have noticed that for a given task and model size, training is not very sensitive to this hyper- parameter and the algorithm behaves well even for rather small thresholds. The algorithm can also be thought of as adapting the learning rate based on the norm of the gradient. Compared to other learning rate adaptation strate- gies, which focus on improving convergence by col- lecting statistics on the gradient (as for example in   Duchiet al.(2011), or Moreira and Fiesler (1995) for an overview), we rely on theinstantaneousgradient.\n\nfilename: materials_formatted.md\nextension: .md\n\nRTE Recognizing Textual Entailment is a bi- nary entailment task similar to MNLI, but with much less training data (Bentivogli et al., 2009).14  WNLI Winograd NLI is a small natural lan- guage inference dataset (Levesque et al., 2011). The GLUE webpage notes that there are issues with the construction of this dataset, 15 and every trained system that\u2019s been submitted to GLUE has performed worse than the 65.1 baseline accuracy of predicting the majority class. We therefore ex- clude this set to be fair to OpenAI GPT. For our GLUE submission, we always predicted the ma-  14Note that we only report single-task \ufb01ne-tuning results in this paper. A multitask \ufb01ne-tuning approach could poten- tially push the performance even further. For example, we did observe substantial improvements on RTE from multi- task training with MNLI.  15https://gluebenchmark.com/faq  BERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphBERTE[CLS]E1 E2 ENCT1 T2 TNSingle Sentence ......BERTTok 1 Tok 2 Tok N...[CLS]E[CLS]E1 E2 ENCT1 T2 TNSingle Sentence B-PEROO......E[CLS]E1 E[SEP]Class Label...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019Start/End SpanClass LabelBERTTok 1 Tok 2 Tok N...[CLS]Tok 1[CLS][CLS]Tok 1 [SEP]...Tok NTok 1...TokMSentence 1...Sentence 2Note that the purpose of the masking strategies is to reduce the mismatch between pre-training and \ufb01ne-tuning, as the [MASK] symbol never ap- pears during the \ufb01ne-tuning stage. We report the Dev results for both MNLI and NER.\n\nfilename: materials_formatted.md\nextension: .md\n\nOn average, we get a MS-SSIM score of 44.3, on 50 samples from the validation set, which shows that our models don\u2019t merely learn to copy training images but generate high-quality images by adding synthesized details on the low resolution input image.  Image Transformer  Input  1D Local Attention \u03c4 = 0.9  \u03c4 = 0.8  \u03c4 = 1.0  \u03c4 = 0.8  2D Local Attention \u03c4 = 0.9  \u03c4 = 1.0  Original  Table 6. Images from our 1D and 2D local attention super-resolution models trained on CelebA, sampled with different temperatures. 2D local attention with \u03c4 = 0.9 scored highest in our human evaluation study.  CIFAR-10 We also trained a super-resolution model on the CIFAR-10 data set. Our model reached a negative log- likelihood of 2.76 using 1D local attention and 2.78 using 2D local attention on the test set. As seen in Figure 2, our model commonly generates plausible looking objects even though the input images seem to barely show any discernible structure beyond coarse shapes.  previously proposed (Mansimov et al., 2015), and tasks combining modalities such as language-driven editing of images.  Fundamentally, we aim to move beyond still images to video (Kalchbrenner et al., 2016) and towards applications in model-based reinforcement learning.  6. Conclusion  In this work we demonstrate that models based on self- attention can operate effectively on modalities other than text, and through local self-attention scale to signi\ufb01cantly larger structures than sentences. With fewer layers, its larger receptive \ufb01elds allow the Image Transformer to signi\ufb01cantly improve over the state of the art in unconditional, probabilis- tic image modeling of comparatively complex images from ImageNet as well as super-resolution.  We further hope to have provided additional evidence that even in the light of generative adversarial networks, likelihood-based models of images is very much a promising area for further research - as is using network architectures such as the Image Transformer in GANs.  References  Ba, Jimmy Lei, Kiros, Jamie Ryan, and Hinton, Geoffrey E. Layer normalization.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nHow does the CLIP model by OpenAI demonstrate robustness in multimodal learning?[/INST] \nAnswer: I'm sorry, but the provided context does not contain information about the CLIP model by OpenAI demonstrating robustness in multimodal learning."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nFor a given sentence example, we select the words, POS tags and arc labels given the schematic defined above, extract their corresponding dense feature representations produced from the embedding matricesEw,Et, andEl, and concatenate these vectors into our inputs[xw,xt,xl]. At training time we backpropagate into the dense vector representations, as well as the parameters at later layers.   Feedforward Neural Network Model:  The network contains an input layer[xw,xt,xl], a hidden layer, and a final softmax layer with a cross-entropy loss function. We can either define a single weight matrix in the hidden layer, to operate on a concatenation of[xw,xt,xl], or we can use three weight matrices [W 1 w,W 1 t,W 1 l], one for each input type, as shown in Figure 3. We then apply a non-linear function and use one more affine layer[W 2 ]so that there are an equivalent number of softmax probabilities to the number of possible transitions (the output dimension).   ## dependency parsing 5   Input layer :[xw,xt,xl]   Hidden layer : h= (W 1 wxw+W 1 txt+W 1 lxl+b 1 )^3   Softmax layer : p = softmax(W 2 h)   words POS tags arc labels   ROOT has_VBZ   He_PRP   nsubj   has_VBZ good_JJ control_NN ._.   Stack Buffer Configuration   Figure 3 : The neural network archi- tecture for greedy, transition-based Note that in Figure 3 ,f(x) =x^3 is the non-linear function used. dependency parsing.  For a more complete explanation of a greedy transition-based neural dependency parser, refer to \"A Fast and Accurate Dependency Parser using Neural Networks\" under Further Reading.  Further reading: Danqi Chen, and Christopher D. Manning. \"A Fast and Accurate Dependency Parser using Neural Networks.\" EMNLP. 2014. Kuebler, Sandra, Ryan McDonald, and Joakim Nivre. \u201cDepen- dency parsing.\u201d Synthesis Lectures on Human Language Technolo- gies 1. 1 ( 2009 ): 1 - 127.\n\nfilename: materials_formatted.md\nextension: .md\n\n## 1 Dependency Grammar and Dependency Structure  Parse trees in NLP, analogous to those in compilers, are used to ana- lyze the syntactic structure of sentences. There are two main types of structures used - constituency structures and dependency structures. Constituency Grammar uses phrase structure grammar to organize words into nested constituents. This will be covered in more detail in following chapters. We now focus on Dependency Parsing. Dependency structure of sentences shows which words depend on (modify or are arguments of) which other words. These binary asymmetric relations between the words are called dependencies and are depicted as arrows going from the head (or governor, superior, regent) to the dependent (or modifier, inferior, subordinate). Usually these dependencies form a tree structure. They are often typed with the name of grammatical relations (subject, prepositional object, apposition, etc.). An example of such a dependency tree is shown in Figure 1. Sometimes a fake ROOT node is added as the head to the whole tree so that every word is a dependent of exactly one node.   Figure 1 : Dependency tree for the sen- tence \"Bills on ports and immigration were submitted by Senator Brownback, Republican of Kansas\"  ### 1. 1 Dependency Parsing  Dependency parsing is the task of analyzing the syntactic depen- dency structure of a given input sentenceS. The output of a depen- dency parser is a dependency tree where the words of the input sen- tence are connected by typed dependency relations. Formally, the de- pendency parsing problem asks to create a mapping from the input sentence with wordsS=w 0 w 1 ...wn(wherew 0 is the ROOT) to its dependency tree graphG. Many different variations of dependency- based methods have been developed in recent years, including neural network-based methods, which we will describe later. To be precise, there are two subproblems in dependency parsing (adapted from Kuebler et al., chapter 1. 2 ):  1. Learning:Given a training setDof sentences annotated with de- pendency graphs, induce a parsing modelMthat can be used to parse new sentences.   ## dependency parsing 2  2. Parsing:Given a parsing modelMand a sentenceS, derive the optimal dependency graphDforSaccording toM.\n\nfilename: materials_formatted.md\nextension: .md\n\n(Pre-condition: buffer has to be non-empty.)  2. Left-Arcr: Add a dependency arc(wj,r,wi)to the arc setA, wherewiis the word second to the top of the stack andwjis the   ## dependency parsing 3   word at the top of the stack. Removewifrom the stack. (Pre- condition: the stack needs to contain at least two items andwi cannot be the ROOT.)  3. Right-Arcr: Add a dependency arc(wi,r,wj)to the arc setA, wherewiis the word second to the top of the stack andwjis the word at the top of the stack. Removewjfrom the stack. (Pre- condition: The stack needs to contain at least two items.)  A more formal definition of these three transitions is presented in Figure 2.  ### 1. 4 Neural Dependency Parsing  While there are many deep models for dependency parsing, this section focuses specifically on greedy, transition-based neural de- pendency parsers. This class of model has demonstrated compara- ble performance and significantly better efficiency than traditional feature-based discriminative dependency parsers. The primary dis- tinction from previous models is the reliance on dense rather than sparse feature representations. The model we will describe employs the arc-standard system for transitions, as presented in section 1. 3. Ultimately, the aim of the model is to predict a transition sequence from some initial con- figurationcto a terminal configuration, in which the dependency parse tree is encoded. As the model is greedy, it attempts to correctly predict one transitionT \u2208 {shift, Left-Arcr, Right-Arcr}at a time, based on features extracted from the current configuration c= ( _\u03c3_ , _\u03b2_ ,A). Recall, _\u03c3_ is the stack, _\u03b2_ the buffer, andAthe set of dependency arcs for a given sentence.   Feature Selection:  Depending on the desired complexity of the model, there is flexi- bility in defining the input to the neural network. The features for a given sentenceSgenerally include some subset of:  1. Sword: Vector representations for some of the words inS(and their dependents) at the top of the stack _\u03c3_ and buffer _\u03b2_.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat does the feedforward neural network model contain in neural dependency parsing?[/INST] \nAnswer: The feedforward neural network model in neural dependency parsing contains an input layer, a hidden layer, and a final softmax layer with a cross-entropy loss function. The input layer is a concatenation of dense feature representations of words, POS tags, and arc labels. The hidden layer applies a non-linear function to the weighted sum of the inputs. The softmax layer then applies the softmax function to the output of the hidden layer to generate probabilities for possible transitions."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nLet\u2019s go through some options.  ### 3. 1 Co-occurrence matrices and document contexts  If you were asked to code up the idea \u201crepresent a word by the distribution of words it appears near\u201d, you might immediately have the following idea:  1. Determine a vocabularyV.  2. Make a matrix of size|V|\u00d7|V|of zeros.  3. Walk through a sequence of documents. For each document, for each wordwin the document, add all the counts of the other wordsw\u2032in the document to the row corresponding towat the column corresponding tow\u2032.  4. Normalize the rows by the sum.    deep learning 7  You\u2019ve just made a document-level co-occurrence matrix for your vocabulary! Call this matrix X. The word embedding X tea\u2208 R |V|(a row of X ) is substantially more immediately useful than the old 1 -hot eteathat we had. One decision we made was document-level co-occurrence. We could instead say that a wordw\u2032is only co-occurring withwif that w\u2032appears much closer, say, within a few words. Here\u2019s an example, where a few relevant windows for co-occurrence are labeled: Larger notions of co-occurrence (e.g., large windows or documents) lead to more semantic or even topic-encoding representations; shorter windows lead to more syntax-encoding representa- tions   [It\u2019s hot and delicious. [I poured [the |{z}tea center word   for] 1 my uncle] 3 .]document  In brief, shorter windows (like the one word window above, labeled 1 ) seem to encode syntactic properties. For example, nouns tend to appear right next totheoris. Plural nouns don\u2019t appear right next to a. Larger windows tend to encode more semantic (and at extremes, topic-like) properties. Note howpouredordeliciousmay occur farther from tea but still be relevant. Document-level windows, for large documents ( 1000 s of words) intuitively represent words by what kinds of documents they appear in (sports, law, medicine, etc.) Another design decision we made was to represent explicit counts of words in|V|-sized vectors. This ends up being a big mistake.\n\nfilename: materials_formatted.md\nextension: .md\n\nDiversity in answers. We automatically catego- rize the answers as follows: We \ufb01rst separate the numerical and non-numerical answers. The non-numerical answers are categorized using con- stituency parses and POS tags generated by Stan- ford CoreNLP. The proper noun phrases are further split into person, location and other entities using NER tags. In Table 2, we can see dates and other numbers make up 19.8% of the data; 32.6% of the answers are proper nouns of three different types; 31.8% are common noun phrases answers; and the remaining 15.8% are made up of adjective phrases, verb phrases, clauses and other types.  Reasoning required to answer questions. To get a better understanding of the reasoning required to answer the questions, we sampled 4 questions from each of the 48 articles in the development set, and then manually labeled the examples with the cate- gories shown in Table 3. The results show that all examples have some sort of lexical or syntactic divergence between the question and the answer in the passage. Note that some examples fall into more than one category.  Reasoning  Description  Example  Lexical variation (synonymy)  Major correspondences between the question and the answer sen- tence are synonyms.  Q: What is the Rankine cycle sometimes called? Sentence: The Rankine cycle is sometimes re- ferred to as a practical Carnot cycle.  Lexical variation (world knowledge)  correspondences between Major the question and the answer sen- tence require world knowledge to resolve.  Q: Which governing bodies have veto power? Sen.: The European Parliament and the Council of the European Union have powers of amendment and veto during the legislative process.  Syntactic variation After the question is paraphrased into declarative form, its syntac- tic dependency structure does not match that of the answer sentence even after local modi\ufb01cations.  Q: What Shakespeare scholar is currently on the faculty? Sen.: Current faculty include the anthropol- ogist Marshall Sahlins, ..., Shakespeare scholar David Bevington.  Multiple sentence reasoning  There is anaphora, or higher-level fusion of multiple sentences is re- quired.\n\nfilename: materials_formatted.md\nextension: .md\n\nTable 1: Examples of five types of semantic and nine types of syntactic questions in the Semantic- Syntactic Word Relationship test set.   Type of relationship Word Pair 1 Word Pair 2 Common capital city Athens Greece Oslo Norway All capital cities Astana Kazakhstan Harare Zimbabwe Currency Angola kwanza Iran rial City-in-state Chicago Illinois Stockton California Man-Woman brother sister grandson granddaughter Adjective to adverb apparent apparently rapid rapidly Opposite possibly impossibly ethical unethical Comparative great greater tough tougher Superlative easy easiest lucky luckiest Present Participle think thinking read reading Nationality adjective Switzerland Swiss Cambodia Cambodian Past tense walking walked swimming swam Plural nouns mouse mice dollar dollars Plural verbs work works speak speaks  4.1 Task Description  To measure quality of the word vectors, we define a comprehensive test set that contains five types of semantic questions, and nine types of syntactic questions. Two examples from each category are shown in Table 1. Overall, there are 8869 semantic and 10675 syntactic questions. The questions in each category were created in two steps: first, a list of similar word pairs was created manually. Then, a large list of questions is formed by connecting two word pairs. For example, we made a list of 68 large American cities and the states they belong to, and formed about 2.5K questions by picking two word pairs at random. We have included in our test set only single token words, thus multi-word entities are not present (such asNew York).  We evaluate the overall accuracy for all question types, and for each question type separately (se- mantic, syntactic). Question is assumed to be correctly answered only if the closest word to the vector computed using the above method is exactly the same as the correct word in the question; synonyms are thus counted as mistakes. This also means that reaching 100% accuracy is likely to be impossible, as the current models do not have any input information about word morphology. However, we believe that usefulness of the word vectors for certain applications should be positively correlated with this accuracy metric. Further progress can be achieved by incorporating information about structure of words, especially for the syntactic questions.  4.2 Maximization of Accuracy  We have used a Google News corpus for training the word vectors. This corpus contains about 6B tokens.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat are the two ways words are discussed in this document?[/INST] \nAnswer: The two ways words are discussed in this document are through their semantic relationships and syntactic relationships."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nNaN) which is easily detectable at runtime; this issue is called theGradient Explosion Problem. When the gradient value goes to zero, however, it can go undetected while dras- tically reducing the learning quality of the model for far-away words in the corpus; this issue is called theVanishing Gradient Problem. Due to vanishing gradients, we don\u2019t know whether there is no dependency between stepstandt+nin the data, or we just cannot capture the true dependency due to this issue. To gain practical intuition about the vanishing gradient problem, you may visit the followingexample website.  #### 2. 4 Solution to the Exploding & Vanishing Gradients  Now that we gained intuition about the nature of the vanishing gradi- ents problem and how it manifests itself in deep neural networks, let us focus on a simple and practical heuristic to solve these problems. To solve the problem of exploding gradients, Thomas Mikolov first introduced a simple heuristic solution thatclipsgradients to a small number whenever they explode. That is, whenever they reach a cer- tain threshold, they are set back to a small number as shown in Algo- rithm 1. Algorithm 1 : Pseudo-code for norm clip-   ping in the gradients whenever they ex- g\u02c6\u2190 \u2202 E plode \u2202 W if \u2016g\u02c6\u2016\u2265threshold then g\u02c6\u2190threshold \u2016g\u02c6\u2016 g\u02c6 end if  Figure 7 visualizes the effect of gradient clipping. It shows the de- cision surface of a small recurrent neural network with respect to its Wmatrix and its bias terms,b. The model consists of a single unit of recurrent neural network running through a small number of time- steps; the solid arrows illustrate the training progress on each gradient descent step. When the gradient descent model hits the high error wall in the objective function, the gradient is pushed off to a far-away loca- tion on the decision surface. The clipping model produces the dashed line where it instead pulls back the error gradient to somewhere close to the original gradient landscape.   On the diculty of training Recurrent Neural Networks   Figure 6.We plot the error surface of a single hidden unit recurrent network, highlighting the existence of high cur- vature walls. The solid lines depicts standard trajectories that gradient descent might follow.\n\nfilename: materials_formatted.md\nextension: .md\n\nThis means that we can handle very abrupt changes in norm, while the other methods would not be able to do so.  3.3. Vanishing gradient regularization  We opt to address the vanishing gradients problem us- ing a regularization term that represents a preference for parameter values such that back-propagated gra- dients neither increase or decrease too much in mag- nitude. Our intuition is that increasing the norm of \u2202xt \u2202xkmeans the error at timetis more sensitive to all inputsut,..,uk(\u2202\u2202xxtkis a factor in \u2202\u2202uEtk). In practice some of these inputs will be irrelevant for the predic- tion at timetand will behave like noise that the net- work needs to learn to ignore. The network can not learn to ignore these irrelevant inputs unless there is an error signal. These two issues can not be solved in parallel, and it seems natural to expect that we need to force the network to increase the norm of\u2202\u2202xxtkat the expense of larger errors (caused by the irrelevant input entries) andthenwait for it to learn to ignore these irrelevant input entries. This suggest that moving to- wards increasing the norm of \u2202\u2202xxtkcan not be always done while following a descent direction of the errorE (which is, for e.g., what a second order method would try to do), and therefore we need to enforce it via a regularization term.\n\nfilename: materials_formatted.md\nextension: .md\n\nWe then compute iteratively:   \u2202Clogadd \u2202\u03b4t\u2212 1 (i)   =\u2211   j   \u2202Clogadd \u2202\u03b4t(j)   e\u03b4t\u2212^1 (i)+[A]i,j \u2211ke\u03b4t\u2212^1 (k)+[A]k,j  ##### ,  where at each steptof the recursion we accumulate of the gradients with respect to the inputsf\u03b8, and the transition scores[A]i,j:   \u2202C \u2202  ##### [   f\u03b8  ##### ]   i,t  ##### +=   \u2202Clogadd \u2202\u03b4t(i)   \u2202\u03b4t(i) \u2202  ##### [   f\u03b8  ##### ]   i,t   =   \u2202Clogadd \u2202\u03b4t(i) \u2202C \u2202[A]i,j  ##### +=   \u2202Clogadd \u2202\u03b4t(j)   \u2202\u03b4t(j) \u2202[A]i,j   =   \u2202Clogadd \u2202\u03b4t(j)   e\u03b4t\u2212^1 (i)+[A]i,j \u2211ke\u03b4t\u2212^1 (k)+[A]k,j  ##### .  A.8 Ranking Criterion  We use the ranking criterion (17) for training our language model. In this case, given a \u201cpositive\u201d examplexand a \u201cnegative\u201d examplex(w), we want to minimize:   C(f\u03b8(x),f\u03b8(xw)) =max  ##### {   0 , 1 \u2212f\u03b8(x)+f\u03b8(x(w))  ##### }  ##### .  Ignoring the non-differentiability ofmax( 0 ,\u00b7)in zero, the gradient is simply given by:   ( \u2202C \u2202f\u03b8(x) \u2202C \u2202f\u03b8(xw)   )   =  #####   #####   #####   #####   #####    (   \u2212 1 1   )   if 1\u2212f\u03b8(x)+f\u03b8(x(w))> 0 ( 0 0   )   otherwise  ##### .    COLLOBERT, WESTON, BOTTOU, KARLEN, KAVUKCUOGLU ANDKUKSA  #### References  R. K. Ando and T. Zhang.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nHow does the paper suggest addressing the vanishing gradients problem?[/INST] \nAnswer: The paper suggests addressing the vanishing gradients problem by using a regularization term that represents a preference for parameter values such that back-propagated gradients neither increase or decrease too much in magnitude. The idea is to increase the norm of the gradient at the expense of larger errors and then wait for the network to learn to ignore these irrelevant input entries. This approach suggests that increasing the norm of the gradient cannot always be done while following a descent direction of the error, and therefore it needs to be enforced via a regularization term."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nLlion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.  \u2020Work performed while at Google Brain. \u2021Work performed while at Google Research.  31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.   Introduction  Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38, 24, 15].  Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht\u22121 and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved significant improvements in computational efficiency through factorization tricks [21] and conditional computation [32], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains.  Attention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms are used in conjunction with a recurrent network.  In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n\nfilename: materials_formatted.md\nextension: .md\n\nAttention Is All You Need  Ashish Vaswani\u2217 Google Brain avaswani@google.com  Noam Shazeer\u2217 Google Brain noam@google.com  Niki Parmar\u2217 Google Research nikip@google.com  Jakob Uszkoreit\u2217 Google Research usz@google.com  Llion Jones\u2217 Google Research llion@google.com  Aidan N. Gomez\u2217 \u2020 University of Toronto aidan@cs.toronto.edu  \u0141ukasz Kaiser\u2217 Google Brain lukaszkaiser@google.com  Illia Polosukhin\u2217 \u2021 illia.polosukhin@gmail.com  Abstract  The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English- to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.  \u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor.\n\nfilename: materials_formatted.md\nextension: .md\n\n(2016) [8] Transformer (4 layers) Zhu et al. (2013) [40] Huang & Harper (2009) [14] McClosky et al. (2006) [26] Vinyals & Kaiser el al. (2014) [37] Transformer (4 layers) Luong et al. (2015) [23] Dyer et al. (2016) [8]  WSJ 23 F1 88.3 90.4 90.4 91.7 91.3 91.3 91.3 92.1 92.1 92.7 93.0 93.3  increased the maximum output length to input length + 300. We used a beam size of 21 and \u03b1 = 0.3 for both WSJ only and the semi-supervised setting.  Our results in Table 4 show that despite the lack of task-specific tuning our model performs sur- prisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8].  In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley- Parser [29] even when training only on the WSJ training set of 40K sentences.  7 Conclusion  In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.  For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles.  We are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours.  The code we used to train and evaluate our models is available at https://github.com/ tensorflow/tensor2tensor.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat does the Transformer model architecture replace in traditional sequence transduction models?[/INST] \nAnswer: The Transformer model architecture replaces the recurrent layers that are commonly used in traditional sequence transduction models. It relies entirely on attention mechanisms, dispensing with recurrence and convolutions entirely."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\n### 2 Recurrent Neural Networks (RNN)  Unlike the conventional translation models, where only a finite win- dow of previous words would be considered for conditioning the language model, Recurrent Neural Networks (RNN) are capable of conditioning the model onallprevious words in the corpus.   xt\u2212 1 xt xt +   ht\u2212 1 ht ht + W\" W\"   yt\u2212 1 yt yt+ 1   Figure 3 : A Recurrent Neural Network (RNN). Three time-steps are shown.  Figure 3 introduces the RNN architecture where each vertical rect- angular box is a hidden layer at a time-step,t. Each such layer holds a number of neurons, each of which performs a linear matrix oper- ation on its inputs followed by a non-linear operation (e.g. tanh()). At each time-step, there are two inputs to the hidden layer: the out- put of the previous layerht\u2212 1 , and the input at that timestepxt. The former input is multiplied by a weight matrixW(hh)and the latter by a weight matrixW(hx)to produce output featuresht, which are multiplied with a weight matrixW(S)and run through a softmax over the vocabulary to obtain a prediction outputy\u02c6of the next word (Equations 5 and 6 ). The inputs and outputs of each single neuron are illustrated in Figure 4.   ht= \u03c3 (W(hh)ht\u2212 1 +W(hx)x[t]) ( 5 )   y\u02c6t=so f tmax(W(S)ht) ( 6 )   ### language models,rnn,gru and lstm 4  What is interesting here is that the same weightsW(hh)andW(hx)are applied repeatedly at each timestep. Thus, the number of parameters the model has to learn is less, and most importantly, is independent of the length of the input sequence - thus defeating the curse of di- mensionality!   Figure 4 : The inputs and outputs to a neuron of a RNN  Below are the details associated with each parameter in the net- work:  - x 1 , ...,xt\u2212 1 ,xt,xt+ 1 , ...xT: the word vectors corresponding to a cor-     pus with T words.\n\nfilename: materials_formatted.md\nextension: .md\n\n2019. End-to-end open-domain question answering with bertserini. arXiv preprint arXiv:1902.01718.     \n\n# Understanding LSTM Networks:  # Understanding LSTM Networks  ### Posted on August 27, 2015  ## Recurrent Neural Networks  Humans don\u2019t start their thinking from scratch every second. As you read this essay, you understand each word based on your understanding of previous words. You don\u2019t throw everything away and start thinking from scratch again. Your thoughts have persistence.  Traditional neural networks can\u2019t do this, and it seems like a major shortcoming. For example, imagine you want to classify what kind of event is happening at every point in a movie. It\u2019s unclear how a traditional neural network could use its reasoning about previous events in the film to inform later ones.  Recurrent neural networks address this issue. They are networks with loops in them, allowing information to persist.   Recurrent Neural Networks have loops.  In the above diagram, a chunk of neural network, , looks at some input and outputs a value. A loop allows information to be passed from one step of the network to the next.  These loops make recurrent neural networks seem kind of mysterious. However, if you think a bit more, it turns out that they aren\u2019t all that different than a normal neural network. A recurrent neural network can be thought of as multiple copies of the same network, each passing a message to a successor. Consider what happens if we unroll the loop:   An unrolled recurrent neural network.  This chain-like nature reveals that recurrent neural networks are intimately related to sequences and lists. They\u2019re the natural architecture of neural network to use for such data.  And they certainly are used! In the last few years, there have been incredible success applying RNNs to a variety of problems: speech recognition, language modeling, translation, image captioning... The list goes on. I\u2019ll leave discussion of the amazing feats one can achieve with RNNs to Andrej Karpathy\u2019s excellent blog post, The Unreasonable Effectiveness of   A xt ht   Recurrent Neural Networks (http://karpathy.github.io/2015/05/21/rnn-effectiveness/). But they really are pretty amazing.\n\nfilename: materials_formatted.md\nextension: .md\n\nThe first technique is that instead of initializingW(hh)ran- domly, start off from an identity matrix initialization. The second technique is to use the Rectified Linear Units (ReLU) in- stead of the sigmoid function. The derivative for the ReLU is either 0 or   ## language models,rnn,gru and lstm 9  1. This way, gradients would flow through the neurons whose deriva- tive is 1 without getting attenuated while propagating back through time-steps.  #### 2. 5 Deep Bidirectional RNNs  So far, we have focused on RNNs that condition on past words to predict the next word in the sequence. It is possible to make predic- tions based on future words by having the RNN model read through the corpus backwards. Irsoy et al. shows a bi-directional deep neu- ral network; at each time-step,t, this network maintains two hidden layers, one for the left-to-right propagation and another for the right- to-left propagation. To maintain two hidden layers at any time, this network consumes twice as much memory space for its weight and bias parameters. The final classification result,y\u02c6t, is generated through combining the score results produced by both RNN hidden layers. Fig- ure 8 shows the bi-directional network architecture, and Equations 17 and 18 show the mathematical formulation behind setting up the bi- directional RNN hidden layer. The only difference between these two relationships is in the direction of recursing through the corpus. Equa- tion 19 shows the classification relationship used for predicting the next word via summarizing past and future word representations.  ## Bidirectionality   RNNs can also be multi-layered. Figure 9 shows a multi-layer bi- directional RNN where each lower layer feeds the next layer. As shown in this figure, in this network architecture, at time-stepteach interme- diate neuron receives one set of parameters from the previous time- step (in the same RNN layer), and two sets of parameters from the previous RNN hidden layer; one input comes from the left-to-right RNN and the other from the right-to-left RNN.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat is the primary purpose of Recurrent Neural Networks (RNNs)?[/INST] \nAnswer: The primary purpose of Recurrent Neural Networks (RNNs) is to address the issue of persistence in neural networks. Unlike traditional neural networks, RNNs are capable of conditioning the model on all previous words in the corpus, allowing information to persist. This makes them particularly useful for tasks related to sequences and lists, such as speech recognition, language modeling, translation, and image captioning. They can consider the entire context or sequence of words or events, rather than starting from scratch with each new input."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\n\u25cb VQA/VQAv2 citations: 4305+1684 \u25cb COCO Captions: 1647 \u25cb Flickr30k: 1228 \u25cf At first the \u201cV\u201d in VQA was found to not matter all that much, so a follow-up VQAv2 dataset was created (Goyal et al., 2017). \u25cf There\u2019s also GQA (Hudson & Manning, 2019).   #### CLEVR (Johnson et al., 2016)  Compositional language and elementary visual reasoning diagnostics in a controlled setting.  Hand crafted for measuring compositionality.   #### Hateful Memes (Kiela et al., 2020)  Motivated by the shortcomings of other V&L datasets: we need something that is harder, more realistic, and requires true multimodal reasoning and understanding.   \u201cMean meme\u201d examples for illustrative purposes \u2013 not actually in the dataset   #### Hateful Memes  Highly trained annotators, so: decent quality but small and expensive  Key concept: benign confounders  A \u201cchallenge set\u201d for the community to do zero-shot/finetuning from pretrained   #### Hateful Memes  Findings in the paper:   \u25cf Big gap with human performance. \u25cf Region features (as opposed to grid) seem to help. \u25cf Earlier fusion is better than middle, is better than late. \u25cf Multimodal pretraining doesn\u2019t really work.   #### Hateful Memes Competition  After the paper came a $100k competition on an unseen test set:  Winner characteristics: frameworks matter, SOTA pretrained models, ensembles, entities, faces and external knowledge. STILL FAR FROM SOLVED.   #### Winoground  How good is CLIP really?  Some relevant ideas/findings from NLP:   \u25cf Winograd schemas \u201cThe [trophy] doesn't fit in the [suitcase] because it is too [large/small]\u201d \u25cf Word order may not matter all that much   #### Winoground   \u25cf Examples written by linguist experts \u25cf Using Getty Images API \u25cf Simple way to measure by comparing scores \u25cf In some cases, very difficult and requiring world knowledge   #### Winoground Findings  SOTA models often perform _below chance_.\n\nfilename: materials_formatted.md\nextension: .md\n\n. C.2 Assessing human feedback quality .  C.3 Labeler demographics .  C.4 Labeler website .  C.5 Instructions for labelers .  C.6 Composition of the labeled dataset  C.7 Example comparison tasks  D Choice of baselines  E CNN/DM lead-3 vs reference summaries  F Controlling for summary length  G Additional results G.1 Value function ablation . G.2 Evaluating policies along axes of quality . G.3 Studying best-of-N optimization . G.4 ROUGE scores G.5 Bigram overlap statistics . G.6 Reward model validation sets .  G.7 Measuring agreement between different evaluation metrics .   H Samples  H.1 Random samples . . H.2 Overoptimized samples .  A TL;DR dataset details  Here, we discuss the pre-processing steps that we apply to the TL;DR dataset. We \ufb01rst remove all duplicate posts by checking the text body, \ufb01nding that there are nearly 20,000 exact duplicates. We then re-parse the TL;DR carefully using a set of heuristics, and \ufb01lter to use only top-level posts (rather than comments). We also \ufb01lter out any post that is from a subreddit not in our \u2018subreddit whitelist\u2019 (see Table 2 for the distribution over subreddits), any post where the title starts with some variant of \u2018Edit\u2019 or \u2018Update\u2019,10 and posts that contain certain topics (such as graphic sex or suicide) using heuristics. Finally, to ensure the posts are short enough to \ufb01t into the context length of our models, we \ufb01lter out any post whose body is longer than 512 tokens. This resulted in a set of 287,790 posts \ufb01ltered by body but not summary, of which we hold out approximately 5% as a validation set. We used this set of posts for RL training since our RL procedure does not require reference summaries.\n\nfilename: materials_formatted.md\nextension: .md\n\n6.2.2 Race  To investigate racial bias in GPT-3, we seeded the model with prompts such as - \"The {race} man was very\", \"The {race} woman was very\" and \"People would describe the {race} person as\" and generated 800 samples for each of the above prompts, with {race} replaced with a term indicating a racial category such as White or Asian. We then measure word co-occurrences in the generated samples. Given prior research demonstrating that language models produce text of differing sentiment when varying features such as occupation [HZJ+19], we explored how race impacted sentiment. We measured sentiment using Senti WordNet [BES10] for the words which co-occurred disproportionately with each race. Each word sentiment varied from 100 to -100, with positive scores indicating positive words (eg. wonderfulness: 100, amicable: 87.5), negative scores indicating negative words (eg. wretched: -87.5 , horrid: -87.5) and a score of 0 indicating neutral words (eg. sloping, chalet).  It should be noted that we were explicitly prompting the models to talk about race and this in turn generated text that focused on racial features; these results are not from the models talking about race in the wild but talking about race in an experimental setup where they have been primed to do so. Additionally, since we are measuring sentiment by simply looking at word co-occurrences, the resulting sentiment can re\ufb02ect socio-historical factors - for instance, text relating to a discussion of slavery will frequently have a negative sentiment, which may lead to a demographic being associated with a negative sentiment under this testing methodology.  Across the models we analyzed, \u2018Asian\u2019 had a consistently high sentiment - it ranked 1st in 3 out of 7 models. On the other hand, \u2019Black\u2019 had a consistently low sentiment - it ranked the lowest in 5 out of 7 models. These differences narrowed marginally on the larger model sizes. This analysis gives a sense of the biases of different models and highlights the need for more sophisticated analysis of the relationship between sentiment, entities, and input data.  9We only used male and female pronouns.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat was the motivation behind creating the Hateful Memes dataset, and what does it aim to test?[/INST] \nAnswer: The Hateful Memes dataset was created to address the shortcomings of other Vision and Language (V&L) datasets. It was designed to be more challenging, more realistic, and to require true multimodal reasoning and understanding. The dataset tests for the understanding of benign confounders and offers a \"challenge set\" for the community to do zero-shot/finetuning from pretrained models. It also examines the effectiveness of different strategies such as region features, fusion timing, and multimodal pretraining."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nThe results are summarized in Table 3.  The results show that while Negative Sampling achieves a respectable accuracy even withk= 5, usingk= 15achieves considerably better performance. Surprisingly,while we found the Hierar- chical Softmax to achieve lower performance when trained without subsampling, it became the best performing method when we downsampled the frequent words. This shows that the subsampling can result in faster training and can also improve accuracy,at least in some cases.  (^2) code.google.com/p/word2vec/source/browse/trunk/questions-phrases.txt Method Dimensionality No subsampling [%] 10 \u2212^5 subsampling [%] NEG-5 300 24 27 NEG-15 300 27 42 HS-Huffman 300 19 47 Table 3: Accuracies of the Skip-gram models on the phrase analogy dataset. The models were trained on approximately one billion words from the news dataset.    NEG-15 with 10 \u2212^5 subsampling HS with 10 \u2212^5 subsampling Vasco de Gama Lingsugur Italian explorer Lake Baikal Great Rift Valley Aral Sea Alan Bean Rebbeca Naomi moonwalker Ionian Sea Ruegen Ionian Islands chess master chess grandmaster Garry Kasparov   Table 4: Examples of the closest entities to the given short phrases, using two different models.   Czech + currency Vietnam + capital German + airlines Russian + river French + actress koruna Hanoi airline Lufthansa Moscow Juliette Binoche Check crown Ho Chi Minh City carrier Lufthansa Volga River Vanessa Paradis Polish zolty Viet Nam flag carrier Lufthansa upriver Charlotte Gainsbourg CTK Vietnamese Lufthansa Russia Cecile De  Table 5: Vector compositionality using element-wise addition. Four closest tokens to the sum of two vectors are shown, using the best Skip-gram model.  To maximize the accuracy on the phrase analogy task, we increased the amount of the training data by using a dataset with about 33 billion words. We used the hierarchical softmax, dimensionality of 1000, and the entire sentence for the context. This resulted in a model that reached an accuracy of72%. We achieved lower accuracy 66% when we reduced the size of the training dataset to 6B words, which suggests that the large amount of the training data is crucial.\n\nfilename: materials_formatted.md\nextension: .md\n\nWhile NCE can be shown to approximately maximize the log probability of the softmax, the Skip- gram model is only concerned with learning high-quality vector representations, so we are free to simplify NCE as long as the vector representations retain their quality. We define Negative sampling (NEG) by the objective  Figure 2:Two-dimensional PCA projection of the 1000-dimensional Skip-gram vectors of countries and their capital cities. The figure illustrates ability of the model to automatically organize concepts and learn implicitly the relationships between them, as during the training we did not provide any supervised information about what a capital city means.  which is used to replace everylogP(wO|wI)term in the Skip-gram objective. Thus the task is to distinguish the target wordwOfrom draws from the noise distributionPn(w)using logistic regres- sion, where there areknegative samples for each data sample. Our experiments indicate that values ofkin the range 5\u201320 are useful for small training datasets, while for large datasets thekcan be as small as 2\u20135. The main difference between the Negative sampling and NCE is that NCE needs both samples and the numerical probabilities of the noise distribution, while Negative sampling uses only samples. And while NCE approximately maximizes the log probability of the softmax, this property is not important for our application.  Both NCE and NEG have the noise distributionPn(w)as a free parameter. We investigated a number of choices forPn(w)and found that the unigram distributionU(w)raised to the 3 / 4 rd power (i.e., U(w)^3 /^4 /Z) outperformed significantly the unigram and the uniform distributions, for both NCE and NEG on every task we tried including language modeling (not reported here).  2.3 Subsampling of Frequent Words  In very large corpora, the most frequent words can easily occur hundreds of millions of times (e.g., \u201cin\u201d, \u201cthe\u201d, and \u201ca\u201d). Such words usually provide less information value than the rare words. For example, while the Skip-gram model benefits from observing the co-occurrences of \u201cFrance\u201d and \u201cParis\u201d, it benefits much less from observing the frequent co-occurrences of \u201cFrance\u201d and \u201cthe\u201d, as nearly every word co-occurs frequently within a sentence with \u201cthe\u201d.\n\nfilename: materials_formatted.md\nextension: .md\n\nDeleting Rare Words ( del ) While it is com- mon to ignore words that are rare in the training corpus,word2vecremoves these tokens from the corpusbeforecreating context windows. As with subsampling, this variation narrows the dis- tance between tokens, inserting new word-context pairs that did not exist in the original corpus with the same window size. Though this variation may also have an effect on performance, preliminary experiments showed that it was small, and we therefore do not investigate its effect in this paper.  3.2 Association Metric Hyperparameters  The PMI (or PPMI) between a word and its con- text is well known to be an effective association measure in the word similarity literature. Levy and Golberg (2014c) show that SGNS is implicitly fac- torizing a word-context matrix whose cell\u2019s val- ues are shifted PMI. Following their analysis, we present two variations of the PMI (and implicitly PPMI) association metric, which we adopt from SGNS. These enhancements of PMI are not di- rectly applicable to GloVe, which, by definition, uses a different association measure.  Shifted PMI ( neg ) SGNS has a natural hyper- parameterk(the number of negative samples), which affects the value that SGNS is trying to op- timize for each(w,c):PMI(w,c)\u2212logk. The shift caused byk > 1 can be applied to distri- butional methods through shifted PPMI (Levy and Goldberg, 2014c):   SPPMI(w,c) = max (PMI(w,c)\u2212logk,0)  It is important to understand that in SGNS,khas two distinct functions. First, it is used to better estimate the distribution of negative examples; a higherkmeans more data and better estimation.  in the original paper (equation 2).   Second, it acts as a prior on the probability of ob- serving a positive example (an actual occurrence of(w,c)in the corpus) versus a negative example; a higherkmeans that negative examples are more probable. Shifted PPMI captures only the second aspect ofk(a prior). We experiment with three values ofk: 1 , 5 , 15.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nHow does subsampling of frequent words improve the Skip-gram model according to the paper?[/INST] \nAnswer: Subsampling of frequent words can improve the Skip-gram model in a couple of ways. Firstly, it can result in faster training. Secondly, it can also enhance accuracy in some cases. This is particularly evident when frequent words are downsampled in the Hierarchical Softmax method, which then becomes the best performing method. The reason behind this is that frequent words like \"in\", \"the\", and \"a\" occur hundreds of millions of times in large corpora and usually provide less information value than rare words. Therefore, the model benefits less from observing the frequent co-occurrences of such words with other words."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nThere are many choices of positional encodings, learned and fixed [9].  In this work, we use sine and cosine functions of different frequencies:  P E(pos,2i) = sin(pos/100002i/dmodel) P E(pos,2i+1) = cos(pos/100002i/dmodel)  where pos is the position and i is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2\u03c0 to 10000 \u00b7 2\u03c0. We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k, P Epos+k can be represented as a linear function of P Epos.  We also experimented with using learned positional embeddings [9] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.  4 Why Self-Attention  In this section we compare various aspects of self-attention layers to the recurrent and convolu- tional layers commonly used for mapping one variable-length sequence of symbol representations (x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi \u2208 Rd, such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata.  One is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required.  The third is the path length between long-range dependencies in the network. Learning long-range dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types.\n\nfilename: materials_formatted.md\nextension: .md\n\nSelf-attention and the feed-forward networks are followed by dropout and bypassed by a residual connection with subsequent layer normalization. The position encodings pq, p1, . . . are added only in the \ufb01rst layer.  softmax++++  cmpcmp cmp q\u2019qm1m2m3pqp1p2p3MatMul WvMatMul Wv\u00b7MatMul WqMatMul Wk    MatMul Wv LayerNormDropoutLocal Self-Attention LayerNormFFNNDropoutImage Transformer  tation generated by the encoder. For both the encoder and decoder, the Image Transformer uses stacks of self-attention and position-wise feed-forward layers, similar to (Vaswani et al., 2017). In addition, the decoder uses an attention mechanism to consume the encoder representation. For un- conditional and class-conditional generation, we employ the Image Transformer in a decoder-only con\ufb01guration.  Before we describe how we scale self-attention to images comprised of many more positions than typically found in sentences, we give a brief description of self-attention.  Each self-attention layer computes a d-dimensional repre- sentation for each position, that is, each channel of each pixel. To recompute the representation for a given posi- tion, it \ufb01rst compares the position\u2019s current representation to other positions\u2019 representations, obtaining an attention distribution over the other positions. This distribution is then used to weight the contribution of the other positions\u2019 representations to the next representation for the position at hand.  Equations 1 and 2 outline the computation in our self- attention and fully-connected feed-forward layers; Figure 1 depicts it. W1 and W2 are the parameters of the feed- forward layer, and are shared across all the positions in a layer. These fully describe all operations performed in every layer, independently for each position, with the exception of multi-head attention. For details of multi-head self-attention, see (Vaswani et al., 2017).\n\nfilename: materials_formatted.md\nextension: .md\n\nFigure 5 presents this.   Embeddings   Add Position Embeddings   Multi-Head Attention   Add & Norm   Feed-For ward   Add & Norm   Block   Linear   Softmax   Repeat for number of   encoder blocks   Probabilities   Encoder Inputs  #### Transformer Encoder   Figure 5 : Diagram of the Transformer Encoder.  In the case that one wants probabilities out of the tokens of a Transformer Encoder (as in masked language modeling for BERT [Devlin et al., 2019 ], which we\u2019ll cover later), one applies a linear transformation to the output space followed by a softmax.  Uses of the Transformer Encoder. A Transformer Encoder is great in contexts where you aren\u2019t trying to generate text autoregressively (there\u2019s no masking in the encoder so each position index can see the whole sequence,) and want strong representations for the whole sequence (again, possible because even the first token can see the whole future of the sequence when building its representation.)  ##### 3. 6 Transformer Decoder  To build a Transformer autoregressive language model, one uses a Transformer Decoder. These differ from Transformer Encoders simply by using future masking at each application of self-attention. This ensures that the informational constraint (no cheating by looking at the future!) holds throughout the architecture. We show a diagram of this architecture in Figure 4. Famous examples of this are GPT- 2 [Radford et al., 2019 ], GPT- 3 [Brown et al., 2020 ] and BLOOM [Workshop et al., 2022 ].  ##### 3. 7 Transformer Encoder-Decoder  A Transformer encoder-decoder takes as input two sequences. Fig- ure 6 shows the whole encoder-decoder structure. The first sequence x 1:nis passed through a Transformer Encoder to build contextual representations. The second sequence y 1:mis encoded through a modified Transformer Decoder architecture in which cross-attention (which we haven\u2019t yet defined!) is applied from the encoded repre- sentation of y 1:mto the output of the Encoder. So, let\u2019s take a quick detour to discuss cross-attention; it\u2019s not too different from what we\u2019ve already seen.    with deep learning 14  Cross-Attention.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat is the role of positional encodings in the Transformer model?[/INST] \nAnswer: Positional encodings are used in the Transformer model to incorporate the position of the words in the sequence. They use sine and cosine functions of different frequencies, allowing each dimension of the positional encoding to correspond to a sinusoid. The chosen function is hypothesized to enable the model to easily learn to attend by relative positions. This is because for any fixed offset, the positional encoding of the position plus the offset can be represented as a linear function of the positional encoding of the position. The sinusoidal version is preferred as it may allow the model to extrapolate to sequence lengths longer than those encountered during training."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nReading comprehension. A data-driven approach to reading comprehension goes back to Hirschman et al. (1999), who curated a dataset of 600 real 3rd\u2013 6th grade reading comprehension questions. Their pattern matching baseline was subsequently im- proved by a rule-based system (Riloff and Thelen, 2000) and a logistic regression model (Ng et al., 2000). More recently, Richardson et al. (2013) cu- rated MCTest, which contains 660 stories created by crowdworkers, with 4 questions per story and 4 answer choices per question. Because many of the questions require commonsense reasoning and reasoning across multiple sentences, the dataset re- mains quite challenging, though there has been no- ticeable progress (Narasimhan and Barzilay, 2015; Sachan et al., 2015; Wang et al., 2015). Both curated datasets, although real and dif\ufb01cult, are too small to support very expressive statistical models.  Some datasets focus on deeper reasoning abili- ties. Algebra word problems require understanding a story well enough to turn it into a system of equa-  tions, which can be easily solved to produce the an- swer (Kushman et al., 2014; Hosseini et al., 2014). BAbI (Weston et al., 2015), a fully synthetic RC dataset, is strati\ufb01ed by different types of reasoning required to solve each task. Clark and Etzioni (2016) describe the task of solving 4th grade science exams, and stress the need to reason with world knowledge.  Open-domain question answering. The goal of open-domain QA is to answer a question from a large collection of documents. The annual eval- uations at the Text REtreival Conference (TREC) (Voorhees and Tice, 2000) led to many advances in open-domain QA, many of which were used in IBM Watson for Jeopardy! (Ferrucci et al., 2013). Recently, Yang et al.\n\nfilename: materials_formatted.md\nextension: .md\n\nWe expect that the remaining gap will be harder to close, but that such efforts will result in signi\ufb01cant advances in reading comprehension.  Reproducibility  All code, data, and experiments for this paper are available on the CodaLab platform: https://worksheets.codalab.org/worksheets/ 0xd53d03a48ef64b329c16b9baf0f99b0c/ .  Acknowledgments  We would like to thank Durim Morina and Professor Michael Bernstein for their help in crowdsourcing the collection of our dataset, both in terms of fund- ing and technical support of the Daemo platform.  References  J. Berant, V. Srikumar, P. Chen, A. V. Linden, B. Harding, B. Huang, P. Clark, and C. D. Manning. 2014. Mod- eling biological processes for reading comprehension. In Empirical Methods in Natural Language Process- ing (EMNLP).  E. Brill, S. Dumais, and M. Banko. 2002. An analysis of the AskMSR question-answering system. In Associa- tion for Computational Linguistics (ACL), pages 257\u2013 264.  D. Chen, J. Bolton, and C. D. Manning.  2016. A thorough examination of the CNN / Daily Mail read- ing comprehension task. In Association for Computa- tional Linguistics (ACL).  P. Clark and O. Etzioni. 2016. My computer is an honor student but how intelligent is it? standardized tests as a measure of AI. AI Magazine, 37(1):5\u201312.  J. Deng, W. Dong, R. Socher, L. Li, K. Li, and L. Fei- Fei. 2009. ImageNet: A large-scale hierarchical im- age database. In Computer Vision and Pattern Recog- nition (CVPR), pages 248\u2013255.\n\nfilename: materials_formatted.md\nextension: .md\n\nOn datasets where question writers do not know the answer\u2014 Natural Questions (Kwiatkowski et al., 2019), WebQuestions (Berant et al., 2013), and Curat- edTrec (Baudis and Sediv\u00b4y, 2015)\u2014we show that learned retrieval is crucial, providing improve- ments of 6 to 19 points in exact match over BM25.  2 Overview  In this section, we introduce notation for open do- main QA that is useful for comparing prior work, baselines, and our proposed model.  2.1 Task  In open domain question answering, the input q is a question string, and the output a is an answer string. Unlike reading comprehension, the source of evidence is a modeling choice rather than a part of the task de\ufb01nition. We compare the assump- tions made by variants of reading comprehension and question answering tasks in Table 1.  Evaluation is exact match with any of the ref- erence answer strings after minor normalization such as lowercasing, following evaluation scripts from DrQA (Chen et al., 2017).  2.2 Formal De\ufb01nitions  Models are de\ufb01ned with respect to an unstruc- tured text corpus that is split into B blocks of ev- idence texts. An answer derivation is a pair (b, s), where 1 \u2264 b \u2264 B indicates the index of an ev- idence block and s denotes a span of text within block b. The start and end token indices of span s are denoted by START(s) and END(s) respectively. Models de\ufb01ne a scoring function S(b, s, q) indi- cating the goodness of an answer derivation (b, s) given a question q. Typically, this scoring func- tion is decomposed over a retrieval component Sretr (b, q) and a reader component Sread (b, s, q):  S(b, s, q) = Sretr (b, q) + Sread (b, s, q)  During inference, the model outputs the answer string of the highest scoring derivation:  a\u2217 = TEXT(argmax  S(b, s, q))  b,s  where TEXT(b, s) deterministically maps answer derivation (b, s) to an answer string.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhy is reading comprehension considered an essential testbed for evaluating natural language understanding by computer systems?[/INST] \nAnswer: Reading comprehension is considered an essential testbed for evaluating natural language understanding by computer systems because it requires the system to understand a story or text well enough to answer questions about it. This often involves commonsense reasoning and reasoning across multiple sentences, making the task challenging and a good measure of a system's ability to understand natural language. Additionally, some reading comprehension tasks require deeper reasoning abilities, such as understanding a story well enough to turn it into a system of equations, or reasoning with world knowledge. Therefore, the performance of a system on reading comprehension tasks can provide a comprehensive evaluation of its natural language understanding capabilities."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nAt the same time, we also identify some datasets where GPT-3\u2019s few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we \ufb01nd that GPT-3 can generate samples of news articles which human evaluators have dif\ufb01culty distinguishing from articles written by humans. We discuss broader societal impacts of this \ufb01nding and of GPT-3 in general.   \u2217Equal contribution \u2020Johns Hopkins University, OpenAI  Author contributions listed at end of paper.              Contents  1 Introduction  2 Approach  2.1 Model and Architectures . . 2.2 Training Dataset . . 2.3 Training Process . . 2.4 Evaluation .  . . .  . . .  . . .  . . .  .  .  . . . .  . . . .  . . . .  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nfilename: materials_formatted.md\nextension: .md\n\nFinally, we train a policy via reinforcement learning (RL) to maximize the score given by the RM; the policy generates a token of text at each \u2018time step\u2019, and is updated using the PPO algorithm [58] based on the RM \u2018reward\u2019 given to the entire generated summary. We can then gather more human data using samples from the resulting policy, and repeat the process. We follow the works of [48, 4] and use large pretrained GPT-3 models with as many as 6.7 billion parameters.  Our main contributions are four-fold.  (1) We show that training with human feedback signi\ufb01cantly outperforms very strong baselines on English summarization. When applying our methods on a version of the Reddit TL;DR dataset [63], we train policies via human feedback that produce better summaries than much larger policies trained via supervised learning. Summaries from our human feedback models are preferred by our labelers to the original human demonstrations in the dataset (see Figure 1).  (2) We show human feedback models generalize much better to new domains than supervised models. Our Reddit-trained human feedback models also generate high-quality summaries of news articles on the CNN/DailyMail (CNN/DM) dataset without any news-speci\ufb01c \ufb01ne-tuning, almost matching the quality of the dataset\u2019s reference summaries. We perform several checks to ensure that these human preferences re\ufb02ect a real quality difference: we consistently monitor agreement rates amongst labelers and researchers, and \ufb01nd researcher-labeler agreement rates are nearly as high as researcher-researcher agreement rates (see Section C.2), and we verify models are not merely optimizing simple metrics like length or amount of copying (see Appendices F and G.7).  4Throughout the paper, error bars represent 1 standard error.  Supervised learningHuman feedbackPretrain onlyReference summaries(3) We conduct extensive empirical analyses of our policy and reward model. We examine the impact of model and data size (Figure 6), study performance as we continue to optimize a given reward model (Section 4.3), and analyze reward model performance using synthetic and human- written perturbations of summaries (Section 4.3).\n\nfilename: materials_formatted.md\nextension: .md\n\n-? 3. Reinforcement Learning from Human Feedback (RLHF)  49  4. What\u2019s next?  #### SEP 775 Lecture Plan: From Language Models to Assistants   #### Limitations of instruction finetuning?  - One limitation of instruction finetuning is obvious: it\u2019s expensive to collect ground-     truth data for tasks. - But there are other, subtler limitations too. Can you think of any? - Problem 1 : tasks like open-ended creative generation have no right answer.     - _Write me a story about a dog and her pet grasshopper._ - Problem 2 : language modeling penalizes all token-level mistakes equally, but some     errors are worse than others. - Even with instruction finetuning _,_ there     a mismatch between the LM     objective and the objective of     \u201csatisfy human preferences\u201d! - Can we explicitly attempt to satisfy     human preferences?   LM   Avatar is a fantasy TV show   is  50   a   adventure fantasy   musical TV show END   1. Zero-Shot (ZS) and Few-Shot (FS) In-Context Learning     + No finetuning needed, prompt engineering (e.g. CoT) can improve performance     - Limits to what you can fit in context     - Complex tasks will probably need gradient steps 2. Instruction finetuning     + Simple and straightforward, generalize to unseen tasks     - Collecting demonstrations for so many tasks is expensive     - Mismatch between LM objective and human preferences 3. Reinforcement Learning from Human Feedback (RLHF)  51  #### SEP 775 Lecture Plan: From Language Models to Assistants  4. What\u2019s next?   1. Zero-Shot (ZS) and Few-Shot (FS) In-Context Learning     + No finetuning needed, prompt engineering (e.g. CoT) can improve performance     - Limits to what you can fit in context     - Complex tasks will probably need gradient steps 2. Instruction finetuning     + Simple and straightforward, generalize to unseen tasks     - Collecting demonstrations for so many tasks is expensive     - Mismatch between LM objective and human preferences 3.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat strategies does the paper propose to address these training issues?[/INST] \nAnswer: The paper proposes several strategies to address the training issues. Firstly, it suggests training a policy via reinforcement learning (RL) to maximize the score given by the Reward Model (RM). This policy generates a token of text at each 'time step', and is updated using the PPO algorithm based on the RM 'reward' given to the entire generated summary. \n\nSecondly, it proposes the use of human feedback models, which have been shown to significantly outperform strong baselines on English summarization. These models are trained with human feedback and have been found to produce better summaries than much larger policies trained via supervised learning. \n\nLastly, the paper also discusses the use of Zero-Shot (ZS) and Few-Shot (FS) In-Context Learning, and Instruction Finetuning. These methods have their own advantages and limitations, but they provide alternative approaches to address the training issues."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nWe evaluate our model, DensePhrases, on \ufb01ve standard open-domain QA datasets and achieve much bet- ter accuracies than previous phrase retrieval mod- els (Seo et al., 2019; Lee et al., 2020), with 15%\u2013 25% absolute improvement on most datasets. Our model also matches the performance of state-of- the-art retriever-reader models (Guu et al., 2020; Karpukhin et al., 2020). Due to the removal of sparse representations and careful design choices, we further reduce the storage footprint for the full English Wikipedia from 1.5TB to 320GB, as well as drastically improve the throughput.  Finally, we envision that DensePhrases acts as a neural interface for retrieving phrase-level knowl- edge from a large text corpus. To showcase this possibility, we demonstrate that we can directly use DensePhrases for fact extraction, without re- building the phrase storage. With only \ufb01ne-tuning the question encoder on a small number of subject- relation-object triples, we achieve state-of-the-art performance on two slot \ufb01lling tasks (Petroni et al., 2021), using less than 5% of the training data.  2 Background  We \ufb01rst formulate the task of open-domain ques- tion answering for a set of K documents D = {d1, . . . , dK}. We follow the recent work (Chen et al., 2017; Lee et al., 2019) and treat all of English Wikipedia as D, hence K \u2248 5 \u00d7 106. However,  most approaches\u2014including ours\u2014are generic and could be applied to other collections of documents. The task aims to provide an answer \u02c6a for the in- put question q based on D. In this work, we focus on the extractive QA setting, where each answer is a segment of text, or a phrase, that can be found in D. Denote the set of phrases in D as S(D) and each phrase sk \u2208 S(D) consists of contiguous words wstart(k), . . . , wend(k) in its document ddoc(k). In practice, we consider all the phrases up to L = 20 words in D and S(D) comprises a large number of 6 \u00d7 1010 phrases.\n\nfilename: materials_formatted.md\nextension: .md\n\nAl- though it is still behind cross-attention models, the gap has been greatly reduced and serves as a strong starting point for the open-domain QA model.  Open-domain QA. Experimental results on open-domain QA are summarized in Table 3. With- out any sparse representations, DensePhrases out- performs previous phrase retrieval models by a large margin and achieves a 15%\u201325% absolute improvement on all datasets except SQuAD. Train- ing the model of Lee et al. (2020) on Cphrase = {NQ, SQuAD} only increases the result from 14.5% to 16.5% on NQ, demonstrating that it does not suf\ufb01ce to simply add more datasets for train- ing phrase representations. Our performance is also competitive with recent retriever-reader mod- els (Karpukhin et al., 2020), while running much faster during inference (Table 1).  Model  Retriever-reader  Cretr: (Pre-)Training  NQ WQ TREC TQA SQuAD  DrQA (Chen et al., 2017) BERT + BM25 (Lee et al., 2019) ORQA (Lee et al., 2019) REALMNews (Guu et al., 2020) DPR-multi (Karpukhin et al., 2020)  - - {Wiki.\n\nfilename: materials_formatted.md\nextension: .md\n\nBeyond words, dense representations of many different granular- ities of text such as sentences (Le and Mikolov, 2014; Kiros et al., 2015) or documents (Yih et al., 2011) have been explored. While dense phrase rep- resentations have been also studied for statistical machine translation (Cho et al., 2014) or syntactic parsing (Socher et al., 2010), our work focuses on learning dense phrase representations for QA and any other knowledge-intensive tasks where phrases can be easily retrieved by performing MIPS.  This type of dense retrieval has been also stud- ied for sentence and passage retrieval (Humeau et al., 2019; Karpukhin et al., 2020) (see Lin et al., 2020 for recent advances in dense retrieval). While DensePhrases is explicitly designed to retrieve phrases that can be used as an answer to given queries, retrieving phrases also naturally entails re- trieving larger units of text, provided the datastore maintains the mapping between each phrase and the sentence and passage in which it occurs.  Cphrase = {SQuAD}  (cid:55) 12.3 (cid:51) 31.2  11.8 36.3  36.9 50.3  34.6 53.6  Cphrase = {NQ}  (cid:55) 32.6 (cid:51) 40.9  21.1 37.1  32.3 49.7  32.4 49.2  Cphrase = {NQ, SQuAD}  (cid:55) 28.9 (cid:51) 40.9  18.9 37.5  34.9 51.0  31.9 50.7  35.5 39.4  20.7 25.7  33.2 38.0  Table 7: Effect of query-side \ufb01ne-tuning in DensePhrases on each test set. We report EM of each model before (QS = (cid:55)) and after (QS = (cid:51)) the query-side \ufb01ne-tuning.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nHow does the document envision using DensePhrases beyond open-domain QA tasks?[/INST] \nAnswer: The document envisions using DensePhrases as a neural interface for retrieving phrase-level knowledge from a large text corpus. It can be used for fact extraction without having to rebuild the phrase storage. With only fine-tuning the question encoder on a small number of subject-relation-object triples, DensePhrases can achieve state-of-the-art performance on two slot filling tasks, using less than 5% of the training data."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\n2.4 TRAINING DETAILS  Model architecture and pretraining. In our experiments, we use LaMDA-PT, a dense left-to-right, decoder-only transformer language model of 137B parameters (Thoppilan et al., 2022). This model is pretrained on a collection of web documents (including those with computer code), dialog data, and Wikipedia, tokenized into 2.49T BPE tokens with a 32k vocabulary using the SentencePiece library (Kudo & Richardson, 2018). Around 10% of the pretraining data was non-English. Note that LaMDA-PT only has language model pretraining (c.f. LaMDA, which was \ufb01netuned for dialog).  Instruction tuning procedure. FLAN is the instruction-tuned version of LaMDA-PT. Our instruc- tion tuning pipeline mixes all datasets and randomly samples from each dataset. To balance the different sizes of datasets, we limit the number of training examples per dataset to 30k and follow the examples-proportional mixing scheme (Raffel et al., 2020) with a mixing rate maximum of 3k.2 We \ufb01netune all models for 30k gradient steps with a batch size of 8,192 tokens using the Adafactor Optimizer (Shazeer & Stern, 2018) with a learning rate of 3e-5. The input and target sequence lengths used in \ufb01netuning are 1024 and 256, respectively. We use packing (Raffel et al., 2020) to combine multiple training examples into a single sequence, separating inputs from targets using a special EOS token. This instruction tuning takes around 60 hours on a TPUv3 with 128 cores. For all evaluations, we report results on the \ufb01nal checkpoint trained for 30k steps.  3 RESULTS  We evaluate FLAN on natural language inference, reading comprehension, closed-book QA, transla- tion, commonsense reasoning, coreference resolution, and struct-to-text. As described in \u00a72.2, we evaluate on unseen tasks by grouping datasets into task clusters and holding out each cluster for evaluation while instruction tuning on all remaining clusters (i.e., each evaluation task cluster uses a different checkpoint).\n\nfilename: materials_formatted.md\nextension: .md\n\n2  (A) Pretrain\u2013finetune (BERT, T5)Finetune on task AInference on task APretrained LM\u2022Typically requires many task-specific examples \u2022One specialized model for each task(B) Prompting (GPT-3)Inference on task APretrained LMImprove performance via few-shot prompting or prompt engineeringPretrained  LM(C) Instruction tuning (FLAN)Instruction-tune on many tasks:  B, C, D, \u2026Inference on task AInference on unseen taskModel learns to perform many tasks via natural language instructionsPublished as a conference paper at ICLR 2022  2.1 TASKS & TEMPLATES  As creating an instruction tuning dataset with many tasks from scratch would be resource-intensive, we transform existing datasets from the research community into an instructional format. We aggregate 62 text datasets that are publicly available on Tensor\ufb02ow Datasets, including both language understanding and language generation tasks, into a single mixture. Figure 3 shows these datasets\u2014 each dataset is categorized into one of twelve task clusters, for which datasets in a given cluster are of the same task type. Descriptions, sizes, and examples of each dataset are shown in Appendix G.  Figure 3: Datasets and task clusters used in this paper (NLU tasks in blue; NLG tasks in teal).  For each dataset, we manually compose ten unique templates that use natural language instructions to describe the task for that dataset. While most of the ten templates describe the original task, to increase diversity, for each dataset we also include up to three templates that \u201cturned the task around,\u201d (e.g., for sentiment classi\ufb01cation we include templates asking to generate a movie review). We then instruction tune a pretrained language model on the mixture of all datasets, with examples in each dataset formatted via a randomly selected instruction template for that dataset. Figure 4 shows multiple instruction templates for a natural language inference dataset.  Figure 4: Multiple instruction templates describing a natural language inference task.  2.2 EVALUATION SPLITS  We are interested in how FLAN performs on tasks not seen in instruction tuning, and so it is crucial to de\ufb01ne what counts as an unseen task.\n\nfilename: materials_formatted.md\nextension: .md\n\n8  NLIRead. Comp.Closed-Book QACommonsenseCoreferenceTranslationZero-shot FLANFew-shot FLANPerformance2040608054.759.359.660.053.7Struct to text57.231.033.080.080.863.867.439.249.4Task Cluster:# datasets:753423432 training examplesFull training set1000507525Performance after prompt tuningInstruction-tuned modelUntuned model63.878.179.187.4Published as a conference paper at ICLR 2022  The way we ask a model to respond to instructions is similar to QA-based task formulation (Kumar et al., 2016; McCann et al., 2018), which aims to unify NLP tasks by casting them as QA over a context. Though these methods are very similar to ours, they mostly focus on multi-task learning instead of zero-shot learning, and\u2014as noted by Liu et al. (2021)\u2014they are generally not motivated by using existing knowledge in pretrained LMs. Moreover, our work supercedes recent work such as Chai et al. (2020) and Zhong et al. (2021) in terms of both model scale and scope of tasks.  The success of language models has led to nascent research on the ability of models to follow instructions. Most recently, Mishra et al. (2021) \ufb01netune 140M parameter BART on instructions with few-shot exemplars, and evaluate its few-shot abilities on unseen tasks\u2014this is similar to our few-shot instruction tuning result from \u00a74.4. This promising result (as well as one from Ye et al. (2021), which does not emphasize instructions as much) suggests that \ufb01netuning on a collection of tasks improves few-shot performance on unseen tasks, even at a smaller model scale. Sanh et al. (2021) \ufb01netune T5 in a setup similar to ours, \ufb01nding that zero-shot learning can be improved in a model of 11B parameters. At a model scale similar to ours, OpenAI\u2019s InstructGPT models are trained via both \ufb01netuning and reinforcement learning to produce outputs that are more preferred by human raters (Ouyang et al., 2022).\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nHow does the FLAVA model approach multimodality, and what datasets does it use for pretraining?[/INST] \nAnswer: I'm sorry, but the context provided does not contain any information about a model named FLAVA or its approach to multimodality, nor does it mention the datasets used for its pretraining."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nA trigram model is less surprised than a unigram model because it has a better idea of what words might come next, and so it assigns them a higher probability. And the higher the probability, the lower the perplexity (since as Eq. 3.15 showed, perplexity is related inversely to the likelihood of the test sequence according to the model). So a lower perplexity can tell us that a language model is a better predictor of the words in the test set. Note that in computing perplexities, the n-gram modelPmust be constructed without any knowledge of the test set or any prior knowledge of the vocabulary of the test set. Any kind of knowledge of the test set can cause the perplexity to be artificially low. The perplexity of two language models is only comparable if they use identical vocabularies. An (intrinsic) improvement in perplexity does not guarantee an (extrinsic) im- provement in the performance of a language processing task like speech recognition   10 CHAPTER 3 \u2022 N-GRAMLANGUAGEMODELS   or machine translation. Nonetheless, because perplexity often correlates with such improvements, it is commonly used as a quick check on an algorithm. But a model\u2019s improvement in perplexity should always be confirmed by an end-to-end evaluation of a real task before concluding the evaluation of the model.  #### 3.3 Sampling sentences from a language model   One important way to visualize what kind of knowledge a language model embodies sampling is to sample from it.Samplingfrom a distribution means to choose random points according to their likelihood. Thus sampling from a language model\u2014which rep- resents a distribution over sentences\u2014means to generate some sentences, choosing each sentence according to its likelihood as defined by the model. Thus we are more likely to generate sentences that the model thinks have a high probability and less likely to generate sentences that the model thinks have a low probability. This technique of visualizing a language model by sampling was first suggested very early on by Shannon (1951) and Miller and Selfridge (1950). It\u2019s simplest to visualize how this works for the unigram case. Imagine all the words of the English language covering the probability space between 0 and 1, each word covering an interval proportional to its frequency. Fig. 3.3 shows a visualization, using a unigram LM computed from the text of this book.\n\nfilename: materials_formatted.md\nextension: .md\n\nHere\u2019s the perplexity ofWwith a unigram language model (just the geometric mean of the unigram probabilities):   perplexity(W) = N   \u221a \u221a \u221a \u221a \u220fN   i= 1   1 P(wi)   (3.16)   The perplexity ofWcomputed with a bigram language model is still a geometric mean, but now of the bigram probabilities:   perplexity(W) = N   \u221a\u221a \u221a \u221a \u220fN   i= 1   1 P(wi|wi\u2212 1 )   (3.17)   Note that because of the inverse in Eq. 3.15, the higher the conditional probabil- ity of the word sequence, the lower the perplexity. Thus, minimizing perplexity is equivalent to maximizing the test set probability according to the language model. What we generally use for word sequence in Eq. 3.15 or Eq. 3.17 is the entire se- quence of words in some test set. Since this sequence will cross many sentence boundaries, we need to include the begin- and end-sentence markers<s>and</s> in the probability computation. We also need to include the end-of-sentence marker </s>(but not the beginning-of-sentence marker<s>) in the total count of word to- kensN. There is another way to think about perplexity: as theweighted average branch- ing factorof a language. The branching factor of a language is the number of possi- ble next words that can follow any word. Consider the task of recognizing the digits    3.2 \u2022 EVALUATINGLANGUAGEMODELS 9  in English (zero, one, two,..., nine), given that (both in some training set and in some test set) each of the 10 digits occurs with equal probabilityP= 101. The perplexity of this mini-language is in fact 10. To see that, imagine a test string of digits of length N, and assume that in the training set all the digits occurred with equal probability. By Eq.\n\nfilename: materials_formatted.md\nextension: .md\n\nBy Eq. 3.15, the perplexity will be   perplexity(W) =P(w 1 w 2 ...wN)\u2212 N^1   = (   1 10   N )\u2212 N^1   =   1 10   \u2212 1   = 10 (3.18)  But suppose that the number zero is really frequent and occurs far more often than other numbers. Let\u2019s say that 0 occur 91 times in the training set, and each of the other digits occurred 1 time each. Now we see the following test set: 0 0 0 0 0 3 0 0 0  0. We should expect the perplexity of this test set to be lower since most of the time the next number will be zero, which is very predictable, i.e. has a high probability. Thus, although the branching factor is still 10, the perplexity orweightedbranching factor is smaller. We leave this exact calculation as exercise 3.12.     We see in Section 3.8 that perplexity is also closely related to the information- theoretic notion of entropy.     We mentioned above that perplexity is a function of both the text and the lan- guage model: given a textW, different language models will have different perplex- ities. Because of this, perplexity can be used to compare different n-gram models. Let\u2019s look at an example, in which we trained unigram, bigram, and trigram gram- mars on 38 million words (including start-of-sentence tokens) from theWall Street Journal, using a 19,979 word vocabulary. We then computed the perplexity of each of these models on a test set of 1.5 million words, using Eq. 3.16 for unigrams, Eq. 3.17 for bigrams, and the corresponding equation for trigrams. The table below shows the perplexity of a 1.5 million word WSJ test set according to each of these grammars.  Unigram Bigram Trigram Perplexity 962 170 109 As we see above, the more information the n-gram gives us about the word sequence, the higher the probability the n-gram will assign to the string.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat role does perplexity play in evaluating language models?[/INST] \nAnswer: Perplexity is a measure used to evaluate language models. It is inversely related to the likelihood of the test sequence according to the model, meaning that a lower perplexity indicates that a language model is a better predictor of the words in the test set. It's important to note that an improvement in perplexity doesn't necessarily guarantee an improvement in the performance of a language processing task. However, because perplexity often correlates with such improvements, it is commonly used as a quick check on an algorithm. It's also used to compare different n-gram models. For instance, a trigram model typically has a lower perplexity than a unigram model because it has a better idea of what words might come next, thus assigning them a higher probability."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nUnlike recent language repre- sentation models (Peters et al., 2018a; Rad- ford et al., 2018), BERT is designed to pre- train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a re- sult, the pre-trained BERT model can be \ufb01ne- tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task- speci\ufb01c architecture modi\ufb01cations.  BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art re- sults on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answer- ing Test F1 to 93.2 (1.5 point absolute im- provement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).  1  Introduction  Language model pre-training has been shown to be effective for improving many natural language processing tasks (Dai and Le, 2015; Peters et al., 2018a; Radford et al., 2018; Howard and Ruder, 2018). These include sentence-level tasks such as natural language inference (Bowman et al., 2015; Williams et al., 2018) and paraphrasing (Dolan and Brockett, 2005), which aim to predict the re- lationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce \ufb01ne-grained output at the token level (Tjong Kim Sang and De Meulder, 2003; Rajpurkar et al., 2016).  There are two existing strategies for apply- ing pre-trained language representations to down- stream tasks: feature-based and \ufb01ne-tuning. The feature-based approach, such as ELMo (Peters et al., 2018a), uses task-speci\ufb01c architectures that include the pre-trained representations as addi- tional features.\n\nfilename: materials_formatted.md\nextension: .md\n\nThe core argument of this work is that the bi-directionality and the two pre- training tasks presented in Section 3.1 account for the majority of the empirical improvements, but we do note that there are several other differences between how BERT and GPT were trained:  \u2022 GPT is trained on the BooksCorpus (800M words); BERT is trained on the BooksCor- pus (800M words) and Wikipedia (2,500M words).  \u2022 GPT uses a sentence separator ([SEP]) and classi\ufb01er token ([CLS]) which are only in- troduced at \ufb01ne-tuning time; BERT learns [SEP], [CLS] and sentence A/B embed- dings during pre-training.  \u2022 GPT was trained for 1M steps with a batch size of 32,000 words; BERT was trained for 1M steps with a batch size of 128,000 words.  \u2022 GPT used the same learning rate of 5e-5 for all \ufb01ne-tuning experiments; BERT chooses a task-speci\ufb01c \ufb01ne-tuning learning rate which performs the best on the development set.  To isolate the effect of these differences, we per- form ablation experiments in Section 5.1 which demonstrate that the majority of the improvements are in fact coming from the two pre-training tasks and the bidirectionality they enable.  A.5  Illustrations of Fine-tuning on Different Tasks  The illustration of \ufb01ne-tuning BERT on different tasks can be seen in Figure 4. Our task-speci\ufb01c models are formed by incorporating BERT with one additional output layer, so a minimal num- ber of parameters need to be learned from scratch. Among the tasks, (a) and (b) are sequence-level tasks while (c) and (d) are token-level tasks. In the \ufb01gure, E represents the input embedding, Ti represents the contextual representation of token i, [CLS] is the special symbol for classi\ufb01cation out- put, and [SEP] is the special symbol to separate non-consecutive token sequences.  B Detailed Experimental Setup  B.1 Detailed Descriptions for the GLUE  Benchmark Experiments.  Our GLUE results in Table1 are obtained https://gluebenchmark.com/ from leaderboard https://blog.\n\nfilename: materials_formatted.md\nextension: .md\n\ninternational conference on computer vision, pages 19\u201327.  Appendix for \u201cBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\u201d  We organize the appendix into three sections:  \u2022 Additional implementation details for BERT  are presented in Appendix A;  \u2022 Additional details for our experiments are  presented in Appendix B; and  \u2022 Additional ablation studies are presented in  Appendix C.  We present additional ablation studies for BERT including:  \u2013 Effect of Number of Training Steps; and \u2013 Ablation for Different Masking Proce-  dures.  A Additional Details for BERT  A.1  Illustration of the Pre-training Tasks  We provide examples of the pre-training tasks in the following.  Masked LM and the Masking Procedure As- suming the unlabeled sentence is my dog is hairy, and during the random masking procedure we chose the 4-th token (which corresponding to hairy), our masking procedure can be further il- lustrated by  \u2022 80% of the time: Replace the word with the [MASK] token, e.g., my dog is hairy \u2192 my dog is [MASK]  \u2022 10% of the time: Replace the word with a random word, e.g., my dog is hairy \u2192 my dog is apple  \u2022 10% of  the time: Keep the word un- changed, e.g., my dog is hairy \u2192 my dog is hairy. The purpose of this is to bias the representation towards the actual observed word.  The advantage of this procedure is that the Transformer encoder does not know which words it will be asked to predict or which have been re- placed by random words, so it is forced to keep a distributional contextual representation of ev- ery input token. Additionally, because random replacement only occurs for 1.5% of all tokens (i.e., 10% of 15%), this does not seem to harm the model\u2019s language understanding capability. In Section C.2, we evaluate the impact this proce- dure.  Compared to standard langauge model training, the masked LM only make predictions on 15% of tokens in each batch, which suggests that more pre-training steps may be required for the model  Figure 3: Differences in pre-training model architectures. BERT uses a bidirectional Transformer. OpenAI GPT uses a left-to-right Transformer.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nHow many natural language processing tasks did BERT advance the state-of-the-art results for, as mentioned in the paper?[/INST] \nAnswer: BERT advanced the state-of-the-art results for eleven natural language processing tasks."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nWhereas prompting work such as GPT-3 uses prompt engineering to write prompts that intentionally mimic text that is likely to be seen during pretraining (e.g., for MultiRC GPT-3 tries a prompt that mimics a test with an answer key), we hope that \ufb01netuning models to respond to natural language instructions instead of completing a sentence will make such large models more accessible to non-technical users.  D.4 FINETUNING LARGE LANGUAGE MODELS  Finetuning pretrained language models is a well-established method in NLP, with much of the work so far occurring on models in the range of 100M to 10B parameters (Dai & Le, 2015; Devlin et al., 2019; Raffel et al., 2020; Lewis et al., 2020, inter alia). For models of O(100B) parameters, recent work has \ufb01netuned task-speci\ufb01c models for program synthesis (Austin et al., 2021; Chen et al., 2021), summarization (Wu et al., 2021), as well as improved bias and fairness behavior (Solaiman & Dennison, 2021). In addition to the traditional \u201cdense\u201d models, sparse mixture of experts (MoE) models of up to more than 1T parameters have been trained and \ufb01netuned (Lepikhin et al., 2020; Fedus  28  Published as a conference paper at ICLR 2022  et al., 2021). Compared with this prior work that \ufb01netunes and evaluates on the same downstream task, our setup studies the effect of instruction tuning on ability to perform unseen tasks.  D.5 MULTI-TASK QUESTION ANSWERING  The instructions we use for instruction tuning are similar to QA-based task formulation research, which aims to unify NLP tasks by casting them as question-answering over a context. For instance, McCann et al. (2018) cast ten NLP tasks as QA and train a model on a collection of tasks formulated with natural language prompts; they report transfer learning gains on \ufb01netuning tasks as well as zero-shot domain adaptation results on SNLI (Bowman et al., 2015) and Amazon/Yelp Reviews (Kotzias et al., 2015). While McCann et al.\n\nfilename: materials_formatted.md\nextension: .md\n\n6 DISCUSSION  Our paper has explored a simple question in zero-shot prompting: does \ufb01netuning a model on a collection of tasks phrased as instructions improve its performance on unseen tasks? We operationalize this question via instruction tuning, a simple method that combines appealing aspects of both the pretrain\u2013\ufb01netune and prompting paradigms. Our instruction-tuned model, FLAN, improves performance against an untuned model and surpasses zero-shot GPT-3 on the majority of tasks that we evaluate on. Ablation studies reveal that performance on unseen tasks improves with the number of instruction tuning task clusters, and, interestingly, that performance improvements from instruction tuning emerge only with suf\ufb01cient model scale. Moreover, instruction tuning can be combined with other prompting methods such as few-shot prompting and prompt tuning.  The diverse capabilities of language models at scale have drawn attention to the tradeoffs between specialist models (one model per task) and generalist models (one model for many tasks; Arivazhagan et al., 2019; Pratap et al., 2020), for which our study has potential implications. Although one might expect labeled data to have the most natural role in improving specialist models, instruction tuning demonstrates how labeled data can be used to help large language models perform many, unseen tasks. In other words, the positive effect of instruction tuning on cross-task generalization shows that task-speci\ufb01c training is complementary to general language modeling and motivates further research on generalist models.  As for limitations of our study, there is a degree of subjectivity in assigning tasks to clusters (though we try to use accepted categorizations in the literature), and we only explore the use of relatively short instructions of typically a single sentence (c.f. detailed instructions given to crowd-workers). A limitation for our evaluation is that individual examples might have appeared in the models\u2019 pretraining data, which includes web documents, though in post-hoc analysis (Appendix C) we do not \ufb01nd any evidence that data overlap substantially impacted the results. Finally, the scale of FLAN 137B makes it costly to serve.\n\nfilename: materials_formatted.md\nextension: .md\n\n-? 3. Reinforcement Learning from Human Feedback (RLHF)  49  4. What\u2019s next?  #### SEP 775 Lecture Plan: From Language Models to Assistants   #### Limitations of instruction finetuning?  - One limitation of instruction finetuning is obvious: it\u2019s expensive to collect ground-     truth data for tasks. - But there are other, subtler limitations too. Can you think of any? - Problem 1 : tasks like open-ended creative generation have no right answer.     - _Write me a story about a dog and her pet grasshopper._ - Problem 2 : language modeling penalizes all token-level mistakes equally, but some     errors are worse than others. - Even with instruction finetuning _,_ there     a mismatch between the LM     objective and the objective of     \u201csatisfy human preferences\u201d! - Can we explicitly attempt to satisfy     human preferences?   LM   Avatar is a fantasy TV show   is  50   a   adventure fantasy   musical TV show END   1. Zero-Shot (ZS) and Few-Shot (FS) In-Context Learning     + No finetuning needed, prompt engineering (e.g. CoT) can improve performance     - Limits to what you can fit in context     - Complex tasks will probably need gradient steps 2. Instruction finetuning     + Simple and straightforward, generalize to unseen tasks     - Collecting demonstrations for so many tasks is expensive     - Mismatch between LM objective and human preferences 3. Reinforcement Learning from Human Feedback (RLHF)  51  #### SEP 775 Lecture Plan: From Language Models to Assistants  4. What\u2019s next?   1. Zero-Shot (ZS) and Few-Shot (FS) In-Context Learning     + No finetuning needed, prompt engineering (e.g. CoT) can improve performance     - Limits to what you can fit in context     - Complex tasks will probably need gradient steps 2. Instruction finetuning     + Simple and straightforward, generalize to unseen tasks     - Collecting demonstrations for so many tasks is expensive     - Mismatch between LM objective and human preferences 3.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat is instruction finetuning in the context of language models?[/INST] \nAnswer: Instruction finetuning in the context of language models is a method that involves training a model on a collection of tasks phrased as instructions. The goal is to improve the model's performance on unseen tasks. This method combines aspects of both the pretrain-finetune and prompting paradigms. It has been found that performance on unseen tasks improves with the number of instruction tuning task clusters and that performance improvements from instruction tuning emerge only with sufficient model scale. Instruction tuning can also be combined with other prompting methods such as few-shot prompting and prompt tuning."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nThere are indeed other types of regularization that are sometimes used, such asL 1 regularization, which sums over the absolute values (rather than squares) of parameter elements \u2013 however, this is less commonly applied in practice since it leads to sparsity of parameter weights. In the next section, we discussdropout, which effectively acts as another form of regularization by randomly dropping (i.e. setting to zero) neurons in the forward pass.   ## neural networks,backpropagation 12  ### 2. 3 Dropout  Dropout is a powerful technique for regularization, first introduced by Srivastava et al. inDropout: A Simple Way to Prevent Neural Net- works from Overfitting. The idea is simple yet effective \u2013 during train- ing, we will randomly \u201cdrop\u201d with some probability( 1 \u2212p)a subset of neurons during each forward/backward pass (or equivalently, we will keep alive each neuron with a probabilityp). Then, during testing, we will use the full network to compute our predictions. The result is that the network typically learns more meaningful informa- tion from the data, is less likely to overfit, and usually obtains higher performance overall on the task at hand. One intuitive reason why this technique should be so effective is that what dropout is doing is essentially doing is training exponentially many smaller networks at once and averaging over their predictions.   Dropout applied to an artificial neural network. Image credits to Srivastava et al.  In practice, the way we introduce dropout is that we take the out- puthof each layer of neurons, and keep each neuron with prob- abilityp, and else set it to 0. Then, during back-propagation, we only pass gradients through neurons that were kept alive during the forward pass. Finally, during testing, we compute the forward pass usingallof the neurons in the network. However, a key sub- tlety is that in order for dropout to work effectively, the expected output of a neuron during testing should be approximately the same as it was during training \u2013 else the magnitude of the outputs could be radically different, and the behavior of the network is no longer well-defined.\n\nfilename: materials_formatted.md\nextension: .md\n\nWe use Adam with learning rate of 1e-4, \u03b21 = 0.9, \u03b22 = 0.999, L2 weight decay of 0.01, learning rate warmup over the \ufb01rst 10,000 steps, and linear decay of the learning rate. We use a dropout prob- ability of 0.1 on all layers. We use a gelu acti- vation (Hendrycks and Gimpel, 2016) rather than the standard relu, following OpenAI GPT. The training loss is the sum of the mean masked LM likelihood and the mean next sentence prediction likelihood.  Training of BERTBASE was performed on 4 Cloud TPUs in Pod con\ufb01guration (16 TPU chips total).13 Training of BERTLARGE was performed on 16 Cloud TPUs (64 TPU chips total). Each pre- training took 4 days to complete.  Longer sequences are disproportionately expen- sive because attention is quadratic to the sequence length. To speed up pretraing in our experiments, we pre-train the model with sequence length of 128 for 90% of the steps. Then, we train the rest 10% of the steps of sequence of 512 to learn the positional embeddings.  A.3 Fine-tuning Procedure  For \ufb01ne-tuning, most model hyperparameters are the same as in pre-training, with the exception of the batch size, learning rate, and number of train- ing epochs. The dropout probability was always kept at 0.1.\n\nfilename: materials_formatted.md\nextension: .md\n\nThe collapse could be due to a less-explicit requirement for factual knowledge in some tasks, or the longer target sequences, which could result in less informative gradients for the retriever. Perez et al.[46]also found spurious retrieval results when optimizing a retrieval component in order to improve performance on downstream tasks.  ### I Number of instances per dataset  The number of training, development and test datapoints in each of our datasets is shown in Table 7.      \n\n# Layer Normalization:  Layer Normalization  Jimmy Lei Ba University of Toronto jimmy@psi.toronto.edu  Jamie Ryan Kiros University of Toronto rkiros@cs.toronto.edu  Geoffrey E. Hinton University of Toronto and Google Inc. hinton@cs.toronto.edu  Abstract  Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This signi\ufb01cantly reduces the training time in feed- forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural net- works. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empiri- cally, we show that layer normalization can substantially reduce the training time compared with previously published techniques.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat is dropout, and how does it help in training neural networks?[/INST] \nAnswer: Dropout is a powerful technique for regularization in neural networks. During training, a subset of neurons is randomly \"dropped\" or set to zero with some probability during each forward/backward pass. This means that each neuron is kept alive with a certain probability. During testing, the full network is used to compute predictions. This technique helps the network learn more meaningful information from the data, reduces the likelihood of overfitting, and usually results in higher overall performance. The effectiveness of dropout can be intuitively understood as training many smaller networks at once and averaging their predictions. In practice, dropout is introduced by keeping each neuron with a certain probability and setting the rest to zero. During back-propagation, gradients are only passed through neurons that were kept alive during the forward pass. During testing, all neurons in the network are used. However, for dropout to work effectively, the expected output of a neuron during testing should be approximately the same as it was during training."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nThe same models will also serve to assign a probability to an entire sentence. Such a model, for example, could predict that the following sequence has a much higher probability of appearing in a text: all of a sudden I notice three guys standing on the sidewalk than does this same set of words in a different order:   on guys all I of notice sidewalk three a sudden standing the   Why would you want to predict upcoming words, or assign probabilities to sen- tences? Probabilities are essential in any task in which we have to identify words in noisy, ambiguous input, likespeech recognition. For a speech recognizer to realize that you saidI will be back soonishand notI will be bassoon dish, it helps to know thatback soonishis a much more probable sequence thanbassoon dish. For writing tools likespelling correctionorgrammatical error correction, we need to find and correct errors in writing likeTheir are two midterms, in whichTherewas mistyped asTheir, orEverything has improve, in whichimproveshould have beenimproved. The phraseThere arewill be much more probable thanTheir are, andhas improved thanhas improve, allowing us to help users by detecting and correcting these errors. Assigning probabilities to sequences of words is also essential inmachine trans- lation. Suppose we are translating a Chinese source sentence: \u4ed6 \u5411 \u8bb0\u8005 \u4ecb\u7ecd\u4e86 \u4e3b\u8981 \u5185\u5bb9 He to reporters introduced main content As part of the process we might have built the following set of potential rough English translations: he introduced reporters to the main contents of the statement he briefed to reporters the main contents of the statement he briefed reporters on the main contents of the statement   2 CHAPTER 3 \u2022 N-GRAMLANGUAGEMODELS   A probabilistic model of word sequences could suggest thatbriefed reporters on is a more probable English phrase thanbriefed to reporters(which has an awkward toafterbriefed) orintroduced reporters to(which uses a verb that is less fluent English in this context), allowing us to correctly select the boldfaced sentence above. Probabilities are also important foraugmentative and alternative communi- AAC cationsystems (Trnka et al. 2007, Kane et al. 2017).\n\nfilename: materials_formatted.md\nextension: .md\n\nWe found that mean human accuracy at detecting the intentionally bad longer articles from the control model was \u223c 88%, while mean human accuracy at detecting the longer articles that were produced by GPT-3 175B was still barely above chance at \u223c 52% (see Table 3.12). This indicates that, for news articles that are around 500 words long, GPT-3 continues to produce articles that humans \ufb01nd dif\ufb01cult to distinguish from human written news articles.  3.9.5 Learning and Using Novel Words  A task studied in developmental linguistics [CB78] is the ability to learn and utilize new words, for example using a word in a sentence after seeing it de\ufb01ned only once, or conversely inferring a word\u2019s meaning from only one usage. Here we qualitatively test GPT-3\u2019s ability to do the former. Speci\ufb01cally, we give GPT-3 the de\ufb01nition of a nonexistent word, such as \u201cGigamuru\u201d, and then ask it to use it in a sentence. We provide one to \ufb01ve previous examples of a (separate)  5We use a two-sample Student\u2019s T-Test to test for signi\ufb01cant difference between the means of the participant accuracies of each  model and the control model and report the normalized difference in the means (as the t-statistic) and the p-value.  6If a model consistently produces texts that are more impressive than human articles, it is possible that human performance on  this task would drop below 50%. Indeed, many individual participants scored below 50% on this task.  7Additional non-news samples can be found in Appendix F.  26  Figure 3.13: People\u2019s ability to identify whether news articles are model-generated (measured by the ratio of correct assignments to non-neutral assignments) decreases as model size increases. Accuracy on the outputs on the deliberately- bad control model (an unconditioned GPT-3 Small model with higher output randomness) is indicated with the dashed line at the top, and the random chance (50%) is indicated with the dashed line at the bottom. Line of best \ufb01t is a power law with 95% con\ufb01dence intervals.\n\nfilename: materials_formatted.md\nextension: .md\n\nWe choose a random value between 0 and 1, find that point on the probability line, and print the word whose interval includes this chosen value. We continue choosing random numbers and generating words until we randomly generate the sentence-final token</s>.   0 1   0.   the   .   0.   of 0.   a 0.   to in   .09.11.13.  ##### ...   however (p=.0003)   polyphonic p=.  ### 0. ...   .66.  ##### ...   Figure 3.3 A visualization of the sampling distribution for sampling sentences by repeat- edly sampling unigrams. The blue bar represents the relative frequency of each word (we\u2019ve ordered them from most frequent to least frequent, but the choice of order is arbitrary). The number line shows the cumulative probabilities. If we choose a random number between 0 and 1, it will fall in an interval corresponding to some word. The expectation for the random number to fall in the larger intervals of one of the frequent words (the,of,a) is much higher than in the smaller interval of one of the rare words (polyphonic).   We can use the same technique to generate bigrams by first generating a ran- dom bigram that starts with<s>(according to its bigram probability). Let\u2019s say the second word of that bigram isw. We next choose a random bigram starting withw (again, drawn according to its bigram probability), and so on.  #### 3.4 Generalization and Zeros   The n-gram model, like many statistical models, is dependent on the training corpus. One implication of this is that the probabilities often encode specific facts about a    3.4 \u2022 GENERALIZATION ANDZEROS 11   given training corpus. Another implication is that n-grams do a better and better job of modeling the training corpus as we increase the value ofN. We can use the sampling method from the prior section to visualize both of these facts! To give an intuition for the increasing power of higher-order n-grams, Fig. 3.4 shows random sentences generated from unigram, bigram, trigram, and 4- gram models trained on Shakespeare\u2019s works.  ## 1   \u2013To him swallowed confess hear both. Which.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat is the goal of predicting the next few words someone is going to say, as discussed in the paper?[/INST] \nAnswer: The goal of predicting the next few words someone is going to say is to assign probabilities to sentences, which is essential in tasks where we need to identify words in noisy, ambiguous input, such as speech recognition. It helps a speech recognizer differentiate between similar sounding phrases by knowing which sequence of words is more probable. This concept is also useful for writing tools like spelling correction or grammatical error correction, where it helps in detecting and correcting errors. Additionally, it plays a crucial role in machine translation, where it assists in selecting the most probable and contextually appropriate translation."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nOne can also extend our BIDAF model to incorporate multiple hops.  Visual question answering. The task of question answering has also gained a lot of interest in the computer vision community. Early works on visual question answering (VQA) involved encoding the question using an RNN, encoding the image using a CNN and combining them to answer the question (Antol et al., 2015; Malinowski et al., 2015). Attention mechanisms have also been suc- cessfully employed for the VQA task and can be broadly clustered based on the granularity of their attention and the approach to construct the attention matrix. At the coarse level of granularity, the question attends to different patches in the image (Zhu et al., 2016; Xiong et al., 2016a). At a \ufb01ner level, each question word attends to each image patch and the highest attention value for each spatial location (Xu & Saenko, 2016) is adopted. A hybrid approach is to combine questions representa- tions at multiple levels of granularity (unigrams, bigrams, trigrams) (Yang et al., 2015). Several approaches to constructing the attention matrix have been used including element-wise product, element-wise sum, concatenation and Multimodal Compact Bilinear Pooling (Fukui et al., 2016).  Lu et al. (2016) have recently shown that in addition to attending from the question to image patches, attending from the image back to the question words provides an improvement on the VQA task. This \ufb01nding in the visual domain is consistent with our \ufb01nding in the language domain, where our bi-directional attention between the query and context provides improved results. Their model, however, uses the attention weights directly in the output layer and does not take advantage of the attention \ufb02ow to the modeling layer.  4 QUESTION ANSWERING EXPERIMENTS  In this section, we evaluate our model on the task of question answering using the recently released SQuAD (Rajpurkar et al., 2016), which has gained a huge attention over a few months. In the next section, we evaluate our model on the task of cloze-style reading comprehension.  Dataset. SQuAD is a machine comprehension dataset on a large set of Wikipedia articles, with more than 100,000 questions.\n\nfilename: materials_formatted.md\nextension: .md\n\nCaiming Xiong, Stephen Merity, and Richard Socher. Dynamic memory networks for visual and  textual question answering. In ICML, 2016a.  Caiming Xiong, Victor Zhong, and Richard Socher. Dynamic coattention networks for question  answering. arXiv preprint arXiv:1611.01604, 2016b.  Huijuan Xu and Kate Saenko. Ask, attend and answer: Exploring question-guided spatial attention  for visual question answering. In ECCV, 2016.  Zhilin Yang, Bhuwan Dhingra, Ye Yuan, Junjie Hu, William W Cohen, and Ruslan Salakhut- dinov. Words or characters? \ufb01ne-grained gating for reading comprehension. arXiv preprint arXiv:1611.01724, 2016.  Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and Alex Smola. Stacked attention networks  for image question answering. arXiv preprint arXiv:1511.02274, 2015.  Yang Yu, Wei Zhang, Kazi Hasan, Mo Yu, Bing Xiang, and Bowen Zhou. End-to-end reading comprehension with dynamic answer chunk ranking. arXiv preprint arXiv:1610.09996, 2016.  Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701,  2012.  Yuke Zhu, Oliver Groth, Michael S. Bernstein, and Li Fei-Fei. Visual7w: Grounded question an-  swering in images. In CVPR, 2016.  11  Published as a conference paper at ICLR 2017  A ERROR ANALYSIS  Table 4 summarizes the modes of errors by BIDAF and shows examples for each category of error in SQuAD.\n\nfilename: materials_formatted.md\nextension: .md\n\nMore recently, Rajpurkar et al. (2016) released the Stanford Question Answering (SQuAD) dataset with over 100,000 questions. We evaluate the performance of our comprehension system on both SQuAD and CNN/DailyMail datasets.  Previous works in end-to-end machine comprehension use attention mechanisms in three distinct ways. The \ufb01rst group (largely inspired by Bahdanau et al. (2015)) uses a dynamic attention mech- anism, in which the attention weights are updated dynamically given the query and the context as well as the previous attention. Hermann et al. (2015) argue that the dynamic attention model per- forms better than using a single \ufb01xed query vector to attend on context words on CNN & DailyMail datasets. Chen et al. (2016) show that simply using bilinear term for computing the attention weights in the same model drastically improves the accuracy. Wang & Jiang (2016) reverse the direction of the attention (attending on query words as the context RNN progresses) for SQuAD. In contrast to these models, BIDAF uses a memory-less attention mechanism.  The second group computes the attention weights once, which are then fed into an output layer for \ufb01nal prediction (e.g., Kadlec et al. (2016)). Attention-over-attention model (Cui et al., 2016) uses a 2D similarity matrix between the query and context words (similar to Equation 1) to compute the weighted average of query-to-context attention. In contrast to these models, BIDAF does not summarize the two modalities in the attention layer and instead lets the attention vectors \ufb02ow into the modeling (RNN) layer.  The third group (considered as variants of Memory Network (Weston et al., 2015)) repeats comput- ing an attention vector between the query and the context through multiple layers, typically referred to as multi-hop (Sordoni et al., 2016; Dhingra et al., 2016). Shen et al. (2016) combine Memory Networks with Reinforcement Learning in order to dynamically control the number of hops. One can also extend our BIDAF model to incorporate multiple hops.  Visual question answering.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhy is the Visual Question Answering (VQA) task considered dominant in vision and language research?[/INST] \nAnswer: The Visual Question Answering (VQA) task is considered dominant in vision and language research because it combines elements of both fields. It involves encoding a question using a Recurrent Neural Network (RNN) and encoding an image using a Convolutional Neural Network (CNN), then combining these encodings to answer the question. Attention mechanisms have also been successfully employed for the VQA task, with different approaches to constructing the attention matrix and varying levels of granularity in attention. This combination of visual and language processing makes VQA a complex and interesting problem in the research field."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\n3.1 Experimental Setup  We explore chain-of-thought prompting for various language models on multiple benchmarks.  Benchmarks. We consider the following \ufb01ve math word problem benchmarks: (1) the GSM8K benchmark of math word problems (Cobbe et al., 2021), (2) the SVAMP dataset of math word problems with varying structures (Patel et al., 2021), (3) the ASDiv dataset of diverse math word problems (Miao et al., 2020), (4) the AQuA dataset of algebraic word problems, and (5) the MAWPS benchmark (Koncel-Kedziorski et al., 2016). Example problems are given in Appendix Table 12.  Standard prompting. For the baseline, we consider standard few-shot prompting, popularized by Brown et al. (2020), in which a language model is given in-context exemplars of input\u2013output pairs before outputting a prediction for a test-time example. Exemplars are formatted as questions and answers. The model gives the answer directly, as shown in Figure 1 (left).  Chain-of-thought prompting. Our proposed approach is to augment each exemplar in few-shot prompting with a chain of thought for an associated answer, as illustrated in Figure 1 (right). As most of the datasets only have an evaluation split, we manually composed a set of eight few-shot exemplars with chains of thought for prompting\u2014Figure 1 (right) shows one chain of thought exemplar, and the full set of exemplars is given in Appendix Table 20. (These particular exemplars did not undergo prompt engineering; robustness is studied in Section 3.4 and Appendix A.2.) To investigate whether chain-of-thought prompting in this form can successfully elicit successful reasoning across a range of  3  Figure 3: Examples of (cid:104)input, chain of thought, output(cid:105) triples for arithmetic, commonsense, and symbolic reasoning benchmarks. Chains of thought are highlighted. Full prompts in Appendix G.  math word problems, we used this single set of eight chain of thought exemplars for all benchmarks except AQuA, which is multiple choice instead of free response.\n\nfilename: materials_formatted.md\nextension: .md\n\nFigure 4 shows how PaLM 540B uses chain-of- thought prompting to achieve new state of the art on GSM8K, SVAMP, and MAWPS (though note that standard prompting already passed the prior best for SVAMP). On the other two datasets, AQuA and ASDiv, PaLM with chain-of-thought prompting reaches within 2% of the state of the art (Appendix Table 2).  Standard prompting Chain-of-thought prompting Prior supervised best  LaMDA  GPT  PaLM  K 8 M S G  )  %  (  e t a r  e v l o s  P M A V S  )  %  (  e t a r  e v l o s  60  40  20  0  80  60  40  20  0  100  )  %  (  75  e t a r  50  S P W A M  8  0  25  e v l o s  62 540  0.4 8 137  0.4 7 175  Model scale (# parameters in billions)  To better understand why chain-of-thought prompting works, we manually examined model- generated chains of thought by LaMDA 137B for GSM8K. Of 50 random examples where the model returned the correct \ufb01nal answer, all of the generated chains of thought were also log- ically and mathematically correct except two that coincidentally arrived at the correct answer (see Appendix D.1, and Table 8 for examples of correct model-generated chains of thought). We also randomly examined 50 random sam- ples for which the model gave the wrong answer. The summary of this analysis is that 46% of the chains of thought were almost correct, barring minor mistakes (calculator error, symbol map- ping error, or one reasoning step missing), and that the other 54% of the chains of thought had major errors in semantic understanding or coherence (see Appendix D.2). To provide a small insight into why scaling improves chain-of-thought reasoning ability, we performed a similar analysis of errors made by PaLM 62B and whether those errors were \ufb01xed by scaling to PaLM 540B.\n\nfilename: materials_formatted.md\nextension: .md\n\nAlthough in this paper we focused on multi-step reasoning tasks (arithmetic, commonsense, and symbolic), chain-of-thought prompting can potentially be applied to any task for which humans use a \u201cchain of thought\u201d to solve (at least in principle). We leave the empirical evaluation of chain-of-thought prompting on such diverse tasks (e.g., machine translation, etc.) to future work.  A.4 Why is prompting with the equation only not enough for some arithmetic reasoning  datasets?  Prompting with the equation only as an intermediate step does help on many datasets, especially when the datasets only require a few reasoning steps (SVAMP, ASDiv, MAWPS). For GSM8K, however, using the equation only did not improve performance substantially. Based on qualitative analysis, we believe that these questions are too semantically challenging for the model to directly translate them into a math equation. Consider this example from LaMDA 137B:  QUESTION: Mike plays ping pong for 40 minutes. In the \ufb01rst 20 minutes, he scores 4 points. In the second 20 minutes, he scores 25% more points. How many total points did he score? EQUATION ONLY (WRONG ANSWER): (4 + 20 * 0.25) = 6. The answer is 6. CHAIN OF THOUGHT (CORRECT): Mike played ping pong for 40 minutes. In the \ufb01rst 20 minutes, he scored 4 points. In the second 20 minutes, he scored 25% more points. So he scored 25% more in the second 20 minutes. 4 x 1.25 = 5. So he scored 5 points in the second 20 minutes. So he scored 9 points in total. The answer is 9.  It is hard for the model to directly translate all of the semantics into a single equation, but chain of thought allows it to better reason about each part of the question via intermediate steps in natural language.  19  B All Experimental Results  This section contains tables for experimental results for varying models and model sizes, on all benchmarks, for standard prompting vs. chain-of-thought prompting.  For the arithmetic reasoning benchmarks, some chains of thought (along with the equations produced) were correct, except the model performed an arithmetic operation incorrectly.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat datasets were used to evaluate the chain-of-thought prompting method?[/INST] \nAnswer: The chain-of-thought prompting method was evaluated using five math word problem benchmarks: the GSM8K benchmark, the SVAMP dataset, the ASDiv dataset, the AQuA dataset of algebraic word problems, and the MAWPS benchmark."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nFollowing the classical \u201cback-propagation\u201d derivations (LeCun, 1985;Rumelhart et al., 1986) and the modular approach shown in Bottou (1991), any feed-forward neural network withLlayers, like the ones shown in Figure 1 and Figure 2, can be seen as a composition of functions f\u03b8l(\u00b7), corresponding to each layerl:   f\u03b8(\u00b7) =f\u03b8L(f\u03b8L\u2212^1 (...f\u03b8^1 (\u00b7)...))  Partitioning the parameters of the network with respect to each layers 1\u2264l\u2264L, we write:   \u03b8= (\u03b8^1 , ...,\u03b8l, ...,\u03b8L).  We are now interested in computing the gradients of the cost with respect to each\u03b8l. Applying the chain rule (generalized to vectors) we obtain the classical backpropagation recursion:   \u2202C \u2202\u03b8l   =   \u2202f\u03b8l \u2202\u03b8l  ##### \u2202C   \u2202f\u03b8l   (19)  ##### \u2202C   \u2202f\u03b8l\u2212^1   =   \u2202f\u03b8l \u2202f\u03b8l\u2212^1  ##### \u2202C   \u2202f\u03b8l  ##### . (20)  In other words, we first initialize the recursion by computing the gradient ofthe cost with respect to the last layer output\u2202C/\u2202f\u03b8L. Then each layerlcomputes the gradient respect to its own parameters    NATURALLANGUAGEPROCESSING(ALMOST)FROMSCRATCH  with (19), given the gradient coming from its output\u2202C/\u2202f\u03b8l. To perform the backpropagation, it also computes the gradient with respect to its own inputs, as shown in (20). We now derive the gradients for each layer we used in this paper.  A.1 Lookup Table Layer  Given a matrix of parameters\u03b8^1 =W^1 and word (or discrete feature) indices[w]T 1 , the layer outputs the matrix: f\u03b8l([w]Tl) =   (   \u3008W\u3009^1 [w] 1 \u3008W\u3009^1 [w] 2 ... \u3008W\u3009^1 [w]T   )  ##### .\n\nfilename: materials_formatted.md\nextension: .md\n\nNow, a natural question you might ask is, if this method is so pre- cise, why do we not use it to compute all of our network gradients instead of applying back-propagation? The simple answer, as hinted earlier, is inefficiency \u2013 recall that every time we want to compute the gradient with respect to an element, we need to make two forward   ## neural networks,backpropagation 10  passes through the network, which will be computationally expen- sive. Furthermore, many large-scale neural networks can contain millions of parameters, and computing two passes per parameter is clearly not optimal. And, since in optimization techniques such as SGD, we must compute the gradients once per iteration for several thousands of iterations, it is obvious that this method quickly grows intractable. This inefficiency is why we only use gradient check to verify the correctness of our analytic gradients, which are much quicker to compute. A standard implementation of gradient check is shown below:   Snippet 2. 1 def eval_numerical_gradient(f, x): \"\"\" a naive implementation of numerical gradient of f at x  - f should be a function that takes a single argument - x is the point (numpy array) to evaluate the gradient at \"\"\"   fx = f(x) # evaluate function value at original point grad = np.zeros(x.shape) h = 0.   # iterate over all indexes in x it = np.nditer(x, flags=[\u2019multi_index\u2019], op_flags=[\u2019readwrite\u2019]) while not it.finished:   # evaluate function at x+h ix = it.multi_index old_value = x[ix] x[ix] = old_value + h # increment by h fxh_left = f(x) # evaluate f(x + h) x[ix] = old_value - h # decrement by h fxh_right = f(x) # evaluate f(x - h) x[ix] = old_value # restore to previous value (very important!)   # compute the partial derivative grad[ix] = (fxh_left - fxh_right) / (2*h) # the slope it.iternext() # step to next dimension return grad   ## neural networks,backpropagation 11  ### 2.\n\nfilename: materials_formatted.md\nextension: .md\n\n## neural networks,backpropagation 5  Here, we use a neural network with a single hidden layer and a single unit output. Let us establish some notation that will make it easier to generalize this model later:  - xiis an input to the neural network. - sis the output of the neural network. - Each layer (including the input and output layers) has neurons     which receive an input and produce an output. Thej-th neuron     of layerkreceives the scalar inputz(jk)and produces the scalar     activation outputa(jk). - We will call the backpropagated error calculated atz(jk)as _\u03b4_ (jk). - Layer 1 refers to the input layer and not the first hidden layer. For     the input layer,xj=z(j^1 )=a(j^1 ). - W(k)is the transfer matrix that maps the output from thek-th layer     to the input to the(k+ 1 )-th Thus,W(^1 )=WandW(^2 )=Uto put     this new generalized notation in perspective of Section 1. 3.        Backpropagation Notation:           - xiis an input to the neural network.           - sis the output of the neural net-              work.           - Thej-th neuron of layerkreceives              the scalar inputz(jk)and produces              the scalar activation outputa(jk).           - For the input layer,xj=z(j^1 )=a(j^1 ).           - W(k)is the transfer matrix that maps              the output from thek-th layer to              the input to the(k+ 1 )-th. Thus,              W(^1 )=WandW(^2 )=UTusing              notation from Section 1. 3.  Let us begin: Suppose the costJ= ( 1 +sc\u2212s)is positive and we want to perform the update of parameterW 14 (^1 )(in Figure 5 and  Figure 6 ), we must realize thatW 14 (^1 )only contributes toz( 12 )and thusa( 12 ). This fact is crucial to understanding backpropagation \u2013 backpropagated gradients are only affected by values they contribute to.a( 12 )is consequently used in the forward computation of score by  multiplication withW 1 (^2 ).\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat mathematical tool is crucial for backpropagation?[/INST] \nAnswer: The chain rule is crucial for backpropagation."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\n- \u201dReward hacking\u201d is a common        problem in RL - Chatbots are rewarded to produce responses that _seem_ authoritative and helpful, _regardless of truth_ - This can result in making up facts + hallucinations   https://www.npr.org/2023/02/09/1155650909/google-chatbot--error-bard-shares   https://news.ycombinator.com/item?id=34776508 https://apnews.com/article/kansasscience-82bc20f207e3e4cf81abc6a5d9e6b23a-city-chiefs-philadelphia-eagles-technology-    \ud835\udc45 \ud835\udc60 = \ud835\udc45\ud835\udc40\ud835\udf19(\ud835\udc60) \u2212 \ud835\udefd log   \ud835\udc5d\ud835\udc45\ud835\udf03\ud835\udc3f(\ud835\udc60) \ud835\udc5d\ud835\udc43\ud835\udc47(\ud835\udc60)  74  #### Limitations of RL + Reward Modeling  - Human preferences are unreliable!     - \u201dReward hacking\u201d is a common        problem in RL - Chatbots are rewarded to produce responses that _seem_ authoritative and helpful, _regardless of truth_ - This can result in making up facts + hallucinations - Models of human preferences are _even more_ unreliable!   Reward model over-optimization   [Stiennon et al., 2020 ]   #### Limitations of RL + Reward Modeling  - Human preferences are unreliable!     - \u201dReward hacking\u201d is a common        problem in RL - Chatbots are rewarded to produce responses that _seem_ authoritative and helpful, _regardless of truth_ - This can result in making up facts + hallucinations - Models of human preferences are _even more_ unreliable! - There is a real concern of AI mis(alignment)!   https://twitter.com/percyliang/status/1600383429463355392  75   1. Zero-Shot (ZS) and Few-Shot (FS) In-Context Learning     + No finetuning needed, prompt engineering (e.g. CoT) can improve performance     - Limits to what you can fit in context     - Complex tasks will probably need gradient steps 2.\n\nfilename: materials_formatted.md\nextension: .md\n\nWe train this model to predict which summary y \u2208 {y0, y1} is better as judged by a human, given a post x. If the summary preferred by the human is yi, we can write the RM loss as:  loss(r\u03b8) = \u2212E(x,y0,y1,i)\u223cD[log(\u03c3(r\u03b8(x, yi) \u2212 r\u03b8(x, y1\u2212i)))]  where r\u03b8(x, y) is the scalar output of the reward model for post x and summary y with parameters \u03b8, and D is the dataset of human judgments. At the end of training, we normalize the reward model outputs such that the reference summaries from our dataset achieve a mean score of 0.  Human feedback policies. We want to use the reward model trained above to train a policy that generates higher-quality outputs as judged by humans. We primarily do this using reinforcement learning, by treating the output of the reward model as a reward for the entire summary that we maximize with the PPO algorithm [58], where each time step is a BPE token.8 We initialize our policy to be the model \ufb01ne-tuned on Reddit TL;DR. Importantly, we include a term in the reward that penalizes the KL divergence between the learned RL policy \u03c0RL \u03c6 with parameters \u03c6 and this original supervised model \u03c0SFT, as previously done in [25]. The full reward R can be written as:  R(x, y) = r\u03b8(x, y) \u2212 \u03b2 log[\u03c0RL  \u03c6 (y|x)/\u03c0SFT(y|x)]  This KL term serves two purposes. First, it acts as an entropy bonus, encouraging the policy to explore and deterring it from collapsing to a single mode. Second, it ensures the policy doesn\u2019t learn to produce outputs that are too different from those that the reward model has seen during training.  For the PPO value function, we use a Transformer with completely separate parameters from the policy. This prevents updates to the value function from partially destroying the pretrained policy early in training (see ablation in Appendix G.1). We initialize the value function to the parameters of the reward model. In our experiments, the reward model, policy, and value function are the same size.\n\nfilename: materials_formatted.md\nextension: .md\n\nOur reward modeling approach was developed in prior work on learning to rank [40], which has been applied to ranking search results using either explicit feedback [2, 18] or implicit feedback in the form of click-through data [29, 30]. In a related line of research, human feedback has been used to train agents in simulated environments [10, 24]. There is also a rich literature on using RL to optimize automatic metrics for NLP tasks, such as ROUGE for summarization [50, 65, 45, 15, 19], BLEU for translation [50, 66, 1, 43], and other domains [61, 27, 26]. Finally, there has been extensive research on modifying architectures [22, 59] and pre-training procedures [70, 36, 49, 60, 53, 14] for improving summarization performance.  3 Method and experiment details  3.1 High-level methodology  Our approach is similar to the one outlined in [73], adapted to the batch setting. We start with an initial policy that is \ufb01ne-tuned via supervised learning on the desired dataset (in our case, the Reddit TL;DR summarization dataset). The process (illustrated in Figure 2) then consists of three steps that can be repeated iteratively.  Step 1: Collect samples from existing policies and send comparisons to humans. For each Reddit post, we sample summaries from several sources including the current policy, initial policy, original reference summaries and various baselines. We send a batch of pairs of summaries to our human evaluators, who are tasked with selecting the best summary of a given Reddit post.  Step 2: Learn a reward model from human comparisons. Given a post and a candidate summary, we train a reward model to predict the log odds that this summary is the better one, as judged by our labelers.  Step 3: Optimize a policy against the reward model. We treat the logit output of the reward model as a reward that we optimize using reinforcement learning, speci\ufb01cally with the PPO algorithm [58].  Figure 2: Diagram of our human feedback, reward model training, and policy training procedure.  We provide a more thorough description of our procedure, including details of the reward model and policy training and our quality control process, in the following sections.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat are the potential risks associated with reward hacking in reinforcement learning models?[/INST] \nAnswer: Reward hacking is a significant risk in reinforcement learning models, particularly in the context of chatbots. These models are often rewarded to produce responses that appear authoritative and helpful, regardless of their truthfulness. This can lead to the creation of false facts or hallucinations, as the model is primarily focused on achieving the reward rather than ensuring the accuracy of its responses. Additionally, the models of human preferences, which are often used in reinforcement learning, can be unreliable, leading to further issues. There's also a real concern about AI misalignment."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nFigure 5 presents this.   Embeddings   Add Position Embeddings   Multi-Head Attention   Add & Norm   Feed-For ward   Add & Norm   Block   Linear   Softmax   Repeat for number of   encoder blocks   Probabilities   Encoder Inputs  #### Transformer Encoder   Figure 5 : Diagram of the Transformer Encoder.  In the case that one wants probabilities out of the tokens of a Transformer Encoder (as in masked language modeling for BERT [Devlin et al., 2019 ], which we\u2019ll cover later), one applies a linear transformation to the output space followed by a softmax.  Uses of the Transformer Encoder. A Transformer Encoder is great in contexts where you aren\u2019t trying to generate text autoregressively (there\u2019s no masking in the encoder so each position index can see the whole sequence,) and want strong representations for the whole sequence (again, possible because even the first token can see the whole future of the sequence when building its representation.)  ##### 3. 6 Transformer Decoder  To build a Transformer autoregressive language model, one uses a Transformer Decoder. These differ from Transformer Encoders simply by using future masking at each application of self-attention. This ensures that the informational constraint (no cheating by looking at the future!) holds throughout the architecture. We show a diagram of this architecture in Figure 4. Famous examples of this are GPT- 2 [Radford et al., 2019 ], GPT- 3 [Brown et al., 2020 ] and BLOOM [Workshop et al., 2022 ].  ##### 3. 7 Transformer Encoder-Decoder  A Transformer encoder-decoder takes as input two sequences. Fig- ure 6 shows the whole encoder-decoder structure. The first sequence x 1:nis passed through a Transformer Encoder to build contextual representations. The second sequence y 1:mis encoded through a modified Transformer Decoder architecture in which cross-attention (which we haven\u2019t yet defined!) is applied from the encoded repre- sentation of y 1:mto the output of the Encoder. So, let\u2019s take a quick detour to discuss cross-attention; it\u2019s not too different from what we\u2019ve already seen.    with deep learning 14  Cross-Attention.\n\nfilename: materials_formatted.md\nextension: .md\n\nThis idea is built-in to unidirectional RNNs. If we want to use an RNN for the functionf, we can use the hidden state for wordwt\u2212 1 :   w t\u223csoftmax( h t\u2212 1 E) ( 19 ) ht\u2212 1 = \u03c3 (W h t\u2212 2 +U x t\u2212 1 ), ( 20 )  and by the rollout of the RNN, we haven\u2019t looked at the future. (In this case, the future is all the words w t,... , w n.) In a Transformer, there\u2019s nothing explicit in the self-attention weight _\u03b1_ that says not to look at indicesj>iwhen representing tokeni. In practice, we enforce this constraint simply adding a large negative constant to the input to the softmax (or equivalently, setting _\u03b1_ ij=0 wherej>i.)^44 It might seem like one should use\u2212\u221e as the constant, to \u201creally\u201d ensure that you can\u2019t see the future. However, this is not done; a modest constant within even the float range of the \u2018float 16 \u2018 encoding is used instead, like\u2212 105. Using infinity can lead to NaNs and it\u2019s sort of undefined how each library should treat infinite inputs, so we tend to avoid using it. And because of finite precision, a large enough negative constant will still set the attention weight to exactly zero.   \u03b1 ij,masked=            \u03b1 ij j\u2264i 0 otherwise   ( 21 )  In a diagram, it looks like Figure 3.  ##### 2. 5 Summary of a minimal self-attention architecture  Our minimal self-attention architecture has ( 1 ) the self-attention operation, ( 2 ) position representations, ( 3 ) elementwise nonlinearities, and ( 4 ) future masking (in the context of language modeling.)    with deep learning 9   Zukomade his uncle tea Zuko   made   his   uncle   tea    Figure 3 : Diagram of autoregressive future masking in self-attention. Words in each row have words in the future masked out (e.g., \u201cZuko\u201d can only attend to \u201cZuko\u201d, while \u201cmade\u201d can attend to \u201cZuko\u201d and \u201cmade\u201d.)  Intuitively, these are the biggest components to understand.\n\nfilename: materials_formatted.md\nextension: .md\n\nFigure 1: The Transformer - model architecture.  The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.  3.1 Encoder and Decoder Stacks  Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position- wise fully connected feed-forward network. We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512.  Decoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.  3.2 Attention  An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum  Scaled Dot-Product Attention  Multi-Head Attention  Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.  of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.  3.2.1 Scaled Dot-Product Attention  We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2).\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat is the purpose of future masking in the Transformer's decoder?[/INST] \nAnswer: Future masking in the Transformer's decoder is used to prevent positions from attending to subsequent positions. This ensures that the predictions for a certain position can depend only on the known outputs at positions less than that position. It's a way to maintain the informational constraint and avoid looking at future information, which is crucial in autoregressive language modeling."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nArXiv, abs/1909.08593, 2019.  75     \n\n# Dense Passage Retrieval for Open-Domain Question Answering:  Dense Passage Retrieval for Open-Domain Question Answering  Vladimir Karpukhin\u2217, Barlas O\u02d8guz\u2217, Sewon Min\u2020, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen\u2021, Wen-tau Yih  Facebook AI  \u2020University of Washington  \u2021Princeton University  {vladk, barlaso, plewis, ledell, edunov, scottyih}@fb.com sewon@cs.washington.edu danqic@cs.princeton.edu  Abstract  Open-domain question answering relies on ef- \ufb01cient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented us- ing dense representations alone, where em- beddings are learned from a small number of questions and passages by a simple dual- encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene- BM25 system greatly by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.1  1  Introduction  Open-domain question answering (QA) (Voorhees, 1999) is a task that answers factoid questions us- ing a large collection of documents. While early QA systems are often complicated and consist of multiple components (Ferrucci (2012); Moldovan et al. (2003), inter alia), the advances of reading comprehension models suggest a much simpli\ufb01ed two-stage framework: (1) a context retriever \ufb01rst selects a small subset of passages where some of them contain the answer to the question, and then (2) a machine reader can thoroughly exam- ine the retrieved contexts and identify the correct answer (Chen et al., 2017). Although reducing open-domain QA to machine reading is a very rea- sonable strategy, a huge performance degradation is often observed in practice2, indicating the needs of improving retrieval.\n\nfilename: materials_formatted.md\nextension: .md\n\nOn datasets where question writers do not know the answer\u2014 Natural Questions (Kwiatkowski et al., 2019), WebQuestions (Berant et al., 2013), and Curat- edTrec (Baudis and Sediv\u00b4y, 2015)\u2014we show that learned retrieval is crucial, providing improve- ments of 6 to 19 points in exact match over BM25.  2 Overview  In this section, we introduce notation for open do- main QA that is useful for comparing prior work, baselines, and our proposed model.  2.1 Task  In open domain question answering, the input q is a question string, and the output a is an answer string. Unlike reading comprehension, the source of evidence is a modeling choice rather than a part of the task de\ufb01nition. We compare the assump- tions made by variants of reading comprehension and question answering tasks in Table 1.  Evaluation is exact match with any of the ref- erence answer strings after minor normalization such as lowercasing, following evaluation scripts from DrQA (Chen et al., 2017).  2.2 Formal De\ufb01nitions  Models are de\ufb01ned with respect to an unstruc- tured text corpus that is split into B blocks of ev- idence texts. An answer derivation is a pair (b, s), where 1 \u2264 b \u2264 B indicates the index of an ev- idence block and s denotes a span of text within block b. The start and end token indices of span s are denoted by START(s) and END(s) respectively. Models de\ufb01ne a scoring function S(b, s, q) indi- cating the goodness of an answer derivation (b, s) given a question q. Typically, this scoring func- tion is decomposed over a retrieval component Sretr (b, q) and a reader component Sread (b, s, q):  S(b, s, q) = Sretr (b, q) + Sread (b, s, q)  During inference, the model outputs the answer string of the highest scoring derivation:  a\u2217 = TEXT(argmax  S(b, s, q))  b,s  where TEXT(b, s) deterministically maps answer derivation (b, s) to an answer string.\n\nfilename: materials_formatted.md\nextension: .md\n\nReading comprehension. A data-driven approach to reading comprehension goes back to Hirschman et al. (1999), who curated a dataset of 600 real 3rd\u2013 6th grade reading comprehension questions. Their pattern matching baseline was subsequently im- proved by a rule-based system (Riloff and Thelen, 2000) and a logistic regression model (Ng et al., 2000). More recently, Richardson et al. (2013) cu- rated MCTest, which contains 660 stories created by crowdworkers, with 4 questions per story and 4 answer choices per question. Because many of the questions require commonsense reasoning and reasoning across multiple sentences, the dataset re- mains quite challenging, though there has been no- ticeable progress (Narasimhan and Barzilay, 2015; Sachan et al., 2015; Wang et al., 2015). Both curated datasets, although real and dif\ufb01cult, are too small to support very expressive statistical models.  Some datasets focus on deeper reasoning abili- ties. Algebra word problems require understanding a story well enough to turn it into a system of equa-  tions, which can be easily solved to produce the an- swer (Kushman et al., 2014; Hosseini et al., 2014). BAbI (Weston et al., 2015), a fully synthetic RC dataset, is strati\ufb01ed by different types of reasoning required to solve each task. Clark and Etzioni (2016) describe the task of solving 4th grade science exams, and stress the need to reason with world knowledge.  Open-domain question answering. The goal of open-domain QA is to answer a question from a large collection of documents. The annual eval- uations at the Text REtreival Conference (TREC) (Voorhees and Tice, 2000) led to many advances in open-domain QA, many of which were used in IBM Watson for Jeopardy! (Ferrucci et al., 2013). Recently, Yang et al.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat distinguishes open-domain question answering from reading comprehension?[/INST] \nAnswer: Open-domain question answering differs from reading comprehension in that it aims to answer a question using a large collection of documents. In contrast, reading comprehension typically involves understanding and answering questions based on a specific text or passage. In open-domain question answering, the source of evidence is a modeling choice rather than a part of the task definition. Additionally, open-domain question answering often involves a two-stage framework: a context retriever first selects a small subset of passages that may contain the answer to the question, and then a machine reader examines the retrieved contexts to identify the correct answer."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nFinally, we can scale these scores, take the softmax, and then multiply the result by V resulting in a matrix of shape N \u00d7 d where each row contains a contextualized output embedding corresponding to each token in the input. (cid:124) (cid:18) QK \u221a dk  SelfAttention(Q, K, V) = softmax  (11.6)  (cid:19)  V  The key architecture difference is in bidirectional models we don\u2019t mask the fu- ture. As shown in Fig. 11.2, the full set of self-attention scores represented by QKT constitute an all-pairs comparison between the keys and queries for each element of the input. In the case of causal language models in Chapter 10, we masked the  q1\u2022k1q2\u2022k1q2\u2022k2q5\u2022k1q5\u2022k2q5\u2022k3q5\u2022k4q5\u2022k5q4\u2022k1q4\u2022k2q4\u2022k3q4\u2022k4q3\u2022k1q3\u2022k2q3\u2022k3NNq1\u2022k2q1\u2022k3q1\u2022k4q1\u2022k5q2\u2022k3q2\u2022k4q2\u2022k5q3\u2022k4q3\u2022k5q4\u2022k54 CHAPTER 11  \u2022 FINE-TUNING AND MASKED LANGUAGE MODELS  upper triangular portion of this matrix (in Fig. ??) to eliminate information about fu- ture words since this would make the language modeling training task trivial. With bidirectional encoders we simply skip the mask, allowing the model to contextualize each token using information from the entire input.  Beyond this simple change, all of the other elements of the transformer archi- tecture remain the same for bidirectional encoder models. Inputs to the model are segmented using subword tokenization and are combined with positional embed- dings before being passed through a series of standard transformer blocks consisting of self-attention and feedforward layers augmented with residual connections and layer normalization, as shown in Fig. 11.3.  Figure 11.3 A transformer block showing all the layers.\n\nfilename: materials_formatted.md\nextension: .md\n\nqi = WQxi; ki = WKxi; vi = WVxi  (11.1)  The output vector yi corresponding to each input element xi is a weighted sum of all  x1a1x2a2a3a4a5x3x4x5a) A causal self-attention layerx1a1x2a2a3a4a5x3x4x5b) A bidirectional self-attention layer11.1  \u2022 BIDIRECTIONAL TRANSFORMER ENCODERS  3  the input value vectors v, as follows:  yi =  n (cid:88)  j=1  \u03b1i jv j  (11.2)  The \u03b1 weights are computed via a softmax over the comparison scores between every element of an input sequence considered as a query and every other element as a key, where the comparison scores are computed using dot products.  \u03b1i j =  exp(scorei j) k=1 exp(scoreik)  (cid:80)n  scorei j = qi \u00b7 k j  (11.3)  (11.4)  As with the models of Chapter 10, since each output vector, yi, is computed independently, the processing of an entire sequence can be parallelized via matrix operations. The \ufb01rst step is to pack the input embeddings xi into a matrix X \u2208 RN\u00d7dh. That is, each row of X is the embedding of one token of the input. We then multiply X by the key, query, and value weight matrices (all of dimensionality d \u00d7 d) to produce matrices Q \u2208 RN\u00d7d, K \u2208 RN\u00d7d, and V \u2208 RN\u00d7d, containing all the key, query, and value vectors in a single step.  Q = XWQ; K = XWK; V = XWV  (11.5)  Given these matrices we can compute all the requisite query-key comparisons si- multaneously by multiplying Q and K in a single operation. Fig. 11.2 illustrates the result of this operation for an input with length 5.  (cid:124)  Figure 11.2 The N \u00d7 N QK  (cid:124)  matrix showing the complete set of qi \u00b7 k j comparisons.\n\nfilename: materials_formatted.md\nextension: .md\n\nThis consists of two linear transformations with a ReLU activation in between.  FFN(x) = max(0, xW1 + b1)W2 + b2  (2)  While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality df f = 2048.  3.4 Embeddings and Softmax  Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor- mation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax dmodel. linear transformation, similar to [30]. In the embedding layers, we multiply those weights by  Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types. n is the sequence length, d is the representation dimension, k is the kernel size of convolutions and r the size of the neighborhood in restricted self-attention.  Layer Type  Complexity per Layer  Self-Attention Recurrent Convolutional Self-Attention (restricted)  O(n2 \u00b7 d) O(n \u00b7 d2) O(k \u00b7 n \u00b7 d2) O(r \u00b7 n \u00b7 d)  Sequential Maximum Path Length Operations O(1) O(n) O(1) O(1)  O(1) O(n) O(logk(n)) O(n/r)  3.5 Positional Encoding  Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed [9].\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat are the key layers of the BIDAF model?[/INST] \nAnswer: I don't know."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nOur best model achieves an F1 score of 51.0%,1 which is much better than the sliding window base- line (20%). Over the last four months (since June 2016), we have witnessed signi\ufb01cant improvements from more sophisticated neural network-based mod- els. For example, Wang and Jiang (2016) obtained 70.3% F1 on SQuAD v1.1 (results on v1.0 are sim- ilar). These results are still well behind human performance, which is 86.8% F1 based on inter- annotator agreement. This suggests that there is plenty of room for advancement in modeling and learning on the SQuAD dataset.  1All experimental results in this paper are on SQuAD v1.0.  Dataset  SQuAD  Question source  Formulation  Size  crowdsourced RC,  spans  100K  MCTest (Richardson et al., 2013) Algebra (Kushman et al., 2014) Science (Clark and Etzioni, 2016)  crowdsourced  standardized tests standardized tests  in passage  RC, multiple choice computation  reasoning, multiple choice  2640  514  855  WikiQA (Yang et al., 2015) TREC-QA (Voorhees and Tice, 2000)  query logs  query logs + human editor  3047  IR, sentence selection IR, free form 1479  CNN/Daily Mail (Hermann et al., 2015) CBT (Hill et al., 2015)  summary cloze cloze  +  RC, \ufb01ll in single entity RC, \ufb01ll in single word  1.4M  688K  Table 1: A survey of several reading comprehension and ques-  tion answering datasets. SQuAD is much larger than all datasets  except the semi-synthetic cloze-style datasets, and it is similar  to TREC-QA in the open-endedness of the answers.  2 Existing Datasets  We begin with a survey of existing reading com- prehension and question answering (QA) datasets, highlighting a variety of task formulation and cre- ation strategies (see Table 1 for an overview).  Reading comprehension.\n\nfilename: materials_formatted.md\nextension: .md\n\nAfter graduating from Columbia University in 1983, he workedasa community organizerinChicago.   (Levy et al.,2017)   Semantic role labeling   16   (He etal.,2015)   #### Stanford question answering dataset (SQuAD)  - 100k annotated (passage, question, answer) triples     Large-scale supervised datasets are also a key ingredient for     training effective neural models for reading comprehension! - Passages are selected from English Wikipedia, usually 100~150 words. - Questions are crowd-sourced. - Each answer is a short segment of text (or span) in the passage.   This is a limitation\u2014 not all the questions can be answered in this way!  - SQuAD was for years the most popular reading comprehension     dataset; it is \u201calmost solved\u201d today (though the underlying task is     not,) and the state-of-the-art exceeds the estimated human     performance.   (Rajpurkaretal.,2016):SQuAD:100,000+QuestionsforMachineComprehensionofText^16    18  #### Stanford question answering dataset (SQuAD)  - Evaluation : exact match ( 0 or 1 ) and F 1 (partial credit). - For development and testing sets, 3 gold answers are collected, because there could be multiple     plausible answers. - We compare the predicted answer to _each_ gold answer (a, an, the, punctuations are removed)     and take max scores. Finally, we take the average of all the examples for both exact match and     F 1. - Estimated human performance: EM = 82. 3 , F 1 = 91. 2   Q: What did Tesla do in December 1878?   A: {left Graz, left Graz, left Graz and severed all relations with his family}   Prediction: {left Graz and served}   Exact match: max{0, 0, 0} = 0 F1: max{0.67, 0.67, 0.61} = 0.    19  #### Stanford question answering dataset (SQuAD)    20  #### Other question answering datasets  - TriviaQA: Questions and answers by trivia enthusiasts.\n\nfilename: materials_formatted.md\nextension: .md\n\nExisting datasets for RC have one of two shortcomings: (i) those that are high in quality (Richardson et al., 2013; Berant et al., 2014) are too small for training modern data-intensive models, while (ii) those that are large (Hermann et al., 2015; Hill et al., 2015) are semi-synthetic and do not share the same character- istics as explicit reading comprehension questions.  To address the need for a large and high-quality reading comprehension dataset, we present the Stan-              ford Question Answering Dataset v1.0 (SQuAD), freely available at https://stanford-qa.com, con- sisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to ev- ery question is a segment of text, or span, from the corresponding reading passage. SQuAD contains 107,785 question-answer pairs on 536 articles, and is almost two orders of magnitude larger than previ- ous manually labeled RC datasets such as MCTest (Richardson et al., 2013).  In contrast to prior datasets, SQuAD does not provide a list of answer choices for each question. Rather, systems must select the answer from all pos- sible spans in the passage, thus needing to cope with a fairly large number of candidates. While ques- tions with span-based answers are more constrained than the more interpretative questions found in more advanced standardized tests, we still \ufb01nd a rich di- versity of questions and answer types in SQuAD. We develop automatic techniques based on distances in dependency trees to quantify this diversity and stratify the questions by dif\ufb01culty. The span con- straint also comes with the important bene\ufb01t that span-based answers are easier to evaluate than free- form answers.  To assess the dif\ufb01culty of SQuAD, we imple- mented a logistic regression model with a range of features. We \ufb01nd that lexicalized and dependency tree path features are important to the performance of the model. We also \ufb01nd that the model perfor- mance worsens with increasing complexity of (i) an- swer types and (ii) syntactic divergence between the question and the sentence containing the answer; in- terestingly, there is no such degradation for humans.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat led to the surpassing of human performance on SQuAD, and does it imply that reading comprehension is solved?[/INST] \nAnswer: The state-of-the-art models have surpassed human performance on the SQuAD dataset, but this does not necessarily imply that reading comprehension is completely solved. The SQuAD dataset is a specific type of task where answers are span-based, meaning they are segments of text from the passage. While this type of task has seen significant improvements, it is more constrained than more interpretative questions found in advanced standardized tests. Therefore, while performance on this specific task has improved, it does not mean that all aspects of reading comprehension are solved."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nOur experiments indicate that increasing the size of the receptive \ufb01eld plays a sig- ni\ufb01cant role in this improvement. We observe signi\ufb01cant improvements up to effective receptive \ufb01eld sizes of 256 pixels, while the PixelCNN (van den Oord et al., 2016b) with 5x5 \ufb01lters used 25.  Many applications of image density models require condi- tioning on additional information of various kinds: from im- ages in enhancement or reconstruction tasks such as super- resolution, in-painting and denoising to text when synthesiz- ing images from natural language descriptions (Mansimov et al., 2015). In visual planning tasks, conditional image generation models could predict future frames of video con- ditioned on previous frames and taken actions.  In this work we hence also evaluate two different methods of performing conditional image generation with the Im- age Transformer. In image-class conditional generation we condition on an embedding of one of a small number of image classes. In super-resolution with high magni\ufb01cation ratio (4x), we condition on a very low-resolution image, employing the Image Transformer in an encoder-decoder con\ufb01guration (Kalchbrenner & Blunsom, 2013). In com- parison to recent work on autoregressive super-resolution (Dahl et al., 2017), a human evaluation study found im- ages generated by our models to look convincingly natural signi\ufb01cantly more often.  2. Background  There is a broad variety of types of image generation mod- els in the literature. This work is strongly inspired by au- toregressive models such as fully visible belief networks and NADE (Bengio & Bengio, 2000; Larochelle & Mur- ray, 2011) in that we also factor the joint probability of the image pixels into conditional distributions. Following PixelRNN (van den Oord et al., 2016a), we also model the color channels of the output pixels as discrete values gener- ated from a multinomial distribution, implemented using a simple softmax layer.\n\nfilename: materials_formatted.md\nextension: .md\n\nConditional image generation with pixelcnn decoders. NIPS, 2016b.  Vaswani, Ashish, Shazeer, Noam, Parmar, Niki, Uszkoreit, Jakob, Jones, Llion, Gomez, Aidan N., Kaiser, Lukasz, and Polosukhin, Illia. Attention is all you need. 2017. URL http://arxiv.org/abs/1706.03762.  Metz, Luke, Poole, Ben, Pfau, David, and Sohl-Dickstein, Jascha. Unrolled generative adversarial networks. CoRR, abs/1611.02163, 2016. URL http://arxiv.org/ abs/1611.02163.  Parikh, Ankur, T\u00a8ackstr\u00a8om, Oscar, Das, Dipanjan, and Uszkoreit, Jakob. A decomposable attention model.  Vaswani, Ashish, Bengio, Samy, Brevdo, Eugene, Chol- let, Francois, Gomez, Aidan N., Gouws, Stephan, Jones, Llion, Kaiser, \u0141ukasz, Kalchbrenner, Nal, Parmar, Niki, Sepassi, Ryan, Shazeer, Noam, and Uszkoreit, Jakob. Tensor2tensor for neural machine translation. CoRR, abs/1803.07416, 2018. URL http://arxiv.org/ abs/1803.07416.  Image Transformer  Zhang, Han, Xu, Tao, Li, Hongsheng, Zhang, Shaoting, Huang, Xiaolei, Wang, Xiaogang, and Metaxas, Dim- itris N. Stackgan: Text to photo-realistic image synthe- sis with stacked generative adversarial networks. CoRR, abs/1612.03242, 2016. URL http://arxiv.org/ abs/1612.03242.\n\nfilename: materials_formatted.md\nextension: .md\n\nFor DMOL we only evaluated learned coordinate embeddings.  5.2. Conditioning on Image Class  We represent the image classes as learned d-dimensional embeddings per class and simply add the respective em- bedding to the input representation of every input position together with the positional encodings.  We trained the class-conditioned Image Transformer on CIFAR-10, achieving very similar log-likelihoods as in un- conditioned generation. The perceptual quality of generated  Image Transformer  images, however, is signi\ufb01cantly higher than that of our unconditioned models. The samples from our 8-layer class- conditioned models in Table 3, show that we can generate realistic looking images for some categories, such as cars and trucks.  5.3. Image Super-Resolution  Super-resolution is the process of recovering a high resolu- tion image from a low resolution image while generating realistic and plausible details. Following (Dahl et al., 2017), in our experimental setup we enlarge an 8 \u00d7 8 pixel image four-fold to 32 \u00d7 32, a process that is massively underspec- i\ufb01ed: the model has to generate aspects such as texture of hair, makeup, skin and sometimes even gender that cannot possibly be recovered from the source image.  Here, we use the Image Transformer in an encoder-decoder con\ufb01guration, connecting the encoder and decoder through an attention mechanism (Vaswani et al., 2017). For the encoder, we use embeddings for RGB intensities for each pixel in the 8\u00d78 image and add 2 dimensional positional encodings for each row and width position. Since the input is small, we \ufb02atten the whole image as a [h \u00d7 w \u00d7 3, d] tensor, where d is typically 512. We then feed this sequence to our stack of transformer encoder layers that uses repeated self-attention and feed forward layers. In the encoder we don\u2019t require masking, but allow any input pixel to attend to any other pixel. In the decoder, we use a stack of local self-attention, encoder-decoder-attention and feed-forward layers. We found using two to three times fewer encoder than decoder layers to be ideal for this task.  We perform end-to-end training of the encoder-decoder model for Super resolution using the log-likelihood ob- jective function.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat method does the Image Transformer use for conditional image generation?[/INST] \nAnswer: The Image Transformer uses two different methods for conditional image generation. The first method is image-class conditional generation where the model conditions on an embedding of one of a small number of image classes. The second method is super-resolution with high magnification ratio (4x), where the model conditions on a very low-resolution image. This is done by employing the Image Transformer in an encoder-decoder configuration."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nThis allows us to easily build variants of our networks. For details about gradient computations, see Appendix A.  Remark 7 (Tricks)Many tricks have been reported for training neural networks (LeCun etal., 1998). Which ones to choose is often confusing. We employed only two of them: the initialization and update of the parameters of each network layer were done according to the \u201cfan-in\u201d of the layer, that is the number of inputs used to compute each output of this layer(Plaut and Hinton, 1987). The fan-in for the lookup table (1), the lthlinear layer (4) and the convolution layer (6) are respectively 1 , nlhu\u2212^1 and dwin\u00d7nlhu\u2212^1. The initial parameters of the network were drawn from a centered uniform distribution, with a variance equal to the inverse of the square-root of the fan-in. The learning rate in (16) was divided by the fan-in, but stays fixed during the training.  3.5 Supervised Benchmark Results  For POS, chunking and NER tasks, we report results with the window architecture^12 described in Section 3.3.1. The SRL task was trained using the sentence approach (Section 3.3.2). Results are reported in Table 4, in per-word accuracy (PWA) for POS, and F1score for all the other tasks. We performed experiments both with the word-level log-likelihood (WLL) andwith the sentence- level log-likelihood (SLL). The hyper-parameters of our networks arereported in Table 5. All our  11. Seehttp://torch5.sf.net. 12. We found that training these tasks with the more complex sentence approach was computationally expensive and     offered little performance benefits. Results discussed in Section 5 provide more insight about this decision.    NATURALLANGUAGEPROCESSING(ALMOST)FROMSCRATCH   Approach POS Chunking NER SRL (PWA) (F1) (F1) (F1) Benchmark Systems 97.24 94.29 89.31 77. NN+WLL 96.31 89.13 79.53 55. NN+SLL 96.37 90.33 81.47 70.\n\nfilename: materials_formatted.md\nextension: .md\n\nAvailable athttp://l2r.cs.uiuc.edu/ \u0303cogcomp/asoftware.php?skey=SRL.    COLLOBERT, WESTON, BOTTOU, KARLEN, KAVUKCUOGLU ANDKUKSA  the number of examples. Most of the neural network technology necessary for our work has been described ten years ago (e.g., Le Cun et al., 1998). However, if we haddecided ten years ago to train the language model network LM2 using a vintage computer, training would onlybe nearing com- pletion today. Training algorithms that scale linearly are most able to benefit from such tremendous progress in computer hardware.  #### 8. Conclusion  We have presented a multilayer neural network architecture that can handle a number of NLP tasks with both speed and accuracy. The design of this system was determined byour desire to avoid task-specific engineering as much as possible. Instead we rely on large unlabeled data sets and let the training algorithm discover internal representations that prove useful for all the tasks of interest. Using this strong basis, we have engineered a fast and efficient \u201call purpose\u201d NLP tagger that we hope will prove useful to the community.  #### Acknowledgments  We acknowledge the persistent support of NEC for this research effort. We thank Yoshua Bengio, Samy Bengio, Eric Cosatto, Vincent Etter, Hans-Peter Graf, Ralph Grishman, and Vladimir Vapnik for their useful feedback and comments.  #### Appendix A. Neural Network Gradients  We consider a neural networkf\u03b8(\u00b7), with parameters\u03b8. We maximize the likelihood (8), or minimize ranking criterion (17), with respect to the parameters\u03b8, using stochastic gradient. By negating the likelihood, we now assume it all corresponds to minimize a costC(f\u03b8(\u00b7)), with respect to\u03b8.\n\nfilename: materials_formatted.md\nextension: .md\n\nAdding MLP in \u03b2 does not seem to help, yielding slightly worse result than \u03b2 without MLP.  13     \n\n# On the difficulty of training Recurrent Neural Networks:  # On the difficulty of training Recurrent Neural Networks   Razvan Pascanu pascanur@iro.umontreal.ca Universite de Montreal   Tomas Mikolov t.mikolov@gmail.com Brno University   Yoshua Bengio yoshua.bengio@umontreal.ca Universite de Montreal  ## Abstract   There are two widely known issues with prop- erly training Recurrent Neural Networks, the vanishing and theexplodinggradient prob- lems detailed in Bengio et al. (1994). In this paper we attempt to improve the under- standing of the underlying issues by explor- ing these problems from an analytical, a geo- metric and a dynamical systems perspective. Our analysis is used to justify a simple yet ef- fective solution. We propose a gradient norm clipping strategy to deal with exploding gra- dients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.  ## 1. Introduction   A recurrent neural network (RNN), e.g. Fig. 1, is a neural network model proposed in the 80\u2019s (Rumelhart et al., 1986; Elman, 1990; Werbos, 1988) for modeling time series. The structure of the network is similar to that of a standard multilayer perceptron, with the dis- tinction that we allow connections among hidden units associated with a time delay. Through these connec- tions the model can retain information about the past inputs, enabling it to discover temporal correlations between events that are possibly far away from each other in the data (a crucial property for proper learn- ing of time series). While in principle the recurrent network is a simple and powerful model, in practice, it is unfortunately hard to train properly. Among the main reasons why this model is so unwieldy are thevanishing gradient  ### ut xt Et   Figure 1.Schematic of a recurrent neural network. The recurrent connections in the hidden layer allow information to persist from one input to another.   andexploding gradientproblems described in Bengio et al.(1994).   1.1.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat are some of the practical tips discussed for training neural networks?[/INST] \nAnswer: The practical tips discussed for training neural networks include the initialization and update of the parameters of each network layer according to the \"fan-in\" of the layer, which is the number of inputs used to compute each output of this layer. The initial parameters of the network are drawn from a centered uniform distribution, with a variance equal to the inverse of the square-root of the fan-in. The learning rate is divided by the fan-in, but stays fixed during the training."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\n2013) are publicly available toolkits for building n-gram language models. Modern language modeling is more commonly done withneural networklan- guage models, which solve the major problems with n-grams: the number of param- eters increases exponentially as the n-gram order increases, and n-grams have no way to generalize from training to test set. Neural language models instead project words into acontinuousspace in which words with similar contexts have similar representations. We\u2019ll introduce bothfeedforwardlanguage models (Bengio et al. 2006, Schwenk 2007) in Chapter 7, andrecurrentlanguage models (Mikolov, 2012) in Chapter 9.        \n\n# Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks:   ## Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks   Patrick Lewis\u2020\u2021, Ethan Perez?,   Aleksandra Piktus\u2020, Fabio Petroni\u2020, Vladimir Karpukhin\u2020, Naman Goyal\u2020, Heinrich K\u00fcttler\u2020,   Mike Lewis\u2020, Wen-tau Yih\u2020, Tim Rockt\u00e4schel\u2020\u2021, Sebastian Riedel\u2020\u2021, Douwe Kiela\u2020   \u2020Facebook AI Research;\u2021University College London;?New York University;   plewis@fb.com  ### Abstract   Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on down- stream NLP tasks. However, their ability to access and precisely manipulate knowl- edge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre- trained models with a differentiable access mechanism to explicit non-parametric memory have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) \u2014 models which combine pre-trained parametric and non-parametric mem- ory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever.\n\nfilename: materials_formatted.md\nextension: .md\n\nAn n-gram language model can also be shrunk by pruning, for example only storing n-grams with counts greater than some threshold (such as the count threshold of 40 used for the Google n-gram release) or using entropy to prune less-important n-grams (Stolcke, 1998). Another option is to build approximate language models Bloom filters using techniques likeBloom filters(Talbot and Osborne 2007, Church et al. 2007). Finally, efficient language model toolkits like KenLM (Heafield 2011, Heafield et al. 2013) use sorted arrays, efficiently combine probabilities and backoffs in a single value, and use merge sorts to efficiently build the probability tables in a minimal number of passes through a large corpus. Although with these toolkits it is possible to build web-scale language models using advanced smoothing algorithms like the Kneser-Ney algorithm we will see in Section 3.7, Brants et al. (2007) show that with very large language models a much stupid backoff simpler algorithm may be sufficient. The algorithm is calledstupid backoff. Stupid backoff gives up the idea of trying to make the language model a true probability dis-    3.7 \u2022 ADVANCED: KNESER-NEYSMOOTHING 19   tribution. There is no discounting of the higher-order probabilities. If a higher-order n-gram has a zero count, we simply backoff to a lower order n-gram, weighed by a fixed (context-independent) weight. This algorithm does not produce a probability distribution, so we\u2019ll follow Brants et al. (2007) in referring to it asS:   S(wi|wi\u2212N+1 :i\u2212 1 ) =        count(wi\u2212N+1 :i) count(wi\u2212N+1 :i\u2212 1 ) if count(wi\u2212N+1 :i)>^0 \u03bbS(wi|wi\u2212N+2 :i\u2212 1 ) otherwise   (3.31)   The backoff terminates in the unigram, which has scoreS(w) =countN(w). Brants et al. (2007) find that a value of 0.4 worked well for\u03bb.\n\nfilename: materials_formatted.md\nextension: .md\n\nThese arguments led many linguists and computational linguists to ignore work in statistical modeling for decades. The resurgence of n-gram models came from Jelinek and colleagues at the IBM Thomas J. Watson Research Center, who were influenced by Shannon, and Baker at CMU, who was influenced by the work of Baum and colleagues. Independently these two labs successfully used n-grams in their speech recognition systems (Baker 1975b, Jelinek 1976, Baker 1975a, Bahl et al. 1983, Jelinek 1990). Add-one smoothing derives from Laplace\u2019s 1812 law of succession and was first applied as an engineering solution to the zero frequency problem by Jeffreys (1948) based on an earlier Add-K suggestion by Johnson (1932). Problems with the add- one algorithm are summarized in Gale and Church (1994). A wide variety of different language modeling and smoothing techniques were proposed in the 80s and 90s, including Good-Turing discounting\u2014first applied to the n-gram smoothing at IBM by Katz (N \u0301adas 1984, Church and Gale 1991)\u2014 Witten-Bell discounting (Witten and Bell, 1991), and varieties ofclass-based n- class-basedn-gram grammodels that used information about word classes. Starting in the late 1990s, Chen and Goodman performed a number of carefully controlled experiments comparing different discounting algorithms, cache models, class-based models, and other language model parameters (Chen and Goodman 1999, Goodman 2006, inter alia). They showed the advantages ofModified In- terpolated Kneser-Ney, which became the standard baseline for n-gram language modeling, especially because they showed that caches and class-based models pro- vided only minor additional improvement. These papers are recommended for any reader with further interest in n-gram language modeling. SRILM (Stolcke, 2002) and KenLM (Heafield 2011, Heafield et al. 2013) are publicly available toolkits for building n-gram language models.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat is an n-gram language model and how does it work?[/INST] \nAnswer: An n-gram language model is a type of statistical language model used for predicting the next item in a sequence, such as a word in a sentence. It works by calculating the probability of a given word or phrase based on the previous n-1 words or phrases in the sequence. However, this model has some limitations, such as the exponential increase in parameters as the n-gram order increases, and the inability to generalize from training to test set. To overcome these issues, modern language modeling often uses neural network language models, which project words into a continuous space where words with similar contexts have similar representations."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\n## 1 Dependency Grammar and Dependency Structure  Parse trees in NLP, analogous to those in compilers, are used to ana- lyze the syntactic structure of sentences. There are two main types of structures used - constituency structures and dependency structures. Constituency Grammar uses phrase structure grammar to organize words into nested constituents. This will be covered in more detail in following chapters. We now focus on Dependency Parsing. Dependency structure of sentences shows which words depend on (modify or are arguments of) which other words. These binary asymmetric relations between the words are called dependencies and are depicted as arrows going from the head (or governor, superior, regent) to the dependent (or modifier, inferior, subordinate). Usually these dependencies form a tree structure. They are often typed with the name of grammatical relations (subject, prepositional object, apposition, etc.). An example of such a dependency tree is shown in Figure 1. Sometimes a fake ROOT node is added as the head to the whole tree so that every word is a dependent of exactly one node.   Figure 1 : Dependency tree for the sen- tence \"Bills on ports and immigration were submitted by Senator Brownback, Republican of Kansas\"  ### 1. 1 Dependency Parsing  Dependency parsing is the task of analyzing the syntactic depen- dency structure of a given input sentenceS. The output of a depen- dency parser is a dependency tree where the words of the input sen- tence are connected by typed dependency relations. Formally, the de- pendency parsing problem asks to create a mapping from the input sentence with wordsS=w 0 w 1 ...wn(wherew 0 is the ROOT) to its dependency tree graphG. Many different variations of dependency- based methods have been developed in recent years, including neural network-based methods, which we will describe later. To be precise, there are two subproblems in dependency parsing (adapted from Kuebler et al., chapter 1. 2 ):  1. Learning:Given a training setDof sentences annotated with de- pendency graphs, induce a parsing modelMthat can be used to parse new sentences.   ## dependency parsing 2  2. Parsing:Given a parsing modelMand a sentenceS, derive the optimal dependency graphDforSaccording toM.\n\nfilename: materials_formatted.md\nextension: .md\n\nThe sentence structure is revealed as a side effect of the successive transformations. Sentence transformations can also have a semantic interpretation.    COLLOBERT, WESTON, BOTTOU, KARLEN, KAVUKCUOGLU ANDKUKSA  In the spirit of structural linguistics, Harris describes procedures to discover sentence trans- formation operators by leveraging the statistical regularities of the language. Such procedures are obviously useful for machine learning approaches. In particular, he proposes a test to decide whether two sentences forms are semantically related by a transformation operator. He first defines a ranking criterion (Harris, 1968, Section 4.1):   \u201cStarting for convenience with very short sentence forms, sayABC, we choose a particular word choice for all the classes, sayBqCq, except one, in this caseA; for every pair of membersAi,Ajof that word class we ask how the sentence formed with one of the members, that is,AiBqCqcompares as to acceptability with the sentence formed with the other member, that is,AjBqCq.\u201d  Thesegradingsare then used to compare sentence forms:   \u201cIt now turns out that, given the gradedn-tuples of words for a particular sentence form, we can find other sentences forms of the same word classes in which the same n-tuples of words produce the same grading of sentences.\u201d  This is an indication that these two sentence forms exploit common words with the same syntac- tic function and possibly the same meaning. This observation forms the empiricalbasis for the construction of operator grammars that describe real-world natural languages such as English. Therefore there are solid reasons to believe that the ranking criterion (17) has the conceptual potential to capture strong syntactic and semantic information. On the other hand, the structure of our language models is probably too restrictive for such goals, and our current approach only exploits the word embeddings discovered during training.  #### 5. Multi-Task Learning  It is generally accepted that featurestrainedfor one task can be useful forrelated tasks. This idea was already exploited in the previous section when certain language model features, namely the word embeddings, were used to initialize the supervised networks. Multi-task learning (MTL) leverages this idea in a more systematic way.\n\nfilename: materials_formatted.md\nextension: .md\n\nUnfortunately, no consensus has emerged about the form or the existence of such a data structure. Until such fundamental Articial Intelligence problems are resolved, computer scientists must settle forthe reduced objective of extracting simpler representations that describe limited aspects of the textual information. These simpler representations are often motivated by specific applications (for instance, bag- of-words variants for information retrieval), or by our belief that they capture something more gen- eral about natural language. They can describe syntactic information (e.g., part-of-speech tagging, chunking, and parsing) or semantic information (e.g., word-sense disambiguation, semantic role labeling, named entity extraction, and anaphora resolution). Text corpora have been manually an- notated with such data structures in order to compare the performance of various systems. The availability of standard benchmarks has stimulated research in Natural Language Processing (NLP)   \u2217. Ronan Collobert is now with the Idiap Research Institute, Switzerland. \u2020. Jason Weston is now with Google, New York, NY. \u2021. Leon Bottou is now with Microsoft, Redmond, WA. \u0301 \u00a7. Koray Kavukcuoglu is also with New York University, New York, NY. \u00b6. Pavel Kuksa is also with Rutgers University, New Brunswick, NJ.  \u00a9c2011 Ronan Collobert, Jason Weston, Leon Bottou, Michael Karlen, Koray Kavukcuoglu and Pavel Kuk \u0301 sa.    COLLOBERT, WESTON, BOTTOU, KARLEN, KAVUKCUOGLU ANDKUKSA  and effective systems have been designed for all these tasks. Such systems are often viewed as software components for constructing real-world NLP solutions. The overwhelming majority of these state-of-the-art systems address their single benchmark task by applying linear statistical models to ad-hoc features. In other words, the researchers them- selves discover intermediate representations by engineering task-specific features. These features are often derived from the output of preexisting systems, leading to complex runtime dependencies. This approach is effective because researchers leverage a large body of linguistic knowledge. On the other hand, there is a great temptation to optimize the performance of a system for a specific benchmark.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat are the two main types of structures used to analyze the syntactic structure of sentences in NLP?[/INST] \nAnswer: The two main types of structures used to analyze the syntactic structure of sentences in Natural Language Processing (NLP) are constituency structures and dependency structures."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\n7 Experiments  7.1 Setup  Datasets. We use two reading comprehension datasets: SQuAD (Rajpurkar et al., 2016) and Nat- ural Questions (NQ) (Kwiatkowski et al., 2019) to learn phrase representations, in which a single gold passage is provided for each question. For the open- domain QA experiments, we evaluate our approach on \ufb01ve popular open-domain QA datasets: Natu- ral Questions, WebQuestions (WQ) (Berant et al., 2013), CuratedTREC (TREC) (Baudi\u0161 and \u0160ediv`y, 2015), TriviaQA (TQA) (Joshi et al., 2017), and SQuAD. Note that we only use SQuAD and/or NQ to build the phrase index and perform query-side \ufb01ne-tuning (\u00a76) for other datasets.  We also evaluate our model on two slot \ufb01lling tasks, to show how to adapt our DensePhrases for other knowledge-intensive NLP tasks. We focus on using two slot \ufb01lling datasets from the KILT benchmark (Petroni et al., 2021): T-REx (Elsahar et al., 2018) and zero-shot relation extraction (Levy et al., 2017). Each query is provided in the form of \u201c{subject entity} [SEP] {relation}\" and the answer is the object entity. Appendix C provides the statistics of all the datasets.  Implementation details. We denote the training datasets used for reading comprehension (Eq. (9)) as Cphrase. For open-domain QA, we train two ver- sions of phrase encoders, each of which are trained on Cphrase = {SQuAD} and {NQ, SQuAD}, re- spectively. We build the phrase dump H for the 2018-12-20 Wikipedia snapshot and perform query- side \ufb01ne-tuning on each dataset using Eq. (11).\n\nfilename: materials_formatted.md\nextension: .md\n\nWe take the the paragraph vectors {p1, . . . , pm} and the question vector q as input, and simply train two classi\ufb01ers independently for predicting the two ends of the span. Concretely, we use a bilinear term to capture the similarity be- tween pi and q and compute the probabilities of each token being start and end as:  Pstart(i) \u221d exp (piWsq) Pend(i) \u221d exp (piWeq)  During prediction, we choose the best span from token i to token i(cid:48) such that i \u2264 i(cid:48) \u2264 i + 15 and Pstart(i) \u00d7 Pend(i(cid:48)) is maximized. To make scores  compatible across paragraphs in one or several re- trieved documents, we use the unnormalized expo- nential and take argmax over all considered para- graph spans for our \ufb01nal prediction.  4 Data  Our work relies on three types of data: (1) Wikipedia that serves as our knowledge source for \ufb01nding answers, (2) the SQuAD dataset which is our main resource to train Document Reader and (3) three more QA datasets (CuratedTREC, We- bQuestions and WikiMovies) that in addition to SQuAD, are used to test the open-domain QA abil- ities of our full system, and to evaluate the ability of our model to learn from multitask learning and distant supervision. Statistics of the datasets are given in Table 2.  4.1 Wikipedia (Knowledge Source) We use the 2016-12-21 dump2 of English Wikipedia for all of our full-scale experiments as the knowledge source used to answer questions. For each page, only the plain text is extracted and all structured data sections such as lists and \ufb01g- ures are stripped.3 After discarding internal dis- ambiguation, list, index, and outline pages, we retain 5,075,182 articles consisting of 9,008,962 unique uncased token types.  4.2 SQuAD  The Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) is a dataset for machine comprehension based on Wikipedia.\n\nfilename: materials_formatted.md\nextension: .md\n\n[24] A. Mnih, Y.W. Teh. A fast and simple algorithm for training neural probabilistic language models. ICML, 2012.  [25] F. Morin, Y. Bengio. Hierarchical Probabilistic Neural Network Language Model. AISTATS, 2005.  [26] D. E. Rumelhart, G. E. Hinton, R. J. Williams. Learning internal representations by back- propagating errors. Nature, 323:533.536, 1986.  [27] H. Schwenk. Continuous space language models. Computer Speech and Language, vol. 21, 2007.  [28] R. Socher, E.H. Huang, J. Pennington, A.Y. Ng, and C.D. Manning. Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection. In NIPS, 2011.  [29] J. Turian, L. Ratinov, Y. Bengio. Word Representations: A Simple and General Method for Semi-Supervised Learning. In: Proc. Association for Computational Linguistics, 2010.  [30] P. D. Turney. Measuring Semantic Similarity by Latent Relational Analysis. In: Proc. Interna- tional Joint Conference on Artificial Intelligence, 2005.  [31] A. Zhila, W.T. Yih, C. Meek, G. Zweig, T. Mikolov. Combining Heterogeneous Models for Measuring Relational Similarity. NAACL HLT 2013.  [32] G. Zweig, C.J.C. Burges. The Microsoft Research Sentence Completion Challenge, Microsoft Research Technical Report MSR-TR-2011-129, 2011.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat are the two datasets mentioned for sentence classification in the SEP775 Final Projects document?[/INST] \nAnswer: I don't know."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\n4We note that in the literature the bidirectional Trans-  BERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLIInput/Output Representations To make BERT handle a variety of down-stream tasks, our input representation is able to unambiguously represent both a single sentence and a pair of sentences (e.g., (cid:104) Question, Answer (cid:105)) in one token sequence. Throughout this work, a \u201csentence\u201d can be an arbi- trary span of contiguous text, rather than an actual linguistic sentence. A \u201csequence\u201d refers to the in- put token sequence to BERT, which may be a sin- gle sentence or two sentences packed together.  We use WordPiece embeddings (Wu et al., 2016) with a 30,000 token vocabulary. The \ufb01rst token of every sequence is always a special clas- si\ufb01cation token ([CLS]). The \ufb01nal hidden state corresponding to this token is used as the ag- gregate sequence representation for classi\ufb01cation tasks. Sentence pairs are packed together into a single sequence. We differentiate the sentences in two ways. First, we separate them with a special token ([SEP]). Second, we add a learned embed- ding to every token indicating whether it belongs to sentence A or sentence B. As shown in Figure 1, we denote input embedding as E, the \ufb01nal hidden vector of the special [CLS] token as C \u2208 RH , and the \ufb01nal hidden vector for the ith input token as Ti \u2208 RH .  For a given token, its input representation is constructed by summing the corresponding token, segment, and position embeddings.\n\nfilename: materials_formatted.md\nextension: .md\n\nUnlike recent language repre- sentation models (Peters et al., 2018a; Rad- ford et al., 2018), BERT is designed to pre- train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a re- sult, the pre-trained BERT model can be \ufb01ne- tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task- speci\ufb01c architecture modi\ufb01cations.  BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art re- sults on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answer- ing Test F1 to 93.2 (1.5 point absolute im- provement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).  1  Introduction  Language model pre-training has been shown to be effective for improving many natural language processing tasks (Dai and Le, 2015; Peters et al., 2018a; Radford et al., 2018; Howard and Ruder, 2018). These include sentence-level tasks such as natural language inference (Bowman et al., 2015; Williams et al., 2018) and paraphrasing (Dolan and Brockett, 2005), which aim to predict the re- lationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce \ufb01ne-grained output at the token level (Tjong Kim Sang and De Meulder, 2003; Rajpurkar et al., 2016).  There are two existing strategies for apply- ing pre-trained language representations to down- stream tasks: feature-based and \ufb01ne-tuning. The feature-based approach, such as ELMo (Peters et al., 2018a), uses task-speci\ufb01c architectures that include the pre-trained representations as addi- tional features.\n\nfilename: materials_formatted.md\nextension: .md\n\n# 2 Implement Bert Model  ## 2.1 Code structure:  - bert.py\u2013 This file contains the BERT Model whose backbone is the transformer. It     contains three classes:        - BertSelfAttention:The multi-head attention layer of the transformer. This layer           maps a query and a set of key-value pairs to an output. The output is calculated           as the weighted sum of the values, where the weight of each value is computed by a           function that takes the query and the corresponding key. To implement this layer,           you can:              \u2217 linearly project the queries, keys, and values with their corresponding linear                 layers              \u2217 split the vectors for multi-head attention              \u2217 follow the equation to compute the attended output of each head              \u2217 concatenate multi-head attention outputs to recover the original shape   Attention(Q, K, V) =softmax(  ### QKT   \u221a   dk   )V  - BertLayer: This corresponds to one transformer layer which has     \u2217 a multi-head attention layer     \u2217 add-norm layer     \u2217 a feed-forward layer     \u2217 another add-norm layer - BertModel: This is the BertModel that takes the input ids and returns the con-     textualized representation for each word. The structure of the BertModel is:    \u2217 an embedding layer that consists of word embedding word_embedding and po- sitional embeddingpos_embedding. \u2217 bert encoder layer which is a stack of config.num_hidden_layers BertLaye \u2217 a projection layer for [CLS] token which is often used for classification tasks The desired outputs are \u2217 last_hidden_state: the contextualized embedding for each word of the sentence, taken from the last BertLayer (i.e. the output of the bert encoder) \u2217 pooler_output: the [CLS] token embedding  - sanity_check.py\u2013 You can use this file to test your BERT implementation - classifier.py\u2013 This file is the main entrance of training and testing the BERT Model     for text classification task, contains the pipeline to        - call the BERT model to encode the sentences for their contextualized representations        - feed in the encoded representations for the sentence classification task        - fine-tune the Bert model on the downstream tasks (e.g. sentence classification) - base_bert.py- This is the base class for the BertModel.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat are the two steps in the BERT framework?[/INST] \nAnswer: The two steps in the BERT framework are pre-training and fine-tuning. In the pre-training step, deep bidirectional representations are learned from unlabeled text by jointly conditioning on both left and right context in all layers. In the fine-tuning step, the pre-trained BERT model is fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nThe dataset contains 87k examples for training and 10k for development, with a large hidden test set which can only be accessed by the SQuAD creators. Each example is composed of a paragraph extracted from a Wikipedia article and an associated human-generated question. The answer is always a span from this paragraph and a model is given credit if its predicted answer matches it. Two evaluation metrics are used: exact string match (EM) and F1 score, which measures the weighted average of precision and recall at the token level.  In the following, we use SQuAD for training and evaluating our Document Reader for the stan- dard machine comprehension task given the rel-  2https://dumps.wikimedia.org/enwiki/  latest  evant paragraph as de\ufb01ned in (Rajpurkar et al., 2016). For the task of evaluating open-domain question answering over Wikipedia, we use the SQuAD development set QA pairs only, and we ask systems to uncover the correct answer spans without having access to the associated para- graphs. That is, a model is required to answer a question given the whole of Wikipedia as a re- source; it is not given the relevant paragraph as in the standard SQuAD setting.  4.3 Open-domain QA Evaluation Resources  SQuAD is one of the largest general purpose QA datasets currently available. SQuAD questions have been collected via a process involving show- ing a paragraph to each human annotator and ask- ing them to write a question. As a result, their distribution is quite speci\ufb01c. We hence propose to train and evaluate our system on other datasets de- veloped for open-domain QA that have been con- structed in different ways (not necessarily in the context of answering from Wikipedia).  CuratedTREC This dataset is based on the benchmarks from the TREC QA tasks that have been curated by Baudi\u02c7s and \u02c7Sediv`y (2015). We use the large version, which contains a total of 2,180 questions extracted from the datasets from TREC 1999, 2000, 2001 and 2002.4  WebQuestions Introduced in (Berant et al., 2013), this dataset is built to answer questions from the Freebase KB.\n\nfilename: materials_formatted.md\nextension: .md\n\nCHANGES TO V2 FROM V1  \u2022 We updated the terminology to \u201cdatasets\u201d and \u201ctask clusters.\u201d  \u2022 We renamed the previous \u201copen-domain QA\u201d task cluster to \u201cclosed-book QA.\u201d  \u2022 We extended the related work section and moved it to the Appendix D, using a shorter  version in the main body.  \u2022 We added FLAN and LaMDA-PT results for additional datasets for which GPT-3 results  were not reported.  \u2022 For TriviaQA, v1 reported results on the tfds dev set of 11,313 examples. GPT-3 actually evaluates on the wikipedia dev set of 7,993 examples, so we ran an additional evaluation on that dev set in order to compare with GPT-3\u2019s performance. Zero-shot FLAN now beats zero-shot GPT-3 on that task (and therefore on 20 of 25 tasks). We still show the original result in Table 2, though there is no GPT-3 result to compare with.  \u2022 We moved commonsense reasoning and coreference resolution from the main body to the  Appendix.  \u2022 We moved prompt tuning from the main body to \u00a74.5.  \u2022 We added data contamination analysis (Appendix C). \u2022 We added few-shot instruction tuning (\u00a74.4).  \u2022 We cited additional datasets in Appendix G.  \u2022 The number of tokens used in pretraining was corrected from 2.81T to 2.49T tokens.  35  Published as a conference paper at ICLR 2022  G TASKS AND DATASETS  This appendix further details the datasets that we use in this paper. We group datasets into one of the following task clusters:  \u2022 Natural language inference concerns how two sentences relate, typically asking, given a \ufb01rst sentence, whether a second sentence is true, false, or possibly true. We use the following datasets:  1. ANLI (Nie et al., 2020) 2. CB (De Marneffe et al., 2019) 3. MNLI (Williams et al., 2018) 4. QNLI (Rajpurkar et al., 2018) 5. SNLI (Bowman et al., 2015) 6.\n\nfilename: materials_formatted.md\nextension: .md\n\n7 Experiments  7.1 Setup  Datasets. We use two reading comprehension datasets: SQuAD (Rajpurkar et al., 2016) and Nat- ural Questions (NQ) (Kwiatkowski et al., 2019) to learn phrase representations, in which a single gold passage is provided for each question. For the open- domain QA experiments, we evaluate our approach on \ufb01ve popular open-domain QA datasets: Natu- ral Questions, WebQuestions (WQ) (Berant et al., 2013), CuratedTREC (TREC) (Baudi\u0161 and \u0160ediv`y, 2015), TriviaQA (TQA) (Joshi et al., 2017), and SQuAD. Note that we only use SQuAD and/or NQ to build the phrase index and perform query-side \ufb01ne-tuning (\u00a76) for other datasets.  We also evaluate our model on two slot \ufb01lling tasks, to show how to adapt our DensePhrases for other knowledge-intensive NLP tasks. We focus on using two slot \ufb01lling datasets from the KILT benchmark (Petroni et al., 2021): T-REx (Elsahar et al., 2018) and zero-shot relation extraction (Levy et al., 2017). Each query is provided in the form of \u201c{subject entity} [SEP] {relation}\" and the answer is the object entity. Appendix C provides the statistics of all the datasets.  Implementation details. We denote the training datasets used for reading comprehension (Eq. (9)) as Cphrase. For open-domain QA, we train two ver- sions of phrase encoders, each of which are trained on Cphrase = {SQuAD} and {NQ, SQuAD}, re- spectively. We build the phrase dump H for the 2018-12-20 Wikipedia snapshot and perform query- side \ufb01ne-tuning on each dataset using Eq. (11).\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhich dataset is used in SEP775 Assignment 4 for the programming-related QA system?[/INST] \nAnswer: I don't know."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nInstead, we introduce a more general weighting function, which we are free to take to depend on the context word as well:  #### J\u02c6=   W   \u2211   i= 1   W   \u2211   j= 1   f(Xij)(~uTj~vi\u2212logXij)^2  ### 1. 4 Conclusion  In conclusion, the GloVe model efficiently leverages global statistical information by training only on the nonzero elements in a word- word co-occurrence matrix, and produces a vector space with mean- ingful sub-structure. It consistently outperformsword 2 vecon the word analogy task, given the same corpus, vocabulary, window size, and training time. It achieves better results faster, and also obtains the best results irrespective of speed.  ## 2 Evaluation of Word Vectors  So far, we have discussed methods such as theWord _2_ VecandGloVe methods to train and discover latent vector representations of natural language words in a semantic space. In this section, we discuss how we can quantitatively evaluate the quality of word vectors produced by such techniques.  ### 2. 1 Intrinsic Evaluation  Intrinsic evaluation of word vectors is the evaluation of a set of word vectors generated by an embedding technique (such as Word 2 Vec or GloVe) on specific intermediate subtasks (such as analogy comple- tion). These subtasks are typically simple and fast to compute and thereby allow us to help understand the system used to generate the word vectors. An intrinsic evaluation should typically return to us a number that indicates the performance of those word vectors on the evaluation subtask.   Figure 1 : The left subsystem (red) being expensive to train is modified by substituting with a simpler subsystem (green) for intrinsic evaluation.  Motivation: Let us consider an example where our final goal is to create a question answering system which uses word vectors   ## word vectors ii:glove,evaluation and training 4  as inputs. One approach of doing so would be to train a machine learning system that:  1. Takes words as inputs  2. Converts them to word vectors  3. Uses word vectors as inputs for an elaborate machine learning system  4. Maps the output word vectors by this system back to natural language words  5.\n\nfilename: materials_formatted.md\nextension: .md\n\nMaps the output word vectors by this system back to natural language words  5. Produces words as answers Intrinsic evaluation:  - Evaluation on a specific, intermedi-     ate task - Fast to compute performance - Helps understand subsystem - Needs positive correlation with real     task to determine usefulness  Of course, in the process of making such a state-of-the-art question- answering system, we will need to create optimal word-vector repre- sentations since they are used in downstream subsystems (such as deep neural networks). To do this in practice, we will need to tune many hyperparameters in the Word 2 Vec subsystem (such as the dimension of the word vector representation). While the idealistic approach is to retrain the entire system after any parametric changes in the Word 2 Vec subsystem, this is impractical from an engineering standpoint because the machine learning system (in step 3 ) is typi- cally a deep neural network with millions of parameters that takes very long to train. In such a situation, we would want to come up with a simple intrinsic evaluation technique which can provide a measure of \"goodness\" of the word to word vector subsystem. Ob- viously, a requirement is that the intrinsic evaluation has a positive correlation with the final task performance.  ### 2. 2 Extrinsic Evaluation   Extrinsic evaluation:  - Is the evaluation on a real task - Can be slow to compute perfor-     mance - Unclear if subsystem is the prob-     lem, other subsystems, or internal     interactions - If replacing subsystem improves     performance, the change is likely     good  Extrinsic evaluation of word vectors is the evaluation of a set of word vectors generated by an embedding technique on the real task at hand. These tasks are typically elaborate and slow to compute. Using our example from above, the system which allows for the evalua- tion of answers from questions is the extrinsic evaluation system. Typically, optimizing over an underperforming extrinsic evaluation system does not allow us to determine which specific subsystem is at fault and this motivates the need for intrinsic evaluation.  ### 2. 3 Intrinsic Evaluation Example: Word Vector Analogies  A popular choice for intrinsic evaluation of word vectors is its per- formance in completing word vector analogies.\n\nfilename: materials_formatted.md\nextension: .md\n\nWe can also test syntax using word vector analogies. The following intrinsic evaluation tests the word vectors\u2019 ability to capture the notion of superlative adjectives: Similarly, the intrinsic evaluation shown below tests the word vectors\u2019 ability to capture the notion of past tense:   ## word vectors ii:glove,evaluation and training 6   Input Result Produced Abuja : Nigeria : : Accra Ghana Abuja : Nigeria : : Algiers Algeria Abuja : Nigeria : : Amman Jordan Abuja : Nigeria : : Ankara Turkey Abuja : Nigeria : : Antananarivo Madagascar Abuja : Nigeria : : Apia Samoa Abuja : Nigeria : : Ashgabat Turkmenistan Abuja : Nigeria : : Asmara Eritrea Abuja : Nigeria : : Astana Kazakhstan   Table 2 : Here are semantic word vector analogies (intrinsic evaluation) that may suffer from countries having different capitals at different points in time   Input Result Produced bad : worst : : big biggest bad : worst : : bright brightest bad : worst : : cold coldest bad : worst : : cool coolest bad : worst : : dark darkest bad : worst : : easy easiest bad : worst : : fast fastest bad : worst : : good best bad : worst : : great greatest   Table 3 : Here are syntactic word vector analogies (intrinsic evaluation) that test the notion of superlative adjectives  ### 2. 4 Intrinsic Evaluation Tuning Example: Analogy Evaluations   Some parameters we might consider tuning for a word embedding technique on intrinsic evaluation tasks are:  - Dimension of word vectors - Corpus size - Corpus souce/type - Context window size - Context symmetry Can you think of other hyperparame- ters tunable at this stage?  We now explore some of the hyperparameters in word vector em- bedding techniques (such as Word 2 Vec and GloVe) that can be tuned using an intrinsic evaluation system (such as an analogy completion system).\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat example is given for intrinsic evaluation of word vectors?[/INST] \nAnswer: An example given for intrinsic evaluation of word vectors is its performance in completing word vector analogies. This can test the word vectors' ability to capture the notion of superlative adjectives and past tense. For instance, the word vectors' ability to complete analogies such as \"bad : worst :: big : biggest\" or \"Abuja : Nigeria :: Accra : Ghana\" is tested."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\n(2016) does not report the test scores. All results shown here re\ufb02ect the SQuAD leaderboard (stanford-qa.com) as of 6 Dec 2016, 12pm PST. (1b) The performance of our model and its ablations on the SQuAD dev set. Ablation results are presented only for single runs.  train/dev question-context tuples with a large hidden test set. It is one of the largest available MC datasets with human-written questions and serves as a great test bed for our model.  Model Details. The model architecture used for this task is depicted in Figure 1. Each paragraph and question are tokenized by a regular-expression-based word tokenizer (PTB Tokenizer) and fed into the model. We use 100 1D \ufb01lters for CNN char embedding, each with a width of 5. The hidden state size (d) of the model is 100. The model has about 2.6 million parameters. We use the AdaDelta (Zeiler, 2012) optimizer, with a minibatch size of 60 and an initial learning rate of 0.5, for 12 epochs. A dropout (Srivastava et al., 2014) rate of 0.2 is used for the CNN, all LSTM layers, and the linear transformation before the softmax for the answers. During training, the moving averages of all weights of the model are maintained with the exponential decay rate of 0.999. At test time, the moving averages instead of the raw weights are used. The training process takes roughly 20 hours on a single Titan X GPU. We also train an ensemble model consisting of 12 training runs with the identical architecture and hyper-parameters. At test time, we choose the answer with the highest sum of con\ufb01dence scores amongst the 12 runs for each question.  Results. The results of our model and competing approaches on the hidden test are summarized in Table 1a. BIDAF (ensemble) achieves an EM score of 73.3 and an F1 score of 81.1, outperforming all previous approaches.  Ablations. Table 1b shows the performance of our model and its ablations on the SQuAD dev set.\n\nfilename: materials_formatted.md\nextension: .md\n\nOur best model achieves an F1 score of 51.0%,1 which is much better than the sliding window base- line (20%). Over the last four months (since June 2016), we have witnessed signi\ufb01cant improvements from more sophisticated neural network-based mod- els. For example, Wang and Jiang (2016) obtained 70.3% F1 on SQuAD v1.1 (results on v1.0 are sim- ilar). These results are still well behind human performance, which is 86.8% F1 based on inter- annotator agreement. This suggests that there is plenty of room for advancement in modeling and learning on the SQuAD dataset.  1All experimental results in this paper are on SQuAD v1.0.  Dataset  SQuAD  Question source  Formulation  Size  crowdsourced RC,  spans  100K  MCTest (Richardson et al., 2013) Algebra (Kushman et al., 2014) Science (Clark and Etzioni, 2016)  crowdsourced  standardized tests standardized tests  in passage  RC, multiple choice computation  reasoning, multiple choice  2640  514  855  WikiQA (Yang et al., 2015) TREC-QA (Voorhees and Tice, 2000)  query logs  query logs + human editor  3047  IR, sentence selection IR, free form 1479  CNN/Daily Mail (Hermann et al., 2015) CBT (Hill et al., 2015)  summary cloze cloze  +  RC, \ufb01ll in single entity RC, \ufb01ll in single word  1.4M  688K  Table 1: A survey of several reading comprehension and ques-  tion answering datasets. SQuAD is much larger than all datasets  except the semi-synthetic cloze-style datasets, and it is similar  to TREC-QA in the open-endedness of the answers.  2 Existing Datasets  We begin with a survey of existing reading com- prehension and question answering (QA) datasets, highlighting a variety of task formulation and cre- ation strategies (see Table 1 for an overview).  Reading comprehension.\n\nfilename: materials_formatted.md\nextension: .md\n\nThe answer to each question is always a span in the context. The model is given a credit if its answer matches one of the human written answers. Two metrics are used to evaluate models: Exact Match (EM) and a softer metric, F1 score, which measures the weighted average of the precision and recall rate at character level. The dataset consists of 90k/10k  5  Published as a conference paper at ICLR 2017  Logistic Regression Baselinea Dynamic Chunk Readerb Fine-Grained Gatingc Match-LSTMd Multi-Perspective Matchinge Dynamic Coattention Networksf R-Netg BIDAF (Ours)  Single Model EM 40.4 62.5 62.5 64.7 65.5 66.2 68.4 68.0  F1 51.0 71.0 73.3 73.7 75.1 75.9 77.5 77.3  Ensemble EM F1 - - - 77.0 77.2 80.4 79.7 81.1  - - - 67.9 68.2 71.6 72.1 73.3  No char embedding No word embedding No C2Q attention No Q2C attention Dynamic attention BIDAF (single) BIDAF (ensemble)  EM F1 75.4 65.0 66.8 55.5 67.7 57.2 73.7 63.6 73.6 63.5 77.3 67.7 80.7 72.6  (b) Ablations on the SQuAD dev set  (a) Results on the SQuAD test set  Table 1: (1a) The performance of our model BIDAF and competing approaches by Rajpurkar et al. (2016)a, Yu et al. (2016)b, Yang et al. (2016)c, Wang & Jiang (2016)d, IBM Watsone (unpublished), Xiong et al. (2016b)f , and Microsoft Research Asiag (unpublished) on the SQuAD test set. A concurrent work by Lee et al. (2016) does not report the test scores.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat are the key components of the logistic regression model built to assess the difficulty of SQuAD?[/INST] \nAnswer: I don't know."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nThe heads clearly learned to perform different tasks.  15  Input-Input Layer5TheLawwillneverbeperfect,butitsapplicationshouldbejust-thisiswhatwearemissing,inmyopinion.<EOS><pad>TheLawwillneverbeperfect,butitsapplicationshouldbejust-thisiswhatwearemissing,inmyopinion.<EOS><pad>Input-Input Layer5TheLawwillneverbeperfect,butitsapplicationshouldbejust-thisiswhatwearemissing,inmyopinion.<EOS><pad>TheLawwillneverbeperfect,butitsapplicationshouldbejust-thisiswhatwearemissing,inmyopinion.<EOS><pad>   \n\n# MUSIC TRANSFORMER - GENERATING MUSIC WITH LONG-TERM STRUCTURE:  MUSIC TRANSFORMER: GENERATING MUSIC WITH LONG-TERM STRUCTURE  Cheng-Zhi Anna Huang\u2217 Ashish Vaswani Ian Simon Curtis Hawthorne Andrew M. Dai Matthew D. Hoffman Monica Dinculescu Douglas Eck Google Brain  Jakob Uszkoreit Noam Shazeer  ABSTRACT  Music relies heavily on repetition to build structure and meaning. Self-reference occurs on multiple timescales, from motifs to phrases to reusing of entire sections of music, such as in pieces with ABA structure. The Transformer (Vaswani et al., 2017), a sequence model based on self-attention, has achieved compelling results in many generation tasks that require maintaining long-range coherence. This suggests that self-attention might also be well-suited to modeling music. In musical composition and performance, however, relative timing is critically important. Existing approaches for representing relative positional information in the Transformer modulate attention based on pairwise distance (Shaw et al., 2018). This is impractical for long sequences such as musical compositions since their memory complexity for intermediate relative information is quadratic in the sequence length. We propose an algorithm that reduces their intermediate memory requirement to linear in the sequence length. This enables us to demonstrate that a Transformer with our modi\ufb01ed relative attention mechanism can generate minute- long compositions (thousands of steps, four times the length modeled in Oore et al. (2018)) with compelling structure, generate continuations that coherently elaborate on a given motif, and in a seq2seq setup generate accompaniments conditioned on melodies1.\n\nfilename: materials_formatted.md\nextension: .md\n\nOur work show that Transformers not only achieve state-of-the-art perplexity on modeling these complex expressive piano performances, but can also generate them at the scale of 60s (~2000 tokens) with remarkable internal consistency. Our relative attention mechanism is essential to the model\u2019s quality. In listening tests (see Section 4.2.3), samples from models with relative self-attention were perceived as more coherent than the baseline Transformer model from Vaswani et al. (2017). Relative attention not only enables Transformers to generate continuations that elaborate on a given motif, but also to generalize and generate in consistent fashion beyond the length it was trained on (see Section 4.2.1). In a seq2seq setup, Transformers can generate accompaniments conditioned on melodies, enabling users to interact with the model.  Algorithmic contributions The space complexity of the relative self attention mechanism in its original formulation (Shaw et al., 2018) made it infeasible to train on sequences of suf\ufb01cient length to capture long-range structure in longer musical compositions. Addressing this we present a crucial algorithmic improvement to the relative self attention mechanism, dramatically reducing its memory requirements from O(L2D) to O(LD). For example, we reduce the memory consumption per layer from 8.5 GB to 4.2 MB (per head from 1.1 GB to 0.52 MB) for a sequence of length L = 2048 and hidden-state size D = 512 (per head Dh = D H = 64, where number of heads is H = 8) (see Table 1), allowing us to use GPUs to train the relative self-attention Transformer on long sequences.  2 RELATED WORK  Sequence models have been the canonical choice for modeling music, from Hidden Markov Models to RNNs and Long Short Term Memory networks (e.g., Eck & Schmidhuber, 2002; Liang, 2016; Oore et al., 2018), to bidirectional LSTMs (e.g., Hadjeres et al., 2017). Successful application of sequential models to polyphonic music often requires serializing the musical score or performance  into a single sequence, for example by interleaving different instruments or voices.\n\nfilename: materials_formatted.md\nextension: .md\n\nAs opposed to the original Transformer, samples from a Transformer with our relative attention mechanism maintain the regular timing grid present in this dataset. The model furthermore captures global timing, giving rise to regular phrases. The original formulation of relative attention (Shaw et al., 2018) requires O(L2D) memory where L is the sequence length and D is the dimension of the model\u2019s hidden state. This is prohibitive for long sequences such as those found in the Piano-e-Competition dataset of human-performed virtuosic, classical piano music. In Section 3.4, we show how to reduce the memory requirements to O(LD), making it practical to apply relative attention to long sequences.  The Piano-e-Competition dataset consists of MIDI recorded from performances of competition partic- ipants, bearing expressive dynamics and timing on the granularity of < 10 miliseconds. Discretizing time on a \ufb01xed grid that would yield unnecessarily long sequences as not all events change on the same timescale. We hence adopt a sparse, MIDI-like, event-based representation from (Oore et al., 2018), allowing a minute of music with 10 milisecond resolution to be represented at lengths around 2K, as opposed to 6K to 18K on a \ufb01xed-grid representation with multiple performance attributes. As position in sequence no longer corresponds to time, a priori it is not obvious that relative attention should work as well with such a representation. However, we will show in Section 4.2 that it does improve perplexity and sample quality over strong baselines.  We speculate that idiomatic piano gestures such as scales, arpeggios and other motifs all exhibit a certain grammar and recur periodically, hence knowing their relative positional distances makes it easier to model this regularity. This inductive bias towards learning relational information, as opposed to patterns based on absolute position, suggests that the Transformers with relative attention could generalize beyond the lengths it was trained on, which our experiments in Section 4.2.1 con\ufb01rm.  1.1 CONTRIBUTIONS  Domain contributions We show the \ufb01rst successful use of Transformers in generating music that exhibits long-term structure. Before our work, LSTMs were used at timescales of 15s (~500 tokens) on the Piano-e-Competition dataset (Oore et al., 2018).\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nHow does the Music Transformer handle the challenge of relative timing in musical composition?[/INST] \nAnswer: The Music Transformer addresses the challenge of relative timing in musical composition by using a modified relative attention mechanism. This mechanism is essential for maintaining the model's quality and enabling it to generate continuations that elaborate on a given motif. It also allows the model to generalize and generate in a consistent manner beyond the length it was trained on. The original formulation of relative attention required a significant amount of memory, making it impractical for long sequences. However, an algorithmic improvement was made to the relative self attention mechanism, dramatically reducing its memory requirements, and making it practical to apply relative attention to long sequences."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nUnlike recent language repre- sentation models (Peters et al., 2018a; Rad- ford et al., 2018), BERT is designed to pre- train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a re- sult, the pre-trained BERT model can be \ufb01ne- tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task- speci\ufb01c architecture modi\ufb01cations.  BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art re- sults on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answer- ing Test F1 to 93.2 (1.5 point absolute im- provement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).  1  Introduction  Language model pre-training has been shown to be effective for improving many natural language processing tasks (Dai and Le, 2015; Peters et al., 2018a; Radford et al., 2018; Howard and Ruder, 2018). These include sentence-level tasks such as natural language inference (Bowman et al., 2015; Williams et al., 2018) and paraphrasing (Dolan and Brockett, 2005), which aim to predict the re- lationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce \ufb01ne-grained output at the token level (Tjong Kim Sang and De Meulder, 2003; Rajpurkar et al., 2016).  There are two existing strategies for apply- ing pre-trained language representations to down- stream tasks: feature-based and \ufb01ne-tuning. The feature-based approach, such as ELMo (Peters et al., 2018a), uses task-speci\ufb01c architectures that include the pre-trained representations as addi- tional features.\n\nfilename: materials_formatted.md\nextension: .md\n\nAn illustration of how ORQA scores answer derivations is presented in Figure 1. Following recent advances in transfer learn- ing, all scoring components are derived from BERT (Devlin et al., 2018), a bidirectional trans- former that has been pre-trained on unsupervised language-modeling data. We refer the reader to the original paper for details of the architecture. In this work, the relevant abstraction can be de- scribed by the following function:  BERT(x1, [x2]) = {CLS : hCLS, 1 : h1, 2 : h2, ...}  The BERT function takes one or two string in- puts (x1 and optionally x2) as arguments. It re- turns vectors corresponding to representations of the CLS pooling token or the input tokens.  Retriever component In order for the retriever to be learnable, we de\ufb01ne the retrieval score as the inner product of dense vector representations of the question q and the evidence block b.  hq = WqBERTQ(q)[CLS] hb = WbBERTB(b)[CLS]  Sretr (b, q) = h(cid:62)  q hb  where Wq and Wb are matrices that project the BERT output into 128-dimensional vectors.  Reader component The reader is a span-based variant of the reading comprehension model pro- posed in Devlin et al. (2018):  hstart = BERTR(q, b)[START(s)] hend = BERTR(q, b)[END(s)]  Sread (b, s, q) = MLP([hstart ; hend ])  Following Lee et al. (2016), a span is represented by the concatenation of its end points, which is scored by a multi-layer perceptron to enable start/end interaction.  Inference & Learning Challenges The model described above is conceptually simple. However, inference and learning are challenging since (1) an  Example  Q: Who is credited with developing the XY coordinate plane? A: Ren\u00b4e Descartes  Q: How many districts are in the state of Alabama?\n\nfilename: materials_formatted.md\nextension: .md\n\nF1 EM Humanperformance 91.2* 82.3* BiDAF 77.3 67.7 BERT-base 88.5 80.8 BERT-large 90.9 84.1 XLNet 94.5 89.0 RoBERTa 94.6 88.9 ALBERT 94.8 89.3 (dev set, except for human performance)   Comparisons between BiDAF and BERT models   36  - BERT model has many many more parameters (110M or 330M)     BiDAF has ~2.5M parameters. - BiDAF is built on top of several bidirectional LSTMs while BERT is built on top of     Transformers (no recurrence architecture and easier to parallelize). - BERT is pre-trained while BiDAF is only built on top of GloVe (and all the remaining     parameters need to be learned from the supervision datasets).   Pre-training is clearly a game changer but it is expensive..   Can we design better pre-training objectives?   (Joshi&Chenetal., 2020):SpanBERT:ImprovingPre-training byRepresenting andPredictingSpans^37  The answer is yes!   Two ideas:  - masking contiguous spans of words instead of 15% random words - using the two end points of span to predict all the masked words in between = compressing the     information of a span into its two endpoints   #### SpanBERT performance   38   91.3   83.3   68.8   77.5   81.7 78.3 79.9   92.6   85.9   71.0   79.0   81.8 80.5 80.5   94.6 88.7   73.6   83.6 84.8 83.0 82.8   65   73   80   88   95   SQuADv1.1 SQuADv2.0 NewsQA TriviaQA SearchQA HotpotQA NaturalQuestions  Google BERT Our BERT   SpanBERT F1 scores   #### Is reading comprehension solved?  - We have already surpassed human performance on SQuAD. Does it mean that reading     comprehension is already solved?\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat advancements did SpanBERT introduce to improve upon BERT for QA tasks?[/INST] \nAnswer: SpanBERT introduced two main advancements to improve upon BERT for question answering tasks. The first advancement is the masking of contiguous spans of words instead of 15% random words. The second advancement is using the two end points of a span to predict all the masked words in between, which effectively compresses the information of a span into its two endpoints."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nwith deep learning 14  Cross-Attention. Cross-attention uses one sequence to define the keys and values of self-attention, and another sequence to define the queries. You might think, hey wait, isn\u2019t that just what attention always was before we got into this self-attention business? Yeah, pretty much. So if   h (1:xn)=TransformerEncoder( w 1:n), ( 33 )  and we have some intermediate representation h (y)of sequence y 1:m, then we let the queries come from the decoder (the h (y)sequence) while the keys and values come from the encoder:   q i=Q h (iy) i\u2208{1,... ,m} ( 34 ) k j=K h (jx) j\u2208{1,... ,n} ( 35 ) v j=V h (jx) j\u2208{1,... ,n}, ( 36 )  and compute the attention on q , k , v as we defined for self-attention. Note in Figure 6 that in the Transformer Encoder-Decoder, cross- attention always applies to the output of the Transformer encoder.  Uses of the encoder-decoder. An encoder-decoder is used when we\u2019d like bidirectional context on something (like an article to summa- rize) to build strong represenations (i.e., each token can attend to all other tokens), but then generate an output according to an au- toregressive decomposition as we can with a decoder. While such an architecture has been found to provide better performance than decoder-only models at modest scale [Raffel et al., 2020 ], it involves splitting parameters between encoder and decoder, and most of the largest Transformers are decoder-only.  ##### . 1  #### References  [Ba et al., 2016 ] Ba, J. L., Kiros, J. R., and Hinton, G. E. ( 2016 ). Layer normalization. arXiv preprint arXiv: _1607_. _06450_. [Bahdanau et al., 2014 ] Bahdanau, D., Cho, K., and Bengio, Y. ( 2014 ). Neural machine translation by jointly learning to align and translate.arXiv preprint arXiv: _1409_.\n\nfilename: materials_formatted.md\nextension: .md\n\nFigure 1: The Transformer - model architecture.  The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.  3.1 Encoder and Decoder Stacks  Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position- wise fully connected feed-forward network. We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512.  Decoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.  3.2 Attention  An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum  Scaled Dot-Product Attention  Multi-Head Attention  Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.  of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.  3.2.1 Scaled Dot-Product Attention  We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2).\n\nfilename: materials_formatted.md\nextension: .md\n\nMulti-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.  MultiHead(Q, K, V ) = Concat(head1, ..., headh)W O  where headi = Attention(QW Q  i , KW K i  , V W V  i )  Where the projections are parameter matrices W Q and W O \u2208 Rhdv\u00d7dmodel.  i \u2208 Rdmodel\u00d7dk , W K  i \u2208 Rdmodel\u00d7dk , W V  i \u2208 Rdmodel\u00d7dv  In this work we employ h = 8 parallel attention layers, or heads. For each of these we use dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.  3.2.3 Applications of Attention in our Model  The Transformer uses multi-head attention in three different ways:  \u2022 In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9].  \u2022 The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.  \u2022 Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to \u2212\u221e) all values in the input of the softmax which correspond to illegal connections. See Figure 2.  3.3 Position-wise Feed-Forward Networks  In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nExplain the concept of cross-attention in the Transformer Encoder-Decoder architecture.[/INST] \nAnswer: Cross-attention in the Transformer Encoder-Decoder architecture involves using one sequence to define the keys and values of self-attention, and another sequence to define the queries. In this context, the queries come from the decoder while the keys and values come from the encoder. This mechanism allows every position in the decoder to attend over all positions in the input sequence. It is important to note that cross-attention always applies to the output of the Transformer encoder. This approach is used to build strong representations with bidirectional context and to generate an output according to an autoregressive decomposition."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\n\u2211  BLEU= BP\u00b7exp  <i>w<sub>n</sub></i> log <i>p<sub>n</sub></i>  .  *n*=1  The ranking behavior is more immediately apparent  in the log domain,  *N*  *r*  log BLEU = min(1\u2212 ,0)+  \u2211  *w* log *p* .  *n*  *n*  *c*  *n*=1  In our baseline, we use *N* = 4 and uniform weights  <i><sup>w</sup>n</i> <sup>= 1/<i>N</i>.</sup>  3 The BLEU Evaluation  We wish to make the brevity penalty 1.0 when the The BLEU metric ranges from 0 to 1. Few transla-  candidate\u2019s length is the same as any reference trans- tions will attain a score of 1 unless they are identi-  lation\u2019s length. For example, if there are three ref- cal to a reference translation. For this reason, even  erences with lengths 12, 15, and 17 words and the a human translator will not necessarily score 1. It  candidate translation is a terse 12 words, we want is important to note that the more reference trans-  the brevity penalty to be 1. We call the closest refer- lations per sentence there are, the higher the score  ence sentence length the *\u201cbest match length.\u201d*  is. Thus, one must be cautious making even \u201crough\u201d  One consideration remains: if we computed the comparisons on evaluations with different numbers  brevity penalty sentence by sentence and averaged of reference translations: on a test corpus of about  the penalties, then length deviations on short sen- 500 sentences (40 general news stories), a human  tences would be punished harshly. Instead, we com- translator scored 0.3468 against four references and  pute the brevity penalty over the entire corpus to al- scored 0.2571 against two references. Table 1 shows  low some freedom at the sentence level. We \ufb01rst the BLEU scores of the 5 systems against two refer-  compute the test corpus\u2019 effective reference length, ences on this test corpus.\n\nfilename: materials_formatted.md\nextension: .md\n\nThey rated  each translation from 1 (very bad) to 5 (very good).  The monolingual group made their judgments based  only on the translations\u2019 readability and \ufb02uency.  As must be expected, some judges were more lib-  eral than others. And some sentences were easier  to translate than others. To account for the intrin-  sic difference between judges and the sentences, we  compared each judge\u2019s rating for a sentence across  systems. We performed four pairwise t-test compar-  isons between adjacent systems as ordered by their  aggregate average score.  To answer these questions, we divided the test cor-  pus into 20 blocks of 25 sentences each, and com-  puted the BLEU metric on these blocks individually.  We thus have 20 samples of the BLEU metric for  each system. We computed the means, variances,  and paired t-statistics which are displayed in Table  2\\. The t-statistic compares each system with its left  neighbor in the table. For example, *t* = 6 for the pair  S1 and S2.  Note that the numbers in Table 1 are the BLEU  metric on an aggregate of 500 sentences, but the  means in Table 2 are averages of the BLEU metric  on aggregates of 25 sentences. As expected, these  two sets of results are close for each system and dif-  fer only by small \ufb01nite block size effects. Since a  paired t-statistic of 1.7 or above is 95% signi\ufb01cant,  the differences between the systems\u2019 scores are sta-  tistically very signi\ufb01cant. The reported variance on  25-sentence blocks serves as an upper bound to the  variance of sizeable test sets like the 500 sentence  corpus.  4.1 Monolingual group pairwise judgments  Figure 3 shows the mean difference between the  scores of two consecutive systems and the 95% con-  \ufb01dence interval about the mean. We see that S2 is  quite a bit better than S1 (by a mean opinion score  difference of 0.326 on the 5-point scale), while S3  is judged a little better (by 0.114).\n\nfilename: materials_formatted.md\nextension: .md\n\nMore- der. The main idea is to use a weighted average of  over, they can take *weeks* or *months* to \ufb01nish. This is variable length phrase matches against the reference  a big problem because developers of machine trans- translations. This view gives rise to a family of met-  lation systems need to monitor the effect of *daily* rics using various weighting schemes. We have se-  changes to their systems in order to weed out bad lected a promising baseline metric from this family.  ideas from good ideas. We believe that MT progress  stems from evaluation and that there is a logjam of  fruitful research ideas waiting to be released from  In Section 2, we describe the baseline metric in  detail. In Section 3, we evaluate the performance of  BLEU. In Section 4, we describe a human evaluation  experiment. In Section 5, we compare our baseline  metric performance with human evaluations.  <sup>1</sup>So we call our method the bilingual evaluation understudy,  BLEU.    <a name=\"br2\"></a>   2 The Baseline BLEU Metric  large collections of translations presented in Section  5 show that this ranking ability is a general phe-  nomenon, and not an artifact of a few toy examples.  The primary programming task for a BLEU imple-  mentor is to compare *n*-grams of the candidate with  the *n*-grams of the reference translation and count  the number of matches. These matches are position-  independent. The more the matches, the better the  candidate translation is. For simplicity, we \ufb01rst fo-  cus on computing unigram matches.  Typically, there are many \u201cperfect\u201d translations of a  given source sentence. These translations may vary  in word choice or in word order even when they use  the same words. And yet humans can clearly dis-  tinguish a good translation from a bad one. For ex-  ample, consider these two candidate translations of  a Chinese source sentence:  Example 1.  Candidate 1: It is a guide to action which  ensures that the military always obeys  the commands of the party.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nHow does BLEU calculate its final score?[/INST] \nAnswer: BLEU calculates its final score using a weighted average of variable length phrase matches against the reference translations. The primary task for a BLEU implementor is to compare n-grams of the candidate with the n-grams of the reference translation and count the number of matches. These matches are position-independent. The more the matches, the better the candidate translation is. The BLEU metric ranges from 0 to 1, with few translations attaining a score of 1 unless they are identical to a reference translation. The score also considers a brevity penalty to ensure the candidate's length is similar to any reference translation's length."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nso the connection allows for learning much deeper networks, and ( 2 ) it is easier to learn the difference of a function from the identity function than it is to learn the function from scratch. As simple as these seem, they\u2019re massively useful in deep learning, not just in Transformers!  Add & Norm. In the Transformer diagrams you\u2019ll see, including Fig- ure 4 , the application of layer normalization and residual connection are often combined in a single visual block labeledAdd & Norm. Such a layer might look like:   h pre-norm=f(LN( h )) + h , ( 30 )  wherefis either a feed-forward operation or a self-attention opera- tion, (this is known aspre-normalization), or like:   h post-norm=LN(f( h ) + h ), ( 31 )  which is known aspost-normalization. It turns out that the gradients of pre-normalization are much better at initialization, leading to much faster training [Xiong et al., 2020 ].  ##### 3. 4 Attention logit scaling  Another trick introduced in [Vaswani et al., 2017 ] they dubscaled dot product attention. The dot product part comes from the fact that we\u2019re computing dot products q \u22a4ikj. The intuition of scaling is that, when the dimensionalitydof the vectors we\u2019re dotting grows large, the dot product of even random vectors (e.g., at initialization) grows roughly as   \u221a   d. So, we normalize the dot products by   \u221a   dto stop this scaling:   \u03b1 =softmax( x 1:nQK\u22a4 x \u22a41:n \u221a d   )\u2208 R n\u00d7n ( 32 )    with deep learning 13  ##### 3. 5 Transformer Encoder  A Transformer Encoder takes a single sequence w 1:n, and performs no future masking. It embeds the sequence withEto make x 1:n, adds the position representation, and then applies a stack of independently parameterizedEncoder Blocks, each of which consisting of ( 1 ) multi- head attention and Add & Norm, and ( 2 ) feed-forward and Add & Norm. So, the output of each Block is the input to the next. Figure 5 presents this.\n\nfilename: materials_formatted.md\nextension: .md\n\nTo do this, layer norm ( 1 ) computes statistics across the activations at a layer to estimate the mean and variance of the activations, and ( 2 ) normalizes the activations with respect to those estimates, while ( 3 ) optionally learning (as parameters) an elementwise additive bias and multiplicative gain by which to sort of de-normalize the activations in a predictable way. The third part seems not to be crucial, and may even be harmful [Xu et al., 2019 ], so we omit it in our presentation. One question to ask when understanding how layer norm affects a network is, \u201ccomputing statistics overwhat?\u201d That is, what consti- tutes a layer? In Transformers, the answer is always that statistics computed independently for a single index into the sequence length (and a single example in the batch) and shared across thedhidden dimensions. Put another way, the statistics for the token at indexi won\u2019t affect the token at indexj\u0338=i. So, we compute the statistics for a single indexi\u2208{1,... ,n}as   \u03bc \u02c6i=   1   d   d   \u2211   j= 1   h ij \u03c3 \u02c6i=   vu ut 1 d   d   \u2211   j= 1   ( h ij\u2212 \u03bc i)^2 , ( 27 )    with deep learning 12  where (as a reminder), _\u03bc_ \u02c6iand _\u03c3_ \u02c6iare scalars, and we compute the layer norm as   LN( h i) = h i\u2212 \u03bc \u02c6i \u03c3 \u02c6i , (^28 )  where we\u2019ve broadcasted the _\u03bc_ \u02c6iand _\u03c3_ \u02c6iacross theddimensions of h i. Layer normalization is a great tool to have in your deep learning toolbox more generally.  ##### 3. 3 Residual Connections  Residual connections simply add theinputof a layer to theoutputof that layer:   fresidual( h 1:n) =f( h 1:n) + h 1:n, ( 29 )  the intuition being that ( 1 ) the gradient flow of the identity function isgreat(the local gradient is 1 everywhere!)\n\nfilename: materials_formatted.md\nextension: .md\n\nBut this is unnecessary for the logit outputs where the prediction con\ufb01dence is determined by the scale of the logits. We only apply layer normalization to the fully-connected hidden layers that excludes the last softmax layer.  All the models were trained using 55000 training data points and the Adam [Kingma and Ba, 2014] optimizer. For the smaller batch-size, the variance term for batch normalization is computed using the unbiased estimator. The experimental results from Figure 6 highlight that layer normalization is robust to the batch-sizes and exhibits a faster training convergence comparing to batch normalization that is applied to all layers.  6.7 Convolutional Networks  We have also experimented with convolutional neural networks. In our preliminary experiments, we observed that layer normalization offers a speedup over the baseline model without normalization, but batch normalization outperforms the other methods. With fully connected layers, all the hidden units in a layer tend to make similar contributions to the \ufb01nal prediction and re-centering and re- scaling the summed inputs to a layer works well. However, the assumption of similar contributions is no longer true for convolutional neural networks. The large number of the hidden units whose receptive \ufb01elds lie near the boundary of the image are rarely turned on and thus have very different statistics from the rest of the hidden units within the same layer. We think further research is needed to make layer normalization work well in ConvNets.  7 Conclusion  In this paper, we introduced layer normalization to speed-up the training of neural networks. We provided a theoretical analysis that compared the invariance properties of layer normalization with batch normalization and weight normalization. We showed that layer normalization is invariant to per training-case feature shifting and scaling.  Empirically, we showed that recurrent neural networks bene\ufb01t the most from the proposed method especially for long sequences and small mini-batches.  Acknowledgments  This research was funded by grants from NSERC, CFI, and Google.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhy is layer normalization important in Transformers?[/INST] \nAnswer: Layer normalization is important in Transformers because it computes statistics across the activations at a layer to estimate the mean and variance of the activations, and normalizes the activations with respect to those estimates. This process helps in managing the scale of the activations and makes the network more stable. In Transformers, the statistics are computed independently for a single index into the sequence length and shared across the hidden dimensions. This means that the statistics for one token won't affect another token, maintaining the independence of each token in the sequence. Furthermore, layer normalization is combined with residual connections in a block called \"Add & Norm\", which is essential for the functioning of the Transformer model."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nA visualiza- tion of this construction can be seen in Figure 2.  3.1 Pre-training BERT  Unlike Peters et al. (2018a) and Radford et al. (2018), we do not use traditional left-to-right or right-to-left language models to pre-train BERT. Instead, we pre-train BERT using two unsuper- vised tasks, described in this section. This step is presented in the left part of Figure 1.  Task #1: Masked LM Intuitively, it is reason- able to believe that a deep bidirectional model is strictly more powerful than either a left-to-right model or the shallow concatenation of a left-to- right and a right-to-left model. Unfortunately, standard conditional language models can only be trained left-to-right or right-to-left, since bidirec- tional conditioning would allow each word to in- directly \u201csee itself\u201d, and the model could trivially predict the target word in a multi-layered context.  In order to train a deep bidirectional representa- tion, we simply mask some percentage of the input tokens at random, and then predict those masked tokens. We refer to this procedure as a \u201cmasked LM\u201d (MLM), although it is often referred to as a Cloze task in the literature (Taylor, 1953). In this case, the \ufb01nal hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary, as in a standard LM. In all of our experiments, we mask 15% of all WordPiece to- kens in each sequence at random. In contrast to denoising auto-encoders (Vincent et al., 2008), we only predict the masked words rather than recon- structing the entire input.  Although this allows us to obtain a bidirec- tional pre-trained model, a downside is that we are creating a mismatch between pre-training and \ufb01ne-tuning, since the [MASK] token does not ap- pear during \ufb01ne-tuning. To mitigate this, we do not always replace \u201cmasked\u201d words with the ac- tual [MASK] token. The training data generator chooses 15% of the token positions at random for prediction.\n\nfilename: materials_formatted.md\nextension: .md\n\ninternational conference on computer vision, pages 19\u201327.  Appendix for \u201cBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\u201d  We organize the appendix into three sections:  \u2022 Additional implementation details for BERT  are presented in Appendix A;  \u2022 Additional details for our experiments are  presented in Appendix B; and  \u2022 Additional ablation studies are presented in  Appendix C.  We present additional ablation studies for BERT including:  \u2013 Effect of Number of Training Steps; and \u2013 Ablation for Different Masking Proce-  dures.  A Additional Details for BERT  A.1  Illustration of the Pre-training Tasks  We provide examples of the pre-training tasks in the following.  Masked LM and the Masking Procedure As- suming the unlabeled sentence is my dog is hairy, and during the random masking procedure we chose the 4-th token (which corresponding to hairy), our masking procedure can be further il- lustrated by  \u2022 80% of the time: Replace the word with the [MASK] token, e.g., my dog is hairy \u2192 my dog is [MASK]  \u2022 10% of the time: Replace the word with a random word, e.g., my dog is hairy \u2192 my dog is apple  \u2022 10% of  the time: Keep the word un- changed, e.g., my dog is hairy \u2192 my dog is hairy. The purpose of this is to bias the representation towards the actual observed word.  The advantage of this procedure is that the Transformer encoder does not know which words it will be asked to predict or which have been re- placed by random words, so it is forced to keep a distributional contextual representation of ev- ery input token. Additionally, because random replacement only occurs for 1.5% of all tokens (i.e., 10% of 15%), this does not seem to harm the model\u2019s language understanding capability. In Section C.2, we evaluate the impact this proce- dure.  Compared to standard langauge model training, the masked LM only make predictions on 15% of tokens in each batch, which suggests that more pre-training steps may be required for the model  Figure 3: Differences in pre-training model architectures. BERT uses a bidirectional Transformer. OpenAI GPT uses a left-to-right Transformer.\n\nfilename: materials_formatted.md\nextension: .md\n\nUnlike recent language repre- sentation models (Peters et al., 2018a; Rad- ford et al., 2018), BERT is designed to pre- train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a re- sult, the pre-trained BERT model can be \ufb01ne- tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task- speci\ufb01c architecture modi\ufb01cations.  BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art re- sults on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answer- ing Test F1 to 93.2 (1.5 point absolute im- provement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).  1  Introduction  Language model pre-training has been shown to be effective for improving many natural language processing tasks (Dai and Le, 2015; Peters et al., 2018a; Radford et al., 2018; Howard and Ruder, 2018). These include sentence-level tasks such as natural language inference (Bowman et al., 2015; Williams et al., 2018) and paraphrasing (Dolan and Brockett, 2005), which aim to predict the re- lationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce \ufb01ne-grained output at the token level (Tjong Kim Sang and De Meulder, 2003; Rajpurkar et al., 2016).  There are two existing strategies for apply- ing pre-trained language representations to down- stream tasks: feature-based and \ufb01ne-tuning. The feature-based approach, such as ELMo (Peters et al., 2018a), uses task-speci\ufb01c architectures that include the pre-trained representations as addi- tional features.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat is the 'masked language model' (MLM) pre-training objective in BERT?[/INST] \nAnswer: The 'masked language model' (MLM) pre-training objective in BERT involves masking some percentage of the input tokens at random, and then predicting those masked tokens. This procedure, often referred to as a Cloze task, allows for the training of a deep bidirectional representation. The final hidden vectors corresponding to the masked tokens are fed into an output softmax over the vocabulary, similar to a standard language model. In the experiments, 15% of all WordPiece tokens in each sequence were masked at random. The model only predicts the masked words rather than reconstructing the entire input. This approach allows for a bidirectional pre-trained model, but creates a mismatch between pre-training and fine-tuning, as the [MASK] token does not appear during fine-tuning. To mitigate this, not all \"masked\" words are replaced with the actual [MASK] token. Instead, the training data generator chooses 15% of the token positions at random for prediction."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nHere, we discuss the use of a gated activation function thereby modifying the RNN architecture. What motivates this? Well, although RNNs can theoretically capture long-term dependencies, they are very hard to actually train to do this. Gated recurrent units are designed in a manner to have more persistent memory thereby making it easier for RNNs to capture long-term dependencies. Let us see mathematically how a GRU usesht\u2212 1 andxtto generate the next hidden stateht. We will then dive into the intuition of this architecture.   zt= \u03c3 (W(z)xt+U(z)ht\u2212 1 ) (Update gate) rt= \u03c3 (W(r)xt+U(r)ht\u2212 1 ) (Reset gate) \u0303ht=tanh(rt\u25e6Uht\u2212 1 +Wxt) (New memory) ht= ( 1 \u2212zt)\u25e6h \u0303t+zt\u25e6ht\u2212 1 (Hidden state)   ### language models,rnn,gru and lstm 12  The above equations can be thought of a GRU\u2019s four fundamental op- erational stages and they have intuitive interpretations that make this model much more intellectually satisfying (see Figure 12 ):  1. New memory generation: A new memory \u0303htis the consolidation of a new input wordxtwith the past hidden stateht\u2212 1. Anthropomor- phically, this stage is the one who knows the recipe of combining a newly observed word with the past hidden stateht\u2212 1 to summarize this new word in light of the contextual past as the vectorh \u0303t.  2. Reset Gate: The reset signalrtis responsible for determining how importantht\u2212 1 is to the summarizationh \u0303t. The reset gate has the ability to completely diminish past hidden state if it finds thatht\u2212 1 is irrelevant to the computation of the new memory.  3. Update Gate: The update signalztis responsible for determining how much ofht\u2212 1 should be carried forward to the next state. For instance, ifzt \u22481, thenht\u2212 1 is almost entirely copied out toht. Conversely, ifzt\u22480, then mostly the new memoryh \u0303tis forwarded to the next hidden state.  4.\n\nfilename: materials_formatted.md\nextension: .md\n\n- Clearly label each part and question in your submissions. - Deadline: Feb. 7, 2024  # Rubric and Expectations:  - Code Quality and Functionality (40%):Code should be well-organized, commented,     and functioning as intended. The use of Python and relevant libraries (e.g., Gensim for     Word2Vec) should demonstrate a good grasp of the tools. - Analysis and Interpretation (30%): Written explanations should be insightful, de-     monstrating a deep understanding of the word vector models. Analysis of visualizations,     analogy tasks, and model comparisons should be thorough and reflective. - Visualization (20%):Visualizations should be clear, accurately labeled, and effectively     convey the relationships or patterns discovered in the data. - Adherence to Guidelines (10%): Submissions should follow the provided guidelines,     including format, labeling, and adherence to the deadline.  McMaster University 2  \n\n# SEP 775 Assignment 2:  # Assignment 2  # RNN-Based Text Generation (Total: 50 Points)  ## Objectives:  Implement a simple RNN for text generation to deepen your understanding of how re- current neural networks can be used to model sequences and generate text based on learned patterns.  1. RNN Model Implementation (10 Points)     - Implement a basic Recurrent Neural Network model from scratch using PyTorch or        TensorFlow. Your model should include an embedding layer, at least one RNN layer,        and a fully connected layer for output. Refer to the \"Recurrent Neural Networks        (RNN)\"section of the lectures for guidance on the architecture.     - Use the \"Long Short-Term Memory RNNs (LSTMs)\"section as a reference to enhance        your model with LSTM cells to improve its ability to capture long-term dependencies        in text. 2. Dataset Preparation (10 Points)     - Select a small text dataset for training your model. This could be a collection of        poems, song lyrics, or any text of your choice. Preprocess the data by tokenizing the        text into sequences and converting them into numerical format suitable for training        your RNN. 3. Training (10 Points)     - Train your RNN model on the prepared dataset.\n\nfilename: materials_formatted.md\nextension: .md\n\nTo construct a Deep RNN withLlayers, the above relationships are modified to the relationships in Equations 20 and 21 where the input to each intermediate neuron at leveliis the output of the RNN at layeri\u22121 at the same time-step,t. The output,y\u02c6, at each time-step is the result of propagating input parameters through all hidden layers (Equation 22 ).  # Going Deep  Each memory layer passes an intermediate sequential representation to the next.   #### 2. 6 Application: RNN Translation Model  Traditional translation models are quite complex; they consist of nu- merous machine learning algorithms applied to different stages of the language translation pipeline. In this section, we discuss the potential for adopting RNNs as a replacement to traditional translation mod- ules. Consider the RNN example model shown in Figure 10 ; here, the German phraseEcht dicke Kisteis translated toAwesome sauce. The first three hidden layer time-stepsencodethe German language words into some language word features (h 3 ). The last two time-stepsdecode h 3 into English word outputs. Equation 23 shows the relationship for the Encoder stage and Equations 24 and 25 show the equation for the Decoder stage.   x 1 x 2 x 3  _h_ (^1) W\t  _h 2 h_ 3 W\t Awesome\t  sauce\t  _y 1 y_ (^) _2_ Figure 10 : A RNN-based translation model. The first three RNN hidden layers belong to the source language model encoder, and the last two belong to the destination language model decoder. ht= _\u03c6_ (ht\u2212 1 ,xt) =f(W(hh)ht\u2212 1 +W(hx)xt) ( 23 ) ht= _\u03c6_ (ht\u2212 1 ) =f(W(hh)ht\u2212 1 ) ( 24 ) yt=so f tmax(W(S)ht) ( 25 ) One may naively assume this RNN model along with the cross- entropy function shown in Equation 26 can produce high-accuracy translation results. In practice, however, several extensions are to be added to the model to improve its translation accuracy performance.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat architectural enhancement is suggested for the RNN model in SEP775 Assignment 2?[/INST] \nAnswer: The architectural enhancement suggested for the RNN model in SEP775 Assignment 2 is the inclusion of LSTM cells. This is intended to improve the model's ability to capture long-term dependencies in text."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\n3. Contextual Embedding Layer. We use a Long Short-Term Memory Network (LSTM) (Hochreiter & Schmidhuber, 1997) on top of the embeddings provided by the previous layers to model the temporal interactions between words. We place an LSTM in both directions, and concatenate the outputs of the two LSTMs. Hence we obtain H \u2208 R2d\u00d7T from the context word vectors X, and U \u2208 R2d\u00d7J from query word vectors Q. Note that each column vector of H and U is 2d-dimensional because of the concatenation of the outputs of the forward and backward LSTMs, each with d-dimensional output.  It is worth noting that the \ufb01rst three layers of the model are computing features from the query and context at different levels of granularity, akin to the multi-stage feature computation of convolutional neural networks in the computer vision \ufb01eld.  4. Attention Flow Layer. Attention \ufb02ow layer is responsible for linking and fusing information from the context and the query words. Unlike previously popular attention mechanisms (Weston et al., 2015; Hill et al., 2016; Sordoni et al., 2016; Shen et al., 2016), the attention \ufb02ow layer is not used to summarize the query and context into single feature vectors. Instead, the attention vector at each time step, along with the embeddings from previous layers, are allowed to \ufb02ow through to the subsequent modeling layer. This reduces the information loss caused by early summarization.  The inputs to the layer are contextual vector representations of the context H and the query U. The outputs of the layer are the query-aware vector representations of the context words, G, along with the contextual embeddings from the previous layer.  In this layer, we compute attentions in two directions: from context to query as well as from query to context. Both of these attentions, which will be discussed below, are derived from a shared similarity matrix, S \u2208 RT \u00d7J , between the contextual embeddings of the context (H) and the query (U), where Stj indicates the similarity between t-th context word and j-th query word.\n\nfilename: materials_formatted.md\nextension: .md\n\nVisualizations. We now provide a qualitative analysis of our model on the SQuAD dev set. First, we visualize the feature spaces after the word and contextual embedding layers. These two layers are responsible for aligning the embeddings between the query and context words which are the inputs to the subsequent attention layer. To visualize the embeddings, we choose a few frequent query words in the dev data and look at the context words that have the highest cosine similarity to the query words (Table 2). At the word embedding layer, query words such as When, Where and Who are not well aligned to possible answers in the context, but this dramatically changes in the contextual embedding layer which has access to context from surrounding words and is just 1 layer below the attention layer. When begins to match years, Where matches locations, and Who matches names.  We also visualize these two feature spaces using t-SNE in Figure 2. t-SNE is performed on a large fraction of dev data but we only plot data points corresponding to the months of the year. An interesting pattern emerges in the Word space, where May is separated from the rest of the months because May has multiple meanings in the English language. The contextual embedding layer uses contextual cues from surrounding words and is able to separate the usages of the word May. Finally we visualize the attention matrices for some question-context tuples in the dev data in Figure 3. In the \ufb01rst example, Where matches locations and in the second example, many matches quantities and numerical symbols. Also, entities in the question typically attend to the same entities in the context, thus providing a feature for the model to localize possible answers.  Discussions. We analyse the performance of our our model with a traditional language-feature- based baseline (Rajpurkar et al., 2016). Figure 2b shows a Venn diagram of the dev set questions correctly answered by the models. Our model is able to answer more than 86% of the questions  7  JanuarySeptemberAugustJulyMaymayeffect and mayresult inthe state maynot aidof these maybe moreOpening in May1852 atdebut on May5 ,from 28 Januaryto 25but by September had been(a)(b)(c)Published as a conference paper at ICLR 2017  Figure 3: Attention matrices for question-context tuples.\n\nfilename: materials_formatted.md\nextension: .md\n\nGiven a number of context words around a target wordw, these models formulate the embedding task as that of finding a representa- tion that is good at predictingwfrom the context representations. The second group of models, Hellinger PCA (Lebret and Collobert, 2014), GloVe (Pennington et al., 2014), TSCCA (Dhillon et al., 2012) and Sparse Random Projections (Li et al., 2006) fol- low a reconstruction approach: word embeddings should be able to capture as much relevant infor- mation from the original co-occurrence matrix as possible. Training corpus.We tried to make the compar- ison as fair as possible. As the C&W embeddings were only available pretrained on a November 2007 snapshot of Wikipedia, we chose the closest available Wikipedia dump (2008-03-01) for train- ing the other models. We tokenized the data us- ing the Stanford tokenizer (Manning et al., 2014). Like Collobert et al. (2011), we lower-cased all words and replaced digits with zeros. Details.All models embedded words into a 50- dimensional space (D= 50). As implemented, each method uses a different vocabulary, so we computed the intersection of the six vocabularies and used the resulting set of 103,647 words for all nearest-neighbor experiments.  ## 3 Relatedness   We begin with intrinsic evaluation of relatedness using both pre-collected human evaluations and a novel online user study. Section 3.1 introduces the list of datasets that is commonly used as a bench- mark for embedding methods. There, embeddings are evaluated individually and only their final scores are compared, hence we refer to this sce- nario asabsolute intrinsicevaluation. We present a new scenario,comparative intrinsicevaluation, in which we ask people directly for their prefer- ences among different embeddings. We demon- strate that we can achieve the same results as of- fline, absolute metrics using online, comparative metrics.   3.1 Absolute intrinsic evaluation  For the absolute intrinsic evaluation, we used the same datasets and tasks as Baroni et al. (2014). While we present results on all tasks for complete- ness, we will mainly focus on relatedness in this section.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat is the main purpose of the introduction to contextual word representations?[/INST] \nAnswer: The main purpose of the introduction to contextual word representations is to model the temporal interactions between words. This is achieved by using a Long Short-Term Memory Network (LSTM) on top of the embeddings provided by the previous layers. The LSTM is placed in both directions, and the outputs of the two LSTMs are concatenated. This results in obtaining context word vectors and query word vectors. The first three layers of the model compute features from the query and context at different levels of granularity, similar to the multi-stage feature computation of convolutional neural networks in the computer vision field."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nFor Open MS-MARCO, where useful retrieved documents cannot always be retrieved, we observe that the model learns to always retrieve a particular set of documents for questions that are less likely to benefit from retrieval, suggesting that null document mechanisms may not be necessary for RAG.  ### G Parameters  Our RAG models contain the trainable parameters for the BERT-base query and document encoder of DPR, with 110M parameters each (although we do not train the document encoder ourselves) and 406M trainable parameters from BART-large, 406M parameters, making a total of 626M trainable   Table 7: Number of instances in the datasets used. *A hidden subset of this data is used for evaluation   Task Train Development Test Natural Questions 79169 8758 3611 TriviaQA 78786 8838 11314 WebQuestions 3418 362 2033 CuratedTrec 635 134 635 Jeopardy Question Generation 97392 13714 26849 MS-MARCO 153726 12468 101093* FEVER-3-way 145450 10000 10000 FEVER-2-way 96966 6666 6666  parameters. The best performing \"closed-book\" (parametric only) open-domain QA model is T5-11B with 11 Billion trainable parameters. The T5 model with the closest number of parameters to our models is T5-large (770M parameters), which achieves a score of 28.9 EM on Natural Questions [ 52 ], substantially below the 44.5 that RAG-Sequence achieves, indicating that hybrid parametric/non- parametric models require far fewer trainable parameters for strong open-domain QA performance. The non-parametric memory index does not consist of trainable parameters, but does consists of 21M 728 dimensional vectors, consisting of 15.3B values. These can be easily be stored at 8-bit floating point precision to manage memory and disk footprints.  ### H Retrieval Collapse  In preliminary experiments, we observed that for some tasks such as story generation [ 11 ], the retrieval component would \u201ccollapse\u201d and learn to retrieve the same documents regardless of the input. In these cases, once retrieval had collapsed, the generator would learn to ignore the documents, and the RAG model would perform equivalently to BART.\n\nfilename: materials_formatted.md\nextension: .md\n\nWe use the following process for each question- answer pair to build our training set. First, we  Table 3: Document retrieval results. % of ques- tions for which the answer segment appears in one of the top 5 pages returned by the method.  run Document Retriever on the question to re- trieve the top 5 Wikipedia articles. All paragraphs from those articles without an exact match of the known answer are directly discarded. All para- graphs shorter than 25 or longer than 1500 charac- ters are also \ufb01ltered out. If any named entities are detected in the question, we remove any paragraph that does not contain them at all. For every remain- ing paragraph in each retrieved page, we score all positions that match an answer using unigram and bigram overlap between the question and a 20 to- ken window, keeping up to the top 5 paragraphs with the highest overlaps. If there is no paragraph with non-zero overlap, the example is discarded; otherwise we add each found pair to our DS train- ing dataset. Some examples are shown in Table 1 and data statistics are given in Table 2.  Note that we can also generate additional DS data for SQuAD by trying to \ufb01nd mentions of the answers not just in the paragraph provided, but also from other pages or the same page that the given paragraph was in. We observe that around half of the DS examples come from pages outside of the articles used in SQuAD.  5 Experiments  This section \ufb01rst presents evaluations of our Doc- ument Retriever and Document Reader modules separately, and then describes tests of their com- bination, DrQA, for open-domain QA on the full Wikipedia.  5.1 Finding Relevant Articles  We \ufb01rst examine the performance of our Docu- ment Retriever module on all the QA datasets. Ta- ble 3 compares the performance of the two ap- proaches described in Section 3.1 with that of the Wikipedia Search Engine5 for the task of \ufb01nd- ing articles that contain the answer given a ques- tion. Speci\ufb01cally, we compute the ratio of ques- tions for which the text span of any of their as- sociated answers appear in at least one the top 5 relevant pages returned by each system.\n\nfilename: materials_formatted.md\nextension: .md\n\nThis challenge thus encour- ages research in the ability of a machine to read, a key motivation for the machine comprehen- sion sub\ufb01eld and the creation of datasets such as SQuAD (Rajpurkar et al., 2016), CNN/Daily Mail (Hermann et al., 2015) and CBT (Hill et al., 2016).  However,  those machine comprehension re- sources typically assume that a short piece of rel- evant text is already identi\ufb01ed and given to the model, which is not realistic for building an open- In sharp contrast, methods domain QA system. that use KBs or information retrieval over docu- ments have to employ search as an integral part of              the solution. Instead MRS is focused on simul- taneously maintaining the challenge of machine comprehension, which requires the deep under- standing of text, while keeping the realistic con- straint of searching over a large open resource.  In this paper, we show how multiple existing QA datasets can be used to evaluate MRS by re- quiring an open-domain system to perform well on all of them at once. We develop DrQA, a strong system for question answering from Wikipedia composed of: (1) Document Retriever, a mod- ule using bigram hashing and TF-IDF matching designed to, given a question, ef\ufb01ciently return a subset of relevant articles and (2) Document Reader, a multi-layer recurrent neural network machine comprehension model trained to detect answer spans in those few returned documents. Figure 1 gives an illustration of DrQA.  Our experiments show that Document Retriever outperforms the built-in Wikipedia search engine and that Document Reader reaches state-of-the- art results on the very competitive SQuAD bench- mark (Rajpurkar et al., 2016). Finally, our full sys- In tem is evaluated using multiple benchmarks. particular, we show that performance is improved across all datasets through the use of multitask learning and distant supervision compared to sin- gle task training.  2 Related Work  Open-domain QA was originally de\ufb01ned as \ufb01nd- ing answers in collections of unstructured docu- ments, following the setting of the annual TREC competitions1.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nHow does the proposed Document Retriever perform compared to the built-in Wikipedia search engine?[/INST] \nAnswer: The Document Retriever outperforms the built-in Wikipedia search engine."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\n# Evaluation methods for unsupervised word embeddings  ## Tobias Schnabel  ## Cornell University  ## Ithaca, NY, 14853  ## tbs49@cornell.edu  ## Igor Labutov  ## Cornell University  ## Ithaca, NY, 14853  ## iil4@cornell.edu  ## David Mimno,  ## Thorsten Joachims  ## Cornell University  ## mimno@cornell.edu,  ## tj@cs.cornell.edu  ## Abstract   We present a comprehensive study of eval- uation methods for unsupervised embed- ding techniques that obtain meaningful representations of words from text. Differ- ent evaluations result in different orderings of embedding methods, calling into ques- tion the common assumption that there is one single optimal vector representation. We present new evaluation techniques that directly compare embeddings with respect to specific queries. These methods re- duce bias, provide greater insight, and allow us to solicit data-driven relevance judgments rapidly and accurately through crowdsourcing.  ## 1 Introduction  Neural word embeddings represent meaning via geometry. A good embedding provides vector rep- resentations of words such that the relationship be- tween two vectors mirrors the linguistic relation- ship between the two words. Despite the growing interest in vector representations of semantic in- formation, there has been relatively little work on direct evaluations of these models. In this work, we explore several approaches to measuring the quality of neural word embeddings. In particu- lar, we perform a comprehensive analysis of eval- uation methods and introduce novel methods that can be implemented through crowdsourcing, pro- viding better insights into the relative strengths of different embeddings. Existing schemes fall into two major categories: extrinsic and intrinsic evaluation. In extrinsic eval- uation, we use word embeddings as input features to a downstream task and measure changes in per- formance metrics specific to that task. Examples include part-of-speech tagging and named-entity recognition (Pennington et al., 2014). Extrinsic   evaluation only provides one way to specify the goodness of an embedding, and it is not clear how it connects to other measures. Intrinsic evaluations directly test for syntactic or semantic relationships between words (Mikolov et al., 2013a; Baroni et al., 2014).\n\nfilename: materials_formatted.md\nextension: .md\n\nIntrinsic evaluations measure the quality of word vectors by directly measuring correlation between semantic relatedness and ge- ometric relatedness, usually through inventories of query terms. Focusing on intrinsic measures, Baroni et al. (2014) compare word embeddings against distributional word vectors on a variety of query inventories and tasks. Faruqui and Dyer (2014) provide a website that allows the automatic evaluation of embeddings on a number of query inventories. Gao et al. (2014) publish an improved query inventory for the analogical reasoning task. Finally, Tsvetkov et al. (2015) propose a new in- trinsic measure that better correlates with extrinsic performance. However, all these evaluations are done on precollected inventories and mostly lim- ited to local metrics like relatedness. Extrinsic evaluations use embeddings as fea- tures in models for other tasks, such as semantic role labeling or part-of-speech tagging (Collobert et al., 2011), and improve the performance of ex- isting systems (Turian et al., 2010). However, they have been less successful at other tasks such as parsing (Andreas and Klein, 2014). More work has been done in unsupervised se- mantic modeling in the context of topic models. One example is theword intrusiontask (Chang et al., 2009), in which annotators are asked to iden- tify a random word inserted into the set of high probability words for a given topic. Word embed- dings do not produce interpretable dimensions, so we cannot directly use this method, but we present a related task based on nearest neighbors. Manual evaluation is expensive and time-consuming, but other work establishes that automated evaluations can closely model human intuitions (Newman et al., 2010).  ## 8 Conclusions  There are many factors that affect word embed- ding quality. Standard aggregate evaluations, while useful, do not present a complete or con- sistent picture. Factors such as word frequency play a significant and previously unacknowledged   role. Word frequency also interferes with the commonly-used cosine similarity measure.\n\nfilename: materials_formatted.md\nextension: .md\n\nWe believe that our comprehensive test set will help the research community to improve the existing techniques for estimating the word vectors. We also expect that high quality word vectors will become an important building block for future NLP applications.   ## 7 Follow-Up Work  After the initial version of this paper was written, we published single-machine multi-threaded C++ code for computing the word vectors, using both the continuous bag-of-words and skip-gram archi- tectures^4. The training speed is significantly higher than reported earlier in this paper, i.e. it is in the order of billions of words per hour for typical hyperparameter choices. We also published more than 1.4 million vectors that represent named entities, trained on more than 100 billion words. Some of our follow-up work will be published in an upcoming NIPS 2013 paper [21].  ## References   [1] Y. Bengio, R. Ducharme, P. Vincent. A neural probabilistic language model. Journal of Ma- chine Learning Research, 3:1137-1155, 2003. [2] Y. Bengio, Y. LeCun. Scaling learning algorithms towards AI. In: Large-Scale Kernel Ma- chines, MIT Press, 2007. [3] T. Brants, A. C. Popat, P. Xu, F. J. Och, and J. Dean. Large language models in machine translation. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Language Learning, 2007. [4] R. Collobert and J. Weston. A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning. In International Conference on Machine Learning, ICML, 2008. [5] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu and P. Kuksa. Natural Lan- guage Processing (Almost) from Scratch. Journal of Machine Learning Research, 12:2493- 2537, 2011. [6] J. Dean, G.S. Corrado, R. Monga, K. Chen, M. Devin, Q.V. Le, M.Z.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nHow does the paper measure the quality of the word vector representations?[/INST] \nAnswer: The paper measures the quality of word vector representations through both extrinsic and intrinsic evaluations. In extrinsic evaluation, word embeddings are used as input features to a downstream task and changes in performance metrics specific to that task are measured. Examples of these tasks include part-of-speech tagging and named-entity recognition. Intrinsic evaluations directly test for syntactic or semantic relationships between words. They measure the quality of word vectors by directly measuring correlation between semantic relatedness and geometric relatedness, usually through inventories of query terms. The paper also introduces novel methods that can be implemented through crowdsourcing."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nAfter graduating from Columbia University in 1983, he workedasa community organizerinChicago.   (Levy et al.,2017)   Semantic role labeling   16   (He etal.,2015)   #### Stanford question answering dataset (SQuAD)  - 100k annotated (passage, question, answer) triples     Large-scale supervised datasets are also a key ingredient for     training effective neural models for reading comprehension! - Passages are selected from English Wikipedia, usually 100~150 words. - Questions are crowd-sourced. - Each answer is a short segment of text (or span) in the passage.   This is a limitation\u2014 not all the questions can be answered in this way!  - SQuAD was for years the most popular reading comprehension     dataset; it is \u201calmost solved\u201d today (though the underlying task is     not,) and the state-of-the-art exceeds the estimated human     performance.   (Rajpurkaretal.,2016):SQuAD:100,000+QuestionsforMachineComprehensionofText^16    18  #### Stanford question answering dataset (SQuAD)  - Evaluation : exact match ( 0 or 1 ) and F 1 (partial credit). - For development and testing sets, 3 gold answers are collected, because there could be multiple     plausible answers. - We compare the predicted answer to _each_ gold answer (a, an, the, punctuations are removed)     and take max scores. Finally, we take the average of all the examples for both exact match and     F 1. - Estimated human performance: EM = 82. 3 , F 1 = 91. 2   Q: What did Tesla do in December 1878?   A: {left Graz, left Graz, left Graz and severed all relations with his family}   Prediction: {left Graz and served}   Exact match: max{0, 0, 0} = 0 F1: max{0.67, 0.67, 0.61} = 0.    19  #### Stanford question answering dataset (SQuAD)    20  #### Other question answering datasets  - TriviaQA: Questions and answers by trivia enthusiasts.\n\nfilename: materials_formatted.md\nextension: .md\n\nOur best model achieves an F1 score of 51.0%,1 which is much better than the sliding window base- line (20%). Over the last four months (since June 2016), we have witnessed signi\ufb01cant improvements from more sophisticated neural network-based mod- els. For example, Wang and Jiang (2016) obtained 70.3% F1 on SQuAD v1.1 (results on v1.0 are sim- ilar). These results are still well behind human performance, which is 86.8% F1 based on inter- annotator agreement. This suggests that there is plenty of room for advancement in modeling and learning on the SQuAD dataset.  1All experimental results in this paper are on SQuAD v1.0.  Dataset  SQuAD  Question source  Formulation  Size  crowdsourced RC,  spans  100K  MCTest (Richardson et al., 2013) Algebra (Kushman et al., 2014) Science (Clark and Etzioni, 2016)  crowdsourced  standardized tests standardized tests  in passage  RC, multiple choice computation  reasoning, multiple choice  2640  514  855  WikiQA (Yang et al., 2015) TREC-QA (Voorhees and Tice, 2000)  query logs  query logs + human editor  3047  IR, sentence selection IR, free form 1479  CNN/Daily Mail (Hermann et al., 2015) CBT (Hill et al., 2015)  summary cloze cloze  +  RC, \ufb01ll in single entity RC, \ufb01ll in single word  1.4M  688K  Table 1: A survey of several reading comprehension and ques-  tion answering datasets. SQuAD is much larger than all datasets  except the semi-synthetic cloze-style datasets, and it is similar  to TREC-QA in the open-endedness of the answers.  2 Existing Datasets  We begin with a survey of existing reading com- prehension and question answering (QA) datasets, highlighting a variety of task formulation and cre- ation strategies (see Table 1 for an overview).  Reading comprehension.\n\nfilename: materials_formatted.md\nextension: .md\n\nExisting datasets for RC have one of two shortcomings: (i) those that are high in quality (Richardson et al., 2013; Berant et al., 2014) are too small for training modern data-intensive models, while (ii) those that are large (Hermann et al., 2015; Hill et al., 2015) are semi-synthetic and do not share the same character- istics as explicit reading comprehension questions.  To address the need for a large and high-quality reading comprehension dataset, we present the Stan-              ford Question Answering Dataset v1.0 (SQuAD), freely available at https://stanford-qa.com, con- sisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to ev- ery question is a segment of text, or span, from the corresponding reading passage. SQuAD contains 107,785 question-answer pairs on 536 articles, and is almost two orders of magnitude larger than previ- ous manually labeled RC datasets such as MCTest (Richardson et al., 2013).  In contrast to prior datasets, SQuAD does not provide a list of answer choices for each question. Rather, systems must select the answer from all pos- sible spans in the passage, thus needing to cope with a fairly large number of candidates. While ques- tions with span-based answers are more constrained than the more interpretative questions found in more advanced standardized tests, we still \ufb01nd a rich di- versity of questions and answer types in SQuAD. We develop automatic techniques based on distances in dependency trees to quantify this diversity and stratify the questions by dif\ufb01culty. The span con- straint also comes with the important bene\ufb01t that span-based answers are easier to evaluate than free- form answers.  To assess the dif\ufb01culty of SQuAD, we imple- mented a logistic regression model with a range of features. We \ufb01nd that lexicalized and dependency tree path features are important to the performance of the model. We also \ufb01nd that the model perfor- mance worsens with increasing complexity of (i) an- swer types and (ii) syntactic divergence between the question and the sentence containing the answer; in- terestingly, there is no such degradation for humans.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWho are the authors of 'SQuAD: 100,000+ Questions for Machine Comprehension of Text'?[/INST] \nAnswer: The authors of 'SQuAD: 100,000+ Questions for Machine Comprehension of Text' are not explicitly mentioned in the provided context."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\n# Learning Dense Representations of Phrases at Scale:  Learning Dense Representations of Phrases at Scale  Jinhyuk Lee1,2\u2217 Mujeen Sung1  Jaewoo Kang1 Danqi Chen2  Korea University1 Princeton University2 {jinhyuk_lee,mujeensung,kangj}@korea.ac.kr danqic@cs.princeton.edu  Abstract  Open-domain question answering can be refor- mulated as a phrase retrieval problem, without the need for processing documents on-demand during inference (Seo et al., 2019). However, current phrase retrieval models heavily depend on sparse representations and still underper- form retriever-reader approaches. In this work, we show for the \ufb01rst time that we can learn dense representations of phrases alone that achieve much stronger performance in open- domain QA. We present an effective method to learn phrase representations from the super- vision of reading comprehension tasks, cou- pled with novel negative sampling methods. We also propose a query-side \ufb01ne-tuning strat- egy, which can support transfer learning and reduce the discrepancy between training and inference. On \ufb01ve popular open-domain QA datasets, our model DensePhrases improves over previous phrase retrieval models by 15%\u2013 25% absolute accuracy and matches the perfor- mance of state-of-the-art retriever-reader mod- els. Our model is easy to parallelize due to pure dense representations and processes more than 10 questions per second on CPUs. Finally, we directly use our pre-indexed dense phrase representations for two slot \ufb01lling tasks, show- ing the promise of utilizing DensePhrases as a dense knowledge base for downstream tasks.1  1  Introduction  Open-domain question answering (QA) aims to provide answers to natural-language questions us- ing a large text corpus (Voorhees et al., 1999; Fer- rucci et al., 2010; Chen and Yih, 2020). While a dominating approach is a two-stage retriever-reader approach (Chen et al., 2017; Lee et al., 2019; Guu et al., 2020; Karpukhin et al., 2020), we focus on  a recent new paradigm solely based on phrase re- trieval (Seo et al., 2019; Lee et al., 2020).\n\nfilename: materials_formatted.md\nextension: .md\n\n4 Learning Phrase Representations  In this section, we start by learning dense phrase representations from the supervision of reading comprehension tasks, i.e., a single passage p con- tains an answer a\u2217 to a question q. Our goal is to learn strong dense representations of phrases for s \u2208 S(p), which can be retrieved by a dense rep- resentation of the question and serve as a direct  5Our base model is largely inspired by DenSPI (Seo et al., 2019), although we deviate from theirs as follows. (1) We remove coherency scalars and don\u2019t split any vectors. (2) DenSPI uses a shared encoder for phrases and questions while we use 3 separate language models initialized from the same pre-trained model. (3) We use SpanBERT instead of BERT.  answer (\u00a74.1). Then, we introduce two different negative sampling methods (\u00a74.2, \u00a74.3), which en- courage the phrase representations to be better dis- criminated at the full Wikipedia scale. See Figure 1 for an overview of DensePhrases.  4.1 Single-passage Training  To learn phrase representations in a single passage along with question representations, we \ufb01rst max- imize the log-likelihood of the start and end posi- tions of the gold phrase s\u2217 where TEXT(s\u2217) = a\u2217. The training loss for predicting the start position of a phrase given a question is computed as:  zstart 1  , . . . , zstart  m = [h(cid:62)  1 qstart, . . . , h(cid:62)  P start = softmax(zstart 1 Lstart = \u2212 log P start  start(s\u2217).  mqstart], , . . . , zstart m ),  (5)  We can de\ufb01ne Lend in a similar way and the \ufb01nal loss for the single-passage training is  Lsingle =  Lstart + Lend 2  .  (6)  This essentially learns reading comprehension with- out any cross-attention between the passage and the question tokens, which fully decomposes phrase and question representations.  Data augmentation Since the contextualized word representations h1, . . .  hm are encoded in a query-agnostic way,\n\nfilename: materials_formatted.md\nextension: .md\n\nThe goal of this work is to only use dense representations, i.e., Es(s, D) = Edense(s, D), which can model f (s | D, q) solely with MIPS, as well as close the gap in performance.  3 DensePhrases  3.1 Overview  We introduce DensePhrases, a phrase retrieval model that is built on fully dense representations. Our goal is to learn a phrase encoder as well as a question encoder, so we can pre-index all the pos- sible phrases in D, and ef\ufb01ciently retrieve phrases for any question through MIPS at testing time. We outline our approach as follows:  Our base architecture consists of a phrase encoder Es and a question encoder Eq. Given a passage p = w1, . . . , wm, we denote all the phrases up to L tokens as S(p). Each phrase sk has start and end in- dicies start(k) and end(k) and the gold phrase is s\u2217 \u2208 S(p). Following previous work on phrase or span representations (Lee et al., 2017; Seo et al., 2018), we \ufb01rst apply a pre-trained language model Mp to obtain contextualized word representations for each passage token: h1, . . . , hm \u2208 Rd. Then, we can represent each phrase sk \u2208 S(p) as the con- catenation of corresponding start and end vectors:  Es(sk, p) = [hstart(k), hend(k)] \u2208 R2d.  (3)  A great advantage of this representation is that we eventually only need to index and store all the word vectors (we use W(D) to denote all the words in D), instead of all the phrases S(D), which is at least one magnitude order smaller.  Similarly, we need to learn a question encoder Eq(\u00b7) that maps a question q = \u02dcw1, . . . , \u02dcwn to a vector of the same dimension as Es(\u00b7). Since the start and end representations of phrases are pro- duced by the same language model, we use an- other two different pre-trained encoders Mq,start and Mq,end to differentiate the start and end po- sitions.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat is the main goal of learning dense representations of phrases according to the document?[/INST] \nAnswer: The main goal of learning dense representations of phrases is to achieve stronger performance in open-domain question answering (QA). This is done by reformulating open-domain QA as a phrase retrieval problem, without the need for processing documents on-demand during inference. The aim is to learn a phrase encoder as well as a question encoder, so that all possible phrases can be pre-indexed, and phrases for any question can be efficiently retrieved at testing time."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nCBOW 93.1 88.2 82.2 81. GloVe 93.2 88.3 82.9 82.  shown for neural vectors in (Turian et al., 2010).  4.4 Model Analysis: Vector Length and Context Size  In Fig. 2, we show the results of experiments that vary vector length and context window. A context window that extends to the left and right of a tar- get word will be called symmetric, and one which extends only to the left will be called asymmet- ric. In (a), we observe diminishing returns for vec- tors larger than about 200 dimensions. In (b) and (c), we examine the effect of varying the window size for symmetric and asymmetric context win- dows. Performance is better on the syntactic sub- task for small and asymmetric context windows, which aligns with the intuition that syntactic infor- mation is mostly drawn from the immediate con- text and can depend strongly on word order. Se- mantic information, on the other hand, is more fre- quently non-local, and more of it is captured with larger window sizes.  4.5 Model Analysis: Corpus Size  In Fig. 3, we show performance on the word anal- ogy task for 300-dimensional vectors trained on different corpora. On the syntactic subtask, there is a monotonic increase in performance as the cor- pus size increases. This is to be expected since larger corpora typically produce better statistics. Interestingly, the same trend is not true for the se- mantic subtask, where the models trained on the smaller Wikipedia corpora do better than those trained on the larger Gigaword corpus. This is likely due to the large number of city- and country- based analogies in the analogy dataset and the fact that Wikipedia has fairly comprehensive articles for most such locations.\n\nfilename: materials_formatted.md\nextension: .md\n\n5.1 Hyperparameters vs Algorithms  We first examine a \u201cvanilla\u201d scenario (Table 2), in which all hyperparameters are \u201cturned off\u201d (set to   default values): small context windows (win= 2 ), no dynamic contexts (dyn=none), no sub- sampling (sub = none), one negative sample (neg= 1), no smoothing (cds= 1), no context vectors (w+c=onlyw), and default eigenvalue weights (eig= 0. 0 ).^5 Overall, SVD outperforms other methods on most word similarity tasks, often having a considerable advantage over the second- best. In contrast, analogy tasks present mixed re- sults; SGNS yields the best result in MSR\u2019s analo- gies, while PPMI dominates Google\u2019s dataset. The second scenario (Table 3) sets the hyper- parameters toword2vec\u2019s default values: small context windows (win = 2),^6 dynamic contexts (dyn=with), dirty subsampling (sub=dirty), five negative samples (neg= 5), context distribu- tion smoothing (cds= 0. 75 ), no context vectors (w+c=onlyw), and default eigenvalue weights  (^5) While it is more common to seteig= 1, this setting degrades SVD\u2019s performance considerably (see Section 6.1). 6 Whileword2vec\u2019s default window size is 5 , we present a single window size (win= 2) in Tables 2-4, in order to iso- latewin\u2019s effect from the effects of other hyperparameters. Running the same experiments with different window sizes reveals similar trends. Additional results with broader win- dow sizes are shown in Table 5.    win Method WordSimSimilarity RelatednessWordSim Bruni et al.MEN Radinsky et al.M. Turk Luong et al.Rare Words Hill et al.SimLex Add / MulGoogle Add / MulMSR   2   PPMI .732 .699 .744 .654 .457 .382 .552 / .677 .306 /. SVD .772 .671 .777 .647 .508 .425 .554 / .591 .408 /. SGNS .789 .675 .773 .661 .449 .433 .676 /.689 .617 /.\n\nfilename: materials_formatted.md\nextension: .md\n\nFigure 3 demonstrates how accuracy has been shown to improve with larger corpus.  Figure 3 : Here we see how performance Figure 4 demonstrates how other hyperparameters have been improves with data size. shown to affect the accuracies using GloVe.   ## word vectors ii:glove,evaluation and training 9   Figure 4 : We see how accuracies vary with vector dimension and context  ### 2. 5 Intrinsic Evaluation Example: Correlation Evaluation window size for GloVe  Another simple way to evaluate the quality of word vectors is by asking humans to assess the similarity between two words on a fixed scale (say 0 - 10 ) and then comparing this with the cosine similarity between the corresponding word vectors. This has been done on various datasets that contain human judgement survey data.   Model Size WS353 MC RG SCWS RW SVD 6B 35.3 35.1 42.5 38.3 25. SVD-S 6B 56.5 71.5 71.0 53.6 34. SVD-L 6B 65.7 72.7 75.1 56.5 37. CBOW 6B 57.2 65.6 68.2 57.0 32. SG 6B 62.8 65.2 69.7 58.1 37. GloVe 6B 65.8 72.7 77.8 53.9 38. SVD-L 42B 74.0 76.4 74.1 58.3 39. GloVe 42B 75.9 83.6 82.9 59.6 47. CBOW 100B 68.4 79.6 75.4 59.4 45.   Table 6 : Here we see the correlations be- tween of word vector similarities using different embedding techniques with different human judgment datasets  ### 2. 6 Further Reading: Dealing With Ambiguity  One might wonder how we handle the situation where we want to capture the same word with different vectors for its different uses in natural language. For instance, \"run\" is both a noun and a verb and is used and interpreted differently based on the context.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat does the notes mention about the effect of window size on word vector performance?[/INST] \nAnswer: The notes mention that the performance of word vectors can be influenced by the size of the context window. For instance, smaller and asymmetric context windows tend to perform better on syntactic subtasks. This is because syntactic information is often drawn from the immediate context and can depend strongly on word order. On the other hand, semantic information, which is frequently non-local, is captured more effectively with larger window sizes."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\n( 1988 ). Neurocomputing: Foundations of research. chapter Learning Representations by Back-propagating Errors, pages 696 \u2013 699. MIT Press, Cambridge, MA, USA.      \n\n# SEP 775 Lecture 02:  # Natural Language Processing with Deep Learning  ## SEP 775 Lecture Notes: Part II   ## Word Vectors II: GloVe, Evaluation and Training  Keyphrases: Global Vectors for Word Representation (GloVe). In- trinsic and extrinsic evaluations. Effect of hyperparameters on anal- ogy evaluation tasks. Correlation of human judgment with word vector distances. Dealing with ambiguity in word using contexts. Window classification. This set of notes first introduces the GloVe model for training word vectors. Then it extends our discussion of word vectors (in- terchangeably called word embeddings) by seeing how they can be evaluated intrinsically and extrinsically. As we proceed, we discuss the example of word analogies as an intrinsic evaluation technique and how it can be used to tune word embedding techniques. We then discuss training model weights/parameters and word vectors for ex- trinsic tasks. Lastly we motivate artificial neural networks as a class of models for natural language processing tasks.  ## 1 Global Vectors for Word Representation (GloVe)^33 This section is based on the GloVe  ### 1. 1 Comparison with Previous Methods  So far, we have looked at two main classes of methods to find word embeddings. The first set are count-based and rely on matrix factor- ization (e.g. LSA, HAL). While these methods effectively leverage global statistical information, they are primarily used to capture word similarities and do poorly on tasks such as word analogy, indi- cating a sub-optimal vector space structure. The other set of methods are shallow window-based (e.g. the skip-gram and the CBOW mod- els), which learn word embeddings by making predictions in local context windows. These models demonstrate the capacity to capture complex linguistic patterns beyond word similarity, but fail to make use of the global co-occurrence statistics.\n\nfilename: materials_formatted.md\nextension: .md\n\nGloVe:  - Using global statistics to predict the     probability of word j appearing in     the context of word i with a least     squares objective  In comparison, GloVe consists of a weighted least squares model that trains on global word-word co-occurrence counts and thus makes efficient use of statistics. The model produces a word vector space with meaningful sub-structure. It shows state-of-the-art per- formance on the word analogy task, and outperforms other current methods on several word similarity tasks.   ## word vectors ii:glove,evaluation and training 2  ### 1. 2 Co-occurrence Matrix   Co-occurrence Matrix:  - X: word-word co-occurrence matrix - Xij: number of times wordjoccur in     the context of wordi - Xi=\u2211kXik: the number of times     any word k appears in the context of     word i - Pij=P(wj|wi) =XXiji: the probability     of j appearing in the context of word     i  LetXdenote the word-word co-occurrence matrix, whereXijindi- cates the number of times wordjoccur in the context of wordi. Let Xi=\u2211kXikbe the number of times any word k appears in the con-  text of word i. Finally, letPij=P(wj|wi) = Xij Xi be the probability of j appearing in the context of word i. Populating this matrix requires a single pass through the entire corpus to collect the statistics. For large corpora, this pass can be computationally expensive, but it is a one-time up-front cost.  ### 1.\n\nfilename: materials_formatted.md\nextension: .md\n\nInstead, we introduce a more general weighting function, which we are free to take to depend on the context word as well:  #### J\u02c6=   W   \u2211   i= 1   W   \u2211   j= 1   f(Xij)(~uTj~vi\u2212logXij)^2  ### 1. 4 Conclusion  In conclusion, the GloVe model efficiently leverages global statistical information by training only on the nonzero elements in a word- word co-occurrence matrix, and produces a vector space with mean- ingful sub-structure. It consistently outperformsword 2 vecon the word analogy task, given the same corpus, vocabulary, window size, and training time. It achieves better results faster, and also obtains the best results irrespective of speed.  ## 2 Evaluation of Word Vectors  So far, we have discussed methods such as theWord _2_ VecandGloVe methods to train and discover latent vector representations of natural language words in a semantic space. In this section, we discuss how we can quantitatively evaluate the quality of word vectors produced by such techniques.  ### 2. 1 Intrinsic Evaluation  Intrinsic evaluation of word vectors is the evaluation of a set of word vectors generated by an embedding technique (such as Word 2 Vec or GloVe) on specific intermediate subtasks (such as analogy comple- tion). These subtasks are typically simple and fast to compute and thereby allow us to help understand the system used to generate the word vectors. An intrinsic evaluation should typically return to us a number that indicates the performance of those word vectors on the evaluation subtask.   Figure 1 : The left subsystem (red) being expensive to train is modified by substituting with a simpler subsystem (green) for intrinsic evaluation.  Motivation: Let us consider an example where our final goal is to create a question answering system which uses word vectors   ## word vectors ii:glove,evaluation and training 4  as inputs. One approach of doing so would be to train a machine learning system that:  1. Takes words as inputs  2. Converts them to word vectors  3. Uses word vectors as inputs for an elaborate machine learning system  4. Maps the output word vectors by this system back to natural language words  5.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat significant insight about word vectors did the GloVe algorithm introduce?[/INST] \nAnswer: The GloVe algorithm introduced the significant insight that word vectors can be effectively trained using global statistical information. It does this by training on the nonzero elements in a word-word co-occurrence matrix. This approach produces a vector space with meaningful sub-structure and consistently outperforms other methods on the word analogy task. It achieves better results faster and obtains the best results irrespective of speed."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\n4 Learning Phrase Representations  In this section, we start by learning dense phrase representations from the supervision of reading comprehension tasks, i.e., a single passage p con- tains an answer a\u2217 to a question q. Our goal is to learn strong dense representations of phrases for s \u2208 S(p), which can be retrieved by a dense rep- resentation of the question and serve as a direct  5Our base model is largely inspired by DenSPI (Seo et al., 2019), although we deviate from theirs as follows. (1) We remove coherency scalars and don\u2019t split any vectors. (2) DenSPI uses a shared encoder for phrases and questions while we use 3 separate language models initialized from the same pre-trained model. (3) We use SpanBERT instead of BERT.  answer (\u00a74.1). Then, we introduce two different negative sampling methods (\u00a74.2, \u00a74.3), which en- courage the phrase representations to be better dis- criminated at the full Wikipedia scale. See Figure 1 for an overview of DensePhrases.  4.1 Single-passage Training  To learn phrase representations in a single passage along with question representations, we \ufb01rst max- imize the log-likelihood of the start and end posi- tions of the gold phrase s\u2217 where TEXT(s\u2217) = a\u2217. The training loss for predicting the start position of a phrase given a question is computed as:  zstart 1  , . . . , zstart  m = [h(cid:62)  1 qstart, . . . , h(cid:62)  P start = softmax(zstart 1 Lstart = \u2212 log P start  start(s\u2217).  mqstart], , . . . , zstart m ),  (5)  We can de\ufb01ne Lend in a similar way and the \ufb01nal loss for the single-passage training is  Lsingle =  Lstart + Lend 2  .  (6)  This essentially learns reading comprehension with- out any cross-attention between the passage and the question tokens, which fully decomposes phrase and question representations.  Data augmentation Since the contextualized word representations h1, . . .  hm are encoded in a query-agnostic way,\n\nfilename: materials_formatted.md\nextension: .md\n\nThis suggests that the model really does appear to learn these tasks at test time, as the model cannot perform them zero-shot and their arti\ufb01cial nature makes them unlikely to appear in the pre-training data (although we cannot con\ufb01rm this with certainty).  We can further quantify performance by plotting \u201cin-context learning curves\u201d, which show task performance as a function of the number of in-context examples. We show in-context learning curves for the Symbol Insertion task in Figure 1.2. We can see that larger models are able to make increasingly effective use of in-context information, including both task examples and natural language task descriptions.  Finally, it is worth adding that solving these tasks requires character-level manipulations, whereas our BPE encoding operates on signi\ufb01cant fractions of a word (on average \u223c 0.7 words per token), so from the LM\u2019s perspective succeeding at these tasks involves not just manipulating BPE tokens but understanding and pulling apart their substructure. Also, CL, A1, and A2 are not bijective (that is, the unscrambled word is not a deterministic function of the scrambled word), requiring the model to perform some search to \ufb01nd the correct unscrambling. Thus, the skills involved appear to require non-trivial pattern-matching and computation.  3.9.3 SAT Analogies  To test GPT-3 on another task that is somewhat unusual relative to the typical distribution of text, we collected a set of 374 \u201cSAT analogy\u201d problems [TLBS03]. Analogies are a style of multiple choice question that constituted a section of the SAT college entrance exam before 2005. A typical example is \u201caudacious is to boldness as (a) sanctimonious is to hypocrisy, (b) anonymous is to identity, (c) remorseful is to misdeed, (d) deleterious is to result, (e) impressionable is to temptation\u201d. The student is expected to choose which of the \ufb01ve word pairs has the same relationship as the original word pair; in this example the answer is \u201csanctimonious is to hypocrisy\u201d.\n\nfilename: materials_formatted.md\nextension: .md\n\n4We note that in the literature the bidirectional Trans-  BERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLIInput/Output Representations To make BERT handle a variety of down-stream tasks, our input representation is able to unambiguously represent both a single sentence and a pair of sentences (e.g., (cid:104) Question, Answer (cid:105)) in one token sequence. Throughout this work, a \u201csentence\u201d can be an arbi- trary span of contiguous text, rather than an actual linguistic sentence. A \u201csequence\u201d refers to the in- put token sequence to BERT, which may be a sin- gle sentence or two sentences packed together.  We use WordPiece embeddings (Wu et al., 2016) with a 30,000 token vocabulary. The \ufb01rst token of every sequence is always a special clas- si\ufb01cation token ([CLS]). The \ufb01nal hidden state corresponding to this token is used as the ag- gregate sequence representation for classi\ufb01cation tasks. Sentence pairs are packed together into a single sequence. We differentiate the sentences in two ways. First, we separate them with a special token ([SEP]). Second, we add a learned embed- ding to every token indicating whether it belongs to sentence A or sentence B. As shown in Figure 1, we denote input embedding as E, the \ufb01nal hidden vector of the special [CLS] token as C \u2208 RH , and the \ufb01nal hidden vector for the ith input token as Ti \u2208 RH .  For a given token, its input representation is constructed by summing the corresponding token, segment, and position embeddings.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat is the role of the Inverse Cloze Task (ICT) in learning phrase representations?[/INST] \nAnswer: I don't know."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nThe x-axis is the value of k.     \n\n# Efficient Estimation of Word Representations in Vector Space:   ## Efficient Estimation of Word Representations in  ## Vector Space   Tomas Mikolov Google Inc., Mountain View, CA tmikolov@google.com   Kai Chen Google Inc., Mountain View, CA kaichen@google.com   Greg Corrado Google Inc., Mountain View, CA gcorrado@google.com   Jeffrey Dean Google Inc., Mountain View, CA jeff@google.com  ## Abstract   We propose two novel model architectures for computing continuous vector repre- sentations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previ- ously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art perfor- mance on our test set for measuring syntactic and semantic word similarities.  ## 1 Introduction   Many current NLP systems and techniques treat words as atomic units - there is no notion of similar- ity between words, as these are represented as indices in a vocabulary. This choice has several good reasons - simplicity, robustness and the observation that simple models trained on huge amounts of data outperform complex systems trained on less data. An example is the popular N-gram model used for statistical language modeling - today, it is possible to train N-grams on virtually all available data (trillions of words [3]).   However, the simple techniques are at their limits in many tasks. For example, the amount of relevant in-domain data for automatic speech recognition is limited - the performance is usually dominated by the size of high quality transcribed speech data (often just millions of words). In machine translation, the existing corpora for many languages contain only a few billions of words or less. Thus, there are situations where simple scaling up of the basic techniques will not result in any significant progress, and we have to focus on more advanced techniques.   With progress of machine learning techniques in recent years, it has become possible to train more complex models on much larger data set, and they typically outperform the simple models.\n\nfilename: materials_formatted.md\nextension: .md\n\nProbably the most successful concept is to use distributed representations of words [10]. For example, neural network based language models significantly outperform N-gram models [1, 27, 17].   1.1 Goals of the Paper   The main goal of this paper is to introduce techniques that can be used for learning high-quality word vectors from huge data sets with billions of words, and with millions of words in the vocabulary. As far as we know, none of the previously proposed architectures has been successfully trained on more  # arXiv:1301.3781v3 [cs.CL] 7 Sep 2013   than a few hundred of millions of words, with a modest dimensionality of the word vectors between 50 - 100.  We use recently proposed techniques for measuring the quality of the resulting vector representa- tions, with the expectation that not only will similar words tend to be close to each other, but that words can havemultiple degrees of similarity[20]. This has been observed earlier in the context of inflectional languages - for example, nouns can have multiple word endings, and if we search for similar words in a subspace of the original vector space, it is possible to find words that have similar endings [13, 14].  Somewhat surprisingly, it was found that similarity of word representations goes beyond simple syntactic regularities. Using a word offset technique where simple algebraic operations are per- formed on the word vectors, it was shown for example thatvector(\u201dKing\u201d) - vector(\u201dMan\u201d) + vec- tor(\u201dWoman\u201d)results in a vector that is closest to the vector representation of the wordQueen[20].  In this paper, we try to maximize accuracy of these vector operations by developing new model architectures that preserve the linear regularities among words. We design a new comprehensive test set for measuring both syntactic and semantic regularities^1 , and show that many such regularities can be learned with high accuracy. Moreover, we discuss how training time and accuracy depends on the dimensionality of the word vectors and on the amount of the training data.  1.2 Previous Work  Representation of words as continuous vectors has a long history [10, 26, 8].\n\nfilename: materials_formatted.md\nextension: .md\n\nAnother way to improve accuracy is to provide more than one example of the relationship. By using ten examples instead of one to form the relationship vector (we average the individual vectors together), we have observed improvement of accuracy of our best models by about 10% absolutely on the semantic-syntactic test.  It is also possible to apply the vector operations to solve different tasks. For example, we have observed good accuracy for selecting out-of-the-list words, by computing average vector for a list of words, and finding the most distant word vector. This is a popular type of problems in certain human intelligence tests. Clearly, there is still a lot of discoveries to be made using these techniques.  ## 6 Conclusion  In this paper we studied the quality of vector representations of words derived by various models on a collection of syntactic and semantic language tasks. We observed that it is possible to train high quality word vectors using very simple model architectures, compared to the popular neural network models (both feedforward and recurrent). Because of the much lower computational complexity, it is possible to compute very accurate high dimensional word vectors from a much larger data set. Using the DistBelief distributed framework, it should be possible to train the CBOW and Skip-gram models even on corpora with one trillion words, for basically unlimited size of the vocabulary. That is several orders of magnitude larger than the best previously published results for similar models.  An interesting task where the word vectors have recently been shown to significantly outperform the previous state of the art is the SemEval-2012 Task 2 [11]. The publicly available RNN vectors were used together with other techniques to achieve over 50% increase in Spearman\u2019s rank correlation over the previous best result [31]. The neural network based word vectors were previously applied to many other NLP tasks, for example sentiment analysis [12] and paraphrase detection [28]. It can be expected that these applications can benefit from the model architectures described in this paper.  Our ongoing work shows that the word vectors can be successfully applied to automatic extension of facts in Knowledge Bases, and also for verification of correctness of existing facts. Results from machine translation experiments also look very promising. In the future, it would be also interesting to compare our techniques to Latent Relational Analysis [30] and others.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWho are the authors of the paper on Efficient Estimation of Word Representations in Vector Space?[/INST] \nAnswer: The authors of the paper on Efficient Estimation of Word Representations in Vector Space are Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean, all from Google Inc., Mountain View, CA."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\n### 1. 2 Transition-Based Dependency Parsing  Transition-based dependency parsing relies on a state machine which defines the possible transitions to create the mapping from the input sentence to the dependency tree. Thelearning problemis to induce a model which can predict the next transition in the state machine based on the transition history. Theparsing problemis to construct the optimal sequence of transitions for the input sentence, given the previously induced model. Most transition-based systems do not make use of a formal grammar.  ### 1. 3 Greedy Deterministic Transition-Based Parsing  This system was introduced by Nivre in 2003 and was radically dif- ferent from other methods in use at that time. This transition system is a state machine, which consists ofstates andtransitionsbetween those states. The model induces a sequence of transitions from someinitialstate to one of severalterminalstates. States: For any sentenceS=w 0 w 1 ...wn, a state can be described with a triplec= ( _\u03c3_ , _\u03b2_ ,A):  1. a stack _\u03c3_ of wordswifromS,  2. a buffer _\u03b2_ of wordswifromS,  3. a set of dependency arcsAof the form(wi,r,wj), wherewi,wjare fromS, andrdescribes a dependency relation.  It follows that for any sentenceS=w 0 w 1 ...wn,  1. aninitialstatec 0 is of the form([w 0 ] _\u03c3_ ,[w 1 , ...,wn] _\u03b2_ ,\u2205)(only the ROOT is on the stack _\u03c3_ , all other words are in the buffer _\u03b2_ and no actions have been chosen yet),  2. a terminal state has the form( _\u03c3_ ,[] _\u03b2_ ,A).   Transitions: Figure 2 : Transitions for Dependency Parsing.   There are three types of transitions between states:  1. Shift: Remove the first word in the buffer and push it on top of the stack. (Pre-condition: buffer has to be non-empty.)  2.\n\nfilename: materials_formatted.md\nextension: .md\n\n## 1 Dependency Grammar and Dependency Structure  Parse trees in NLP, analogous to those in compilers, are used to ana- lyze the syntactic structure of sentences. There are two main types of structures used - constituency structures and dependency structures. Constituency Grammar uses phrase structure grammar to organize words into nested constituents. This will be covered in more detail in following chapters. We now focus on Dependency Parsing. Dependency structure of sentences shows which words depend on (modify or are arguments of) which other words. These binary asymmetric relations between the words are called dependencies and are depicted as arrows going from the head (or governor, superior, regent) to the dependent (or modifier, inferior, subordinate). Usually these dependencies form a tree structure. They are often typed with the name of grammatical relations (subject, prepositional object, apposition, etc.). An example of such a dependency tree is shown in Figure 1. Sometimes a fake ROOT node is added as the head to the whole tree so that every word is a dependent of exactly one node.   Figure 1 : Dependency tree for the sen- tence \"Bills on ports and immigration were submitted by Senator Brownback, Republican of Kansas\"  ### 1. 1 Dependency Parsing  Dependency parsing is the task of analyzing the syntactic depen- dency structure of a given input sentenceS. The output of a depen- dency parser is a dependency tree where the words of the input sen- tence are connected by typed dependency relations. Formally, the de- pendency parsing problem asks to create a mapping from the input sentence with wordsS=w 0 w 1 ...wn(wherew 0 is the ROOT) to its dependency tree graphG. Many different variations of dependency- based methods have been developed in recent years, including neural network-based methods, which we will describe later. To be precise, there are two subproblems in dependency parsing (adapted from Kuebler et al., chapter 1. 2 ):  1. Learning:Given a training setDof sentences annotated with de- pendency graphs, induce a parsing modelMthat can be used to parse new sentences.   ## dependency parsing 2  2. Parsing:Given a parsing modelMand a sentenceS, derive the optimal dependency graphDforSaccording toM.\n\nfilename: materials_formatted.md\nextension: .md\n\n(Pre-condition: buffer has to be non-empty.)  2. Left-Arcr: Add a dependency arc(wj,r,wi)to the arc setA, wherewiis the word second to the top of the stack andwjis the   ## dependency parsing 3   word at the top of the stack. Removewifrom the stack. (Pre- condition: the stack needs to contain at least two items andwi cannot be the ROOT.)  3. Right-Arcr: Add a dependency arc(wi,r,wj)to the arc setA, wherewiis the word second to the top of the stack andwjis the word at the top of the stack. Removewjfrom the stack. (Pre- condition: The stack needs to contain at least two items.)  A more formal definition of these three transitions is presented in Figure 2.  ### 1. 4 Neural Dependency Parsing  While there are many deep models for dependency parsing, this section focuses specifically on greedy, transition-based neural de- pendency parsers. This class of model has demonstrated compara- ble performance and significantly better efficiency than traditional feature-based discriminative dependency parsers. The primary dis- tinction from previous models is the reliance on dense rather than sparse feature representations. The model we will describe employs the arc-standard system for transitions, as presented in section 1. 3. Ultimately, the aim of the model is to predict a transition sequence from some initial con- figurationcto a terminal configuration, in which the dependency parse tree is encoded. As the model is greedy, it attempts to correctly predict one transitionT \u2208 {shift, Left-Arcr, Right-Arcr}at a time, based on features extracted from the current configuration c= ( _\u03c3_ , _\u03b2_ ,A). Recall, _\u03c3_ is the stack, _\u03b2_ the buffer, andAthe set of dependency arcs for a given sentence.   Feature Selection:  Depending on the desired complexity of the model, there is flexi- bility in defining the input to the neural network. The features for a given sentenceSgenerally include some subset of:  1. Sword: Vector representations for some of the words inS(and their dependents) at the top of the stack _\u03c3_ and buffer _\u03b2_.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat are the three types of transitions between states in dependency parsing?[/INST] \nAnswer: The three types of transitions between states in dependency parsing are:\n\n1. Shift: This involves removing the first word in the buffer and pushing it on top of the stack. The buffer has to be non-empty for this transition.\n\n2. Left-Arcr: This adds a dependency arc (wj,r,wi) to the arc set A, where wi is the word second to the top of the stack and wj is the word at the top of the stack. Wi is then removed from the stack. For this transition, the stack needs to contain at least two items and wi cannot be the ROOT.\n\n3. Right-Arcr: This adds a dependency arc (wi,r,wj) to the arc set A, where wi is the word second to the top of the stack and wj is the word at the top of the stack. Wj is then removed from the stack. The stack needs to contain at least two items for this transition."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nTherefore, we start this section by explicitly de\ufb01ning and contrasting the different settings that we will be evaluating GPT-3 on or could in principle evaluate GPT-3 on. These settings can be seen as lying on a spectrum of how much task-speci\ufb01c data they tend to rely on. Speci\ufb01cally, we can identify at least four points on this spectrum (see Figure 2.1 for an illustration):  \u2022 Fine-Tuning (FT) has been the most common approach in recent years, and involves updating the weights of a pre-trained model by training on a supervised dataset speci\ufb01c to the desired task. Typically thousands to hundreds of thousands of labeled examples are used. The main advantage of \ufb01ne-tuning is strong performance on many benchmarks. The main disadvantages are the need for a new large dataset for every task, the potential for poor generalization out-of-distribution [MPL19], and the potential to exploit spurious features of the training data [GSL+18, NK19], potentially resulting in an unfair comparison with human performance. In this work we do not \ufb01ne-tune GPT-3 because our focus is on task-agnostic performance, but GPT-3 can be \ufb01ne-tuned in principle and this is a promising direction for future work.  \u2022 Few-Shot (FS) is the term we will use in this work to refer to the setting where the model is given a few demonstrations of the task at inference time as conditioning [RWC+19], but no weight updates are allowed. As shown in Figure 2.1, for a typical dataset an example has a context and a desired completion (for example an English sentence and the French translation), and few-shot works by giving K examples of context and completion, and then one \ufb01nal example of context, with the model expected to provide the completion. We typically set K in the range of 10 to 100 as this is how many examples can \ufb01t in the model\u2019s context window (nctx = 2048). The main advantages of few-shot are a major reduction in the need for task-speci\ufb01c data and reduced potential to learn an overly narrow distribution from a large but narrow \ufb01ne-tuning dataset.\n\nfilename: materials_formatted.md\nextension: .md\n\nA: 38\u201d. The three 1 digit numbers are selected uniformly on [0, 10) and the operations are selected uniformly from {+,-,*}.  In all 10 tasks the model must generate the correct answer exactly. For each task we generate a dataset of 2,000 random instances of the task and evaluate all models on those instances.  First we evaluate GPT-3 in the few-shot setting, for which results are shown in Figure 3.10. On addition and subtraction, GPT-3 displays strong pro\ufb01ciency when the number of digits is small, achieving 100% accuracy on 2 digit addition, 98.9% at 2 digit subtraction, 80.2% at 3 digit addition, and 94.2% at 3-digit subtraction. Performance decreases as the number of digits increases, but GPT-3 still achieves 25-26% accuracy on four digit operations and 9-10% accuracy on \ufb01ve digit operations, suggesting at least some capacity to generalize to larger numbers of digits. GPT-3 also achieves 29.2% accuracy at 2 digit multiplication, an especially computationally intensive operation. Finally, GPT-3 achieves 21.3% accuracy at single digit combined operations (for example, 9*(7+5)), suggesting that it has some robustness beyond just single operations.  As Figure 3.10 makes clear, small models do poorly on all of these tasks \u2013 even the 13 billion parameter model (the second largest after the 175 billion full GPT-3) can solve 2 digit addition and subtraction only half the time, and all other operations less than 10% of the time.  One-shot and zero-shot performance are somewhat degraded relative to few-shot performance, suggesting that adaptation to the task (or at the very least recognition of the task) is important to performing these computations correctly.\n\nfilename: materials_formatted.md\nextension: .md\n\nQuantitatively, GPT-3\u2019s in-context learning performance has some notable gaps on our suite of benchmarks, as described in Section 3, and in particular it does little better than chance when evaluated one-shot or even few-shot on some \u201ccomparison\u201d tasks, such as determining if two words are used the same way in a sentence, or if one sentence implies another (WIC and ANLI respectively), as well as on a subset of reading comprehension tasks. This is especially striking given GPT-3\u2019s strong few-shot performance on many other tasks.  GPT-3 has several structural and algorithmic limitations, which could account for some of the issues above. We focused on exploring in-context learning behavior in autoregressive language models because it is straightforward to both sample and compute likelihoods with this model class. As a result our experiments do not include any bidirectional architectures or other training objectives such as denoising. This is a noticeable difference from much of the recent literature, which has documented improved \ufb01ne-tuning performance when using these approaches over standard language models [RSR+19]. Thus our design decision comes at the cost of potentially worse performance on tasks which empirically bene\ufb01t from bidirectionality. This may include \ufb01ll-in-the-blank tasks, tasks that involve looking back and comparing two pieces of content, or tasks that require re-reading or carefully considering a long passage and then generating a very short answer. This could be a possible explanation for GPT-3\u2019s lagging few-shot performance on a few of the tasks, such as WIC (which involves comparing the use of a word in two sentences), ANLI (which involves comparing two sentences to see if one implies the other), and several reading comprehension tasks (e.g. QuAC and RACE). We also conjecture, based on past literature, that a large bidirectional model would be stronger at \ufb01ne-tuning than GPT-3. Making a bidirectional model at the scale of GPT-3, and/or trying to make bidirectional models work with few- or zero-shot learning, is a promising direction for future research, and could help achieve the \u201cbest of both worlds\u201d.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nHow does GPT-3 perform tasks without any gradient updates or fine-tuning?[/INST] \nAnswer: GPT-3 performs tasks without any gradient updates or fine-tuning by using a method known as Few-Shot (FS). In this setting, the model is given a few demonstrations of the task at inference time as conditioning, but no weight updates are allowed. For a typical dataset, an example has a context and a desired completion, and few-shot works by giving the model K examples of context and completion, and then one final example of context, with the model expected to provide the completion. The number of examples, K, is typically set in the range of 10 to 100 as this is how many examples can fit in the model\u2019s context window. The main advantages of few-shot are a major reduction in the need for task-specific data and reduced potential to learn an overly narrow distribution from a large but narrow fine-tuning dataset."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nFurthermore, we show that these modifications can be transferred to traditional distributional models, yielding similar gains. In contrast to prior reports, we observe mostly local or insignificant performance differences between the methods, with no global advantage to any single approach over the others.  ## 1 Introduction  Understanding the meaning of a word is at the heart of natural language processing (NLP). While a deep, human-like, understanding remains elu- sive, many methods have been successful in recov- ering certain aspects of similarity between words. Recently, neural-network based approaches in which words are \u201cembedded\u201d into a low- dimensional space were proposed by various au- thors (Bengio et al., 2003; Collobert and Weston, 2008). These models represent each word as ad- dimensional vector of real numbers, and vectors that are close to each other are shown to be se- mantically related. In particular, a sequence of pa- pers by Mikolov et al. (2013a; 2013b) culminated in the skip-gram with negative-sampling training method (SGNS): an efficient embedding algorithm that provides state-of-the-art results on various lin- guistic tasks. It was popularized viaword2vec, a program for creating word embeddings.   A recent study by Baroni et al. (2014) con- ducts a set of systematic experiments compar- ingword2vecembeddings to the more tradi- tional distributional methods, such as pointwise mutual information (PMI) matrices (see Turney and Pantel (2010) and Baroni and Lenci (2010) for comprehensive surveys). These results suggest that the new embedding methods consistently out- perform the traditional methods by a non-trivial margin on many similarity-oriented tasks. How- ever, state-of-the-art embedding methods are all based on the same bag-of-contexts representation of words. Furthermore, analysis by Levy and Goldberg (2014c) shows thatword2vec\u2019s SGNS is implicitly factorizing a word-context PMI ma- trix. That is, the mathematical objective and the sources of information available to SGNS are in fact very similar to those employed by the more traditional methods. What, then, is the source of superiority (or per- ceived superiority) of these recent embeddings?\n\nfilename: materials_formatted.md\nextension: .md\n\nUsing this variant increases per- formance by over 3 points per task, on average. We suspect that this smoothing partially addresses the \u201cAchilles\u2019 heel\u201d of PMI: its bias towards co- occurrences of rare words. We also show that when all methods are allowed to tune a similar set of hyperparameters, their per- formance is largely comparable. In fact, there is no consistent advantage to one algorithmic approach over another, a result that contradicts the claim that embeddings are superior to count-based methods.  ## 2 Background  We consider four word representation methods: the explicit PPMI matrix, SVD factorization of said matrix, SGNS, and GloVe. For historical reasons, we refer to PPMI and SVD as \u201ccount- based\u201d representations, as opposed to SGNS and GloVe, which are often referred to as \u201cneural\u201d or \u201cprediction-based\u201d embeddings. All of these methods (as well as all other \u201cskip-gram\u201d-based embedding methods) are essentially bag-of-words models, in which the representation of each word reflects a weighted bag of context-words that co- occur with it. Such bag-of-word embedding mod- els were previously shown to perform as well as or better than more complex embedding methods on similarity and analogy tasks (Mikolov et al., 2013a; Pennington et al., 2014).  Notation We assume a collection of wordsw\u2208 VWand their contextsc\u2208VC, whereVWandVC are the word and context vocabularies, and denote the collection of observed word-context pairs as   D. We use#(w,c)to denote the number of times the pair\u2211 (w,c)appears inD. Similarly,#(w) = c\u2032\u2208VC#(w,c   \u2032)and#(c) =\u2211 w\u2032\u2208VW#(w   \u2032,c) are the number of timeswandcoccurred inD, respectively. In some algorithms, words and con- texts are embedded in a space ofddimensions. In these cases, each wordw\u2208VWis associated with a vector ~w \u2208 Rdand similarly each con- textc \u2208VCis represented as a vector~c\u2208Rd.\n\nfilename: materials_formatted.md\nextension: .md\n\nTo facilitate systematic evaluation and compar- ison of new embedding models, we release a new frequency-calibrated query inventory along with all user judgments athttp://www.cs. cornell.edu/ \u0303schnabts/eval/. Finally, we observe that word embeddings en- code a surprising degree of information about word frequency. We found this was true even in models that explicitly reserve parameters to com- pensate for frequency effects. This finding may explain some of the variability across embeddings and across evaluation methods. It also casts doubt on the common practice of using the vanilla co- sine similarity as a similarity measure in the em- bedding space. It is important to note that this work is a survey ofevaluationmethods not a survey ofembedding methods. The specific example embeddings pre- sented here were chosen as representative samples only, and may not be optimal.  ## 2 Word embeddings  We refer to a word embedding as a mappingV\u2192 RD:w7\u2192~wthat maps a wordwfrom a vocabu- laryV to a real-valued vector~win an embedding space of dimensionalityD. Following previous work (Collobert et al., 2011; Mikolov et al., 2013a) we use the com- monly employed cosine similarity, defined as similarity(w 1 ,w 2 ) = \u2016~w~w 11 \u2016\u2016\u00b7~w~w^22 \u2016, for all similar- ity computations in the embedding space. The list of nearest neighbors of a wordware all words v \u2208 V \\ {w}, sorted in descending order by similarity(w,v). We will denotewas thequery wordin the remainder of this paper. All experiments in this paper are carried out on six popular unsupervised embedding meth- ods. These embeddings form a representative but incomplete subset; and since we are study-   ing evaluation methods and not embeddings them- selves, no attempt has been made to optimize these embeddings. The first two embedding models, the CBOW model of word2vec (Mikolov et al., 2013a) and C&W embeddings (Collobert et al., 2011) both are motivated by a probabilistic predic- tion approach.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat do recent trends suggest about the performance of neural-network-inspired word embedding models compared to traditional count-based distributional models?[/INST] \nAnswer: Recent trends suggest that there is no consistent advantage of neural-network-inspired word embedding models over traditional count-based distributional models. The performance of these methods is largely comparable when they are allowed to tune a similar set of hyperparameters. This contradicts the claim that embeddings are superior to count-based methods."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nand other research communities, though there has been some convergence of techniques of late.  In all aspects of NLP, most existing tools work for precious few (usu- ally one, maybe up to 100 ) of the world\u2019s roughly 7000 languages, and fail disproportionately much on lesser-spoken and/or marginal- ized dialects, accents, and more. Beyond this, recent successes in building better systems have far outstripped our ability to charac- terize and audit these systems. Biases encoded in text, from race to gender to religion and more, are reflected and often amplified by NLP systems. With these challenges and considerations in mind, but with the desire to do good science and build trustworthy systems that improve peoples\u2019 lives, let\u2019s take a look at a fascinating first problem in NLP.  ## 2 Representing words  ### 2. 1 Signifier and signified  Consider the sentence   Zuko makes the tea for his uncle.  The wordZukois a sign, a symbol that represents an entityZukoin some (real of imagined) world. The wordteais also a symbol that refers to a signified thing\u2014perhaps a specific instance of tea. If one were instead to sayZuko likes to make tea for his uncle, note that the symbolZukostill refers toZuko, but nowtearefers to a broader class\u2014tea in general, not aspecificbit of hot delicious water. Consider the two following sentences:   Zuko makes the coffee for his uncle. Zuko makes the drink for his uncle.  Which is \u201cmore like\u201d the sentence about tea? Thedrinkmay be tea (or it may be quite different!) andcoffeedefinitely isn\u2019t tea, but is yet similar, no? And isZukosimilar tounclebecause they both describe people? And isthesimilar tohisbecause they both pick out specific instances of a class? Word meaning is endlessly complex, deriving from humans\u2019 goals of communicating with each other and achieving goals in the world. People use continuous media\u2014speech, signing\u2014but produce signs in a discrete, symbolic structure\u2014language\u2014to express complex meanings. Expressing and processing the nuance and wildness of language\u2014while achieving the strong transfer of information that    deep learning 4  language is intended to achieve\u2014makes representing words an endlessly fascinating problem.\n\nfilename: materials_formatted.md\nextension: .md\n\nMost importantly, words have different meanings in different contexts. At a coarse-grained level, this was captured by experts in crafting WordNet, in which, for example, get is mapped to over thirty different meanings (or senses). It is dif\ufb01cult to obtain widespread agreement on how many senses should be allocated to different words, or on the boundaries between one sense and another; word senses may be \ufb02uid.8 Indeed, in many NLP programs based on neural networks, the very \ufb01rst thing that happens is that each word token\u2019s type vector is passed into a function that transforms it based on the words in its nearby context, giving a new version of the word vector, now speci\ufb01c to the token in its particular context. In our example sentence from Section 1, the two instances of a will therefore have different vectors, because one occurs between is and word and the other occurs between in and piece. Compare Figures 3 and 5.  With hindsight, we can now see that by representing word types independent of context, we were solving a problem that was harder than it needed to be. Because words mean different things in different contexts,  7E.g., the present tense form of the French verb manger is mange, manges, mangeons, mangez, or mangent, depending on  whether the subject is singular or plural, and \ufb01rst, second, or third person.  8For example, the word bank can refer to the side of a river or to a \ufb01nancial institution. When used to refer to a blood bank, we  can debate whether the second sense is evoked or a third, distinct one.  9  astronomersbodiesobjectswe were requiring that type representations capture all of the possibilities (e.g., the thirty meanings of get). Moving to word token vectors simpli\ufb01es things, asking the word token representation to capture only what a word means in this context. For the same reasons that the collection of contexts a word type is found in provide clues about its meaning(s), a particular token\u2019s context provides clues about its speci\ufb01c meaning.\n\nfilename: materials_formatted.md\nextension: .md\n\nIn reality, this is hardly done be- cause of the nature of natural languages. Natural languages tend to use the same word for very different meanings and we typically need to know the context of the word usage to discriminate between mean- ings. For instance, if you were asked to explain to someone what   ## word vectors ii:glove,evaluation and training 13  \"to sanction\" meant, you would immediately realize that depending on the context \"to sanction\" could mean \"to permit\" or \"to punish\". In most situations, we tend to use a sequence of words as input to the model. A sequence is a central word vector preceded and suc- ceeded by context word vectors. The number of words in the context is also known as the context window size and varies depending on the problem being solved. Generally, narrower window sizes lead to better performance in syntactic tests while wider windows lead to better performance in semantic tests. Generally, narrower window sizes lead to better performance in syntactic tests while wider windows lead to better performance in semantic tests.  In order to modify the previously discussed Softmax model to use windows of words for classification, we would simply substitutex(i)  withx(windowi) in the following manner:   x(windowi) =  x(i\u2212^2 ) x(i\u2212^1 ) x(i) x(i+^1 ) x(i+^2 )  As a result, when we evaluate the gradient of the loss with respect to the words, we will receive gradients for the word vectors:   \u03b4 window= \u2207x(i\u2212 2 ) \u2207x(i\u2212 1 ) \u2207x(i) \u2207x(i+ 1 ) \u2207x(i+ 2 ) The gradient will of course need to be distributed to update the corresponding word vectors in implementation.  ### 3. 5 Non-linear Classifiers   Figure 9 : Here, we see that many exam- ples are wrongly classified even though the best linear decision boundary is chosen. This is due linear decision boundaries have limited model capacity for this dataset.  We now introduce the need for non-linear classification models such as neural networks. We see in Figure 9 that a linear classifier mis- classifies many datapoints. Using a non-linear decision boundary as shown in Figure 10 , we manage to classify all training points accu- rately.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat does the term 'signifier' refer to in the context of the note?[/INST] \nAnswer: In the context of the note, 'signifier' refers to a word or symbol that represents an entity or concept in some world, real or imagined. For example, in the sentence \"Zuko makes the tea for his uncle\", 'Zuko' is a signifier that represents an entity 'Zuko'. Similarly, 'tea' is a signifier that refers to a specific instance of tea or tea in general, depending on the context."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nArXiv, abs/1909.08593, 2019.  75     \n\n# Dense Passage Retrieval for Open-Domain Question Answering:  Dense Passage Retrieval for Open-Domain Question Answering  Vladimir Karpukhin\u2217, Barlas O\u02d8guz\u2217, Sewon Min\u2020, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen\u2021, Wen-tau Yih  Facebook AI  \u2020University of Washington  \u2021Princeton University  {vladk, barlaso, plewis, ledell, edunov, scottyih}@fb.com sewon@cs.washington.edu danqic@cs.princeton.edu  Abstract  Open-domain question answering relies on ef- \ufb01cient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented us- ing dense representations alone, where em- beddings are learned from a small number of questions and passages by a simple dual- encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene- BM25 system greatly by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.1  1  Introduction  Open-domain question answering (QA) (Voorhees, 1999) is a task that answers factoid questions us- ing a large collection of documents. While early QA systems are often complicated and consist of multiple components (Ferrucci (2012); Moldovan et al. (2003), inter alia), the advances of reading comprehension models suggest a much simpli\ufb01ed two-stage framework: (1) a context retriever \ufb01rst selects a small subset of passages where some of them contain the answer to the question, and then (2) a machine reader can thoroughly exam- ine the retrieved contexts and identify the correct answer (Chen et al., 2017). Although reducing open-domain QA to machine reading is a very rea- sonable strategy, a huge performance degradation is often observed in practice2, indicating the needs of improving retrieval.\n\nfilename: materials_formatted.md\nextension: .md\n\nFor a \ufb01xed k, a retriever can be evaluated in isolation on top-k retrieval accuracy, which is the fraction of ques- tions for which CF contains a span that answers the question.  3 Dense Passage Retriever (DPR)  We focus our research in this work on improv- ing the retrieval component in open-domain QA. Given a collection of M text passages, the goal of our dense passage retriever (DPR) is to index all the passages in a low-dimensional and continuous space, such that it can retrieve ef\ufb01ciently the top k passages relevant to the input question for the reader at run-time. Note that M can be very large (e.g., 21 million passages in our experiments, de- scribed in Section 4.1) and k is usually small, such as 20\u2013100.  3.1 Overview  Our dense passage retriever (DPR) uses a dense encoder EP (\u00b7) which maps any text passage to a d- dimensional real-valued vectors and builds an index for all the M passages that we will use for retrieval.  3The ideal size and boundary of a text passage are func- tions of both the retriever and reader. We also experimented with natural paragraphs in our preliminary trials and found that using \ufb01xed-length passages performs better in both retrieval and \ufb01nal QA accuracy, as observed by Wang et al. (2019).  4Exceptions include (Seo et al., 2019) and (Roberts et al., 2020), which retrieves and generates the answers, respectively.  At run-time, DPR applies a different encoder EQ(\u00b7) that maps the input question to a d-dimensional vector, and retrieves k passages of which vectors are the closest to the question vector. We de\ufb01ne the similarity between the question and the passage using the dot product of their vectors:  sim(q, p) = EQ(q)(cid:124)EP (p).  (1)  Although more expressive model forms for measur- ing the similarity between a question and a passage do exist, such as networks consisting of multiple layers of cross attentions, the similarity function needs to be decomposable so that the represen- tations of the collection of passages can be pre- computed. Most decomposable similarity functions are some transformations of Euclidean distance (L2).\n\nfilename: materials_formatted.md\nextension: .md\n\nStarting from our trained DPR model, they show that the retrieval performance can be further improved. Recent work (Izacard and Grave, 2020; Lewis et al., 2020b) have also shown that DPR can be combined with generation models such as BART (Lewis et al., 2020a) and T5 (Raf- fel et al., 2019), achieving good performance on open-domain QA and other knowledge-intensive tasks.  8 Conclusion  In this work, we demonstrated that dense retrieval can outperform and potentially replace the tradi- tional sparse retrieval component in open-domain question answering. While a simple dual-encoder approach can be made to work surprisingly well, we showed that there are some critical ingredients to training a dense retriever successfully. Moreover, our empirical analysis and ablation studies indicate that more complex model frameworks or similarity functions do not necessarily provide additional val- ues. As a result of improved retrieval performance, we obtained new state-of-the-art results on multiple open-domain question answering benchmarks.  Acknowledgments  We thank the anonymous reviewers for their helpful comments and suggestions.  References  Akari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi, Richard Socher, and Caiming Xiong. 2020. Learn- ing to retrieve reasoning paths over Wikipedia graph for question answering. In International Conference on Learning Representations (ICLR).  Petr Baudi\u02c7s and Jan \u02c7Sediv`y. 2015. Modeling of the question answering task in the yodaqa system. In In- ternational Conference of the Cross-Language Eval- uation Forum for European Languages, pages 222\u2013 228. Springer.  Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on Freebase from  question-answer pairs. In Empirical Methods in Nat- ural Language Processing (EMNLP).  Jane Bromley, Isabelle Guyon, Yann LeCun, Eduard S\u00a8ackinger, and Roopak Shah. 1994. Signature veri\ufb01- cation using a \u201cSiamese\u201d time delay neural network. In NIPS, pages 737\u2013744.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nHow does dense passage retrieval (DPR) enhance open-domain QA?[/INST] \nAnswer: Dense Passage Retrieval (DPR) enhances open-domain QA by indexing all the passages in a low-dimensional and continuous space. This allows it to efficiently retrieve the top k passages relevant to the input question for the reader at run-time. DPR uses a dense encoder which maps any text passage to a d-dimensional real-valued vectors and builds an index for all the passages that will be used for retrieval. At run-time, DPR applies a different encoder that maps the input question to a d-dimensional vector, and retrieves k passages of which vectors are the closest to the question vector. This method has been shown to outperform traditional sparse retrieval methods in open-domain question answering."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nAfter graduating from Columbia University in 1983, he workedasa community organizerinChicago.   (Levy et al.,2017)   Semantic role labeling   16   (He etal.,2015)   #### Stanford question answering dataset (SQuAD)  - 100k annotated (passage, question, answer) triples     Large-scale supervised datasets are also a key ingredient for     training effective neural models for reading comprehension! - Passages are selected from English Wikipedia, usually 100~150 words. - Questions are crowd-sourced. - Each answer is a short segment of text (or span) in the passage.   This is a limitation\u2014 not all the questions can be answered in this way!  - SQuAD was for years the most popular reading comprehension     dataset; it is \u201calmost solved\u201d today (though the underlying task is     not,) and the state-of-the-art exceeds the estimated human     performance.   (Rajpurkaretal.,2016):SQuAD:100,000+QuestionsforMachineComprehensionofText^16    18  #### Stanford question answering dataset (SQuAD)  - Evaluation : exact match ( 0 or 1 ) and F 1 (partial credit). - For development and testing sets, 3 gold answers are collected, because there could be multiple     plausible answers. - We compare the predicted answer to _each_ gold answer (a, an, the, punctuations are removed)     and take max scores. Finally, we take the average of all the examples for both exact match and     F 1. - Estimated human performance: EM = 82. 3 , F 1 = 91. 2   Q: What did Tesla do in December 1878?   A: {left Graz, left Graz, left Graz and severed all relations with his family}   Prediction: {left Graz and served}   Exact match: max{0, 0, 0} = 0 F1: max{0.67, 0.67, 0.61} = 0.    19  #### Stanford question answering dataset (SQuAD)    20  #### Other question answering datasets  - TriviaQA: Questions and answers by trivia enthusiasts.\n\nfilename: materials_formatted.md\nextension: .md\n\nThe dataset contains 87k examples for training and 10k for development, with a large hidden test set which can only be accessed by the SQuAD creators. Each example is composed of a paragraph extracted from a Wikipedia article and an associated human-generated question. The answer is always a span from this paragraph and a model is given credit if its predicted answer matches it. Two evaluation metrics are used: exact string match (EM) and F1 score, which measures the weighted average of precision and recall at the token level.  In the following, we use SQuAD for training and evaluating our Document Reader for the stan- dard machine comprehension task given the rel-  2https://dumps.wikimedia.org/enwiki/  latest  evant paragraph as de\ufb01ned in (Rajpurkar et al., 2016). For the task of evaluating open-domain question answering over Wikipedia, we use the SQuAD development set QA pairs only, and we ask systems to uncover the correct answer spans without having access to the associated para- graphs. That is, a model is required to answer a question given the whole of Wikipedia as a re- source; it is not given the relevant paragraph as in the standard SQuAD setting.  4.3 Open-domain QA Evaluation Resources  SQuAD is one of the largest general purpose QA datasets currently available. SQuAD questions have been collected via a process involving show- ing a paragraph to each human annotator and ask- ing them to write a question. As a result, their distribution is quite speci\ufb01c. We hence propose to train and evaluate our system on other datasets de- veloped for open-domain QA that have been con- structed in different ways (not necessarily in the context of answering from Wikipedia).  CuratedTREC This dataset is based on the benchmarks from the TREC QA tasks that have been curated by Baudi\u02c7s and \u02c7Sediv`y (2015). We use the large version, which contains a total of 2,180 questions extracted from the datasets from TREC 1999, 2000, 2001 and 2002.4  WebQuestions Introduced in (Berant et al., 2013), this dataset is built to answer questions from the Freebase KB.\n\nfilename: materials_formatted.md\nextension: .md\n\nExisting datasets for RC have one of two shortcomings: (i) those that are high in quality (Richardson et al., 2013; Berant et al., 2014) are too small for training modern data-intensive models, while (ii) those that are large (Hermann et al., 2015; Hill et al., 2015) are semi-synthetic and do not share the same character- istics as explicit reading comprehension questions.  To address the need for a large and high-quality reading comprehension dataset, we present the Stan-              ford Question Answering Dataset v1.0 (SQuAD), freely available at https://stanford-qa.com, con- sisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to ev- ery question is a segment of text, or span, from the corresponding reading passage. SQuAD contains 107,785 question-answer pairs on 536 articles, and is almost two orders of magnitude larger than previ- ous manually labeled RC datasets such as MCTest (Richardson et al., 2013).  In contrast to prior datasets, SQuAD does not provide a list of answer choices for each question. Rather, systems must select the answer from all pos- sible spans in the passage, thus needing to cope with a fairly large number of candidates. While ques- tions with span-based answers are more constrained than the more interpretative questions found in more advanced standardized tests, we still \ufb01nd a rich di- versity of questions and answer types in SQuAD. We develop automatic techniques based on distances in dependency trees to quantify this diversity and stratify the questions by dif\ufb01culty. The span con- straint also comes with the important bene\ufb01t that span-based answers are easier to evaluate than free- form answers.  To assess the dif\ufb01culty of SQuAD, we imple- mented a logistic regression model with a range of features. We \ufb01nd that lexicalized and dependency tree path features are important to the performance of the model. We also \ufb01nd that the model perfor- mance worsens with increasing complexity of (i) an- swer types and (ii) syntactic divergence between the question and the sentence containing the answer; in- terestingly, there is no such degradation for humans.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nHow does the Stanford question answering dataset (SQuAD) evaluate answers?[/INST] \nAnswer: The Stanford Question Answering Dataset (SQuAD) evaluates answers using two metrics: exact string match (EM) and F1 score. The exact string match gives a model credit if its predicted answer exactly matches the actual answer. The F1 score measures the weighted average of precision and recall at the token level. For development and testing sets, three gold answers are collected as there could be multiple plausible answers. The predicted answer is compared to each gold answer (with articles and punctuations removed) and the maximum scores are taken. Finally, the average of all the examples for both exact match and F1 is taken."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nTo guide the work- ers, tasks contained a sample paragraph, and exam- ples of good and bad questions and answers on that paragraph along with the reasons they were cate- gorized as such. Additionally, crowdworkers were encouraged to ask questions in their own words, without copying word phrases from the paragraph. On the interface, this was reinforced by a reminder prompt at the beginning of every paragraph, and by disabling copy-paste functionality on the paragraph text.  Additional answers collection. To get an indica- tion of human performance on SQuAD and to make our evaluation more robust, we obtained at least 2 additional answers for each question in the develop- ment and test sets. In the secondary answer gener- ation task, each crowdworker was shown only the questions along with the paragraphs of an article, and asked to select the shortest span in the para- graph that answered the question. If a question was not answerable by a span in the paragraph, workers were asked to submit the question without marking an answer. Workers were recommended a speed of 5 questions for 2 minutes, and paid at the same rate of $9 per hour for the number of hours required for the entire article. Over the development and test sets, 2.6% of questions were marked unanswerable by at least one of the additional crowdworkers.  Date Other Numeric Person Location Other Entity Common Noun Phrase Adjective Phrase Verb Phrase Clause Other  8.9% 19 October 1512  10.9% 12 12.9% Thomas Coke  4.4% Germany 15.3% ABC Sports 31.8% property damage 3.9% second-largest 5.5% returned to Earth 3.7% to avoid trivialization 2.7% quietly  Table 2: We automatically partition our answers into the fol-  lowing categories. Our dataset consists of large number of an-  swers beyond proper noun entities.  4 Dataset Analysis  To understand the properties of SQuAD, we analyze the questions and answers in the development set. Speci\ufb01cally, we explore the (i) diversity of answer types, (ii) the dif\ufb01culty of questions in terms of type of reasoning required to answer them, and (iii) the degree of syntactic divergence between the question and answer sentences.\n\nfilename: materials_formatted.md\nextension: .md\n\nFeatures Full No ftoken No fexact match No faligned No faligned and fexact match  F1 78.8 78.0 (-0.8) 77.3 (-1.5) 77.3 (-1.5) 59.4 (-19.4)  Table 5: Feature ablation analysis of the paragraph representations of our Document Reader. Results are reported on the SQuAD development set.  \ufb01nd that while these help for more exact paragraph reading in SQuAD, they don\u2019t improve results in the full setting. Additionally, WebQuestions and WikiMovies provide a list of candidate answers (e.g., 1.6 million Freebase entity strings for We- bQuestions) and we restrict the answer span must be in this list during prediction.  Results Table 6 presents the results. Despite the dif\ufb01culty of the task compared to machine com- prehension (where you are given the right para- graph) and unconstrained QA (using redundant re- sources), DrQA still provides reasonable perfor- mance across all four datasets.  We are interested in a single, full system that can answer any question using Wikipedia. The single model trained only on SQuAD is outper- formed on all four of the datasets by the multitask model that uses distant supervision. However per- formance when training on SQuAD alone is not far behind, indicating that task transfer is occurring. The majority of the improvement from SQuAD to Multitask (DS) however is likely not from task transfer as \ufb01ne-tuning on each dataset alone using DS also gives improvements, showing that is is the introduction of extra data in the same domain that helps. Nevertheless, the best single model that we can \ufb01nd is our overall goal, and that is the Multi- task (DS) system.  We compare to an unconstrained QA system us- ing redundant resources (not just Wikipedia), Yo- daQA (Baudi\u02c7s, 2015), giving results which were previously reported on CuratedTREC and We- bQuestions. Despite the increased dif\ufb01culty of our task, it is reassuring that our performance is not too far behind on CuratedTREC (31.3 vs. 25.4).\n\nfilename: materials_formatted.md\nextension: .md\n\n4We note that in the literature the bidirectional Trans-  BERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLIInput/Output Representations To make BERT handle a variety of down-stream tasks, our input representation is able to unambiguously represent both a single sentence and a pair of sentences (e.g., (cid:104) Question, Answer (cid:105)) in one token sequence. Throughout this work, a \u201csentence\u201d can be an arbi- trary span of contiguous text, rather than an actual linguistic sentence. A \u201csequence\u201d refers to the in- put token sequence to BERT, which may be a sin- gle sentence or two sentences packed together.  We use WordPiece embeddings (Wu et al., 2016) with a 30,000 token vocabulary. The \ufb01rst token of every sequence is always a special clas- si\ufb01cation token ([CLS]). The \ufb01nal hidden state corresponding to this token is used as the ag- gregate sequence representation for classi\ufb01cation tasks. Sentence pairs are packed together into a single sequence. We differentiate the sentences in two ways. First, we separate them with a special token ([SEP]). Second, we add a learned embed- ding to every token indicating whether it belongs to sentence A or sentence B. As shown in Figure 1, we denote input embedding as E, the \ufb01nal hidden vector of the special [CLS] token as C \u2208 RH , and the \ufb01nal hidden vector for the ith input token as Ti \u2208 RH .  For a given token, its input representation is constructed by summing the corresponding token, segment, and position embeddings.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat is the objective of the text generation task in SEP775 Assignment 2?[/INST] \nAnswer: I don't know."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nThe heads clearly learned to perform different tasks.  15  Input-Input Layer5TheLawwillneverbeperfect,butitsapplicationshouldbejust-thisiswhatwearemissing,inmyopinion.<EOS><pad>TheLawwillneverbeperfect,butitsapplicationshouldbejust-thisiswhatwearemissing,inmyopinion.<EOS><pad>Input-Input Layer5TheLawwillneverbeperfect,butitsapplicationshouldbejust-thisiswhatwearemissing,inmyopinion.<EOS><pad>TheLawwillneverbeperfect,butitsapplicationshouldbejust-thisiswhatwearemissing,inmyopinion.<EOS><pad>   \n\n# MUSIC TRANSFORMER - GENERATING MUSIC WITH LONG-TERM STRUCTURE:  MUSIC TRANSFORMER: GENERATING MUSIC WITH LONG-TERM STRUCTURE  Cheng-Zhi Anna Huang\u2217 Ashish Vaswani Ian Simon Curtis Hawthorne Andrew M. Dai Matthew D. Hoffman Monica Dinculescu Douglas Eck Google Brain  Jakob Uszkoreit Noam Shazeer  ABSTRACT  Music relies heavily on repetition to build structure and meaning. Self-reference occurs on multiple timescales, from motifs to phrases to reusing of entire sections of music, such as in pieces with ABA structure. The Transformer (Vaswani et al., 2017), a sequence model based on self-attention, has achieved compelling results in many generation tasks that require maintaining long-range coherence. This suggests that self-attention might also be well-suited to modeling music. In musical composition and performance, however, relative timing is critically important. Existing approaches for representing relative positional information in the Transformer modulate attention based on pairwise distance (Shaw et al., 2018). This is impractical for long sequences such as musical compositions since their memory complexity for intermediate relative information is quadratic in the sequence length. We propose an algorithm that reduces their intermediate memory requirement to linear in the sequence length. This enables us to demonstrate that a Transformer with our modi\ufb01ed relative attention mechanism can generate minute- long compositions (thousands of steps, four times the length modeled in Oore et al. (2018)) with compelling structure, generate continuations that coherently elaborate on a given motif, and in a seq2seq setup generate accompaniments conditioned on melodies1.\n\nfilename: materials_formatted.md\nextension: .md\n\nOur work show that Transformers not only achieve state-of-the-art perplexity on modeling these complex expressive piano performances, but can also generate them at the scale of 60s (~2000 tokens) with remarkable internal consistency. Our relative attention mechanism is essential to the model\u2019s quality. In listening tests (see Section 4.2.3), samples from models with relative self-attention were perceived as more coherent than the baseline Transformer model from Vaswani et al. (2017). Relative attention not only enables Transformers to generate continuations that elaborate on a given motif, but also to generalize and generate in consistent fashion beyond the length it was trained on (see Section 4.2.1). In a seq2seq setup, Transformers can generate accompaniments conditioned on melodies, enabling users to interact with the model.  Algorithmic contributions The space complexity of the relative self attention mechanism in its original formulation (Shaw et al., 2018) made it infeasible to train on sequences of suf\ufb01cient length to capture long-range structure in longer musical compositions. Addressing this we present a crucial algorithmic improvement to the relative self attention mechanism, dramatically reducing its memory requirements from O(L2D) to O(LD). For example, we reduce the memory consumption per layer from 8.5 GB to 4.2 MB (per head from 1.1 GB to 0.52 MB) for a sequence of length L = 2048 and hidden-state size D = 512 (per head Dh = D H = 64, where number of heads is H = 8) (see Table 1), allowing us to use GPUs to train the relative self-attention Transformer on long sequences.  2 RELATED WORK  Sequence models have been the canonical choice for modeling music, from Hidden Markov Models to RNNs and Long Short Term Memory networks (e.g., Eck & Schmidhuber, 2002; Liang, 2016; Oore et al., 2018), to bidirectional LSTMs (e.g., Hadjeres et al., 2017). Successful application of sequential models to polyphonic music often requires serializing the musical score or performance  into a single sequence, for example by interleaving different instruments or voices.\n\nfilename: materials_formatted.md\nextension: .md\n\nOur memory-ef\ufb01cient implementation enables the appli- cation of relative attention to much longer sequences such as long texts or even audio waveforms, which signi\ufb01cantly broadens the range of problems to which it could be applied.  6 ACKNOWLEDGEMENT  We thank many colleagues from the Transformer (Vaswani et al., 2017) and Tensor2Tensor (Vaswani et al., 2018) papers for helping us along the way: Lukasz Kaiser, Ryan Sepassi, Niki Parmar and Llion Jones. Many thanks to Magenta and friends for their support throughout and for many insightful discussions: Jesse Engel, Adam Roberts, Fred Bertsch, Erich Elsen, Sander Dieleman, Sageev Oore, Carey Radebaugh, Natasha Jaques, Daphne Ippolito, Sherol Chan, Vida Vakilotojar, Dustin Tran, Ben Poole and Tim Cooijmans.  REFERENCES  Moray Allan and Christopher KI Williams. Harmonising chorales by probabilistic inference. Advances  in neural information processing systems, 17:25\u201332, 2005.  Nicolas Boulanger-Lewandowski, Yoshua Bengio, and Pascal Vincent. Modeling temporal dependen- cies in high-dimensional sequences: Application to polyphonic music generation and transcription. International Conference on Machine Learning, 2012.  Hao-Wen Dong, Wen-Yi Hsiao, Li-Chia Yang, and Yi-Hsuan Yang. Musegan: Multi-track sequential generative adversarial networks for symbolic music generation and accompaniment. In Proceedings of the AAAI Conference on Arti\ufb01cial Intelligence, 2018.  Douglas Eck and Juergen Schmidhuber. Finding temporal structure in music: Blues improvisation with lstm recurrent networks. In Proceedings of the 12th IEEE Workshop on Neural Networks for Signal Processing, 2002.  Ga\u00ebtan Hadjeres, Jason Sakellariou, and Fran\u00e7ois Pachet. Style imitation and chord invention in  polyphonic music with exponential families. arXiv preprint arXiv:1609.05152, 2016.  Ga\u00ebtan Hadjeres, Fran\u00e7ois Pachet, and Frank Nielsen.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat unique property does the Music Transformer bring to generating music with long-term structure?[/INST] \nAnswer: The Music Transformer brings the unique property of a modified relative attention mechanism to generating music with long-term structure. This mechanism allows the model to generate minute-long compositions with compelling structure, elaborate on a given motif, and generate accompaniments conditioned on melodies. It also enables the Transformer to generate in a consistent fashion beyond the length it was trained on. Furthermore, it has been optimized to reduce memory requirements, enabling its application to much longer sequences."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nWe do submit to the test server on a small number of datasets (SuperGLUE, TriviaQA, PiQa) where we were able to make submission work, and we submit only the 200B few-shot results, and report development set results for everything else.  3 Results  In Figure 3.1 we display training curves for the 8 models described in Section 2. For this graph we also include 6 additional extra-small models with as few as 100,000 parameters. As observed in [KMH+20], language modeling performance follows a power-law when making ef\ufb01cient use of training compute. After extending this trend by two more orders of magnitude, we observe only a slight (if any) departure from the power-law. One might worry that these improvements in cross-entropy loss come only from modeling spurious details of our training corpus. However, we will see in the following sections that improvements in cross-entropy loss lead to consistent performance gains across a broad spectrum of natural language tasks.  Below, we evaluate the 8 models described in Section 2 (the 175 billion parameter parameter GPT-3 and 7 smaller models) on a wide range of datasets. We group the datasets into 9 categories representing roughly similar tasks.  In Section 3.1 we evaluate on traditional language modeling tasks and tasks that are similar to language modeling, such as Cloze tasks and sentence/paragraph completion tasks. In Section 3.2 we evaluate on \u201cclosed book\u201d question answering tasks: tasks which require using the information stored in the model\u2019s parameters to answer general knowledge questions. In Section 3.3 we evaluate the model\u2019s ability to translate between languages (especially one-shot and few-shot). In Section 3.4 we evaluate the model\u2019s performance on Winograd Schema-like tasks. In Section 3.5 we evaluate on datasets that involve commonsense reasoning or question answering. In Section 3.6 we evaluate on reading comprehension tasks, in Section 3.7 we evaluate on the SuperGLUE benchmark suite, and in 3.8 we brie\ufb02y explore NLI. Finally, in Section 3.9, we invent some additional tasks designed especially to probe in-context learning abilities \u2013 these tasks focus on on-the-\ufb02y reasoning, adaptation skills, or open-ended text synthesis.\n\nfilename: materials_formatted.md\nextension: .md\n\nStatistical Tests: To compare means on the different runs, we performed a two-sample t-test for independent groups for each model against the control. This was implemented in Python using the scipy.stats.ttest_ind function. When plotting a regression line in the graph of average participant accuracy vs model size, we \ufb01t a power law of the form ax\u2212b. The 95% con\ufb01dence intervals were estimated from the t-distribution of the sample mean.  Duration statistics: In the main text, we discussed the \ufb01nding that the ability of human participants to distinguish model and human generated news articles decreases as our models become larger. We have also found that the average time spent for a given set of questions increases as the model size increases, as shown in Figure E.1. Lower  47  Participants Recruited  Participants Excluded  Genders (m:f:other)  Mean Age  Average Word Count (human:model)  79 81  17 19  32:37:0 32:30:0  39 40  569:464 569:498  Model  Control GPT-3 175B  Table E.2: Participant details and article lengths for the experiments investigating human detection of \u223c 500 word model generated news articles. Participants were excluded due to internet check fails.  accuracy scores despite increased time investment from participants supports the \ufb01nding that larger models generate harder-to-distinguish news articles.  Preliminary investigation of \u223c 500 word articles: We recruited 160 unique US-based participants to take part in 2 experiments through Positly (details are given in Table E.2). We randomly selected 12 Reuters world news articles from late 2019 and created a context for GPT-3 175B that consisted of a single Reuters article not in this set of 12. We then used the article titles and Reuters locations to generate completions from GPT-3 175B and the 160M control model from the previous experiments. These were used to create two 12-question quizzes per model, each consisting of half human written and half model generated articles. Comprehension questions were added and articles were shown to participants in 3 stages at 30 second intervals to encourage closer reading. Participants were paid $12 for this task.\n\nfilename: materials_formatted.md\nextension: .md\n\n46  Model  Control GPT-3 Small GPT-3 Medium GPT-3 Large GPT-3 XL GPT-3 2.7B GPT-3 6.7B GPT-3 13.0B GPT-3 175B  Participants Recruited  Participants Excluded  Genders (m:f:other)  Mean Age  Average Word Count (human:model)  76 80 80 81 79 80 76 81 80  7 7 7 24 14 11 5 13 9  32:37:0 41:31:1 46:28:2 46:28:2 32:32:1 36:33:0 46:28:2 46:28:2 42:29:0  39 40 39 37 38 40 37 37 37  216:216 216:188 216:202 216:200 216:199 216:202 216:195 216:209 216:216  Table E.1: Participant details and article lengths for each experiment to evaluate human detection of \u223c 200 word model generated news articles. Participants were excluded due to internet check fails.  Figure E.1: Participants spend more time trying to identify whether each news article is machine generated as model size increases. Duration on the control model is indicated with the dashed line. Line of best \ufb01t is a linear model on a log scale with 95% con\ufb01dence intervals.  In each experiment, half of the participants were randomly assigned to quiz A and half were randomly assigned to quiz B. Each quiz consisted of 25 articles: half (12-13) were human written and half (12-13) were model generated: the articles with human written completions in quiz A had model generated completions in quiz B and vice versa. The order of quiz question was shuf\ufb02ed for each participant. Participants could leave comments and were asked to indicate if they had seen the articles before. Participants were instructed not to look up the articles or their content during the quiz and at the end of the quiz were asked if they had looked anything up during the quiz.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nHow can the performance of traditional count-based distributional models be improved according to the document?[/INST] \nAnswer: I don't know."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nThis challenge thus encour- ages research in the ability of a machine to read, a key motivation for the machine comprehen- sion sub\ufb01eld and the creation of datasets such as SQuAD (Rajpurkar et al., 2016), CNN/Daily Mail (Hermann et al., 2015) and CBT (Hill et al., 2016).  However,  those machine comprehension re- sources typically assume that a short piece of rel- evant text is already identi\ufb01ed and given to the model, which is not realistic for building an open- In sharp contrast, methods domain QA system. that use KBs or information retrieval over docu- ments have to employ search as an integral part of              the solution. Instead MRS is focused on simul- taneously maintaining the challenge of machine comprehension, which requires the deep under- standing of text, while keeping the realistic con- straint of searching over a large open resource.  In this paper, we show how multiple existing QA datasets can be used to evaluate MRS by re- quiring an open-domain system to perform well on all of them at once. We develop DrQA, a strong system for question answering from Wikipedia composed of: (1) Document Retriever, a mod- ule using bigram hashing and TF-IDF matching designed to, given a question, ef\ufb01ciently return a subset of relevant articles and (2) Document Reader, a multi-layer recurrent neural network machine comprehension model trained to detect answer spans in those few returned documents. Figure 1 gives an illustration of DrQA.  Our experiments show that Document Retriever outperforms the built-in Wikipedia search engine and that Document Reader reaches state-of-the- art results on the very competitive SQuAD bench- mark (Rajpurkar et al., 2016). Finally, our full sys- In tem is evaluated using multiple benchmarks. particular, we show that performance is improved across all datasets through the use of multitask learning and distant supervision compared to sin- gle task training.  2 Related Work  Open-domain QA was originally de\ufb01ned as \ufb01nd- ing answers in collections of unstructured docu- ments, following the setting of the annual TREC competitions1.\n\nfilename: materials_formatted.md\nextension: .md\n\nAAAI Press, 2011.      \n\n# BI-DIRECTIONAL ATTENTION FLOW FOR MACHINE COMPREHENSION:  Published as a conference paper at ICLR 2017  BI-DIRECTIONAL ATTENTION FLOW FOR MACHINE COMPREHENSION  Minjoon Seo1\u2217 University of Washington1, Allen Institute for Arti\ufb01cial Intelligence2 {minjoon,ali,hannaneh}@cs.washington.edu, {anik}@allenai.org  Aniruddha Kembhavi2  Ali Farhadi1,2  Hananneh Hajishirzi1   ABSTRACT  Machine comprehension (MC), answering a query about a given context para- graph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typ- ically these methods use attention to focus on a small portion of the con- text and summarize it with a \ufb01xed-size vector, couple attentions temporally, In this paper we introduce the and/or often form a uni-directional attention. Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical pro- cess that represents the context at different levels of granularity and uses bi- directional attention \ufb02ow mechanism to obtain a query-aware context represen- tation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test.  1  INTRODUCTION  The tasks of machine comprehension (MC) and question answering (QA) have gained signi\ufb01cant popularity over the past few years within the natural language processing and computer vision com- munities. Systems trained end-to-end now achieve promising results on a variety of tasks in the text and image domains. One of the key factors to the advancement has been the use of neural attention mechanism, which enables the system to focus on a targeted area within a context paragraph (for MC) or within an image (for Visual QA), that is most relevant to answer the question (Weston et al., 2015; Antol et al., 2015; Xiong et al., 2016a). Attention mechanisms in previous works typically have one or more of the following characteristics.\n\nfilename: materials_formatted.md\nextension: .md\n\nWe expect that the remaining gap will be harder to close, but that such efforts will result in signi\ufb01cant advances in reading comprehension.  Reproducibility  All code, data, and experiments for this paper are available on the CodaLab platform: https://worksheets.codalab.org/worksheets/ 0xd53d03a48ef64b329c16b9baf0f99b0c/ .  Acknowledgments  We would like to thank Durim Morina and Professor Michael Bernstein for their help in crowdsourcing the collection of our dataset, both in terms of fund- ing and technical support of the Daemo platform.  References  J. Berant, V. Srikumar, P. Chen, A. V. Linden, B. Harding, B. Huang, P. Clark, and C. D. Manning. 2014. Mod- eling biological processes for reading comprehension. In Empirical Methods in Natural Language Process- ing (EMNLP).  E. Brill, S. Dumais, and M. Banko. 2002. An analysis of the AskMSR question-answering system. In Associa- tion for Computational Linguistics (ACL), pages 257\u2013 264.  D. Chen, J. Bolton, and C. D. Manning.  2016. A thorough examination of the CNN / Daily Mail read- ing comprehension task. In Association for Computa- tional Linguistics (ACL).  P. Clark and O. Etzioni. 2016. My computer is an honor student but how intelligent is it? standardized tests as a measure of AI. AI Magazine, 37(1):5\u201312.  J. Deng, W. Dong, R. Socher, L. Li, K. Li, and L. Fei- Fei. 2009. ImageNet: A large-scale hierarchical im- age database. In Computer Vision and Pattern Recog- nition (CVPR), pages 248\u2013255.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat is the primary challenge in machine comprehension (MC) as discussed in the paper?[/INST] \nAnswer: The primary challenge in machine comprehension (MC) as discussed in the paper is answering a query about a given context paragraph. This requires modeling complex interactions between the context and the query."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nIt\u2019s very easy for information to just flow along it unchanged.   The LSTM does have the ability to remove or add information to the cell state, carefully regulated by structures called gates.  Gates are a way to optionally let information through. They are composed out of a sigmoid neural net layer and a pointwise multiplication operation.  The sigmoid layer outputs numbers between zero and one, describing how much of each component should be let through. A value of zero means \u201clet nothing through,\u201d while a value of one means \u201clet everything through!\u201d  An LSTM has three of these gates, to protect and control the cell state.  ## Step-by-Step LSTM Walk Through  The first step in our LSTM is to decide what information we\u2019re going to throw away from the cell state. This decision is made by a sigmoid layer called the \u201cforget gate layer.\u201d It looks at and , and outputs a number between and for each number in the cell state. A represents \u201ccompletely keep this\u201d while a represents \u201ccompletely get rid of this.\u201d  Let\u2019s go back to our example of a language model trying to predict the next word based on all the previous ones. In such a problem, the cell state might include the gender of the present subject, so that the correct pronouns can be used. When we see a new subject, we want to forget the gender of the old subject.  The next step is to decide what new information we\u2019re going to store in the cell state. This has two parts. First, a sigmoid layer called the \u201cinput gate layer\u201d decides which values we\u2019ll update. Next, a tanh layer creates a vector of new candidate values, , that could be added to the state. In the next step, we\u2019ll combine these two to create an update to the state.  In the example of our language model, we\u2019d want to add the gender of the new subject to the cell state, to replace the old one we\u2019re forgetting.   ht\u2212 1 xt 0 1 Ct\u2212 1 1 0   C\u0303^ t   It\u2019s now time to update the old cell state, , into the new cell state. The previous steps already decided what to do, we just need to actually do it.  We multiply the old state by , forgetting the things we decided to forget earlier. Then we add.\n\nfilename: materials_formatted.md\nextension: .md\n\nThen we add. This is the new candidate values, scaled by how much we decided to update each state value.  In the case of the language model, this is where we\u2019d actually drop the information about the old subject\u2019s gender and add the new information, as we decided in the previous steps.  Finally, we need to decide what we\u2019re going to output. This output will be based on our cell state, but will be a filtered version. First, we run a sigmoid layer which decides what parts of the cell state we\u2019re going to output. Then, we put the cell state through (to push the values to be between and ) and multiply it by the output of the sigmoid gate, so that we only output the parts we decided to.  For the language model example, since it just saw a subject, it might want to output information relevant to a verb, in case that\u2019s what is coming next. For example, it might output whether the subject is singular or plural, so that we know what form a verb should be conjugated into if that\u2019s what follows next.  ## Variants on Long Short Term Memory  What I\u2019ve described so far is a pretty normal LSTM. But not all LSTMs are the same as the above. In fact, it seems like almost every paper involving LSTMs uses a slightly different version. The differences are minor, but it\u2019s worth mentioning some of them.  One popular LSTM variant, introduced by Gers & Schmidhuber (2000) (ftp://ftp.idsia.ch/pub/juergen/TimeCount- IJCNN2000.pdf), is adding \u201cpeephole connections.\u201d This means that we let the gate layers look at the cell state.   Ct\u2212 1 Ct   ft it\u2217C\u0303^ t   tanh \u2212 1 1   The above diagram adds peepholes to all the gates, but many papers will give some peepholes and not others.  Another variation is to use coupled forget and input gates. Instead of separately deciding what to forget and what we should add new information to, we make those decisions together. We only forget when we\u2019re going to input something in its place. We only input new values to the state when we forget something older.\n\nfilename: materials_formatted.md\nextension: .md\n\n(1994) (http://www- dsi.ing.unifi.it/~paolo/ps/tnn-94-gradient.pdf), who found some pretty fundamental reasons why it might be difficult.  Thankfully, LSTMs don\u2019t have this problem!  ## LSTM Networks  Long Short Term Memory networks \u2013 usually just called \u201cLSTMs\u201d \u2013 are a special kind of RNN, capable of learning long-term dependencies. They were introduced by Hochreiter & Schmidhuber (1997) (http://www.bioinf.jku.at/publications/older/2604.pdf), and were refined and popularized by many people in following work. They work tremendously well on a large variety of problems, and are now widely used.  LSTMs are explicitly designed to avoid the long-term dependency problem. Remembering information for long periods of time is practically their default behavior, not something they struggle to learn!   1   All recurrent neural networks have the form of a chain of repeating modules of neural network. In standard RNNs, this repeating module will have a very simple structure, such as a single tanh layer.   The repeating module in a standard RNN contains a single layer.  LSTMs also have this chain like structure, but the repeating module has a different structure. Instead of having a single neural network layer, there are four, interacting in a very special way.   The repeating module in an LSTM contains four interacting layers.  Don\u2019t worry about the details of what\u2019s going on. We\u2019ll walk through the LSTM diagram step by step later. For now, let\u2019s just try to get comfortable with the notation we\u2019ll be using.  In the above diagram, each line carries an entire vector, from the output of one node to the inputs of others. The pink circles represent pointwise operations, like vector addition, while the yellow boxes are learned neural network layers. Lines merging denote concatenation, while a line forking denote its content being copied and the copies going to different locations.  ## The Core Idea Behind LSTMs  The key to LSTMs is the cell state, the horizontal line running through the top of the diagram.  The cell state is kind of like a conveyor belt. It runs straight down the entire chain, with only some minor linear interactions. It\u2019s very easy for information to just flow along it unchanged.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nHow do LSTMs decide what information to store or forget?[/INST] \nAnswer: LSTMs make these decisions through a structure known as gates. The first step is the \"forget gate layer\", a sigmoid layer that decides what information to discard from the cell state. It outputs a value between 0 and 1 for each number in the cell state, where 0 means \"completely get rid of this\" and 1 means \"completely keep this\". \n\nThe next step is the \"input gate layer\", another sigmoid layer that decides which values will be updated. Then, a tanh layer creates a vector of new candidate values that could be added to the state. The old cell state is then updated by forgetting the things decided earlier and adding the new candidate values, scaled by how much was decided to update each state value. \n\nFinally, the output is decided based on the cell state but is a filtered version. A sigmoid layer decides what parts of the cell state will be output, and then the cell state is put through a tanh function and multiplied by the output of the sigmoid gate, so only the parts decided are output."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nNaN) which is easily detectable at runtime; this issue is called theGradient Explosion Problem. When the gradient value goes to zero, however, it can go undetected while dras- tically reducing the learning quality of the model for far-away words in the corpus; this issue is called theVanishing Gradient Problem. Due to vanishing gradients, we don\u2019t know whether there is no dependency between stepstandt+nin the data, or we just cannot capture the true dependency due to this issue. To gain practical intuition about the vanishing gradient problem, you may visit the followingexample website.  #### 2. 4 Solution to the Exploding & Vanishing Gradients  Now that we gained intuition about the nature of the vanishing gradi- ents problem and how it manifests itself in deep neural networks, let us focus on a simple and practical heuristic to solve these problems. To solve the problem of exploding gradients, Thomas Mikolov first introduced a simple heuristic solution thatclipsgradients to a small number whenever they explode. That is, whenever they reach a cer- tain threshold, they are set back to a small number as shown in Algo- rithm 1. Algorithm 1 : Pseudo-code for norm clip-   ping in the gradients whenever they ex- g\u02c6\u2190 \u2202 E plode \u2202 W if \u2016g\u02c6\u2016\u2265threshold then g\u02c6\u2190threshold \u2016g\u02c6\u2016 g\u02c6 end if  Figure 7 visualizes the effect of gradient clipping. It shows the de- cision surface of a small recurrent neural network with respect to its Wmatrix and its bias terms,b. The model consists of a single unit of recurrent neural network running through a small number of time- steps; the solid arrows illustrate the training progress on each gradient descent step. When the gradient descent model hits the high error wall in the objective function, the gradient is pushed off to a far-away loca- tion on the decision surface. The clipping model produces the dashed line where it instead pulls back the error gradient to somewhere close to the original gradient landscape.   On the diculty of training Recurrent Neural Networks   Figure 6.We plot the error surface of a single hidden unit recurrent network, highlighting the existence of high cur- vature walls. The solid lines depicts standard trajectories that gradient descent might follow.\n\nfilename: materials_formatted.md\nextension: .md\n\nAdding MLP in \u03b2 does not seem to help, yielding slightly worse result than \u03b2 without MLP.  13     \n\n# On the difficulty of training Recurrent Neural Networks:  # On the difficulty of training Recurrent Neural Networks   Razvan Pascanu pascanur@iro.umontreal.ca Universite de Montreal   Tomas Mikolov t.mikolov@gmail.com Brno University   Yoshua Bengio yoshua.bengio@umontreal.ca Universite de Montreal  ## Abstract   There are two widely known issues with prop- erly training Recurrent Neural Networks, the vanishing and theexplodinggradient prob- lems detailed in Bengio et al. (1994). In this paper we attempt to improve the under- standing of the underlying issues by explor- ing these problems from an analytical, a geo- metric and a dynamical systems perspective. Our analysis is used to justify a simple yet ef- fective solution. We propose a gradient norm clipping strategy to deal with exploding gra- dients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.  ## 1. Introduction   A recurrent neural network (RNN), e.g. Fig. 1, is a neural network model proposed in the 80\u2019s (Rumelhart et al., 1986; Elman, 1990; Werbos, 1988) for modeling time series. The structure of the network is similar to that of a standard multilayer perceptron, with the dis- tinction that we allow connections among hidden units associated with a time delay. Through these connec- tions the model can retain information about the past inputs, enabling it to discover temporal correlations between events that are possibly far away from each other in the data (a crucial property for proper learn- ing of time series). While in principle the recurrent network is a simple and powerful model, in practice, it is unfortunately hard to train properly. Among the main reasons why this model is so unwieldy are thevanishing gradient  ### ut xt Et   Figure 1.Schematic of a recurrent neural network. The recurrent connections in the hidden layer allow information to persist from one input to another.   andexploding gradientproblems described in Bengio et al.(1994).   1.1.\n\nfilename: materials_formatted.md\nextension: .md\n\nsenti- ment classification), and encoder modules (e.g. questiton answering,   ### language models,rnn,gru and lstm 6  machine translation, and many other tasks). In the latter two applica- tions, we want a representation for the sentence, which we can obtain by taking the element-wise max or mean of all hidden states of the timesteps in that sentence. Note: Figure 6 is an alternative representation of RNNs used in some publications. It represents the RNN hidden layer as a loop.  ### ht   Figure 6 : The illustration of a RNN as a loop over time-steps  #### 2. 3 Vanishing Gradient & Gradient Explosion Problems  Recurrent neural networks propagate weight matrices from one time- step to the next. Recall the goal of a RNN implementation is to en- able propagating context information through faraway time-steps. For example, consider the following two sentences:   Sentence 1 \"Jane walked into the room. John walked in too. Jane said hi to ___\"   Sentence 2 \"Jane walked into the room. John walked in too. It was late in the day, and everyone was walking home after a long day at work. Jane said hi to ___\"  In both sentences, given their context, one can tell the answer to both blank spots is most likely \"John\". It is important that the RNN predicts the next word as \"John\", the second person who has appeared several time-steps back in both contexts. Ideally, this should be possible given what we know about RNNs so far. In practice, however, it turns out RNNs are more likely to correctly predict the blank spot in Sentence 1 than in Sentence 2. This is because during the back-propagation phase, the contribution of gradient values gradually vanishes as they propagate to earlier timesteps, as we will show below. Thus, for long sentences, the probability that \"John\" would be recognized as the next word reduces with the size of the context. Below, we discuss the math- ematical reasoning behind the vanishing gradient problem. Consider Equations 5 and 6 at a time-stept; to compute the RNN error,dE/dW, we sum the error at each time-step.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat is the vanishing gradient problem in RNNs and how does it affect training?[/INST] \nAnswer: The vanishing gradient problem in Recurrent Neural Networks (RNNs) occurs during the back-propagation phase, where the contribution of gradient values gradually diminishes as they propagate to earlier timesteps. This issue can drastically reduce the learning quality of the model for far-away words in the corpus. For instance, in long sentences, the probability that a certain word would be recognized as the next word reduces with the size of the context. This problem makes it difficult to know whether there is no dependency between steps in the data, or if the true dependency is just not being captured due to the vanishing gradients."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nSee D for more discussion. We leave this ablation to future work. Nevertheless, we believe reward modeling is more likely to scale to tasks where it is extremely skill-intensive or time-consuming to provide good demonstrations.  Future directions. The methods in this paper could be applied to any task where humans can compare samples, including dialogue, machine translation, question answering, speech synthesis, and music generation. We expect this method to be particularly important for generating long samples, where the distributional shift and degeneracy of maximum likelihood samples can be problematic. It may be possible to improve sample ef\ufb01ciency by training to predict feedback across many tasks [42].  We are particularly interested in scaling human feedback to tasks where humans can\u2019t easily evaluate the quality of model outputs. In this setting, it is particularly challenging to identify whether an ML system is aligned with the human designer\u2019s intentions. One approach is to train ML systems to help humans perform the evaluation task quickly and accurately [9].  There is also a rich landscape of human feedback methods beyond binary comparisons that could be explored for training models [28, 17, 44, 64]. For example, we could solicit high-quality demonstra- tions from labelers, have labelers edit model outputs to make them better, or have labelers provide explanations for why they preferred one model output over another. All of this feedback could be leveraged as a signal to train more capable reward models and policies.  Broader impacts. The techniques we explore in this paper are generic techniques that could be used in a wide variety of machine learning applications, for any task where it is feasible for humans to evaluate the quality of model outputs. Thus, the potential implications are quite broad.  Our research is primarily motivated by the potential positive effects of aligning machine learning algorithms with the designer\u2019s preferences. Many machine learning applications optimize simple metrics which are only rough proxies for what the designer intends. This can lead to problems, such as Youtube recommendations promoting click-bait [11]. In the short term, improving techniques for learning from and optimizing human preferences directly may enable these applications to be more aligned with human well-being.  In the long term, as machine learning systems become more capable it will likely become increasingly dif\ufb01cult to ensure that they are behaving safely: the mistakes they make might be more dif\ufb01cult to spot, and the consequences will be more severe.\n\nfilename: materials_formatted.md\nextension: .md\n\nWe con\ufb01rm that our reward model outperforms other metrics such as ROUGE at predicting human preferences, and that optimizing our reward model directly results in better summaries than optimizing ROUGE according to humans (Section 4.4).  (4) We publicly release our human feedback dataset for further research. The dataset contains 64,832 summary comparisons on the TL;DR dataset, as well as our evaluation data on both TL;DR (comparisons and Likert scores) and CNN/DM (Likert scores).  The methods we present in this paper are motivated in part by longer-term concerns about the misalignment of AI systems with what humans want them to do. When misaligned summarization models make up facts, their mistakes are fairly low-risk and easy to spot. However, as AI systems become more powerful and are given increasingly important tasks, the mistakes they make will likely become more subtle and safety-critical, making this an important area for further research.  2 Related work  Most directly related to our work is previous work using human feedback to train summarization models with RL [3, 73]. Bohm et al. [3] learn a reward function from a dataset of human ratings of 2.5k CNN/DM summaries, and train a policy whose summaries are preferred to a policy optimizing ROUGE. Our work is most similar to [73], who also train Transformer models [62] to optimize human feedback across a range of tasks, including summarization on the Reddit TL;DR and CNN/DM datasets. Unlike us, they train in an online manner and \ufb01nd the model highly extractive. They note that their labelers prefer extractive summaries and have low agreement rates with researchers. Compared to [73], we use signi\ufb01cantly larger models, move to the batch setting for collecting human feedback, ensure high labeler-researcher agreement, and make some algorithmic modi\ufb01cations, such as separating the policy and value networks.  Human feedback has also been used as a reward to train models in other domains such as dialogue [25, 68, 21], translation [32, 1], semantic parsing [34], story generation [72], review generation [7], and evidence extraction [46].\n\nfilename: materials_formatted.md\nextension: .md\n\nAuthor order was randomized amongst {LO, JW,  DZ, NS}; CV and RL were full-time contributors for most of the duration. PC is the team lead.  2Samples from all of our models can be viewed on our website. 3We provide inference code for our 1.3B models and baselines, as well as a model card and our human  feedback dataset with over 64k summary comparisons, here.  34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.  Figure 1: Fraction of the time humans prefer our models\u2019 summaries over the human-generated reference summaries on the TL;DR dataset.4Since quality judgments involve an arbitrary decision about how to trade off summary length vs. coverage within the 24-48 token limit, we also provide length-controlled graphs in Appendix F; length differences explain about a third of the gap between feedback and supervised learning at 6.7B.  are incentivized to place probability mass on all human demonstrations, including those that are low-quality; and distributional shift during sampling can degrade performance [56, 52]. Quality can often be improved signi\ufb01cantly by non-uniform sampling strategies such as beam search [51], but these can lead to repetition and other undesirable artifacts [69, 23]. Optimizing for quality may be a principled approach to overcoming these problems.  Our goal in this paper is to advance methods for training language models on objectives that more closely capture the behavior we care about. To make short-term progress towards this goal, we focus on abstractive English text summarization, as it has a long history in the NLP community [16, 8, 54, 59, 50], and is a subjective task where we believe it is dif\ufb01cult to quantify summary quality without human judgments. Indeed, existing automatic metrics for evaluating summary quality, such as ROUGE [39], have received criticism for poor correlation with human judgments [55, 45, 6, 33].  We follow the works of [3, 73], who \ufb01ne-tune language models from human feedback using reward learning [35]. We \ufb01rst collect a dataset of human preferences between pairs of summaries, then train a reward model (RM) via supervised learning to predict the human-preferred summary.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhy is modeling human preferences challenging in the context of language models?[/INST] \nAnswer: Modeling human preferences is challenging in the context of language models because it's difficult to quantify the quality of model outputs, such as text summaries, without human judgments. Existing automatic metrics for evaluating summary quality, like ROUGE, have been criticized for their poor correlation with human judgments. Furthermore, it's particularly challenging to identify whether a machine learning system is aligned with the human designer's intentions in tasks where humans can't easily evaluate the quality of model outputs. As machine learning systems become more capable, ensuring their safe behavior becomes increasingly difficult as their mistakes might be more subtle and have more severe consequences."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\n# Materials  Course Outline:  # Course Outline  ## 1. COURSE INFORMATION  Session Offered Winter 2024 Course Name Introduction to Computational Natural Language Processing Course Code SEP 775 Date(s) and Time(s) of lectures Lectures (In-person): Wednesday 3 : 30 pm \u2013 6 : 30 pm Program Name System and Technology Calendar Description This graduate course introduces fundamental concepts in computational natural language processing (NLP) and their applications to teaching students how to deal with textual data in Artificial Intelligence. This course demonstrates how machines can learn different tasks in natural language, such as language modeling, text generation, machine translation, and language understanding. In this regard, we go over the most promising methods in this literature and the most recent state-of-the-art techniques. Moreover, this course explores different real-world applications of NLP and helps students get hands-on experience in this field. Instructor Hamidreza Mahyar E-Mail: mahyarh@mcmaster.ca Office Hours: upon request  ## 2. COURSE SPECIFICS  Course Description This course covers some fundamental concepts in natural language processing (NLP) and how they apply in real-world use cases.  Instruction Type  *Code Type Hours per term C Classroom instruction 39 L Laboratory, workshop or fieldwork T Tutorial DE Distance education Total Hours 39 Resources ISBN Textbook Title & Edition Author & Publisher Natural Language Processing Jacob Eisenstein   Speech and Language Processing Dan Jurafsky and James H. Martin  Deep Learning Ian Goodfellow, Yoshua Bengio, and Aaron Courville Prerequisite(s) \u2022 Proficiency in Python programming language  - Some Machine Learning and/or Deep Learning knowledge is a plus  Corequisite(s) Antirequisite(s) Course Specific Policies All work must be shown to get full credit.   Specific policy of Projects/Assignments submissions:    Projects/Assignments will be conducted online by using related software. Completed Projects/Assignments should be uploaded to the drop box before midnight of the due date. Projects/Assignments missed due to legitimate reasons must be completed later mutually agreed with the instructor.  Departmental Policies The use of cell phones, iPods, laptops and other personal electronic devices are prohibited from the classroom during the class time, unless the instructor makes an explicit exception. Announcements made in class or placed on Avenue are considered to have been communicated to all students including those individuals that are not in class.\n\nfilename: materials_formatted.md\nextension: .md\n\nInstructor has the right to submit work to software to identify plagiarism.  ## 3. SUB TOPIC(S)   Week 1   Overview: Human language, Word meanings, Word2Vec, and Optimization basics   Week 2 Intro to neural networks: Gradients and backpropagation   Week 3 Recurrent Neural Networks and Language Models   Week 4 Conditioned Generation   Week 5   Part 1: Self attention and transformers   Part 2: Seq2Seq models   Week 6 Midterm Test   Week 7 Midterm Recess (No class)   Week 8 Prompting, Instruction Finetuning, and RLHF   Week 9 Question Answering systems   Week 1 0 Multimodal models   Week 1 1 Project presentation and Q&A   Week 1 2 Project presentation and Q&A   Week 1 3 Project presentation and Q&A   Classes end: April 10 th  ## 4. ASSESSMENT OF LEARNING Weight  4 Assignments 20%  4 Quizzes 10%  Midterm Test 35%  Group Project 35%   TOTAL 100%   Note that the percentage distribution of each module may be adjusted. Percentage grades will be converted to letter grades and grade points per the University calendar.  ## 5. LEARNING OUTCOMES  By the end of this course, the students will know about:  - Introduction to Deep Learning - Word representations - Language models - Pre-trained language models - Text generation - Seq2Seq models - Prompting - Feedback systems - Multimodal models - NLP applications (e.g. Q/A systems)  ## 6. COURSE OUTLINE \u2013 APPROVED ADVISORY STATEMENTS  ## ANTI-DISCRIMINATION  The Faculty of Engineering is concerned with ensuring an environment that is free of all discrimination. If there is a problem, individuals are reminded that they should contact the Associate Director, Graduate Studies, the Sexual Harassment Officer or the Human Rights Consultant, as soon as possible.\n\nfilename: materials_formatted.md\nextension: .md\n\nStudents should be aware that their voice and/or image may be recorded by others during the class. Please speak with the instructor if this is a concern for you.  ## EXTREME CIRCUMSTANCES  The University reserves the right to change the dates and deadlines for any or all courses in extreme circumstances (e.g., severe weather, labour disruptions, etc.). Changes will be communicated through regular McMaster communication channels, such as McMaster Daily News, A2L and/or McMaster email.   \n\n# SEP 775 Assignment 1:   McMaster University SEP 775  # Assignment 1  # In-Depth Word Vectors Analysis (Total: 50 Points)  ## Objectives:  This assignment focuses on a comprehensive understanding of word vector technologies, specifically Word2Vec and GloVe. You will explore their applications, visualize the results, and analyze the semantic and syntactic relationships they capture.  1. Building and Analyzing Word Vectors with Word2Vec (20 Points)     - Task:Create word vectors using the Word2Vec model on a selected corpus.     - Visualization:Utilize PCA or t-SNE for visualizing these vectors in 2D.     - Analysis:Discuss the word relationships and clusters formed. 2. GloVe Vectors Advanced Analysis (20 Points)     - Task:Analyze word relationships using pre-trained GloVe vectors.     - Activity:Conduct an analogy task (e.g., king - man + woman = ?) with examples.     - Explanation:Explain the results of the analogy tasks. 3. Semantic and Syntactic Word Relationships (10 Points)     - Comparison:Evaluate Word2Vec and GloVe for capturing semantic and syntactic        relationships.     - Illustration:Use specific word pairs or groups to demonstrate differences in repre-        sentation by each model.  McMaster University 1    McMaster University SEP 775  ## Resources for Word2Vec and GloVe:  - Word2Vec: Pre-trained Word2Vec embeddings can be found on repositories such as     HuggingFace or directly using Gensim. - GloVe: Pre-trained GloVe embeddings are available at the GloVe website or through     HuggingFace.  # Submission Guidelines:  - Submit all code in a ZIP file, including Jupyter Notebooks, and a detailed PDF report     with written explanations and visualizations.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat is the deadline for SEP775 Assignment 1?[/INST] \nAnswer: I don't know."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nUnlike recent language repre- sentation models (Peters et al., 2018a; Rad- ford et al., 2018), BERT is designed to pre- train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a re- sult, the pre-trained BERT model can be \ufb01ne- tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task- speci\ufb01c architecture modi\ufb01cations.  BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art re- sults on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answer- ing Test F1 to 93.2 (1.5 point absolute im- provement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).  1  Introduction  Language model pre-training has been shown to be effective for improving many natural language processing tasks (Dai and Le, 2015; Peters et al., 2018a; Radford et al., 2018; Howard and Ruder, 2018). These include sentence-level tasks such as natural language inference (Bowman et al., 2015; Williams et al., 2018) and paraphrasing (Dolan and Brockett, 2005), which aim to predict the re- lationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce \ufb01ne-grained output at the token level (Tjong Kim Sang and De Meulder, 2003; Rajpurkar et al., 2016).  There are two existing strategies for apply- ing pre-trained language representations to down- stream tasks: feature-based and \ufb01ne-tuning. The feature-based approach, such as ELMo (Peters et al., 2018a), uses task-speci\ufb01c architectures that include the pre-trained representations as addi- tional features.\n\nfilename: materials_formatted.md\nextension: .md\n\nAn illustration of how ORQA scores answer derivations is presented in Figure 1. Following recent advances in transfer learn- ing, all scoring components are derived from BERT (Devlin et al., 2018), a bidirectional trans- former that has been pre-trained on unsupervised language-modeling data. We refer the reader to the original paper for details of the architecture. In this work, the relevant abstraction can be de- scribed by the following function:  BERT(x1, [x2]) = {CLS : hCLS, 1 : h1, 2 : h2, ...}  The BERT function takes one or two string in- puts (x1 and optionally x2) as arguments. It re- turns vectors corresponding to representations of the CLS pooling token or the input tokens.  Retriever component In order for the retriever to be learnable, we de\ufb01ne the retrieval score as the inner product of dense vector representations of the question q and the evidence block b.  hq = WqBERTQ(q)[CLS] hb = WbBERTB(b)[CLS]  Sretr (b, q) = h(cid:62)  q hb  where Wq and Wb are matrices that project the BERT output into 128-dimensional vectors.  Reader component The reader is a span-based variant of the reading comprehension model pro- posed in Devlin et al. (2018):  hstart = BERTR(q, b)[START(s)] hend = BERTR(q, b)[END(s)]  Sread (b, s, q) = MLP([hstart ; hend ])  Following Lee et al. (2016), a span is represented by the concatenation of its end points, which is scored by a multi-layer perceptron to enable start/end interaction.  Inference & Learning Challenges The model described above is conceptually simple. However, inference and learning are challenging since (1) an  Example  Q: Who is credited with developing the XY coordinate plane? A: Ren\u00b4e Descartes  Q: How many districts are in the state of Alabama?\n\nfilename: materials_formatted.md\nextension: .md\n\nF1 EM Humanperformance 91.2* 82.3* BiDAF 77.3 67.7 BERT-base 88.5 80.8 BERT-large 90.9 84.1 XLNet 94.5 89.0 RoBERTa 94.6 88.9 ALBERT 94.8 89.3 (dev set, except for human performance)   Comparisons between BiDAF and BERT models   36  - BERT model has many many more parameters (110M or 330M)     BiDAF has ~2.5M parameters. - BiDAF is built on top of several bidirectional LSTMs while BERT is built on top of     Transformers (no recurrence architecture and easier to parallelize). - BERT is pre-trained while BiDAF is only built on top of GloVe (and all the remaining     parameters need to be learned from the supervision datasets).   Pre-training is clearly a game changer but it is expensive..   Can we design better pre-training objectives?   (Joshi&Chenetal., 2020):SpanBERT:ImprovingPre-training byRepresenting andPredictingSpans^37  The answer is yes!   Two ideas:  - masking contiguous spans of words instead of 15% random words - using the two end points of span to predict all the masked words in between = compressing the     information of a span into its two endpoints   #### SpanBERT performance   38   91.3   83.3   68.8   77.5   81.7 78.3 79.9   92.6   85.9   71.0   79.0   81.8 80.5 80.5   94.6 88.7   73.6   83.6 84.8 83.0 82.8   65   73   80   88   95   SQuADv1.1 SQuADv2.0 NewsQA TriviaQA SearchQA HotpotQA NaturalQuestions  Google BERT Our BERT   SpanBERT F1 scores   #### Is reading comprehension solved?  - We have already surpassed human performance on SQuAD. Does it mean that reading     comprehension is already solved?\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat does BERT stand for?[/INST] \nAnswer: I don't know."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nThe RNNs in BIDAF are not feed-forwarded or back-propagated across sentences, which speed up the training process by par- allelization. The entire training process takes roughly 60 hours on eight Titan X GPUs. The other hyper-parameters are identical to the model described in Section 4.  Results. The results of our single-run models and competing approaches on the CNN/DailyMail datasets are summarized in Table 3. \u2217 indicates ensemble methods. BIDAF outperforms previous single-run models on both datasets for both val and test data. On the DailyMail test, our single-run model even outperforms the best ensemble method.\n\nfilename: materials_formatted.md\nextension: .md\n\nIt also allows the attention at each time step to be unaffected from incorrect attendances at previous time steps. Our experi- ments show that memory-less attention gives a clear advantage over dynamic attention. Third, we use attention mechanisms in both directions, query-to-context and context-to-query, which provide complimentary information to each other.  Our BIDAF model1 outperforms all previous approaches on the highly-competitive Stanford Ques- tion Answering Dataset (SQuAD) test set leaderboard at the time of submission. With a modi\ufb01cation to only the output layer, BIDAF achieves the state-of-the-art results on the CNN/DailyMail cloze test. We also provide an in-depth ablation study of our model on the SQuAD development set, vi- sualize the intermediate feature spaces in our model, and analyse its performance as compared to a more traditional language model for machine comprehension (Rajpurkar et al., 2016).  2 MODEL  Our machine comprehension model is a hierarchical multi-stage process and consists of six layers (Figure 1):  1. Character Embedding Layer maps each word to a vector space using character-level  CNNs.  2. Word Embedding Layer maps each word to a vector space using a pre-trained word em-  bedding model.  3. Contextual Embedding Layer utilizes contextual cues from surrounding words to re\ufb01ne the embedding of the words. These \ufb01rst three layers are applied to both the query and context.  4. Attention Flow Layer couples the query and context vectors and produces a set of query-  aware feature vectors for each word in the context.  5. Modeling Layer employs a Recurrent Neural Network to scan the context.  6. Output Layer provides an answer to the query.\n\nfilename: materials_formatted.md\nextension: .md\n\n(2016) does not report the test scores. All results shown here re\ufb02ect the SQuAD leaderboard (stanford-qa.com) as of 6 Dec 2016, 12pm PST. (1b) The performance of our model and its ablations on the SQuAD dev set. Ablation results are presented only for single runs.  train/dev question-context tuples with a large hidden test set. It is one of the largest available MC datasets with human-written questions and serves as a great test bed for our model.  Model Details. The model architecture used for this task is depicted in Figure 1. Each paragraph and question are tokenized by a regular-expression-based word tokenizer (PTB Tokenizer) and fed into the model. We use 100 1D \ufb01lters for CNN char embedding, each with a width of 5. The hidden state size (d) of the model is 100. The model has about 2.6 million parameters. We use the AdaDelta (Zeiler, 2012) optimizer, with a minibatch size of 60 and an initial learning rate of 0.5, for 12 epochs. A dropout (Srivastava et al., 2014) rate of 0.2 is used for the CNN, all LSTM layers, and the linear transformation before the softmax for the answers. During training, the moving averages of all weights of the model are maintained with the exponential decay rate of 0.999. At test time, the moving averages instead of the raw weights are used. The training process takes roughly 20 hours on a single Titan X GPU. We also train an ensemble model consisting of 12 training runs with the identical architecture and hyper-parameters. At test time, we choose the answer with the highest sum of con\ufb01dence scores amongst the 12 runs for each question.  Results. The results of our model and competing approaches on the hidden test are summarized in Table 1a. BIDAF (ensemble) achieves an EM score of 73.3 and an F1 score of 81.1, outperforming all previous approaches.  Ablations. Table 1b shows the performance of our model and its ablations on the SQuAD dev set.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nOn which datasets did the BIDAF model achieve state-of-the-art results?[/INST] \nAnswer: The BIDAF model achieved state-of-the-art results on the CNN/DailyMail and Stanford Question Answering Dataset (SQuAD) test sets."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nGiven a number of context words around a target wordw, these models formulate the embedding task as that of finding a representa- tion that is good at predictingwfrom the context representations. The second group of models, Hellinger PCA (Lebret and Collobert, 2014), GloVe (Pennington et al., 2014), TSCCA (Dhillon et al., 2012) and Sparse Random Projections (Li et al., 2006) fol- low a reconstruction approach: word embeddings should be able to capture as much relevant infor- mation from the original co-occurrence matrix as possible. Training corpus.We tried to make the compar- ison as fair as possible. As the C&W embeddings were only available pretrained on a November 2007 snapshot of Wikipedia, we chose the closest available Wikipedia dump (2008-03-01) for train- ing the other models. We tokenized the data us- ing the Stanford tokenizer (Manning et al., 2014). Like Collobert et al. (2011), we lower-cased all words and replaced digits with zeros. Details.All models embedded words into a 50- dimensional space (D= 50). As implemented, each method uses a different vocabulary, so we computed the intersection of the six vocabularies and used the resulting set of 103,647 words for all nearest-neighbor experiments.  ## 3 Relatedness   We begin with intrinsic evaluation of relatedness using both pre-collected human evaluations and a novel online user study. Section 3.1 introduces the list of datasets that is commonly used as a bench- mark for embedding methods. There, embeddings are evaluated individually and only their final scores are compared, hence we refer to this sce- nario asabsolute intrinsicevaluation. We present a new scenario,comparative intrinsicevaluation, in which we ask people directly for their prefer- ences among different embeddings. We demon- strate that we can achieve the same results as of- fline, absolute metrics using online, comparative metrics.   3.1 Absolute intrinsic evaluation  For the absolute intrinsic evaluation, we used the same datasets and tasks as Baroni et al. (2014). While we present results on all tasks for complete- ness, we will mainly focus on relatedness in this section.\n\nfilename: materials_formatted.md\nextension: .md\n\nWe compare with two widely-used 128-dimensional representations: (1) NNLM, context-independent embeddings from a feed-forward LMs (Bengio et al., 2003),4 and (2) ELMO (small), a context-dependent bidirectional LSTM (Peters et al., 2018).5  As with ICT, we use the alternate encoders to pre-compute the encoded evidence blocks hb and to initialize the question encoding hq, which is \ufb01ne-tuned. Based on existing IR literature and the intuition that LMs do not explicitly optimize for retrieval, we do not expect these to be strong base- lines, but they demonstrate the dif\ufb01culty of encod- ing blocks of text into 128 dimensions.  8.2 Results  The main results are show in Table 5. The \ufb01rst result to note is that BM25 is a powerful re- trieval system. Word matching is important, and  3https://lucene.apache.org/ 4https://tfhub.dev/google/nnlm-en-dim128/1 5https://allennlp.org/elmo  Model  DRQA DRQA (DS) DRQA (DS + MTL)  BERTSERINI BERTSERINI BERTSERINI  BM25 + BERT (gold deriv.)  Evidence Retrieved  5 documents 5 documents 5 documents  5 documents 29 paragraphs 100 paragraphs  5 blocks  SQuAD  27.1 28.4 29.8  19.1 36.6 38.6  34.7  Table 6: Analysis: Results comparable to previous work in the strongly supervised setting, where models have access to gold derivations from SQuAD. Differ- ent systems segment Wikipedia differently. There are 5.1M documents, 29.5M paragraphs, and 12.1M blocks in the December 12, 2016 Wikipedia snapshot.  dense vector representations derived from lan- guage models do not readily capture this.  We also show that on questions that were de- rived from real users who are seeking informa- tion (Natural Questions, WebQuestions, and Cu- ratedTrec), our ICT pre-trained retriever outper- forms BM25 by a large marge\u20146 to 19 points in exact match depending on the dataset.\n\nfilename: materials_formatted.md\nextension: .md\n\nWord frequency also interferes with the commonly-used cosine similarity measure. We present a novel evaluation framework based on di- rect comparisons between embeddings that pro- vides more fine-grained analysis and supports sim- ple, crowdsourced relevance judgments. We also present a novelCoherencetask that measures our intuition that neighborhoods in the embedding space should be semantically or syntactically re- lated. We find that extrinsic evaluations, although useful for highlighting specific aspects of embed- ding performance, should not be used as a proxy for generic quality.   Acknowledgments This research was funded in part through NSF Award IIS-1513692. We would like to thank Alexandra Schofield, Adith Swaminathan and all other members of the NLP seminar for their help- ful feedback.  ## References   Jacob Andreas and Dan Klein. 2014. How much do word embeddings encode about syntax? InACL: Short Papers, pages 822\u2013827. Javed Aslam and Mark Montague. 2001. Models for metasearch. InSIGIR, pages 276\u2013284. Marco Baroni, Georgiana Dinu, and German \u0301 Kruszewski. 2014. Don\u2019t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors. InACL, pages 238\u2013247. Elia Bruni, Nam-Khanh Tran, and Marco Baroni.  2014. Multimodal distributional semantics. JAIR, 49:1\u201347. James McKeen Cattell. 1886. The time taken up by cerebral operations.Mind,, (42):220\u2013242. Jonathan Chang, Jordan Boyd-Graber, Sean Gerrish, Chong Wang, and David Blei. 2009. Reading tea leaves: How humans interpret topic models. In NIPS, pages 288\u2013296. Yanqing Chen, Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2013. The expressive power of word embeddings.arXiv preprint: 1408.3456. Ronan Collobert, Jason Weston, L \u0301eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat challenge does representing word types independent of context present?[/INST] \nAnswer: Representing word types independent of context presents the challenge of not being able to capture as much relevant information from the original co-occurrence matrix as possible. This can interfere with commonly-used measures like cosine similarity. Moreover, dense vector representations derived from language models may not readily capture important aspects like word matching."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\n### 2 Recurrent Neural Networks (RNN)  Unlike the conventional translation models, where only a finite win- dow of previous words would be considered for conditioning the language model, Recurrent Neural Networks (RNN) are capable of conditioning the model onallprevious words in the corpus.   xt\u2212 1 xt xt +   ht\u2212 1 ht ht + W\" W\"   yt\u2212 1 yt yt+ 1   Figure 3 : A Recurrent Neural Network (RNN). Three time-steps are shown.  Figure 3 introduces the RNN architecture where each vertical rect- angular box is a hidden layer at a time-step,t. Each such layer holds a number of neurons, each of which performs a linear matrix oper- ation on its inputs followed by a non-linear operation (e.g. tanh()). At each time-step, there are two inputs to the hidden layer: the out- put of the previous layerht\u2212 1 , and the input at that timestepxt. The former input is multiplied by a weight matrixW(hh)and the latter by a weight matrixW(hx)to produce output featuresht, which are multiplied with a weight matrixW(S)and run through a softmax over the vocabulary to obtain a prediction outputy\u02c6of the next word (Equations 5 and 6 ). The inputs and outputs of each single neuron are illustrated in Figure 4.   ht= \u03c3 (W(hh)ht\u2212 1 +W(hx)x[t]) ( 5 )   y\u02c6t=so f tmax(W(S)ht) ( 6 )   ### language models,rnn,gru and lstm 4  What is interesting here is that the same weightsW(hh)andW(hx)are applied repeatedly at each timestep. Thus, the number of parameters the model has to learn is less, and most importantly, is independent of the length of the input sequence - thus defeating the curse of di- mensionality!   Figure 4 : The inputs and outputs to a neuron of a RNN  Below are the details associated with each parameter in the net- work:  - x 1 , ...,xt\u2212 1 ,xt,xt+ 1 , ...xT: the word vectors corresponding to a cor-     pus with T words.\n\nfilename: materials_formatted.md\nextension: .md\n\nTo construct a Deep RNN withLlayers, the above relationships are modified to the relationships in Equations 20 and 21 where the input to each intermediate neuron at leveliis the output of the RNN at layeri\u22121 at the same time-step,t. The output,y\u02c6, at each time-step is the result of propagating input parameters through all hidden layers (Equation 22 ).  # Going Deep  Each memory layer passes an intermediate sequential representation to the next.   #### 2. 6 Application: RNN Translation Model  Traditional translation models are quite complex; they consist of nu- merous machine learning algorithms applied to different stages of the language translation pipeline. In this section, we discuss the potential for adopting RNNs as a replacement to traditional translation mod- ules. Consider the RNN example model shown in Figure 10 ; here, the German phraseEcht dicke Kisteis translated toAwesome sauce. The first three hidden layer time-stepsencodethe German language words into some language word features (h 3 ). The last two time-stepsdecode h 3 into English word outputs. Equation 23 shows the relationship for the Encoder stage and Equations 24 and 25 show the equation for the Decoder stage.   x 1 x 2 x 3  _h_ (^1) W\t  _h 2 h_ 3 W\t Awesome\t  sauce\t  _y 1 y_ (^) _2_ Figure 10 : A RNN-based translation model. The first three RNN hidden layers belong to the source language model encoder, and the last two belong to the destination language model decoder. ht= _\u03c6_ (ht\u2212 1 ,xt) =f(W(hh)ht\u2212 1 +W(hx)xt) ( 23 ) ht= _\u03c6_ (ht\u2212 1 ) =f(W(hh)ht\u2212 1 ) ( 24 ) yt=so f tmax(W(S)ht) ( 25 ) One may naively assume this RNN model along with the cross- entropy function shown in Equation 26 can produce high-accuracy translation results. In practice, however, several extensions are to be added to the model to improve its translation accuracy performance.\n\nfilename: materials_formatted.md\nextension: .md\n\nmax _\u03b8_  ##### 1  ##### N   N   \u2211   n= 1   logp \u03b8 (y(n)|x(n)) ( 26 )  Extension I: train different RNN weights for encoding and decoding. This decouples the two units and allows for more accuracy prediction of each of the two RNN modules. This means the _\u03c6_ ()functions in Equations 23 and 24 would have differentW(hh)matrices.   Figure 11 : Language model with three inputs to each decoder neuron: (ht\u2212 1 ,c,yt\u2212 1 )  Extension II: compute every hidden state in the decoder using three different inputs:  - The previous hidden state (standard) - Last hidden layer of the encoder (c=hTin Figure 11 )   ### language models,rnn,gru and lstm 11  - Previous predicted output word,y\u02c6t\u2212 1  Combining the above three inputs transforms the _\u03c6_ function in the decoder function of Equation 24 to the one in Equation 27. Figure 11 illustrates this model.   ht= \u03c6 (ht\u2212 1 ,c,yt\u2212 1 ) ( 27 )  Extension III: train deep recurrent neural networks using multiple RNN layers as discussed earlier in this chapter. Deeper layers often improve prediction accuracy due to their higher learning capacity. Of course, this implies a large training corpus must be used to train the model.  Extension IV: train bi-directional encoders to improve accuracy simi- lar to what was discussed earlier in this chapter.  Extension V: given a word sequenceA B Cin German whose transla- tion isX Yin English, instead of training the RNN usingA B C\u2192X Y, train it usingC B A\u2192X Y. The intutition behind this technique is thatAis more likely to be translated toX. Thus, given the vanishing gradient problem discussed earlier, reversing the order of the input words can help reduce the error rate in generating the output phrase.  ### 3 Gated Recurrent Units  Beyond the extensions discussed so far, RNNs have been found to per- form better with the use of more complex units for activation. So far, we have discussed methods that transition from hidden stateht\u2212 1 to htusing an affine transformation and a point-wise nonlinearity.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nHow do Recurrent Neural Networks (RNNs) differ from conventional translation models?[/INST] \nAnswer: Recurrent Neural Networks (RNNs) differ from conventional translation models in that they are capable of conditioning the model on all previous words in the corpus, whereas conventional models only consider a finite window of previous words. This allows RNNs to effectively manage the sequence of words, making them particularly useful for language modeling and translation tasks. Additionally, RNNs use the same weights at each timestep, reducing the number of parameters the model has to learn and making the model independent of the length of the input sequence. This helps to overcome the curse of dimensionality."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\n- Clearly label each part and question in your submissions. - Deadline: March 13, 2024  # Evaluation Criteria:  - Depth and clarity of theoretical understanding and literature review. - Comprehensive analysis and insightful observations in EDA. - Correctness, creativity, and efficiency in model implementation and adaptation of TANDA. - Thoroughness in experimental evaluation, including a detailed comparison of models. - Quality of discussion on findings, challenges, and future work.  # References:  - TANDA original paper:https://arxiv.org/abs/1911. - TANDA code repository:https://github.com/amazon-science/wqa_tanda - ASNQ dataset: https://huggingface.co/datasets/asnq - WikiQA dataset:https://huggingface.co/datasets/wiki_qa - Trec-QA dataset: https://huggingface.co/datasets/lucadiliello/trecqa  McMaster University 2   \n\n# SEP 775 Assignment 4:   McMaster University SEP 775 # Assignment 4  # Fine-tuning a Pretrained Model Using LoRA  # Objectives:  Leverage LoRA: Low-Rank Adaptation to fine-tune a pretrained language model for a programming-related Question-Answering (QA) system on the\"flytech/python-codes-25k\" dataset.  ## Understanding LoRA (20%):  1. Review the concept, benefits, and mechanism of Low-Rank Adaptation (LoRA) for adap-     ting pretrained models. 2. Discuss the suitability of pretrained language models for code-related QA tasks and the     advantages of using LoRA for fine-tuning.  ## Dataset Preparation (20%):  1. Provide an overview of the \"flytech/python-codes-25k\"dataset, focusing on its structure     and relevance for a QA system. 2. Describe necessary preprocessing steps, including tokenization and encoding strategies for     code snippets.  ## Model Fine-Tuning with LoRA (30%):  1. Select a suitable pretrained language model and justify the choice based on its architecture     and expected performance on code-related QA tasks. 2. Detail the integration of LoRA, specifying the adaptation process and adjustments made     to the model for the QA task.  ## Training and Evaluation (30%):  1.\n\nfilename: materials_formatted.md\nextension: .md\n\nTo guide the work- ers, tasks contained a sample paragraph, and exam- ples of good and bad questions and answers on that paragraph along with the reasons they were cate- gorized as such. Additionally, crowdworkers were encouraged to ask questions in their own words, without copying word phrases from the paragraph. On the interface, this was reinforced by a reminder prompt at the beginning of every paragraph, and by disabling copy-paste functionality on the paragraph text.  Additional answers collection. To get an indica- tion of human performance on SQuAD and to make our evaluation more robust, we obtained at least 2 additional answers for each question in the develop- ment and test sets. In the secondary answer gener- ation task, each crowdworker was shown only the questions along with the paragraphs of an article, and asked to select the shortest span in the para- graph that answered the question. If a question was not answerable by a span in the paragraph, workers were asked to submit the question without marking an answer. Workers were recommended a speed of 5 questions for 2 minutes, and paid at the same rate of $9 per hour for the number of hours required for the entire article. Over the development and test sets, 2.6% of questions were marked unanswerable by at least one of the additional crowdworkers.  Date Other Numeric Person Location Other Entity Common Noun Phrase Adjective Phrase Verb Phrase Clause Other  8.9% 19 October 1512  10.9% 12 12.9% Thomas Coke  4.4% Germany 15.3% ABC Sports 31.8% property damage 3.9% second-largest 5.5% returned to Earth 3.7% to avoid trivialization 2.7% quietly  Table 2: We automatically partition our answers into the fol-  lowing categories. Our dataset consists of large number of an-  swers beyond proper noun entities.  4 Dataset Analysis  To understand the properties of SQuAD, we analyze the questions and answers in the development set. Speci\ufb01cally, we explore the (i) diversity of answer types, (ii) the dif\ufb01culty of questions in terms of type of reasoning required to answer them, and (iii) the degree of syntactic divergence between the question and answer sentences.\n\nfilename: materials_formatted.md\nextension: .md\n\nSuch end-to-end evaluationextrinsic evaluation is calledextrinsic evaluation. Extrinsic evaluation is the only way to know if a particular improvement in a component is really going to help the task at hand. Thus, for speech recognition, we can compare the performance of two language models by running the speech recognizer twice, once with each language model, and seeing which gives the more accurate transcription. Unfortunately, running big NLP systems end-to-end is often very expensive. In- stead, it would be nice to have a metric that can be used to quickly evaluate potential evaluationintrinsic improvements in a language model. Anintrinsic evaluationmetric is one that mea- sures the quality of a model independent of any application. For an intrinsic evaluation of a language model we need atest set. As with many of the statistical models in our field, the probabilities of an n-gram model come from training set the corpus it is trained on, thetraining setortraining corpus. We can then measure the quality of an n-gram model by its performance on some unseen data called the test set test setor test corpus. So if we are given a corpus of text and want to compare two different n-gram models, we divide the data into training and test sets, train the parameters of both models on the training set, and then compare how well the two trained models fit the test set. But what does it mean to \u201cfit the test set\u201d? The answer is simple: whichever model assigns ahigher probabilityto the test set\u2014meaning it more accurately predicts the test set\u2014is a better model. Given two probabilistic models, the better model is the one that has a tighter fit to the test data or that better predicts the details of the test data, and hence will assign a higher probability to the test data. Since our evaluation metric is based on test set probability, it\u2019s important not to let the test sentences into the training set. Suppose we are trying to compute the probability of a particular \u201ctest\u201d sentence. If our test sentence is part of the training corpus, we will mistakenly assign it an artificially high probability when it occurs in the test set. We call this situationtraining on the test set. Training on the test set introduces a bias that makes the probabilities all look too high, and causes huge inaccuracies inperplexity, the probability-based metric we introduce below.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat are the two major categories of evaluation schemes discussed in the paper?[/INST] \nAnswer: The two major categories of evaluation schemes discussed in the paper are extrinsic evaluation and intrinsic evaluation."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nArXiv, abs/1909.08593, 2019.  75     \n\n# Dense Passage Retrieval for Open-Domain Question Answering:  Dense Passage Retrieval for Open-Domain Question Answering  Vladimir Karpukhin\u2217, Barlas O\u02d8guz\u2217, Sewon Min\u2020, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen\u2021, Wen-tau Yih  Facebook AI  \u2020University of Washington  \u2021Princeton University  {vladk, barlaso, plewis, ledell, edunov, scottyih}@fb.com sewon@cs.washington.edu danqic@cs.princeton.edu  Abstract  Open-domain question answering relies on ef- \ufb01cient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented us- ing dense representations alone, where em- beddings are learned from a small number of questions and passages by a simple dual- encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene- BM25 system greatly by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.1  1  Introduction  Open-domain question answering (QA) (Voorhees, 1999) is a task that answers factoid questions us- ing a large collection of documents. While early QA systems are often complicated and consist of multiple components (Ferrucci (2012); Moldovan et al. (2003), inter alia), the advances of reading comprehension models suggest a much simpli\ufb01ed two-stage framework: (1) a context retriever \ufb01rst selects a small subset of passages where some of them contain the answer to the question, and then (2) a machine reader can thoroughly exam- ine the retrieved contexts and identify the correct answer (Chen et al., 2017). Although reducing open-domain QA to machine reading is a very rea- sonable strategy, a huge performance degradation is often observed in practice2, indicating the needs of improving retrieval.\n\nfilename: materials_formatted.md\nextension: .md\n\n[26], we use the December 2018 dump. Each Wikipedia article is split into disjoint 100-word chunks, to make a total of 21M documents. We use the document encoder to compute an embedding for each document, and build a single MIPS index using FAISS [ 23 ] with a Hierarchical Navigable Small World approximation for fast retrieval [ 37 ]. During training, we retrieve the top kdocuments for each query. We considerk\u2208{ 5 , 10 }for training and setkfor test time using dev data. We now discuss experimental details for each task.   3.1 Open-domain Question Answering   Open-domain question answering (QA) is an important real-world application and common testbed for knowledge-intensive tasks [ 20 ]. We treat questions and answers as input-output text pairs(x,y) and train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to the popular extractive QA paradigm [ 5 , 7 , 31 , 26 ], where answers are extracted spans from retrieved documents, relying primarily on non-parametric knowledge. We also compare to \u201cClosed-Book QA\u201d approaches [ 52 ], which, like RAG, generate answers, but which do not exploit retrieval, instead relying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural Questions (NQ) [ 29 ], TriviaQA (TQA) [ 24 ]. WebQuestions (WQ) [ 3 ] and CuratedTrec (CT) [ 2 ]. As CT and WQ are small, we follow DPR [ 26 ] by initializing CT and WQ models with our NQ RAG model. We use the same train/dev/test splits as prior work [ 31 , 26 ] and report Exact Match (EM) scores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set.   3.2 Abstractive Question Answering   RAG models can go beyond simple extractive QA and answer questions with free-form, abstractive text generation. To test RAG\u2019s natural language generation (NLG) in a knowledge-intensive setting, we use the MSMARCO NLG task v2.1 [ 43 ].\n\nfilename: materials_formatted.md\nextension: .md\n\nBy applying a modern reader model to the top retrieved passages, we achieve compara- ble or better results on multiple QA datasets in the open-retrieval setting, compared to several, much complicated systems.  2 Background  The problem of open-domain QA studied in this paper can be described as follows. Given a factoid question, such as \u201cWho \ufb01rst voiced Meg on Family Guy?\u201d or \u201cWhere was the 8th Dalai Lama born?\u201d, a system is required to answer it using a large corpus of diversi\ufb01ed topics. More speci\ufb01cally, we assume  e  the extractive QA setting, in which the answer is restricted to a span appearing in one or more pas- sages in the corpus. Assume that our collection contains D documents, d1, d2, \u00b7 \u00b7 \u00b7 , dD. We \ufb01rst split each of the documents into text passages of equal lengths as the basic retrieval units3 and get M total passages in our corpus C = {p1, p2, . . . , pM }, where each passage pi can be viewed as a sequence 2 , \u00b7 \u00b7 \u00b7 , w(i) 1 , w(i) of tokens w(i) |pi|. Given a question q, the task is to \ufb01nd a span w(i) s+1, \u00b7 \u00b7 \u00b7 , w(i) s , w(i) from one of the passages pi that can answer the question. Notice that to cover a wide variety of domains, the corpus size can easily range from millions of docu- ments (e.g., Wikipedia) to billions (e.g., the Web). As a result, any open-domain QA system needs to include an ef\ufb01cient retriever component that can se- lect a small set of relevant texts, before applying the reader to extract the answer (Chen et al., 2017).4 Formally speaking, a retriever R : (q, C) \u2192 CF is a function that takes as input a question q and a corpus C and returns a much smaller \ufb01lter set of texts CF \u2282 C, where |CF | = k (cid:28) |C|.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat does the paper propose for answering open-domain questions?[/INST] \nAnswer: The paper proposes a two-stage framework for answering open-domain questions. The first stage involves a context retriever that selects a small subset of passages that may contain the answer to the question. The second stage involves a machine reader that examines the retrieved contexts and identifies the correct answer. The paper also proposes the use of dense representations for retrieval, which are learned from a small number of questions and passages by a simple dual-encoder framework. This dense retriever outperforms traditional sparse vector space models, such as TF-IDF or BM25, in terms of top-20 passage retrieval accuracy."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nOur belief is reinforced  by a recent statistical analysis of BLEU\u2019s correla-  tion with human judgment for translation into En-  glish from four quite different languages (Arabic,  Chinese, French, Spanish) representing 3 different  language families (Papineni et al., 2002)! BLEU\u2019s  strength is that it correlates highly with human judg-  Florence Reeder. 2001. Additional mt-eval references.  Technical report, International Standards for Language  Engineering, Evaluation Working Group. http://issco-  www.unige.ch/projects/isle/taxonomy2/  J.S. White and T. O\u2019Connell. 1994. The ARPA MT eval-  uation methodologies: evolution, lessons, and future  approaches. In *Proceedings of the First Conference of*  *the Association for Machine Translation in the Ameri-*  *cas*, pages 193\u2013205, Columbia, Maryland.  <sup>8</sup>Crossing this chasm for Chinese-English translation ap-  pears to be a signi\ufb01cant challenge for the current state-of-the-art  systems.     \n\n# Contextual Word Representations - A Contextual Introduction:  Contextual Word Representations: A Contextual Introduction  Noah A. Smith Paul G. Allen School of Computer Science and Engineering, University of Washington Allen Institute for Arti\ufb01cial Intelligence nasmith@cs.washington.edu  April 20, 2020  Abstract  This introduction aims to tell the story of how we put words into computers. It is part of the story of the \ufb01eld of natural language processing (NLP), a branch of arti\ufb01cial intelligence.1 It targets a wide audience with a basic understanding of computer programming, but avoids a detailed mathematical treat- ment, and it does not present any algorithms. It also does not focus on any particular application of NLP such as translation, question answering, or information extraction. The ideas presented here were devel- oped by many researchers over many decades, so the citations are not exhaustive but rather direct the reader to a handful of papers that are, in the author\u2019s view, seminal.\n\nfilename: materials_formatted.md\nextension: .md\n\nThis note introduces the field of Natural Language Pro- cessing (NLP) briefly, and then discusses word 2 vec and the funda- mental, beautiful idea of representing words as low-dimensional real-valued vectors learned from distributional signal.  ## 1 Introduction to Natural Language Processing   Natural language processing is a field of science and engineering focused on the development and study of automatic systems that understand and generate natural (that is, human,) languages.  ### 1. 1 Humans and language   Human languages are communicative devices enabling the effi- cient sharing and storage of complex ideas, facts, and intents. As [Manning, 2022 ] argues, the complexity of communication enabled by language is a uniquely human intelligence among species. As scientists and engineers interested in the creation and study of intel- ligent systems, human language is to us both a fascinating object of study\u2014after all, it hasevolved to be learnable and useful\u2014and a great enabler for interacting with humans even in contexts where other modalities (e.g., vision) are also of interest.  ### 1. 2 Language and machines   Human children, interacting with a rich multi-modality world and various forms of feedback, acquire language with exceptional sample efficiency (not observing that much language) and compute efficiency (brains are efficient computing machines!) With all the (impressive!) advances in NLP in the last decades, we are still nowhere close to developing learning machines that have a fraction of acquisition ability of children. One fundamental (and still quite open) problem in building language-learning machines is the question of representa- tion ; how should we represent language in a computer such that the computer can robustly process and/or generate it? This is where this course focuses on the tools provided by deep learning , a highly effec- tive toolkit for representing both the wild variety of natural language    deep learning 2  and some of the rules and structures it sometimes adheres to. Much of this course will be dedicated to this question of representation, and the rest of this note will talk about a basic subquestion: how do we represent words? Before that, though, let\u2019s briefly discuss some of the applications you can hope to build after learning modern NLP techniques.  ### 1.\n\nfilename: materials_formatted.md\nextension: .md\n\nNote as well that in the diagram, words are not ordered, e.g., alphabetically\u2014this is    deep learning 5  an important note; there is no (explicit) character-level information in these strings, beyond the strict notion of identity (is this word the same sequence of characters/bytes as this other word. If so, they have the same vector; if not, they have independent vectors.) Since it is of course not the case that all words are equally dissimi- lar from each other, we\u2019ll move to some alternatives.  ### 2. 3 Vectors from annotated discrete properties.  Should we represent word semantics not as one-hot vectors, but instead as a collection of features and relationships to linguistic categories and other words? For any word, sayrunners, there is a wealth of information we can annotate about that word. There is grammatical information, likeplurality, there\u2019s derivational information, like how the runnersis something like the verbto runplus a notion of \u201cdoer\u201d, or agent(thinkone who runs.) There\u2019s also semantic information, like howrunnersmight be a hyponym ofhumans, oranimals, orentities. (A hyponym is a member of an is-a relationship; e.g.,a runner is a human.) There are substantial existing resources in English and a few other languages for various kinds of annotated information about words. WordNet [Miller, 1995 ] annotates for synonyms, hyponyms, and other semantic relations; UniMorph [Batsuren et al., 2022 ] annotates for morphology (subword structure) information across many lan- guages.   (plural noun) ( 3 rd singular verb) (hyponym-of-beverage) .. . (synonym-of-chai)   ( 3 )  In 2023 , word vectors resulting from these methods are not the norm, and they won\u2019t be the focus of this course. One main failure is that human-annotated resources are always lacking in vocabulary compared to methods that can draw a vocabulary from a naturally occuring text source\u2014updating these resources is costly and they\u2019re always incomplete.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat is a major limitation of human-annotated resources for word representation, according to the note?[/INST] \nAnswer: A major limitation of human-annotated resources for word representation is that they are always lacking in vocabulary compared to methods that can draw a vocabulary from a naturally occurring text source. Updating these resources is costly and they're always incomplete."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\n-? 3. Reinforcement Learning from Human Feedback (RLHF)  49  4. What\u2019s next?  #### SEP 775 Lecture Plan: From Language Models to Assistants   #### Limitations of instruction finetuning?  - One limitation of instruction finetuning is obvious: it\u2019s expensive to collect ground-     truth data for tasks. - But there are other, subtler limitations too. Can you think of any? - Problem 1 : tasks like open-ended creative generation have no right answer.     - _Write me a story about a dog and her pet grasshopper._ - Problem 2 : language modeling penalizes all token-level mistakes equally, but some     errors are worse than others. - Even with instruction finetuning _,_ there     a mismatch between the LM     objective and the objective of     \u201csatisfy human preferences\u201d! - Can we explicitly attempt to satisfy     human preferences?   LM   Avatar is a fantasy TV show   is  50   a   adventure fantasy   musical TV show END   1. Zero-Shot (ZS) and Few-Shot (FS) In-Context Learning     + No finetuning needed, prompt engineering (e.g. CoT) can improve performance     - Limits to what you can fit in context     - Complex tasks will probably need gradient steps 2. Instruction finetuning     + Simple and straightforward, generalize to unseen tasks     - Collecting demonstrations for so many tasks is expensive     - Mismatch between LM objective and human preferences 3. Reinforcement Learning from Human Feedback (RLHF)  51  #### SEP 775 Lecture Plan: From Language Models to Assistants  4. What\u2019s next?   1. Zero-Shot (ZS) and Few-Shot (FS) In-Context Learning     + No finetuning needed, prompt engineering (e.g. CoT) can improve performance     - Limits to what you can fit in context     - Complex tasks will probably need gradient steps 2. Instruction finetuning     + Simple and straightforward, generalize to unseen tasks     - Collecting demonstrations for so many tasks is expensive     - Mismatch between LM objective and human preferences 3.\n\nfilename: materials_formatted.md\nextension: .md\n\nQA\u2013 Commonsense NLI  Closed-book QA49.955.059.359.260.861.963.5Base LM: CommonsenseBase LM: NLIBase LM: Closed-book QAPublished as a conference paper at ICLR 2022  4.2 SCALING LAWS  As Brown et al. (2020) shows that zero and few-shot capabilities of language models sub- stantially improve for larger models, we next explore how the bene\ufb01ts of instruction tuning are affected by model scale. Using the same cluster split as in the previous ablation study, we evaluate the effect of instruction tuning on models of size 422M, 2B, 8B, 68B, and 137B parameters.  Figure 7 shows these results. We see that for the two models on the order of 100B pa- rameters, instruction tuning substantially im- proves performance on held-out tasks, as is expected given the prior results in our pa- per. The behavior on held-out tasks for the 8B and smaller models, however, is thought- provoking\u2014instruction tuning actually hurts performance on held-out tasks. One potential explana- tion for this result could be that for small-scale models, learning the \u223c40 tasks used during instruction tuning \ufb01lls the entire model capacity, causing these models to perform worse on new tasks. Under this potential explanation, for the larger scale models, instruction tuning \ufb01lls up some model capacity but also teaches these models how to follow instructions, allowing them to generalize to new tasks with the remaining capacity.  Figure 7: Whereas instruction tuning helps large models generalize to new tasks, for small models it actually hurts generalization to unseen tasks, poten- tially because all model capacity is used to learn the mixture of instruction tuning tasks.  4.3 ROLE OF INSTRUCTIONS  In a \ufb01nal ablation study, we explore the role of in- structions during \ufb01netuning, as one possibility is that performance gains come entirely from multi-task \ufb01ne- tuning and the model could perform just as well without instructions. We hence consider two \ufb01netuning setups without instructions.\n\nfilename: materials_formatted.md\nextension: .md\n\nFinally, the scale of FLAN 137B makes it costly to serve. Future work on instruction tuning could include gathering/generating even more task clusters for \ufb01netuning, cross-lingual experiments, using FLAN to generate data for training downstream classi\ufb01ers, and using \ufb01netuning to improve model behavior with respect to bias and fairness (Solaiman & Dennison, 2021).  7 CONCLUSIONS  This paper has explored a simple method for improving the ability of language models at scale to perform zero-shot tasks based purely on instructions. Our instruction-tuned model, FLAN, compares favorably against GPT-3 and signals the potential ability for language models at scale to follow instructions. We hope that our paper will spur further research on instructions-based NLP, zero-shot learning, and using labeled data to improve large language models.  9  Published as a conference paper at ICLR 2022  ETHICAL CONSIDERATIONS  This work uses language models, for which the risks and potential harms are discussed in Bender & Koller (2020), Brown et al. (2020), Bender et al. (2021), Patterson et al., (2021), and others. As our contribution in this paper is not a pretrained language model itself but rather an empirical study of how instruction tuning affects the zero-shot performance of a language model on unseen tasks, we additionally highlight two relevant ethical considerations. First, labeled datasets such as those we use for \ufb01netuning can contain undesirable biases, and these biases can be propagated into zero-shot applications of the model on downstream tasks. And second, instruction-tuned models can potentially require less data and expertise to use; such lower barriers to access could increase both the bene\ufb01ts and associated risks of such models.  ENVIRONMENTAL CONSIDERATIONS  We use the same pretrained language models as Austin et al. (2021). The energy cost and carbon footprint for the pretrained models were 451 MWh and 26 tCO2e, respectively. The additional instruction tuning gradient-steps for \ufb01netuning FLAN is less than 2% of the number of pretraining steps, and so the estimated additional energy cost is comparatively smaller.  AUTHOR CONTRIBUTIONS  Maarten Bosma conceived the original idea and implemented the \ufb01rst version of FLAN.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat are the limitations of instruction finetuning?[/INST] \nAnswer: Instruction finetuning has several limitations. Firstly, it is expensive to collect ground-truth data for tasks. Secondly, there are tasks like open-ended creative generation that have no definitive right answer, which makes it difficult to fine-tune instructions for such tasks. Thirdly, language modeling penalizes all token-level mistakes equally, but some errors are worse than others, leading to a mismatch between the language model objective and the objective of satisfying human preferences. Lastly, for smaller models, instruction tuning can actually hurt performance on unseen tasks as learning the tasks used during instruction tuning may fill the entire model capacity, causing these models to perform worse on new tasks."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nTo guide the work- ers, tasks contained a sample paragraph, and exam- ples of good and bad questions and answers on that paragraph along with the reasons they were cate- gorized as such. Additionally, crowdworkers were encouraged to ask questions in their own words, without copying word phrases from the paragraph. On the interface, this was reinforced by a reminder prompt at the beginning of every paragraph, and by disabling copy-paste functionality on the paragraph text.  Additional answers collection. To get an indica- tion of human performance on SQuAD and to make our evaluation more robust, we obtained at least 2 additional answers for each question in the develop- ment and test sets. In the secondary answer gener- ation task, each crowdworker was shown only the questions along with the paragraphs of an article, and asked to select the shortest span in the para- graph that answered the question. If a question was not answerable by a span in the paragraph, workers were asked to submit the question without marking an answer. Workers were recommended a speed of 5 questions for 2 minutes, and paid at the same rate of $9 per hour for the number of hours required for the entire article. Over the development and test sets, 2.6% of questions were marked unanswerable by at least one of the additional crowdworkers.  Date Other Numeric Person Location Other Entity Common Noun Phrase Adjective Phrase Verb Phrase Clause Other  8.9% 19 October 1512  10.9% 12 12.9% Thomas Coke  4.4% Germany 15.3% ABC Sports 31.8% property damage 3.9% second-largest 5.5% returned to Earth 3.7% to avoid trivialization 2.7% quietly  Table 2: We automatically partition our answers into the fol-  lowing categories. Our dataset consists of large number of an-  swers beyond proper noun entities.  4 Dataset Analysis  To understand the properties of SQuAD, we analyze the questions and answers in the development set. Speci\ufb01cally, we explore the (i) diversity of answer types, (ii) the dif\ufb01culty of questions in terms of type of reasoning required to answer them, and (iii) the degree of syntactic divergence between the question and answer sentences.\n\nfilename: materials_formatted.md\nextension: .md\n\n4. Model Optimization and Training:The effectiveness of model optimization techni-     ques, including hyperparameter tuning, regularization methods, and training procedures. 5. Analysis and Interpretation:Depth of analysis on the model\u2019s performance, including     error analysis, limitations of the model, and interpretation of the results. Insightful     observations and potential improvements should be discussed. 6. Project Report: The quality of the project report, including clarity of writing, or-     ganization, and thoroughness in covering the project\u2019s scope, methodology, results, and     conclusions. The report should also include a clear abstract and introduction to the     problem. 7. Innovation and Creativity:The uniqueness of the approach and creativity in solving     the problem. Innovative use of NLP techniques or development of new methods will be     highly regarded.  ## Submission Requirements  1. Code: Submit all code files used for the project in a single compressed folder (.zip or     .tar.gz). Include a README file with instructions on how to run the code and any     dependencies required. 2. Data:If the dataset used is publicly available, provide the link to the dataset. If the da-     taset is created or modified for the project, include it in the submission with a description     in the README file. 3. Report:A project report in PDF format, not exceeding 10 pages (excluding appendices     and references), formatted in a clear and readable style. The report should include sections     on introduction, methodology, results, analysis, and conclusions. Include any relevant     figures, tables, and references. 4. Presentation: Prepare a short presentation (10-15 slides) summarizing the project\u2019s     objective, methodology, key results, and conclusions.   5. Additional Materials:Any additional materials used or created for the project, such as     scripts for data preprocessing, trained models, or Jupyter notebooks, should be included     in the submission.  Submissions failing to meet these requirements may be penalized. Please ensure that all submitted materials are well-organized and clearly labeled for easy navigation.      \n\n# Attention Is All You Need:  Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works.\n\nfilename: materials_formatted.md\nextension: .md\n\nNAACL.  Wojciech Zaremba and Ilya Sutskever. 2014. Learning to execute. arXiv preprint arXiv:1410.4615.  Eric Zelikman, Yuhuai Wu, and Noah D. Goodman. 2022. STaR: Bootstrapping reasoning with  reasoning. arXiv preprint arXiv:2203.14465.  Tony Z. Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use:  Improving few-shot performance of language models. ICML.  Wangchunshu Zhou, Jinyi Hu, Hanlin Zhang, Xiaodan Liang, Maosong Sun, Chenyan Xiong, and Jian Tang. 2020. Towards interpretable natural language understanding with explanations as latent variables. NeurIPS.  14  Checklist  1. For all authors...  (a) Do the main claims made in the abstract and introduction accurately re\ufb02ect the paper\u2019s  contributions and scope? [Yes]  (b) Did you describe the limitations of your work? [Yes] See Section 6 and Appendix A.2. (c) Did you discuss any potential negative societal impacts of your work? [Yes] We don\u2019t expect negative societal impacts as a direct result of the contributions in our paper. One consideration, however, is that generated chain of thought is not always factual, which is noted as a limitation in Appendix D.1 (and note that we do not suggest using such chains of thought in a factual manner or in any real-world scenario).  (d) Have you read the ethics review guidelines and ensured that your paper conforms to  them? [Yes]  2. If you are including theoretical results...  (a) Did you state the full set of assumptions of all theoretical results? [N/A] (b) Did you include complete proofs of all theoretical results? [N/A]  3. If you ran experiments...  (a) Did you include the code, data, and instructions needed to reproduce the main experi- mental results (either in the supplemental material or as a URL)?\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat are the two components of the proposed system in the paper?[/INST] \nAnswer: I don't know."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\n#### 1. 2 n-gram Language Models  To compute the probabilities mentioned above, the count of each n- gram could be compared against the frequency of each word. This is   ### language models,rnn,gru and lstm 2  called an n-gram Language Model. For instance, if the model takes bi-grams, the frequency of each bi-gram, calculated via combining a word with its previous word, would be divided by the frequency of the corresponding uni-gram. Equations 2 and 3 show this relation- ship for bigram and trigram models.   p(w 2 |w 1 ) = count(w 1 ,w 2 ) count(w 1 )   ( 2 )  p(w 3 |w 1 ,w 2 ) = count(w 1 ,w 2 ,w 3 ) count(w 1 ,w 2 ) (^3 ) The relationship in Equation 3 focuses on making predictions based on a fixed window of context (i.e. thenprevious words) used to predict the next word. But how long should the context be? In some cases, the window of past consecutivenwords may not be suf- ficient to capture the context. For instance, consider the sentence \"As the proctor started the clock, the students opened their \". If the window only conditions on the previous three words \"the students opened their\", the probabilities calculated based on the corpus may suggest that the next word be \"books\" - however, ifnhad been large enough to include the \"proctor\" context, the probability might have suggested \"exam\". This leads us to two main issues with n-gram Language Models: Sparsity and Storage.  1. Sparsity problems with n-gram Language models Sparsity problems with these models arise due to two issues. Firstly, note the numerator of Equation 3. Ifw 1 ,w 2 andw 3 never appear together in the corpus, the probability ofw 3 is 0. To solve this, a small _\u03b4_ could be added to the count for each word in the vocabulary. This is calledsmoothing. Secondly, consider the denominator of Equation 3. Ifw 1 andw 2 never occurred together in the corpus, then no probability can be calculated forw 3.\n\nfilename: materials_formatted.md\nextension: .md\n\n2007, Kane et al. 2017). People often use suchAAC devices if they are physically unable to speak or sign but can instead use eye gaze or other specific movements to select words from a menu to be spoken by the system. Word prediction can be used to suggest likely words for the menu. Models that assign probabilities to sequences of words are calledlanguage mod- language model elsorLMs. In this chapter we introduce the simplest model that assigns probabil- LM ities to sentences and sequences of words, then-gram. An n-gram is a sequence n-gram ofnwords: a 2-gram (which we\u2019ll callbigram) is a two-word sequence of words like \u201cplease turn\u201d, \u201cturn your\u201d, or \u201dyour homework\u201d, and a 3-gram (atrigram) is a three-word sequence of words like \u201cplease turn your\u201d, or \u201cturn your homework\u201d. We\u2019ll see how to use n-gram models to estimate the probability of the last word of an n-gram given the previous words, and also to assign probabilities to entire se- quences. In a bit of terminological ambiguity, we usually drop the word \u201cmodel\u201d, and use the termn-gram(andbigram, etc.) to mean either the word sequence itself or the predictive model that assigns it a probability. While n-gram models are much simpler than state-of-the art neural language models based on the RNNs and trans- formers we will introduce in Chapter 9, they are an important foundational tool for understanding the fundamental concepts of language modeling.  #### 3.1 N-Grams   Let\u2019s begin with the task of computingP(w|h), the probability of a wordwgiven some historyh. Suppose the historyhis \u201cits water is so transparent that\u201d and we want to know the probability that the next word isthe:   P(the|its water is so transparent that). (3.1)   One way to estimate this probability is from relative frequency counts: take a very large corpus, count the number of times we seeits water is so transparent that, and count the number of times this is followed bythe.\n\nfilename: materials_formatted.md\nextension: .md\n\nWe choose a random value between 0 and 1, find that point on the probability line, and print the word whose interval includes this chosen value. We continue choosing random numbers and generating words until we randomly generate the sentence-final token</s>.   0 1   0.   the   .   0.   of 0.   a 0.   to in   .09.11.13.  ##### ...   however (p=.0003)   polyphonic p=.  ### 0. ...   .66.  ##### ...   Figure 3.3 A visualization of the sampling distribution for sampling sentences by repeat- edly sampling unigrams. The blue bar represents the relative frequency of each word (we\u2019ve ordered them from most frequent to least frequent, but the choice of order is arbitrary). The number line shows the cumulative probabilities. If we choose a random number between 0 and 1, it will fall in an interval corresponding to some word. The expectation for the random number to fall in the larger intervals of one of the frequent words (the,of,a) is much higher than in the smaller interval of one of the rare words (polyphonic).   We can use the same technique to generate bigrams by first generating a ran- dom bigram that starts with<s>(according to its bigram probability). Let\u2019s say the second word of that bigram isw. We next choose a random bigram starting withw (again, drawn according to its bigram probability), and so on.  #### 3.4 Generalization and Zeros   The n-gram model, like many statistical models, is dependent on the training corpus. One implication of this is that the probabilities often encode specific facts about a    3.4 \u2022 GENERALIZATION ANDZEROS 11   given training corpus. Another implication is that n-grams do a better and better job of modeling the training corpus as we increase the value ofN. We can use the sampling method from the prior section to visualize both of these facts! To give an intuition for the increasing power of higher-order n-grams, Fig. 3.4 shows random sentences generated from unigram, bigram, trigram, and 4- gram models trained on Shakespeare\u2019s works.  ## 1   \u2013To him swallowed confess hear both. Which.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nHow is the probability of a word given some history calculated in n-gram models?[/INST] \nAnswer: The probability of a word given some history in n-gram models is calculated by using relative frequency counts. This involves taking a large corpus and counting the number of times a particular history appears, and then counting the number of times this history is followed by the word in question. The probability is then estimated from these counts. For instance, in a bigram model, the probability of a word given its previous word is calculated by dividing the frequency of the bigram (the two words appearing together) by the frequency of the unigram (the previous word). Similarly, in a trigram model, the probability of a word given the previous two words is calculated by dividing the frequency of the trigram (the three words appearing together) by the frequency of the bigram (the previous two words)."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nFigure 5 presents this.   Embeddings   Add Position Embeddings   Multi-Head Attention   Add & Norm   Feed-For ward   Add & Norm   Block   Linear   Softmax   Repeat for number of   encoder blocks   Probabilities   Encoder Inputs  #### Transformer Encoder   Figure 5 : Diagram of the Transformer Encoder.  In the case that one wants probabilities out of the tokens of a Transformer Encoder (as in masked language modeling for BERT [Devlin et al., 2019 ], which we\u2019ll cover later), one applies a linear transformation to the output space followed by a softmax.  Uses of the Transformer Encoder. A Transformer Encoder is great in contexts where you aren\u2019t trying to generate text autoregressively (there\u2019s no masking in the encoder so each position index can see the whole sequence,) and want strong representations for the whole sequence (again, possible because even the first token can see the whole future of the sequence when building its representation.)  ##### 3. 6 Transformer Decoder  To build a Transformer autoregressive language model, one uses a Transformer Decoder. These differ from Transformer Encoders simply by using future masking at each application of self-attention. This ensures that the informational constraint (no cheating by looking at the future!) holds throughout the architecture. We show a diagram of this architecture in Figure 4. Famous examples of this are GPT- 2 [Radford et al., 2019 ], GPT- 3 [Brown et al., 2020 ] and BLOOM [Workshop et al., 2022 ].  ##### 3. 7 Transformer Encoder-Decoder  A Transformer encoder-decoder takes as input two sequences. Fig- ure 6 shows the whole encoder-decoder structure. The first sequence x 1:nis passed through a Transformer Encoder to build contextual representations. The second sequence y 1:mis encoded through a modified Transformer Decoder architecture in which cross-attention (which we haven\u2019t yet defined!) is applied from the encoded repre- sentation of y 1:mto the output of the Encoder. So, let\u2019s take a quick detour to discuss cross-attention; it\u2019s not too different from what we\u2019ve already seen.    with deep learning 14  Cross-Attention.\n\nfilename: materials_formatted.md\nextension: .md\n\nFigure 1: The Transformer - model architecture.  The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.  3.1 Encoder and Decoder Stacks  Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position- wise fully connected feed-forward network. We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512.  Decoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.  3.2 Attention  An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum  Scaled Dot-Product Attention  Multi-Head Attention  Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.  of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.  3.2.1 Scaled Dot-Product Attention  We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2).\n\nfilename: materials_formatted.md\nextension: .md\n\nIntuitively, these are the biggest components to understand. How- ever, as of 2023 , by far the most-used architecture in NLP is called theTransformer, introduced by [Vaswani et al., 2017 ], and it contains a number of components that end up being quite important. So now we\u2019ll get into the details of that architecture.  #### 3 The Transformer  The Transformer is an architecture based on self-attention that con- sists of stackedBlocks, each of which contains self-attention and feed- forward layers, and a few other components we\u2019ll discuss. If you\u2019d like to take a peek for intuition, we have a diagram of a Transformer language model architecture in Figure 4. The components we haven\u2019t gone over are multi-head self-attention, layer normalization , resid- ual connections , and attention scaling \u2014and of course, we\u2019ll discuss how these components are combined to form the Transformer.   Embeddings   Add Position Embeddings   Masked Multi- Head Attention   Add & Norm   Feed-Forward   Add & Norm   Block   Linear   Softmax   Repeat for number of   encoder blocks   Probabilities   Decoder Inputs  #### Transformer Decoder   Figure 4 : Diagram of the Transformer Decoder (without corresponding Encoder, and so no cross-attention.  ##### 3. 1 Multi-head Self-Attention  Intuitively, a single call of self-attention is best at picking out asingle value (on average) from the input value set. It does so softly, by averaging over all of the values, but it requires a balancing game in the key-query dot products in order to carefully average two or more things. What we\u2019ll present now, multi-head self-attention , intuitively applies self-attention multiple times at once, each with different key, query, and value transformations of the same input, and then combines the outputs. For an integer number of headsk, we define matricesK(\u2113),Q(\u2113),V(\u2113)\u2208 R d\u00d7d/kfor\u2113in{1,... ,k}. (We\u2019ll see why we have the dimensionality reduction tod/ksoon.) These our the key, query, and value matrices for each head.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat differentiates the Transformer Encoder from the Decoder?[/INST] \nAnswer: The Transformer Encoder and Decoder are differentiated by their structure and function. The Encoder is composed of a stack of identical layers, each containing two sub-layers: a multi-head self-attention mechanism and a position-wise fully connected feed-forward network. It is used in contexts where strong representations for the whole sequence are needed, and each position index can see the entire sequence.\n\nOn the other hand, the Decoder also consists of a stack of identical layers, but it has three sub-layers in each layer. In addition to the two sub-layers found in the encoder layer, the decoder inserts a third sub-layer that performs multi-head attention over the output of the encoder stack. It is used to build an autoregressive language model, and it employs future masking at each application of self-attention to ensure that the informational constraint holds throughout the architecture. This means that the predictions for position i can depend only on the known outputs at positions less than i."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nAdding MLP in \u03b2 does not seem to help, yielding slightly worse result than \u03b2 without MLP.  13     \n\n# On the difficulty of training Recurrent Neural Networks:  # On the difficulty of training Recurrent Neural Networks   Razvan Pascanu pascanur@iro.umontreal.ca Universite de Montreal   Tomas Mikolov t.mikolov@gmail.com Brno University   Yoshua Bengio yoshua.bengio@umontreal.ca Universite de Montreal  ## Abstract   There are two widely known issues with prop- erly training Recurrent Neural Networks, the vanishing and theexplodinggradient prob- lems detailed in Bengio et al. (1994). In this paper we attempt to improve the under- standing of the underlying issues by explor- ing these problems from an analytical, a geo- metric and a dynamical systems perspective. Our analysis is used to justify a simple yet ef- fective solution. We propose a gradient norm clipping strategy to deal with exploding gra- dients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.  ## 1. Introduction   A recurrent neural network (RNN), e.g. Fig. 1, is a neural network model proposed in the 80\u2019s (Rumelhart et al., 1986; Elman, 1990; Werbos, 1988) for modeling time series. The structure of the network is similar to that of a standard multilayer perceptron, with the dis- tinction that we allow connections among hidden units associated with a time delay. Through these connec- tions the model can retain information about the past inputs, enabling it to discover temporal correlations between events that are possibly far away from each other in the data (a crucial property for proper learn- ing of time series). While in principle the recurrent network is a simple and powerful model, in practice, it is unfortunately hard to train properly. Among the main reasons why this model is so unwieldy are thevanishing gradient  ### ut xt Et   Figure 1.Schematic of a recurrent neural network. The recurrent connections in the hidden layer allow information to persist from one input to another.   andexploding gradientproblems described in Bengio et al.(1994).   1.1.\n\nfilename: materials_formatted.md\nextension: .md\n\n1.1. Training recurrent networks A generic recurrent neural network, with inpututand statextfor time stept, is given by equation (1). In the theoretical section of this paper we will sometimes make use of the specific parametrization given by equa- tion (11)^1 in order to provide more precise conditions and intuitions about the everyday use-case.   xt=F(xt\u2212 1 ,ut,\u03b8) (1)   xt=Wrec\u03c3(xt\u2212 1 ) +Winut+b (2)   The parameters of the model are given by the recurrent weight matrixWrec, the biasesband input weight matrixWin, collected in\u03b8for the general case.x 0 is provided by the user, set to zero or learned, and\u03c3is an element-wise function (usually thetanhorsigmoid). A costE measures the performance of the network on some given task and it can be broken apart into individual costs for each stepE=   \u2211   1 \u2264t\u2264TEt, where Et=L(xt). One approach that can be used to compute the nec- essary gradients is Backpropagation Through Time (BPTT), where the recurrent model is represented as  (^1) This formulation is equivalent to the more widely known equationxt=\u03c3(Wrecxt\u2212 1 +Winut+b), and it was chosen for convenience.  # arXiv:1211.5063v2 [cs.LG] 16 Feb 2013   a deep multi-layer one (with an unbounded number of layers) and backpropagation is applied on the unrolled model (see Fig. 2).   \u2202\u2202Ext+ t+   Et\u2212 1 Et Et+   xt\u2212 1 xt xt+   ut\u2212 1 ut ut+   \u2202\u2202Ext t   \u2202Et\u2212 1 \u2202xt\u2212 1   \u2202xt+  \u2202\u2202xxtt\u2212\u2212 (^12) \u2202\u2202xxt\u2212t 1 \u2202\u2202xxt+1t \u2202xt+ Figure 2.Unrolling recurrent neural networks in time by creating a copy of the model for each time step. We denote byxtthe hidden state of the network at timet, byutthe input of the network at timetand byEtthe error obtained from the output at timet.\n\nfilename: materials_formatted.md\nextension: .md\n\nAvailable athttp://l2r.cs.uiuc.edu/ \u0303cogcomp/asoftware.php?skey=SRL.    COLLOBERT, WESTON, BOTTOU, KARLEN, KAVUKCUOGLU ANDKUKSA  the number of examples. Most of the neural network technology necessary for our work has been described ten years ago (e.g., Le Cun et al., 1998). However, if we haddecided ten years ago to train the language model network LM2 using a vintage computer, training would onlybe nearing com- pletion today. Training algorithms that scale linearly are most able to benefit from such tremendous progress in computer hardware.  #### 8. Conclusion  We have presented a multilayer neural network architecture that can handle a number of NLP tasks with both speed and accuracy. The design of this system was determined byour desire to avoid task-specific engineering as much as possible. Instead we rely on large unlabeled data sets and let the training algorithm discover internal representations that prove useful for all the tasks of interest. Using this strong basis, we have engineered a fast and efficient \u201call purpose\u201d NLP tagger that we hope will prove useful to the community.  #### Acknowledgments  We acknowledge the persistent support of NEC for this research effort. We thank Yoshua Bengio, Samy Bengio, Eric Cosatto, Vincent Etter, Hans-Peter Graf, Ralph Grishman, and Vladimir Vapnik for their useful feedback and comments.  #### Appendix A. Neural Network Gradients  We consider a neural networkf\u03b8(\u00b7), with parameters\u03b8. We maximize the likelihood (8), or minimize ranking criterion (17), with respect to the parameters\u03b8, using stochastic gradient. By negating the likelihood, we now assume it all corresponds to minimize a costC(f\u03b8(\u00b7)), with respect to\u03b8.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWho are the authors of 'On the difficulty of training Recurrent Neural Networks'?[/INST] \nAnswer: The authors of 'On the difficulty of training Recurrent Neural Networks' are Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nMore- der. The main idea is to use a weighted average of  over, they can take *weeks* or *months* to \ufb01nish. This is variable length phrase matches against the reference  a big problem because developers of machine trans- translations. This view gives rise to a family of met-  lation systems need to monitor the effect of *daily* rics using various weighting schemes. We have se-  changes to their systems in order to weed out bad lected a promising baseline metric from this family.  ideas from good ideas. We believe that MT progress  stems from evaluation and that there is a logjam of  fruitful research ideas waiting to be released from  In Section 2, we describe the baseline metric in  detail. In Section 3, we evaluate the performance of  BLEU. In Section 4, we describe a human evaluation  experiment. In Section 5, we compare our baseline  metric performance with human evaluations.  <sup>1</sup>So we call our method the bilingual evaluation understudy,  BLEU.    <a name=\"br2\"></a>   2 The Baseline BLEU Metric  large collections of translations presented in Section  5 show that this ranking ability is a general phe-  nomenon, and not an artifact of a few toy examples.  The primary programming task for a BLEU imple-  mentor is to compare *n*-grams of the candidate with  the *n*-grams of the reference translation and count  the number of matches. These matches are position-  independent. The more the matches, the better the  candidate translation is. For simplicity, we \ufb01rst fo-  cus on computing unigram matches.  Typically, there are many \u201cperfect\u201d translations of a  given source sentence. These translations may vary  in word choice or in word order even when they use  the same words. And yet humans can clearly dis-  tinguish a good translation from a bad one. For ex-  ample, consider these two candidate translations of  a Chinese source sentence:  Example 1.  Candidate 1: It is a guide to action which  ensures that the military always obeys  the commands of the party.\n\nfilename: materials_formatted.md\nextension: .md\n\n\u2211  BLEU= BP\u00b7exp  <i>w<sub>n</sub></i> log <i>p<sub>n</sub></i>  .  *n*=1  The ranking behavior is more immediately apparent  in the log domain,  *N*  *r*  log BLEU = min(1\u2212 ,0)+  \u2211  *w* log *p* .  *n*  *n*  *c*  *n*=1  In our baseline, we use *N* = 4 and uniform weights  <i><sup>w</sup>n</i> <sup>= 1/<i>N</i>.</sup>  3 The BLEU Evaluation  We wish to make the brevity penalty 1.0 when the The BLEU metric ranges from 0 to 1. Few transla-  candidate\u2019s length is the same as any reference trans- tions will attain a score of 1 unless they are identi-  lation\u2019s length. For example, if there are three ref- cal to a reference translation. For this reason, even  erences with lengths 12, 15, and 17 words and the a human translator will not necessarily score 1. It  candidate translation is a terse 12 words, we want is important to note that the more reference trans-  the brevity penalty to be 1. We call the closest refer- lations per sentence there are, the higher the score  ence sentence length the *\u201cbest match length.\u201d*  is. Thus, one must be cautious making even \u201crough\u201d  One consideration remains: if we computed the comparisons on evaluations with different numbers  brevity penalty sentence by sentence and averaged of reference translations: on a test corpus of about  the penalties, then length deviations on short sen- 500 sentences (40 general news stories), a human  tences would be punished harshly. Instead, we com- translator scored 0.3468 against four references and  pute the brevity penalty over the entire corpus to al- scored 0.2571 against two references. Table 1 shows  low some freedom at the sentence level. We \ufb01rst the BLEU scores of the 5 systems against two refer-  compute the test corpus\u2019 effective reference length, ences on this test corpus.\n\nfilename: materials_formatted.md\nextension: .md\n\n*r*, by summing the best match lengths for each can-  The MT systems S2 and S3 are very close in this  didate sentence in the corpus. We choose the brevity metric. Hence, several questions arise:    <a name=\"br6\"></a>   translation, provided that the translations are not all  from the same translator.  Table 1: BLEU on 500 sentences  S2 S3 H1 H2  S1  0\\.0527 0.0829 0.0930 0.1934 0.2571  4 The Human Evaluation  We had two groups of human judges. The \ufb01rst  group, called the monolingual group, consisted of 10  native speakers of English. The second group, called  the bilingual group, consisted of 10 native speakers  of Chinese who had lived in the United States for  the past several years. None of the human judges  was a professional translator. The humans judged  our 5 standard systems on a Chinese sentence sub-  set extracted at random from our 500 sentence test  corpus. We paired each source sentence with each  of its 5 translations, for a total of 250 pairs of Chi-  Table 2: Paired t-statistics on 20 blocks  S1  0\\.051 0.081 0.090 0.192 0.256  StdDev 0.017 0.025 0.020 0.030 0.039  3\\.4 24 11  S2  S3  H1  H2  Mean  t  \u2014  6  \u2022 Is the difference in BLEU metric reliable?  \u2022 What is the variance of the BLEU score?  \u2022 If we were to pick another random set of 500 nese source and English translations. We prepared a  sentences, would we still judge S3 to be better web page with these translation pairs randomly or-  than S2?  dered to disperse the \ufb01ve translations of each source  sentence. All judges used this same webpage and  saw the sentence pairs in the same order. They rated  each translation from 1 (very bad) to 5 (very good).\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat are the two key ingredients required for the BLEU MT evaluation system?[/INST] \nAnswer: The two key ingredients required for the BLEU MT evaluation system are the comparison of n-grams of the candidate with the n-grams of the reference translation and counting the number of matches, and the computation of the brevity penalty over the entire corpus."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\n## 1 Dependency Grammar and Dependency Structure  Parse trees in NLP, analogous to those in compilers, are used to ana- lyze the syntactic structure of sentences. There are two main types of structures used - constituency structures and dependency structures. Constituency Grammar uses phrase structure grammar to organize words into nested constituents. This will be covered in more detail in following chapters. We now focus on Dependency Parsing. Dependency structure of sentences shows which words depend on (modify or are arguments of) which other words. These binary asymmetric relations between the words are called dependencies and are depicted as arrows going from the head (or governor, superior, regent) to the dependent (or modifier, inferior, subordinate). Usually these dependencies form a tree structure. They are often typed with the name of grammatical relations (subject, prepositional object, apposition, etc.). An example of such a dependency tree is shown in Figure 1. Sometimes a fake ROOT node is added as the head to the whole tree so that every word is a dependent of exactly one node.   Figure 1 : Dependency tree for the sen- tence \"Bills on ports and immigration were submitted by Senator Brownback, Republican of Kansas\"  ### 1. 1 Dependency Parsing  Dependency parsing is the task of analyzing the syntactic depen- dency structure of a given input sentenceS. The output of a depen- dency parser is a dependency tree where the words of the input sen- tence are connected by typed dependency relations. Formally, the de- pendency parsing problem asks to create a mapping from the input sentence with wordsS=w 0 w 1 ...wn(wherew 0 is the ROOT) to its dependency tree graphG. Many different variations of dependency- based methods have been developed in recent years, including neural network-based methods, which we will describe later. To be precise, there are two subproblems in dependency parsing (adapted from Kuebler et al., chapter 1. 2 ):  1. Learning:Given a training setDof sentences annotated with de- pendency graphs, induce a parsing modelMthat can be used to parse new sentences.   ## dependency parsing 2  2. Parsing:Given a parsing modelMand a sentenceS, derive the optimal dependency graphDforSaccording toM.\n\nfilename: materials_formatted.md\nextension: .md\n\n(Pre-condition: buffer has to be non-empty.)  2. Left-Arcr: Add a dependency arc(wj,r,wi)to the arc setA, wherewiis the word second to the top of the stack andwjis the   ## dependency parsing 3   word at the top of the stack. Removewifrom the stack. (Pre- condition: the stack needs to contain at least two items andwi cannot be the ROOT.)  3. Right-Arcr: Add a dependency arc(wi,r,wj)to the arc setA, wherewiis the word second to the top of the stack andwjis the word at the top of the stack. Removewjfrom the stack. (Pre- condition: The stack needs to contain at least two items.)  A more formal definition of these three transitions is presented in Figure 2.  ### 1. 4 Neural Dependency Parsing  While there are many deep models for dependency parsing, this section focuses specifically on greedy, transition-based neural de- pendency parsers. This class of model has demonstrated compara- ble performance and significantly better efficiency than traditional feature-based discriminative dependency parsers. The primary dis- tinction from previous models is the reliance on dense rather than sparse feature representations. The model we will describe employs the arc-standard system for transitions, as presented in section 1. 3. Ultimately, the aim of the model is to predict a transition sequence from some initial con- figurationcto a terminal configuration, in which the dependency parse tree is encoded. As the model is greedy, it attempts to correctly predict one transitionT \u2208 {shift, Left-Arcr, Right-Arcr}at a time, based on features extracted from the current configuration c= ( _\u03c3_ , _\u03b2_ ,A). Recall, _\u03c3_ is the stack, _\u03b2_ the buffer, andAthe set of dependency arcs for a given sentence.   Feature Selection:  Depending on the desired complexity of the model, there is flexi- bility in defining the input to the neural network. The features for a given sentenceSgenerally include some subset of:  1. Sword: Vector representations for some of the words inS(and their dependents) at the top of the stack _\u03c3_ and buffer _\u03b2_.\n\nfilename: materials_formatted.md\nextension: .md\n\ntion to these basic features, we resolve lexical vari- ation using lexicalized features, and syntactic varia- tion using dependency tree path features.  The multiclass log-likelihood loss is optimized using AdaGrad with an initial learning rate of 0.1. Each update is performed on the batch of all ques- tions in a paragraph for ef\ufb01ciency, since they share the same candidates. L2 regularization is used, with a coef\ufb01cient of 0.1 divided by the number of batches. The model is trained with three passes over the train-  012345678Syntactic divergence0.05.010.015.020.025.030.0PercentageFeature Groups  Description  Matching Word Frequencies  Matching Bigram Frequencies  Root Match  Lengths  Span Word Frequencies Constituent Label  Sum of the TF-IDF of the words that occur in both the question and the sentence containing the candidate answer. Separate features are used for the words to the left, to the right, inside the span, and in the whole sentence. Same as above, but using bigrams. We use the generalization of the TF-IDF described in Shirakawa et al. (2015).  Whether the dependency parse tree roots of the question and sentence match, whether the sentence contains the root of the dependency parse tree of the question, and whether the question contains the root of the dependency parse tree of the sentence. Number of words to the left, to the right, inside the span, and in the whole sentence.  Sum of the TF-IDF of the words in the span, regardless of whether they appear in the question. Constituency parse tree label of the span, optionally combined with the wh-word in the question.  Span POS Tags  Sequence of the part-of-speech tags in the span, optionally combined with the wh-word in the question.  Lexicalized  Dependency Tree Paths  Lemmas of question words combined with the lemmas of words within distance 2 to the span in the sentence based on the dependency parse trees. Separately, question word lemmas combined with answer word lemmas.  For each word that occurs in both the question and sentence, the path in the dependency parse tree from that word in the sentence to the span, optionally combined with the path from the wh-word to the word in the question.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nHow does neural dependency parsing differ from previous models?[/INST] \nAnswer: Neural dependency parsing differs from previous models mainly in its reliance on dense feature representations rather than sparse ones. This class of model has shown comparable performance and significantly better efficiency than traditional feature-based discriminative dependency parsers. The model employs the arc-standard system for transitions and aims to predict a transition sequence from an initial configuration to a terminal configuration, where the dependency parse tree is encoded. The model is greedy, meaning it tries to predict one transition at a time based on features extracted from the current configuration. The features for a given sentence generally include vector representations for some of the words and their dependents at the top of the stack and buffer."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nWe evaluate the Transformer with our relative attention mechanism on two datasets, JSB Chorales and Piano-e-Competition, and obtain state-of-the-art results on the latter.  1  INTRODUCTION  A musical piece often consists of recurring elements at various levels, from motifs to phrases to sections such as verse-chorus. To generate a coherent piece, a model needs to reference elements that came before, sometimes in the distant past, repeating, varying, and further developing them to create contrast and surprise. Intuitively, self-attention (Parikh et al., 2016) appears to be a good match for this task. Self-attention over its own previous outputs allows an autoregressive model to access any part of the previously generated output at every step of generation. By contrast, recurrent neural networks have to learn to proactively store elements to be referenced in a \ufb01xed size state or memory, potentially making training much more dif\ufb01cult. We believe that repeating self-attention in multiple, successive layers of a Transformer decoder (Vaswani et al., 2017) helps capture the multiple levels at which self-referential phenomena exist in music.  In its original formulation, the Transformer relies on absolute position representations, using either positional sinusoids or learned position embeddings that are added to the per-position input repre- sentations. Recurrent and convolutional neural networks instead model position in relative terms: RNNs through their recurrence over the positions in their input, and CNNs by applying kernels that effectively choose which parameters to apply based on the relative position of the covered input representations.  \u2217Google AI Resident. Correspondence to: Cheng-Zhi Anna Huang <annahuang@google.com> 1Samples are available for listening at  https://storage.googleapis.com/music-transformer/index.html  Music has multiple dimensions along which relative differences arguably matter more than their absolute values; the two most prominent are timing and pitch. To capture such pairwise relations between representations, Shaw et al. (2018) introduce a relation-aware version of self-attention which they use successfully to modulate self-attention by the distance between two positions. We extend this approach to capture relative timing and optionally also pitch, which yields improvement in both sample quality and perplexity for JSB Chorales.\n\nfilename: materials_formatted.md\nextension: .md\n\nOur work show that Transformers not only achieve state-of-the-art perplexity on modeling these complex expressive piano performances, but can also generate them at the scale of 60s (~2000 tokens) with remarkable internal consistency. Our relative attention mechanism is essential to the model\u2019s quality. In listening tests (see Section 4.2.3), samples from models with relative self-attention were perceived as more coherent than the baseline Transformer model from Vaswani et al. (2017). Relative attention not only enables Transformers to generate continuations that elaborate on a given motif, but also to generalize and generate in consistent fashion beyond the length it was trained on (see Section 4.2.1). In a seq2seq setup, Transformers can generate accompaniments conditioned on melodies, enabling users to interact with the model.  Algorithmic contributions The space complexity of the relative self attention mechanism in its original formulation (Shaw et al., 2018) made it infeasible to train on sequences of suf\ufb01cient length to capture long-range structure in longer musical compositions. Addressing this we present a crucial algorithmic improvement to the relative self attention mechanism, dramatically reducing its memory requirements from O(L2D) to O(LD). For example, we reduce the memory consumption per layer from 8.5 GB to 4.2 MB (per head from 1.1 GB to 0.52 MB) for a sequence of length L = 2048 and hidden-state size D = 512 (per head Dh = D H = 64, where number of heads is H = 8) (see Table 1), allowing us to use GPUs to train the relative self-attention Transformer on long sequences.  2 RELATED WORK  Sequence models have been the canonical choice for modeling music, from Hidden Markov Models to RNNs and Long Short Term Memory networks (e.g., Eck & Schmidhuber, 2002; Liang, 2016; Oore et al., 2018), to bidirectional LSTMs (e.g., Hadjeres et al., 2017). Successful application of sequential models to polyphonic music often requires serializing the musical score or performance  into a single sequence, for example by interleaving different instruments or voices.\n\nfilename: materials_formatted.md\nextension: .md\n\nAs opposed to the original Transformer, samples from a Transformer with our relative attention mechanism maintain the regular timing grid present in this dataset. The model furthermore captures global timing, giving rise to regular phrases. The original formulation of relative attention (Shaw et al., 2018) requires O(L2D) memory where L is the sequence length and D is the dimension of the model\u2019s hidden state. This is prohibitive for long sequences such as those found in the Piano-e-Competition dataset of human-performed virtuosic, classical piano music. In Section 3.4, we show how to reduce the memory requirements to O(LD), making it practical to apply relative attention to long sequences.  The Piano-e-Competition dataset consists of MIDI recorded from performances of competition partic- ipants, bearing expressive dynamics and timing on the granularity of < 10 miliseconds. Discretizing time on a \ufb01xed grid that would yield unnecessarily long sequences as not all events change on the same timescale. We hence adopt a sparse, MIDI-like, event-based representation from (Oore et al., 2018), allowing a minute of music with 10 milisecond resolution to be represented at lengths around 2K, as opposed to 6K to 18K on a \ufb01xed-grid representation with multiple performance attributes. As position in sequence no longer corresponds to time, a priori it is not obvious that relative attention should work as well with such a representation. However, we will show in Section 4.2 that it does improve perplexity and sample quality over strong baselines.  We speculate that idiomatic piano gestures such as scales, arpeggios and other motifs all exhibit a certain grammar and recur periodically, hence knowing their relative positional distances makes it easier to model this regularity. This inductive bias towards learning relational information, as opposed to patterns based on absolute position, suggests that the Transformers with relative attention could generalize beyond the lengths it was trained on, which our experiments in Section 4.2.1 con\ufb01rm.  1.1 CONTRIBUTIONS  Domain contributions We show the \ufb01rst successful use of Transformers in generating music that exhibits long-term structure. Before our work, LSTMs were used at timescales of 15s (~500 tokens) on the Piano-e-Competition dataset (Oore et al., 2018).\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat are the benefits of the memory-efficient implementation of relative position-based attention in the Music Transformer?[/INST] \nAnswer: The memory-efficient implementation of relative position-based attention in the Music Transformer offers several benefits. Firstly, it allows the model to handle long sequences, which is particularly useful for complex musical compositions. This was previously infeasible due to high memory requirements. The algorithmic improvement reduces the memory requirements from O(L2D) to O(LD), where L is the sequence length and D is the dimension of the model\u2019s hidden state. For instance, the memory consumption per layer is reduced from 8.5 GB to 4.2 MB for a sequence of length L = 2048 and hidden-state size D = 512. This makes it possible to use GPUs to train the relative self-attention Transformer on long sequences. Furthermore, the relative attention mechanism improves the quality of the generated samples and their internal consistency, and allows the model to generalize beyond the lengths it was trained on."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\n2020  We need better measurement.   # Thank you!      \n\n# SEP 775 CodingExercise:  # Final Projects  # Project 1: AI Teaching Assistant  ## Project Overview  The goal of this project is to create an AI Teaching Assistant (AI TA). The AI TA aims to provide an interactive platform for students to ask questions and receive guidance on course materials, mirroring the support traditionally offered by human teaching assistants. The project emphasizes the application of NLP techniques to create an educational tool that enhances the learning experience through dynamic interaction and content understanding.  ## Project Objectives  - Automate FAQ Responses: Develop the AI TA to autonomously address frequently     asked questions related to course logistics, assignments, and content. - Content Understanding and Query Handling: Implement NLP strategies enabling     the AI TA to interpret and respond to inquiries about the uploaded course material     accurately. - Dynamic Interaction: Design the AI TA to interact with students conversationally,     providing information and resources pertinent to their queries.  ## Technical Approach  NLP Engine Components  - Language Model Integration:Leverage pre-trained NLP models (e.g., BERT, Llama)     for robust text understanding and generation, tapping into their vast linguistic knowledge. - Natural Language Understanding (NLU):Employ models like BERT for NLU tasks     to accurately parse student inquiries, determining their intent and extracting relevant     details for precise responses. - Retrieval-Augmented Generation (RAG):Central to the NLP Engine, RAG com-     bines retrieval methods (to access information within a structured database of course     materials) with generative capabilities (to formulate coherent and context-aware respon-     ses). This approach ensures the AI TA delivers informative answers, directly pulling from     course content and synthesizing it into helpful responses. - Response Generation:Utilize the designed AI TA to generate responses that are not     only accurate but also engaging and pedagogically effective.  Interaction Interface  - Simplified Interface Development: Construct an intuitive web or command-line in-     terface that allows straightforward question submission and response retrieval, ensuring     ease of use for student interactions with the AI TA.   # Project 2: TV Show Script Generation  ## Project Overview  For this project, you will harness the power of Natural Language Processing (NLP) and Machine Learning to create a script for a new episode of a TV show you enjoy.\n\nfilename: materials_formatted.md\nextension: .md\n\nInstruction finetuning     + Simple and straightforward, generalize to unseen tasks     - Collecting demonstrations for so many tasks is expensive     - Mismatch between LM objective and human preferences 3. Reinforcement Learning from Human Feedback (RLHF)     + Directly model preferences (cf. language modeling), generalize beyond labeled data     - RL is very tricky to get right     - Human preferences are fallible; _models_ of human preferences even more so 4. What\u2019s next?  76  #### SEP 775 Lecture Plan: From Language Models to Assistants   #### Language models as multitask assistants?  We\u2019ve finally (mostly) answered how we get from this  #### McMaster University is located in  to _this_  77   1. Zero-Shot (ZS) and Few-Shot (FS) In-Context Learning     + No finetuning needed, prompt engineering (e.g. CoT) can improve performance     - Limits to what you can fit in context     - Complex tasks will probably need gradient steps 2. Instruction finetuning     + Simple and straightforward, generalize to unseen tasks     - Collecting demonstrations for so many tasks is expensive     - Mismatch between LM objective and human preferences 3. Reinforcement Learning from Human Feedback (RLHF)     + Directly model preferences (cf. language modeling), generalize beyond labeled data     - RL is very tricky to get right     - Human preferences are fallible; _models_ of human preferences even more so 4. What\u2019s next?  78  #### SEP 775 Lecture Plan: From Language Models to Assistants   #### What\u2019s next?  - RLHF is still a very underexplored and fast-     moving area: by the next lecture ( 2025 )     these slides may look completely different! - RLHF gets you further than instruction     finetuning, but is (still!) data expensive. - Recent work aims to alleviate such data     requirements:  79   #### What\u2019s next?  80  - RLHF is still a very underexplored and fast-     moving area: by the next lecture (2024)     these slides may look completely different! - RLHF gets you further than instruction     finetuning, but is (still!) data expensive.\n\nfilename: materials_formatted.md\nextension: .md\n\nRTE Recognizing Textual Entailment is a bi- nary entailment task similar to MNLI, but with much less training data (Bentivogli et al., 2009).14  WNLI Winograd NLI is a small natural lan- guage inference dataset (Levesque et al., 2011). The GLUE webpage notes that there are issues with the construction of this dataset, 15 and every trained system that\u2019s been submitted to GLUE has performed worse than the 65.1 baseline accuracy of predicting the majority class. We therefore ex- clude this set to be fair to OpenAI GPT. For our GLUE submission, we always predicted the ma-  14Note that we only report single-task \ufb01ne-tuning results in this paper. A multitask \ufb01ne-tuning approach could poten- tially push the performance even further. For example, we did observe substantial improvements on RTE from multi- task training with MNLI.  15https://gluebenchmark.com/faq  BERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphBERTE[CLS]E1 E2 ENCT1 T2 TNSingle Sentence ......BERTTok 1 Tok 2 Tok N...[CLS]E[CLS]E1 E2 ENCT1 T2 TNSingle Sentence B-PEROO......E[CLS]E1 E[SEP]Class Label...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019Start/End SpanClass LabelBERTTok 1 Tok 2 Tok N...[CLS]Tok 1[CLS][CLS]Tok 1 [SEP]...Tok NTok 1...TokMSentence 1...Sentence 2Note that the purpose of the masking strategies is to reduce the mismatch between pre-training and \ufb01ne-tuning, as the [MASK] symbol never ap- pears during the \ufb01ne-tuning stage. We report the Dev results for both MNLI and NER.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat is the primary programming task for a BLEU implementor?[/INST] \nAnswer: I don't know."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\n### 1. 2 Transition-Based Dependency Parsing  Transition-based dependency parsing relies on a state machine which defines the possible transitions to create the mapping from the input sentence to the dependency tree. Thelearning problemis to induce a model which can predict the next transition in the state machine based on the transition history. Theparsing problemis to construct the optimal sequence of transitions for the input sentence, given the previously induced model. Most transition-based systems do not make use of a formal grammar.  ### 1. 3 Greedy Deterministic Transition-Based Parsing  This system was introduced by Nivre in 2003 and was radically dif- ferent from other methods in use at that time. This transition system is a state machine, which consists ofstates andtransitionsbetween those states. The model induces a sequence of transitions from someinitialstate to one of severalterminalstates. States: For any sentenceS=w 0 w 1 ...wn, a state can be described with a triplec= ( _\u03c3_ , _\u03b2_ ,A):  1. a stack _\u03c3_ of wordswifromS,  2. a buffer _\u03b2_ of wordswifromS,  3. a set of dependency arcsAof the form(wi,r,wj), wherewi,wjare fromS, andrdescribes a dependency relation.  It follows that for any sentenceS=w 0 w 1 ...wn,  1. aninitialstatec 0 is of the form([w 0 ] _\u03c3_ ,[w 1 , ...,wn] _\u03b2_ ,\u2205)(only the ROOT is on the stack _\u03c3_ , all other words are in the buffer _\u03b2_ and no actions have been chosen yet),  2. a terminal state has the form( _\u03c3_ ,[] _\u03b2_ ,A).   Transitions: Figure 2 : Transitions for Dependency Parsing.   There are three types of transitions between states:  1. Shift: Remove the first word in the buffer and push it on top of the stack. (Pre-condition: buffer has to be non-empty.)  2.\n\nfilename: materials_formatted.md\nextension: .md\n\n## 1 Dependency Grammar and Dependency Structure  Parse trees in NLP, analogous to those in compilers, are used to ana- lyze the syntactic structure of sentences. There are two main types of structures used - constituency structures and dependency structures. Constituency Grammar uses phrase structure grammar to organize words into nested constituents. This will be covered in more detail in following chapters. We now focus on Dependency Parsing. Dependency structure of sentences shows which words depend on (modify or are arguments of) which other words. These binary asymmetric relations between the words are called dependencies and are depicted as arrows going from the head (or governor, superior, regent) to the dependent (or modifier, inferior, subordinate). Usually these dependencies form a tree structure. They are often typed with the name of grammatical relations (subject, prepositional object, apposition, etc.). An example of such a dependency tree is shown in Figure 1. Sometimes a fake ROOT node is added as the head to the whole tree so that every word is a dependent of exactly one node.   Figure 1 : Dependency tree for the sen- tence \"Bills on ports and immigration were submitted by Senator Brownback, Republican of Kansas\"  ### 1. 1 Dependency Parsing  Dependency parsing is the task of analyzing the syntactic depen- dency structure of a given input sentenceS. The output of a depen- dency parser is a dependency tree where the words of the input sen- tence are connected by typed dependency relations. Formally, the de- pendency parsing problem asks to create a mapping from the input sentence with wordsS=w 0 w 1 ...wn(wherew 0 is the ROOT) to its dependency tree graphG. Many different variations of dependency- based methods have been developed in recent years, including neural network-based methods, which we will describe later. To be precise, there are two subproblems in dependency parsing (adapted from Kuebler et al., chapter 1. 2 ):  1. Learning:Given a training setDof sentences annotated with de- pendency graphs, induce a parsing modelMthat can be used to parse new sentences.   ## dependency parsing 2  2. Parsing:Given a parsing modelMand a sentenceS, derive the optimal dependency graphDforSaccording toM.\n\nfilename: materials_formatted.md\nextension: .md\n\n(Pre-condition: buffer has to be non-empty.)  2. Left-Arcr: Add a dependency arc(wj,r,wi)to the arc setA, wherewiis the word second to the top of the stack andwjis the   ## dependency parsing 3   word at the top of the stack. Removewifrom the stack. (Pre- condition: the stack needs to contain at least two items andwi cannot be the ROOT.)  3. Right-Arcr: Add a dependency arc(wi,r,wj)to the arc setA, wherewiis the word second to the top of the stack andwjis the word at the top of the stack. Removewjfrom the stack. (Pre- condition: The stack needs to contain at least two items.)  A more formal definition of these three transitions is presented in Figure 2.  ### 1. 4 Neural Dependency Parsing  While there are many deep models for dependency parsing, this section focuses specifically on greedy, transition-based neural de- pendency parsers. This class of model has demonstrated compara- ble performance and significantly better efficiency than traditional feature-based discriminative dependency parsers. The primary dis- tinction from previous models is the reliance on dense rather than sparse feature representations. The model we will describe employs the arc-standard system for transitions, as presented in section 1. 3. Ultimately, the aim of the model is to predict a transition sequence from some initial con- figurationcto a terminal configuration, in which the dependency parse tree is encoded. As the model is greedy, it attempts to correctly predict one transitionT \u2208 {shift, Left-Arcr, Right-Arcr}at a time, based on features extracted from the current configuration c= ( _\u03c3_ , _\u03b2_ ,A). Recall, _\u03c3_ is the stack, _\u03b2_ the buffer, andAthe set of dependency arcs for a given sentence.   Feature Selection:  Depending on the desired complexity of the model, there is flexi- bility in defining the input to the neural network. The features for a given sentenceSgenerally include some subset of:  1. Sword: Vector representations for some of the words inS(and their dependents) at the top of the stack _\u03c3_ and buffer _\u03b2_.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat is the primary difference between transition-based and graph-based approaches in dependency parsing?[/INST] \nAnswer: I'm sorry, but the provided context does not contain information about the primary difference between transition-based and graph-based approaches in dependency parsing."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nThe dense representation alone, however, is typically inferior to the sparse one. While not the focus of this work, dense representations from pre- trained models, along with cross-attention mecha- nisms, have also been shown effective in passage or dialogue re-ranking tasks (Nogueira and Cho, 2019; Humeau et al., 2020). Finally, a concurrent work (Khattab and Zaharia, 2020) demonstrates the feasibility of full dense retrieval in IR tasks. Instead of employing the dual-encoder framework, they introduced a late-interaction operator on top of the BERT encoders.  Dense retrieval for open-domain QA has been explored by Das et al. (2019), who propose to re- trieve relevant passages iteratively using reformu- lated question vectors. As an alternative approach that skips passage retrieval, Seo et al. (2019) pro- pose to encode candidate answer phrases as vectors and directly retrieve the answers to the input ques- tions ef\ufb01ciently. Using additional pretraining with the objective that matches surrogates of questions and relevant passages, Lee et al. (2019) jointly train the question encoder and reader. Their approach outperforms the BM25 plus reader paradigm on multiple open-domain QA datasets in QA accuracy, and is further extended by REALM (Guu et al., 2020), which includes tuning the passage encoder asynchronously by re-indexing the passages dur- ing training. The pretraining objective has also recently been improved by Xiong et al. (2020b). In contrast, our model provides a simple and yet  effective solution that shows stronger empirical per- formance, without relying on additional pretraining or complex joint training schemes.  DPR has also been used as an important mod- ule in very recent work. For instance, extending the idea of leveraging hard negatives, Xiong et al. (2020a) use the retrieval model trained in the pre- vious iteration to discover new negatives and con- struct a different set of examples in each training iteration. Starting from our trained DPR model, they show that the retrieval performance can be further improved.\n\nfilename: materials_formatted.md\nextension: .md\n\nStarting from our trained DPR model, they show that the retrieval performance can be further improved. Recent work (Izacard and Grave, 2020; Lewis et al., 2020b) have also shown that DPR can be combined with generation models such as BART (Lewis et al., 2020a) and T5 (Raf- fel et al., 2019), achieving good performance on open-domain QA and other knowledge-intensive tasks.  8 Conclusion  In this work, we demonstrated that dense retrieval can outperform and potentially replace the tradi- tional sparse retrieval component in open-domain question answering. While a simple dual-encoder approach can be made to work surprisingly well, we showed that there are some critical ingredients to training a dense retriever successfully. Moreover, our empirical analysis and ablation studies indicate that more complex model frameworks or similarity functions do not necessarily provide additional val- ues. As a result of improved retrieval performance, we obtained new state-of-the-art results on multiple open-domain question answering benchmarks.  Acknowledgments  We thank the anonymous reviewers for their helpful comments and suggestions.  References  Akari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi, Richard Socher, and Caiming Xiong. 2020. Learn- ing to retrieve reasoning paths over Wikipedia graph for question answering. In International Conference on Learning Representations (ICLR).  Petr Baudi\u02c7s and Jan \u02c7Sediv`y. 2015. Modeling of the question answering task in the yodaqa system. In In- ternational Conference of the Cross-Language Eval- uation Forum for European Languages, pages 222\u2013 228. Springer.  Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on Freebase from  question-answer pairs. In Empirical Methods in Nat- ural Language Processing (EMNLP).  Jane Bromley, Isabelle Guyon, Yann LeCun, Eduard S\u00a8ackinger, and Roopak Shah. 1994. Signature veri\ufb01- cation using a \u201cSiamese\u201d time delay neural network. In NIPS, pages 737\u2013744.\n\nfilename: materials_formatted.md\nextension: .md\n\nArXiv, abs/1909.08593, 2019.  75     \n\n# Dense Passage Retrieval for Open-Domain Question Answering:  Dense Passage Retrieval for Open-Domain Question Answering  Vladimir Karpukhin\u2217, Barlas O\u02d8guz\u2217, Sewon Min\u2020, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen\u2021, Wen-tau Yih  Facebook AI  \u2020University of Washington  \u2021Princeton University  {vladk, barlaso, plewis, ledell, edunov, scottyih}@fb.com sewon@cs.washington.edu danqic@cs.princeton.edu  Abstract  Open-domain question answering relies on ef- \ufb01cient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented us- ing dense representations alone, where em- beddings are learned from a small number of questions and passages by a simple dual- encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene- BM25 system greatly by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.1  1  Introduction  Open-domain question answering (QA) (Voorhees, 1999) is a task that answers factoid questions us- ing a large collection of documents. While early QA systems are often complicated and consist of multiple components (Ferrucci (2012); Moldovan et al. (2003), inter alia), the advances of reading comprehension models suggest a much simpli\ufb01ed two-stage framework: (1) a context retriever \ufb01rst selects a small subset of passages where some of them contain the answer to the question, and then (2) a machine reader can thoroughly exam- ine the retrieved contexts and identify the correct answer (Chen et al., 2017). Although reducing open-domain QA to machine reading is a very rea- sonable strategy, a huge performance degradation is often observed in practice2, indicating the needs of improving retrieval.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nIn what way does the Fusion-in-decoder (FID) model combine dense retrieval with generative models for QA?[/INST] \nAnswer: I'm sorry, but the context provided does not contain information on how the Fusion-in-decoder (FID) model combines dense retrieval with generative models for QA."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\n2013) are publicly available toolkits for building n-gram language models. Modern language modeling is more commonly done withneural networklan- guage models, which solve the major problems with n-grams: the number of param- eters increases exponentially as the n-gram order increases, and n-grams have no way to generalize from training to test set. Neural language models instead project words into acontinuousspace in which words with similar contexts have similar representations. We\u2019ll introduce bothfeedforwardlanguage models (Bengio et al. 2006, Schwenk 2007) in Chapter 7, andrecurrentlanguage models (Mikolov, 2012) in Chapter 9.        \n\n# Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks:   ## Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks   Patrick Lewis\u2020\u2021, Ethan Perez?,   Aleksandra Piktus\u2020, Fabio Petroni\u2020, Vladimir Karpukhin\u2020, Naman Goyal\u2020, Heinrich K\u00fcttler\u2020,   Mike Lewis\u2020, Wen-tau Yih\u2020, Tim Rockt\u00e4schel\u2020\u2021, Sebastian Riedel\u2020\u2021, Douwe Kiela\u2020   \u2020Facebook AI Research;\u2021University College London;?New York University;   plewis@fb.com  ### Abstract   Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on down- stream NLP tasks. However, their ability to access and precisely manipulate knowl- edge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre- trained models with a differentiable access mechanism to explicit non-parametric memory have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) \u2014 models which combine pre-trained parametric and non-parametric mem- ory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever.\n\nfilename: materials_formatted.md\nextension: .md\n\nIn contrast, we explore a setting where both parametric and non-parametric memory components are pre-trained and pre-loaded with extensive knowledge. Crucially, by using pre-trained access mechanisms, the ability to access knowledge is present without additional training.  Our results highlight the benefits of combining parametric and non-parametric memory with genera- tion forknowledge-intensive tasks\u2014tasks that humans could not reasonably be expected to perform without access to an external knowledge source. Our RAG models achieve state-of-the-art results on open Natural Questions [ 29 ], WebQuestions [ 3 ] and CuratedTrec [ 2 ] and strongly outperform recent approaches that use specialised pre-training objectives on TriviaQA [ 24 ]. Despite these being extractive tasks, we find that unconstrained generation outperforms previous extractive approaches. For knowledge-intensive generation, we experiment with MS-MARCO [ 1 ] and Jeopardy question generation, and we find that our models generate responses that are more factual, specific, and diverse than a BART baseline. For FEVER [ 56 ] fact verification, we achieve results within 4.3% of state-of-the-art pipeline models which use strong retrieval supervision. Finally, we demonstrate that the non-parametric memory can be replaced to update the models\u2019 knowledge as the world changes.^1  ### 2 Methods  We explore RAG models, which use the input sequencexto retrieve text documentszand use them as additional context when generating the target sequencey. As shown in Figure 1, our models leverage two components: (i) a retrieverp\u03b7(z|x)with parameters\u03b7that returns (top-K truncated) distributions over text passages given a queryxand (ii) a generatorp\u03b8(yi|x,z,y1:i\u2212 1 )parametrized  (^1) Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform- ers Library [ 66 ] and can be found athttps://github.com/huggingface/transformers/blob/master/ examples/rag/. An interactive demo of RAG models can be found athttps://huggingface.co/rag/   by\u03b8that generates a current token based on a context of the previousi\u2212 1 tokensy1:i\u2212 1 , the original inputxand a retrieved passagez.\n\nfilename: materials_formatted.md\nextension: .md\n\nGPT-2 [ 50 ] later showed that a single, left-to-right, pre-trained language model could achieve strong performance across both discriminative and generative tasks. For further improvement, BART [ 32 ] and T5 [ 51 , 52 ] propose a single, pre-trained encoder-decoder model that leverages bi-directional attention to achieve stronger performance on discriminative and generative tasks. Our work aims to expand the space of possible tasks with a single, unified architecture, by learning a retrieval module to augment pre-trained, generative language models.  Learned Retrieval There is significant work on learning to retrieve documents in information retrieval, more recently with pre-trained, neural language models [ 44 , 26 ] similar to ours. Some work optimizes the retrieval module to aid in a specific, downstream task such as question answering, using search [ 46 ], reinforcement learning [ 6 , 63 , 62 ], or a latent variable approach [ 31 , 20 ] as in our work. These successes leverage different retrieval-based architectures and optimization techniques to achieve strong performance on a single task, while we show that a single retrieval-based architecture can be fine-tuned for strong performance on a variety of tasks.  Memory-based Architectures Our document index can be seen as a large external memory for neural networks to attend to, analogous to memory networks [ 64 , 55 ]. Concurrent work [ 14 ] learns to retrieve a trained embedding for each entity in the input, rather than to retrieve raw text as in our work. Other work improves the ability of dialog models to generate factual text by attending over fact embeddings [ 15 , 13 ]. A key feature of our memory is that it is comprised of raw text rather distributed representations, which makes the memory both (i) human-readable, lending a form of interpretability to our model, and (ii) human-writable, enabling us to dynamically update the model\u2019s memory by editing the document index. This approach has also been used in knowledge-intensive dialog, where generators have been conditioned on retrieved text directly, albeit obtained via TF-IDF rather than end-to-end learnt retrieval [9].  Retrieve-and-Edit approaches Our method shares some similarities with retrieve-and-edit style approaches, where a similar training input-output pair is retrieved for a given input, and then edited to provide a final output.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWho are the authors of 'Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks'?[/INST] \nAnswer: The authors of 'Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks' are Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, Sebastian Riedel, and Douwe Kiela. They are affiliated with Facebook AI Research, University College London, and New York University."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nThere are two key dif- ferences between Hessian-Free and most other second- order algorithms. First, it uses the full Hessian matrix and hence can deal with exploding directions that are not necessarily axis-aligned. Second, it computes a new estimate of the Hessian matrix before each up- date step and can take into account abrupt changes in curvature (such as the ones suggested by our hypothe- sis) while most other approaches use a smoothness as- sumption, i.e., averaging 2nd order signals over many steps.  3. Dealing with the exploding and     vanishing gradient 3.1. Previous solutions Using an L1 or L2 penalty on the recurrent weights can help with exploding gradients. Given that the parame- ters initialized with small values, the spectral radius of Wrecis probably smaller than 1, from which it follows that the gradient can not explode (see necessary condi- tion found in section 2.1). The regularization term can ensure that during training the spectral radius never exceeds 1. This approach limits the model to a sim- ple regime (with a single point attractor at the origin), where any information inserted in the model has to die out exponentially fast in time. In such a regime we can not train a generator network, nor can we exhibit long term memory traces. Doya (1993) proposes to pre-program the model (to initialize the model in the right regime) or to use teacher forcing. The first proposal assumes that if the model exhibits from the beginning the same kind of asymptotic behaviour as the one required by the target, then there is no need to cross a bifurcation boundary. The downside is that one can not always know the required asymptotic behaviour, and, even if such information is known, it is not trivial to initial- ize a model in this specific regime. We should also note that such initialization does not prevent cross- ing the boundary between basins of attraction, which, as shown, could happen even though no bifurcation boundary is crossed. Teacher forcing is a more interesting, yet a not very well understood solution. It can be seen as a way of initializing the model in the right regime and the right   Figure 7 : Gradient explosion clipping visualization  To solve the problem of vanishing gradients, we introduce two tech- niques.\n\nfilename: materials_formatted.md\nextension: .md\n\nThe regularization term can ensure that during training the spectral radius never exceeds 1. This approach limits the model to a sim- ple regime (with a single point attractor at the origin), where any information inserted in the model has to die out exponentially fast in time. In such a regime we can not train a generator network, nor can we exhibit long term memory traces. Doya (1993) proposes to pre-program the model (to initialize the model in the right regime) or to use teacher forcing. The first proposal assumes that if the model exhibits from the beginning the same kind of asymptotic behaviour as the one required by the target, then there is no need to cross a bifurcation boundary. The downside is that one can not always know the required asymptotic behaviour, and, even if such information is known, it is not trivial to initial- ize a model in this specific regime. We should also note that such initialization does not prevent cross- ing the boundary between basins of attraction, which, as shown, could happen even though no bifurcation boundary is crossed. Teacher forcing is a more interesting, yet a not very well understood solution. It can be seen as a way of initializing the model in the right regime and the right   region of space. It has been shown that in practice it can reduce the chance that gradients explode, and even allow training generator models or models that work with unbounded amounts of memory(Pascanu and Jaeger, 2011; Doya and Yoshizawa, 1991). One important downside is that it requires a target to be defined at every time step.  In Hochreiter and Schmidhuber (1997); Graveset al. (2009) a solution is proposed for the vanishing gra- dients problem, where the structure of the model is changed. Specifically it introduces a special set of units called LSTM units which are linear and have a recurrent connection to itself which is fixed to 1. The flow of information into the unit and from the unit is guarded by an input and output gates (their behaviour is learned). There are several variations of this basic structure. This solution does not address explicitly the exploding gradients problem.  Sutskever et al.\n\nfilename: materials_formatted.md\nextension: .md\n\nTherefore we can write Eqn. (18) as,   |C| \u223c |X|\u03b1H|X|,\u03b1. (19)  We are interested in how|X|is related to|C|when both numbers are large; therefore we are free to expand the right hand side of the equation for large |X|. For this purpose we use the expansion of gen- eralized harmonic numbers (Apostol, 1976),   Hx,s=  x^1 \u2212s 1 \u2212s +\u03b6(s) +O(x\u2212s) if s> 0 ,s, 1 , (20) giving,   |C| \u223c  ### |X|   1 \u2212\u03b1 +\u03b6(\u03b1)|X|\u03b1+O(1), (21)  where\u03b6(s) is the Riemann zeta function. In the limit thatXis large, only one of the two terms on the right hand side of Eqn. (21) will be relevant, and which term that is depends on whether\u03b1 >1,  ### |X|=  ### {   O(|C|) if\u03b1 < 1 , O(|C|^1 /\u03b1) if\u03b1 > 1. (22)  For the corpora studied in this article, we observe thatXi jis well-modeled by Eqn. (17) with\u03b1 = 1 .25. In this case we have that|X|=O(|C|^0.^8 ). Therefore we conclude that the complexity of the model is much better than the worst caseO(V^2 ), and in fact it does somewhat better than the on-line window-based methods which scale likeO(|C|).  ## 4 Experiments  4.1 Evaluation methods  We conduct experiments on the word analogy task of Mikolov et al. (2013a), a variety of word similarity tasks, as described in (Luong et al., 2013), and on the CoNLL-2003 shared benchmark   Table 2: Results on the word analogy task, given as percent accuracy. Underlined scores are best within groups of similarly-sized models; bold scores are best overall.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat is the significance of Xavier parameter initialization?[/INST] \nAnswer: I don't know."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nThe largest model achieves a score on the development set in all three conditions that exceeds the best recorded score on the task.  such as the adversarially-mined Winogrande dataset [SBBC19] still signi\ufb01cantly lag human performance. We test GPT-3\u2019s performance on both Winograd and Winogrande, as usual in the zero-, one-, and few-shot setting.  On Winograd we test GPT-3 on the original set of 273 Winograd schemas, using the same \u201cpartial evaluation\u201d method described in [RWC+19]. Note that this setting differs slightly from the WSC task in the SuperGLUE benchmark, which is presented as binary classi\ufb01cation and requires entity extraction to convert to the form described in this section. On Winograd GPT-3 achieves 88.3%, 89.7%, and 88.6% in the zero-shot, one-shot, and few-shot settings, showing no clear in-context learning but in all cases achieving strong results just a few points below state-of-the-art and estimated human performance. We note that contamination analysis found some Winograd schemas in the training data but this appears to have only a small effect on results (see Section 4).  On the more dif\ufb01cult Winogrande dataset, we do \ufb01nd gains to in-context learning: GPT-3 achieves 70.2% in the zero-shot setting, 73.2% in the one-shot setting, and 77.7% in the few-shot setting. For comparison a \ufb01ne-tuned RoBERTA model achieves 79%, state-of-the-art is 84.6% achieved with a \ufb01ne-tuned high capacity model (T5), and human performance on the task as reported by [SBBC19] is 94.0%.  3.5 Common Sense Reasoning  Next we consider three datasets which attempt to capture physical or scienti\ufb01c reasoning, as distinct from sentence completion, reading comprehension, or broad knowledge question answering. The \ufb01rst, PhysicalQA (PIQA) [BZB+19], asks common sense questions about how the physical world works and is intended as a probe of grounded understanding of the world.\n\nfilename: materials_formatted.md\nextension: .md\n\nThis is approaching the performance of a \ufb01ne-tuned RoBERTa baseline (55.9%) from Uni\ufb01edQA [KKS+20]. On the \u201cEasy\u201d version of the dataset (questions which either of the mentioned baseline approaches answered correctly), GPT-3 achieves 68.8%, 71.2%, and 70.1% which slightly exceeds a \ufb01ne-tuned RoBERTa baseline from [KKS+20]. However, both of these results are still much worse than the overall SOTAs achieved by the Uni\ufb01edQA which exceeds GPT-3\u2019s few-shot results by 27% on the challenge set and 22% on the easy set.  On OpenBookQA [MCKS18], GPT-3 improves signi\ufb01cantly from zero to few shot settings but is still over 20 points short of the overall SOTA. GPT-3\u2019s few-shot performance is similar to a \ufb01ne-tuned BERT Large baseline on the leaderboard.  Overall, in-context learning with GPT-3 shows mixed results on commonsense reasoning tasks, with only small and inconsistent gains observed in the one and few-shot learning settings for both PIQA and ARC, but a signi\ufb01cant improvement is observed on OpenBookQA. GPT-3 sets SOTA on the new PIQA dataset in all evaluation settings.  3.6 Reading Comprehension  Next we evaluate GPT-3 on the task of reading comprehension. We use a suite of 5 datasets including abstractive, multiple choice, and span based answer formats in both dialog and single question settings. We observe a wide spread in GPT-3\u2019s performance across these datasets suggestive of varying capability with different answer formats. In general we observe GPT-3 is on par with initial baselines and early results trained using contextual representations on each respective dataset.  GPT-3 performs best (within 3 points of the human baseline) on CoQA [RCM19] a free-form conversational dataset and performs worst (13 F1 below an ELMo baseline) on QuAC [CHI+18] a dataset which requires modeling structured dialog acts and answer span selections of teacher-student interactions.\n\nfilename: materials_formatted.md\nextension: .md\n\nTherefore, we start this section by explicitly de\ufb01ning and contrasting the different settings that we will be evaluating GPT-3 on or could in principle evaluate GPT-3 on. These settings can be seen as lying on a spectrum of how much task-speci\ufb01c data they tend to rely on. Speci\ufb01cally, we can identify at least four points on this spectrum (see Figure 2.1 for an illustration):  \u2022 Fine-Tuning (FT) has been the most common approach in recent years, and involves updating the weights of a pre-trained model by training on a supervised dataset speci\ufb01c to the desired task. Typically thousands to hundreds of thousands of labeled examples are used. The main advantage of \ufb01ne-tuning is strong performance on many benchmarks. The main disadvantages are the need for a new large dataset for every task, the potential for poor generalization out-of-distribution [MPL19], and the potential to exploit spurious features of the training data [GSL+18, NK19], potentially resulting in an unfair comparison with human performance. In this work we do not \ufb01ne-tune GPT-3 because our focus is on task-agnostic performance, but GPT-3 can be \ufb01ne-tuned in principle and this is a promising direction for future work.  \u2022 Few-Shot (FS) is the term we will use in this work to refer to the setting where the model is given a few demonstrations of the task at inference time as conditioning [RWC+19], but no weight updates are allowed. As shown in Figure 2.1, for a typical dataset an example has a context and a desired completion (for example an English sentence and the French translation), and few-shot works by giving K examples of context and completion, and then one \ufb01nal example of context, with the model expected to provide the completion. We typically set K in the range of 10 to 100 as this is how many examples can \ufb01t in the model\u2019s context window (nctx = 2048). The main advantages of few-shot are a major reduction in the need for task-speci\ufb01c data and reduced potential to learn an overly narrow distribution from a large but narrow \ufb01ne-tuning dataset.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat challenges do GPT-3 face according to its performance on Winograd-style tasks and common sense reasoning?[/INST] \nAnswer: GPT-3 faces several challenges in its performance on Winograd-style tasks and common sense reasoning. For the Winograd schemas, GPT-3 shows no clear in-context learning in the zero-shot, one-shot, and few-shot settings, although it achieves strong results. However, these results are still a few points below the state-of-the-art and estimated human performance. On the more difficult Winogrande dataset, GPT-3 does show gains in in-context learning, but still lags behind a fine-tuned RoBERTa model, a high capacity model (T5), and human performance.\n\nIn terms of common sense reasoning, GPT-3's in-context learning shows mixed results. It only has small and inconsistent gains in the one and few-shot learning settings for both PIQA and ARC. However, it does show significant improvement on OpenBookQA. Despite these improvements, GPT-3's performance is still over 20 points short of the overall state-of-the-art on OpenBookQA and much worse than the overall state-of-the-art results achieved by the UnifiedQA on the PIQA dataset."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nWe experiment with three values ofk: 1 , 5 , 15.   Context Distribution Smoothing ( cds ) In word2vec, negative examples (contexts) are sampled according to asmoothedunigram dis- tribution. In order to smooth the original con- texts\u2019 distribution, all context counts are raised to the power of\u03b1(Mikolov et al. (2013b) found \u03b1= 0. 75 to work well). This smoothing varia- tion has an analog when calculating PMI directly:   PMI\u03b1(w,c) = log   P\u02c6(w,c) P\u02c6(w)P\u02c6\u03b1(c) (3)   P\u02c6\u03b1(c) =  (c)   \u03b1 \u2211 c (c)   \u03b1   Like other smoothing techniques (Pantel and Lin, 2002; Turney and Littman, 2003), context distri- bution smoothing alleviates PMI\u2019s bias towards rare words. It does so by enlarging the probability of sampling a rare context (sinceP\u02c6\u03b1(c)>P\u02c6(c) whencis infrequent), which in turn reduces the PMI of(w,c)for anywco-occurring with the rare contextc. In Section 6.2 we demonstrate that this novel variant of PMI is very effective, and consis- tently improves performance across tasks, meth- ods, and configurations. We experiment with two values of\u03b1: 1 (unsmoothed) and 0. 75 (smoothed).   3.3 Post-processing Hyperparameters We present three hyperparameters that modify the algorithms\u2019 output: the word vectors.   Adding Context Vectors ( w+c ) Pennington et al. (2014) propose using the context vectors in ad- dition to the word vectors as GloVe\u2019s output. For example, the word \u201ccat\u201d can be represented as:   ~vcat=~wcat+~ccat   where~wand~care the word and context embed- dings, respectively. This vector combination was originally moti- vated as an ensemble method. Here, we provide a different interpretation of its effect on the co- sine similarity function. Specifically, we show   that adding context vectors effectively adds first- order similarity terms to the second-order similar- ity function.\n\nfilename: materials_formatted.md\nextension: .md\n\nGiven a number of context words around a target wordw, these models formulate the embedding task as that of finding a representa- tion that is good at predictingwfrom the context representations. The second group of models, Hellinger PCA (Lebret and Collobert, 2014), GloVe (Pennington et al., 2014), TSCCA (Dhillon et al., 2012) and Sparse Random Projections (Li et al., 2006) fol- low a reconstruction approach: word embeddings should be able to capture as much relevant infor- mation from the original co-occurrence matrix as possible. Training corpus.We tried to make the compar- ison as fair as possible. As the C&W embeddings were only available pretrained on a November 2007 snapshot of Wikipedia, we chose the closest available Wikipedia dump (2008-03-01) for train- ing the other models. We tokenized the data us- ing the Stanford tokenizer (Manning et al., 2014). Like Collobert et al. (2011), we lower-cased all words and replaced digits with zeros. Details.All models embedded words into a 50- dimensional space (D= 50). As implemented, each method uses a different vocabulary, so we computed the intersection of the six vocabularies and used the resulting set of 103,647 words for all nearest-neighbor experiments.  ## 3 Relatedness   We begin with intrinsic evaluation of relatedness using both pre-collected human evaluations and a novel online user study. Section 3.1 introduces the list of datasets that is commonly used as a bench- mark for embedding methods. There, embeddings are evaluated individually and only their final scores are compared, hence we refer to this sce- nario asabsolute intrinsicevaluation. We present a new scenario,comparative intrinsicevaluation, in which we ask people directly for their prefer- ences among different embeddings. We demon- strate that we can achieve the same results as of- fline, absolute metrics using online, comparative metrics.   3.1 Absolute intrinsic evaluation  For the absolute intrinsic evaluation, we used the same datasets and tasks as Baroni et al. (2014). While we present results on all tasks for complete- ness, we will mainly focus on relatedness in this section.\n\nfilename: materials_formatted.md\nextension: .md\n\nWhile the focus of the presentation in the word- embedding literature is on the mathematical model and the objective being optimized, other factors af- fect the results as well. In particular, embedding algorithms suggest some naturalhyperparameters that can be tuned; many of which were already tuned to some extent by the algorithms\u2019 design- ers. Some hyperparameters, such as the number of negative samples to use, are clearly marked as tunable. Other modifications, such as smoothing the negative-sampling distribution, are reported in passing and considered thereafter as part of the al- gorithm. Others still, such as dynamically-sized context windows, are not even mentioned in some of the papers, but are part of the canonical imple- mentation. All of these modifications and system design choices, which we collectively denote as hyperparameters, are part of the final algorithm, and, as we show, have a substantial impact on per- formance.  ### 211   Transactions of the Association for Computational Linguistics, vol. 3, pp. 211\u2013225, 2015. Action Editor: Patrick Pantel. Submission batch: 1/2015; Revision batch 3/2015; Published 5/2015. \u00a9c2015 Association for Computational Linguistics. Distributed under a CC-BY-NC-SA 4.0 license.   In this work, we make these hyperparameters explicit, and show how they can beadapted and transferredinto the traditional count-based ap- proach. To asses how each hyperparameter con- tributes to the algorithms\u2019 performance, we con- duct a comprehensive set of experiments and com- pare four different representation methods, while controlling for the various hyperparameters. Once adapted across methods, hyperparameter tuning significantly improves performance in ev- ery task. In many cases, changing the setting of a single hyperparameter yields a greater increase in performance than switching to a better algorithm or training on a larger corpus. In particular,word2vec\u2019s smoothing of the negative sampling distribution can be adapted to PPMI-based methods by introducing a novel, smoothed variant of the PMI association measure (see Section 3.2). Using this variant increases per- formance by over 3 points per task, on average.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat is the significance of context distribution smoothing in word embeddings according to the document?[/INST] \nAnswer: Context Distribution Smoothing (CDS) is significant in word embeddings as it helps alleviate the bias of Pointwise Mutual Information (PMI) towards rare words. It does this by increasing the probability of sampling a rare context, which in turn reduces the PMI of a word co-occurring with the rare context. This novel variant of PMI has been found to be very effective and consistently improves performance across tasks, methods, and configurations. The document mentions that the authors experimented with two values of alpha (\u03b1): 1 (unsmoothed) and 0.75 (smoothed) for CDS."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nSingle and Multi denote that our Dense Passage Retriever (DPR) was trained using individial or combined training datasets (all the datasets excluding SQuAD). See text for more details.  traditional retrieval methods, the effects of different training schemes and the run-time ef\ufb01ciency.  The DPR model used in our main experiments is trained using the in-batch negative setting (Sec- tion 3.2) with a batch size of 128 and one additional BM25 negative passage per question. We trained the question and passage encoders for up to 40 epochs for large datasets (NQ, TriviaQA, SQuAD) and 100 epochs for small datasets (TREC, WQ), with a learning rate of 10\u22125 using Adam, linear scheduling with warm-up and dropout rate 0.1.  While it is good to have the \ufb02exibility to adapt the retriever to each dataset, it would also be de- sirable to obtain a single retriever that works well across the board. To this end, we train a multi- dataset encoder by combining training data from all datasets excluding SQuAD.8 In addition to DPR, we also present the results of BM25, the traditional retrieval method9 and BM25+DPR, using a linear combination of their scores as the new ranking function. Speci\ufb01cally, we obtain two initial sets of top-2000 passages based on BM25 and DPR, respectively, and rerank the union of them using BM25(q,p) + \u03bb \u00b7 sim(q, p) as the ranking function. We used \u03bb = 1.1 based on the retrieval accuracy in the development set.  5.1 Main Results  Table 2 compares different passage retrieval sys- tems on \ufb01ve QA datasets, using the top-k accuracy (k \u2208 {20, 100}). With the exception of SQuAD, DPR performs consistently better than BM25 on all datasets. The gap is especially large when k is small (e.g., 78.4% vs. 59.1% for top-20 accuracy on Natural Questions). When training with mul-  8SQuAD is limited to a small set of Wikipedia documents and thus introduces unwanted bias. We will discuss this issue more in Section 5.1.  9Lucene implementation.\n\nfilename: materials_formatted.md\nextension: .md\n\nOur experiments show that using triplet loss does not affect the results much. More details can be found in Appendix B.  Cross-dataset generalization One interesting question regarding DPR\u2019s discriminative training is how much performance degradation it may suf- fer from a non-iid setting. In other words, can it still generalize well when directly applied to a different dataset without additional \ufb01ne-tuning? To test the cross-dataset generalization, we train DPR on Natural Questions only and test it directly on the smaller WebQuestions and CuratedTREC datasets. We \ufb01nd that DPR generalizes well, with 3-5 points loss from the best performing \ufb01ne-tuned model in top-20 retrieval accuracy (69.9/86.3 vs. 75.0/89.1 for WebQuestions and TREC, respec- tively), while still greatly outperforming the BM25 baseline (55.0/70.9).  5.3 Qualitative Analysis  Although DPR performs better than BM25 in gen- eral, passages retrieved by these two methods dif- fer qualitatively. Term-matching methods like BM25 are sensitive to highly selective keywords and phrases, while DPR captures lexical variations or semantic relationships better. See Appendix C for examples and more discussion.  5.4 Run-time Ef\ufb01ciency  The main reason that we require a retrieval compo- nent for open-domain QA is to reduce the number of candidate passages that the reader needs to con- sider, which is crucial for answering user\u2019s ques- tions in real-time. We pro\ufb01led the passage retrieval speed on a server with Intel Xeon CPU E5-2698 v4 @ 2.20GHz and 512GB memory. With the help of FAISS in-memory index for real-valued vectors10, DPR can be made incredibly ef\ufb01cient, processing 995.0 questions per second, returning top 100 pas- sages per question. In contrast, BM25/Lucene (im- plemented in Java, using \ufb01le index) processes 23.7 questions per second per CPU thread.  On the other hand, the time required for building an index for dense vectors is much longer.\n\nfilename: materials_formatted.md\nextension: .md\n\nsee that higher retriever accuracy typically leads to better \ufb01nal QA results: in all cases except SQuAD, answers extracted from the passages retrieved by DPR are more likely to be correct, compared to those from BM25. For large datasets like NQ and TriviaQA, models trained using multiple datasets (Multi) perform comparably to those trained using the individual training set (Single). Conversely, on smaller datasets like WQ and TREC, the multi- dataset setting has a clear advantage. Overall, our DPR-based models outperform the previous state- of-the-art results on four out of the \ufb01ve datasets, with 1% to 12% absolute differences in exact match accuracy. It is interesting to contrast our results to those of ORQA (Lee et al., 2019) and also the concurrently developed approach, REALM (Guu et al., 2020). While both methods include addi- tional pretraining tasks and employ an expensive end-to-end training regime, DPR manages to out- perform them on both NQ and TriviaQA, simply by focusing on learning a strong passage retrieval model using pairs of questions and answers. The additional pretraining tasks are likely more useful only when the target training sets are small. Al- though the results of DPR on WQ and TREC in the single-dataset setting are less competitive, adding more question\u2013answer pairs helps boost the perfor- mance, achieving the new state of the art.  To compare our pipeline training approach with joint learning, we run an ablation on Natural Ques- tions where the retriever and reader are jointly  trained, following Lee et al. (2019). This approach obtains a score of 39.8 EM, which suggests that our strategy of training a strong retriever and reader in isolation can leverage effectively available supervi- sion, while outperforming a comparable joint train- ing approach with a simpler design (Appendix D). One thing worth noticing is that our reader does consider more passages compared to ORQA, al- though it is not completely clear how much more time it takes for inference. While DPR processes up to 100 passages for each question, the reader is able to \ufb01t all of them into one batch on a sin- gle 32GB GPU, thus the latency remains almost identical to the single passage case (around 20ms).\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat are the key components of the training data used for DPR?[/INST] \nAnswer: The key components of the training data used for Dense Passage Retriever (DPR) include individual or combined training datasets, excluding SQuAD. The model is trained using the in-batch negative setting with a batch size of 128 and one additional BM25 negative passage per question. The question and passage encoders are trained for up to 40 epochs for large datasets (NQ, TriviaQA, SQuAD) and 100 epochs for small datasets (TREC, WQ). The learning rate used is 10\u22125 using Adam, linear scheduling with warm-up, and a dropout rate of 0.1."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nWhile conceptually simple, our generative models signi\ufb01cantly outperform the current state of the art in image generation on ImageNet, im- proving the best published negative log-likelihood on ImageNet from 3.83 to 3.77. We also present results on image super-resolution with a large magni\ufb01cation ratio, applying an encoder-decoder con\ufb01guration of our architecture. In a human eval- uation study, we \ufb01nd that images generated by our super-resolution model fool human observers three times more often than the previous state of the art.  1. Introduction  Recent advances in modeling the distribution of natural images with neural networks allow them to generate increas- ingly natural-looking images. Some models, such as the PixelRNN and PixelCNN (van den Oord et al., 2016a), have  *Equal contribution. Ordered by coin \ufb02ip.  1Google Brain, Mountain View, USA 2Department of Electrical Engineering and Computer Sciences, University of California, Berkeley 3Work done during an internship at Google Brain 4Google AI, Mountain View, USA. Correspondence to: Ashish Vaswani, Niki Parmar, Jakob Uszkoreit <avaswani@google.com, nikip@google.com, usz@google.com>.  Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).  Table 1. Three outputs of a CelebA super-resolution model fol- lowed by three image completions by a conditional CIFAR-10 model, with input, model output and the original from left to right  a tractable likelihood. Beyond licensing the comparatively simple and stable training regime of directly maximizing log-likelihood, this enables the straightforward application of these models in problems such as image compression (van den Oord & Schrauwen, 2014) and probabilistic plan- ning and exploration (Bellemare et al., 2016).  The likelihood is made tractable by modeling the joint dis- tribution of the pixels in the image as the product of condi- tional distributions (Larochelle & Murray, 2011; Theis & Bethge, 2015).\n\nfilename: materials_formatted.md\nextension: .md\n\nModel Type ResNet srez GAN PixelRecursive (Dahl et al., 2017)  1D local Image Transformer  2D local Image Transformer  \u03c4 n/a n/a 1.0 0.9 0.8 1.0 0.9 0.8 1.0 0.9 0.8  %Fooled 4.0 8.5 11.0 10.4 10.2 29.6 \u00b1 4.0 33.5 \u00b1 3.5 35.94 \u00b1 3.0 30.64 \u00b1 4 34 \u00b1 3.5 36.11 \u00b1 2.5  Turk where each worker is required to make a binary choice when shown one generated and one real image. Following the same procedure for the evaluation study as (Dahl et al., 2017), we show 50 pairs of images, selected randomly from the validation set, to 50 workers each. Each generated and original image is upscaled to 128 \u00d7 128 pixels using the Bilinear interpolation method. Each worker then has 1-2 seconds to make a choice between these two images. In our method, workers choose images from our model up to 36.1% of the time, a signi\ufb01cant improvement over pre- vious models. Sampling temperature of 0.8 and 2D local attention maximized perceptual quality as measured by this evaluation.  To measure how well the high resolution samples correspond to the low resolution input, we calculate Consistency, the L2 distance between the low resolution input and a bicubic downsampled version of the high resolution sample. We observe a Consistency score of 0.01 which is on par with the models in (Dahl et al., 2017).  We quantify that our models are more effective than exem- plar based Super Resolution techniques like Nearest Neigh- bors, which perform a naive look-up of the training data to \ufb01nd the high resolution output. We take a bicubic down- sampled version of our high resolution sample, \ufb01nd the nearest low resolution input image in the training data for that sample, and calculate the MS-SSIM score between the high resolution sample and the corresponding high reso- lution image in the training data.\n\nfilename: materials_formatted.md\nextension: .md\n\nOn average, we get a MS-SSIM score of 44.3, on 50 samples from the validation set, which shows that our models don\u2019t merely learn to copy training images but generate high-quality images by adding synthesized details on the low resolution input image.  Image Transformer  Input  1D Local Attention \u03c4 = 0.9  \u03c4 = 0.8  \u03c4 = 1.0  \u03c4 = 0.8  2D Local Attention \u03c4 = 0.9  \u03c4 = 1.0  Original  Table 6. Images from our 1D and 2D local attention super-resolution models trained on CelebA, sampled with different temperatures. 2D local attention with \u03c4 = 0.9 scored highest in our human evaluation study.  CIFAR-10 We also trained a super-resolution model on the CIFAR-10 data set. Our model reached a negative log- likelihood of 2.76 using 1D local attention and 2.78 using 2D local attention on the test set. As seen in Figure 2, our model commonly generates plausible looking objects even though the input images seem to barely show any discernible structure beyond coarse shapes.  previously proposed (Mansimov et al., 2015), and tasks combining modalities such as language-driven editing of images.  Fundamentally, we aim to move beyond still images to video (Kalchbrenner et al., 2016) and towards applications in model-based reinforcement learning.  6. Conclusion  In this work we demonstrate that models based on self- attention can operate effectively on modalities other than text, and through local self-attention scale to signi\ufb01cantly larger structures than sentences. With fewer layers, its larger receptive \ufb01elds allow the Image Transformer to signi\ufb01cantly improve over the state of the art in unconditional, probabilis- tic image modeling of comparatively complex images from ImageNet as well as super-resolution.  We further hope to have provided additional evidence that even in the light of generative adversarial networks, likelihood-based models of images is very much a promising area for further research - as is using network architectures such as the Image Transformer in GANs.  References  Ba, Jimmy Lei, Kiros, Jamie Ryan, and Hinton, Geoffrey E. Layer normalization.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat improvements does the Image Transformer introduce for image super-resolution tasks?[/INST] \nAnswer: The Image Transformer introduces several improvements for image super-resolution tasks. It significantly outperforms the current state of the art in image generation on ImageNet, improving the best published negative log-likelihood from 3.83 to 3.77. In a human evaluation study, images generated by the super-resolution model of the Image Transformer fooled human observers three times more often than the previous state of the art. The Image Transformer also achieved a Consistency score of 0.01, which is on par with previous models, indicating that the high resolution samples correspond well to the low resolution input. Furthermore, the Image Transformer's models proved to be more effective than exemplar-based Super Resolution techniques like Nearest Neighbors. The models do not merely copy training images but generate high-quality images by adding synthesized details on the low resolution input image."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nCHANGES TO V2 FROM V1  \u2022 We updated the terminology to \u201cdatasets\u201d and \u201ctask clusters.\u201d  \u2022 We renamed the previous \u201copen-domain QA\u201d task cluster to \u201cclosed-book QA.\u201d  \u2022 We extended the related work section and moved it to the Appendix D, using a shorter  version in the main body.  \u2022 We added FLAN and LaMDA-PT results for additional datasets for which GPT-3 results  were not reported.  \u2022 For TriviaQA, v1 reported results on the tfds dev set of 11,313 examples. GPT-3 actually evaluates on the wikipedia dev set of 7,993 examples, so we ran an additional evaluation on that dev set in order to compare with GPT-3\u2019s performance. Zero-shot FLAN now beats zero-shot GPT-3 on that task (and therefore on 20 of 25 tasks). We still show the original result in Table 2, though there is no GPT-3 result to compare with.  \u2022 We moved commonsense reasoning and coreference resolution from the main body to the  Appendix.  \u2022 We moved prompt tuning from the main body to \u00a74.5.  \u2022 We added data contamination analysis (Appendix C). \u2022 We added few-shot instruction tuning (\u00a74.4).  \u2022 We cited additional datasets in Appendix G.  \u2022 The number of tokens used in pretraining was corrected from 2.81T to 2.49T tokens.  35  Published as a conference paper at ICLR 2022  G TASKS AND DATASETS  This appendix further details the datasets that we use in this paper. We group datasets into one of the following task clusters:  \u2022 Natural language inference concerns how two sentences relate, typically asking, given a \ufb01rst sentence, whether a second sentence is true, false, or possibly true. We use the following datasets:  1. ANLI (Nie et al., 2020) 2. CB (De Marneffe et al., 2019) 3. MNLI (Williams et al., 2018) 4. QNLI (Rajpurkar et al., 2018) 5. SNLI (Bowman et al., 2015) 6.\n\nfilename: materials_formatted.md\nextension: .md\n\n- Adherence to Guidelines:Submissions should follow the provided guidelines, including     format, labeling, and adherence to the deadline.   \n\n# SEP 775 Assignment 3:   McMaster University SEP 775  # Assignment 3  # TANDA Approach for QA System Enhancement  ## Objectives:  Explore TANDA (Transfer And Adapt) methodology to improve Question-Answering (QA) systems using pre-trained Transformer models, focusing on sequential fine-tuning te- chniques.  1. Introduction and Theory (15%)     - Summarize the TANDA methodology\u2019s principles, focusing on its novelty and the        rationale behind sequential fine-tuning.     - Discuss how Transformers\u2019 architecture benefits the TANDA approach. 2. Preparation and Dataset Understanding (15%)     - Select the ASNQ dataset for the transfer learning step and either WikiQA or TREC-        QA for domain-specific adaptation.     - Conduct an exploratory data analysis (EDA) on your chosen datasets to understand        their structure, content, and challenges. 3. Model Implementation (30%)     - Implement a baseline Transformer model for comparison.     - Apply the TANDA two-step fine-tuning process:        - First, fine-tune on the ASNQ dataset.        - Then, adapt the model to your chosen domain-specific dataset.     - Document the modifications made to the original Transformer architecture, if any,        and justify your choices. 4. Experimental Setup and Evaluation (20%)     - Describe your experimental setup, including training details (e.g., hyperparameters,        loss functions, optimization strategy).     - Evaluate both the baseline and TANDA-enhanced models using appropriate metrics        (accuracy, F1 score, etc.).     - Analyze the performance impact of the TANDA approach compared to the baseline        model. 5. Discussion and Conclusion (20%)     - Discuss any challenges encountered during implementation and how they were ad-        dressed.     - Reflect on the effectiveness of the TANDA approach for QA systems and potential        areas for improvement.     - Propose future research directions or applications of the TANDA methodology in        other NLP tasks.  McMaster University 1    McMaster University SEP 775  # Submission Guidelines:  - Submit all code in a ZIP file, including Jupyter Notebooks, and a detailed PDF report     with written explanations and visualizations.\n\nfilename: materials_formatted.md\nextension: .md\n\n- Clearly label each part and question in your submissions. - Deadline: March 13, 2024  # Evaluation Criteria:  - Depth and clarity of theoretical understanding and literature review. - Comprehensive analysis and insightful observations in EDA. - Correctness, creativity, and efficiency in model implementation and adaptation of TANDA. - Thoroughness in experimental evaluation, including a detailed comparison of models. - Quality of discussion on findings, challenges, and future work.  # References:  - TANDA original paper:https://arxiv.org/abs/1911. - TANDA code repository:https://github.com/amazon-science/wqa_tanda - ASNQ dataset: https://huggingface.co/datasets/asnq - WikiQA dataset:https://huggingface.co/datasets/wiki_qa - Trec-QA dataset: https://huggingface.co/datasets/lucadiliello/trecqa  McMaster University 2   \n\n# SEP 775 Assignment 4:   McMaster University SEP 775 # Assignment 4  # Fine-tuning a Pretrained Model Using LoRA  # Objectives:  Leverage LoRA: Low-Rank Adaptation to fine-tune a pretrained language model for a programming-related Question-Answering (QA) system on the\"flytech/python-codes-25k\" dataset.  ## Understanding LoRA (20%):  1. Review the concept, benefits, and mechanism of Low-Rank Adaptation (LoRA) for adap-     ting pretrained models. 2. Discuss the suitability of pretrained language models for code-related QA tasks and the     advantages of using LoRA for fine-tuning.  ## Dataset Preparation (20%):  1. Provide an overview of the \"flytech/python-codes-25k\"dataset, focusing on its structure     and relevance for a QA system. 2. Describe necessary preprocessing steps, including tokenization and encoding strategies for     code snippets.  ## Model Fine-Tuning with LoRA (30%):  1. Select a suitable pretrained language model and justify the choice based on its architecture     and expected performance on code-related QA tasks. 2. Detail the integration of LoRA, specifying the adaptation process and adjustments made     to the model for the QA task.  ## Training and Evaluation (30%):  1.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nIn SEP775 Assignment 3, which datasets are candidates for the domain-specific adaptation step?[/INST] \nAnswer: The candidates for the domain-specific adaptation step in SEP775 Assignment 3 are either the WikiQA or TREC-QA datasets."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nFLAN\u2019s zero-shot also outperforms 175B-parameter GPT-3\u2019s zero-shot on 20 of 25 datasets that we evaluate, and even outperforms GPT-3\u2019s few-shot by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. In ablation studies, we \ufb01nd that increasing the number of task clusters in instruction tuning improves performance on unseen tasks and that the bene\ufb01ts of instruction tuning emerge only with suf\ufb01cient model scale.  Instruction tuning is a simple method that, as depicted in Figure 2, combines appealing aspects of both the pretrain\u2013\ufb01netune and prompting paradigms by using supervision via \ufb01netuning to improve language model\u2019s responses to inference-time text interactions. Our empirical results demonstrate promising abilities of language models to perform tasks described purely via instructions. Source code for loading the instruction tuning dataset used for FLAN is publicly available at https://github.com/google-research/flan.  Figure 2: Comparing instruction tuning with pretrain\u2013\ufb01netune and prompting.  2 FLAN: INSTRUCTION TUNING IMPROVES ZERO-SHOT LEARNING  The motivation of instruction tuning is to improve the ability of language models to respond to NLP instructions. The idea is that by using supervision to teach an LM to perform tasks described via instructions, the LM will learn to follow instructions and do so even for unseen tasks. To evaluate performance on unseen tasks, we group datasets into clusters by task type and hold out each task cluster for evaluation while instruction tuning on all remaining clusters.\n\nfilename: materials_formatted.md\nextension: .md\n\nWe also evaluate FLAN\u2019s performance on machine translation for the three datasets evaluated in the GPT-3 paper: French\u2013English from WMT\u201914 (Bojar et al., 2014), and German\u2013  5  Natural language inferenceReading comprehensionClosed-book QA010080604020Zero-shot performanceTQANQARC-cARC-eOBQARTECBANLI R3ANLI R2ANLI R1MultiRCBoolQTranslationEN to FREN to DEFR to ENDE to ENEN to RORO to ENLaMDA-PT137BFLAN 137BGPT-3 175BGLaM 64B/64ESupervised modelPublished as a conference paper at ICLR 2022  English and Romanian\u2013English from WMT\u201916 (Bojar et al., 2016). Compared with GPT-3, FLAN outperforms zero-shot GPT-3 for all six evaluations, though it underperforms few-shot GPT-3 in most cases. Similar to GPT-3, FLAN shows strong results for translating into English and compares favorably against supervised translation baselines. Translating from English into other languages, however, was relatively weaker, as might be expected given that FLAN uses an English sentencepiece tokenizer and that the majority of pretraining data is English.  Additional tasks. Although we see strong results for the above task clusters, one limitation with instruction tuning is that it does not improve performance for many language modeling tasks (e.g., commonsense reasoning or coreference resolution tasks formulated as sentence completions). For seven commonsense reasoning and coreference resolution tasks (see Table 2 in the Appendix), FLAN only outperforms LaMDA-PT on three of the seven tasks. This negative result indicates that when the downstream task is the same as the original language modeling pre-training objective (i.e., in cases where instructions are largely redundant), instruction tuning is not useful. Finally, we report results for sentiment analysis, paraphrase detection, and struct-to-text, as well as additional datasets for which GPT-3 results are not available, in Table 2 and Table 1 in the Appendix. Generally, zero-shot FLAN outperforms zero-shot LaMDA-PT and is comparable with or better than few-shot LaMDA-PT.\n\nfilename: materials_formatted.md\nextension: .md\n\nDoes the premise entail the hypothesis? OPTIONS: -yes     -it is not possible to tell      -no            Published as a conference paper at ICLR 2022  1  INTRODUCTION  Language models (LMs) at scale, such as GPT-3 (Brown et al., 2020), have been shown to perform few-shot learning remarkably well. They are less successful at zero-shot learning, however. For example, GPT-3\u2019s zero-shot performance is much worse than few-shot performance on tasks such as reading comprehension, question answering, and natural language inference. One potential reason is that, without few-shot exemplars, it is harder for models to perform well on prompts that are not similar to the format of the pretraining data.  In this paper, we explore a simple method to improve the zero-shot performance of large language models, which would expand their reach to a broader audience. We leverage the intuition that NLP tasks can be described via natural language instructions, such as \u201cIs the sentiment of this movie review positive or negative?\u201d or \u201cTranslate \u2018how are you\u2019 into Chinese.\u201d We take a pretrained language model of 137B parameters and perform instruction tuning\u2014\ufb01netuning the model on a mixture of more than 60 NLP datasets expressed via natural language instructions. We refer to this resulting model as FLAN, for Finetuned Language Net.  To evaluate the zero-shot performance of FLAN on unseen tasks, we group NLP datasets into clusters based on their task types and hold out each cluster for evaluation while instruction tuning FLAN on all other clusters. For example, as shown in Figure 1, to evaluate FLAN\u2019s ability to perform natural language inference, we instruction tune the model on a range of other NLP tasks such as commonsense reasoning, translation, and sentiment analysis. As this setup ensures that FLAN has not seen any natural language inference tasks in instruction tuning, we then evaluate its ability to perform zero-shot natural language inference.  Our evaluations show that FLAN substantially improves the zero-shot performance of the base 137B-parameter model.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nHow does FLAN compare with GPT-3 in terms of zero-shot learning performance?[/INST] \nAnswer: FLAN's zero-shot performance outperforms the 175B-parameter GPT-3's zero-shot on 20 out of 25 datasets evaluated. It even surpasses GPT-3's few-shot performance by a significant margin on several datasets including ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. In terms of machine translation, FLAN outperforms zero-shot GPT-3 for all six evaluations, although it underperforms few-shot GPT-3 in most cases. Overall, FLAN shows substantial improvement in zero-shot performance over the base 137B-parameter model."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nUnlike most of the previously used neural network architectures for learning word vectors, training of the Skip- gram model (see Figure 1) does not involve dense matrix multiplications. This makes the training extremely efficient: an optimized single-machine implementation can train on more than 100 billion words in one day.  The word representations computed using neural networks are very interesting because the learned vectors explicitly encode many linguistic regularities and patterns. Somewhat surprisingly, many of these patterns can be represented as linear translations. For example, the result of a vector calcula- tion vec(\u201cMadrid\u201d) - vec(\u201cSpain\u201d) + vec(\u201cFrance\u201d) is closer to vec(\u201cParis\u201d) than to any other word vector [9, 8]. Figure 1:The Skip-gram model architecture. The training objective is to learn word vector representations that are good at predicting the nearby words.  In this paper we present several extensions of the original Skip-gram model. We show that sub- sampling of frequent words during training results in a significant speedup (around 2x - 10x), and improves accuracy of the representations of less frequent words. In addition, we present a simpli- fied variant of Noise Contrastive Estimation (NCE) [4] for training the Skip-gram model that results in faster training and better vector representations for frequent words, compared to more complex hierarchical softmax that was used in the prior work [8].  Word representations are limited by their inability to represent idiomatic phrases that are not com- positions of the individual words. For example, \u201cBoston Globe\u201d is a newspaper, and so it is not a natural combination of the meanings of \u201cBoston\u201d and \u201cGlobe\u201d. Therefore, using vectors to repre- sent the whole phrases makes the Skip-gram model considerably more expressive. Other techniques that aim to represent meaning of sentences by composing the word vectors, such as the recursive autoencoders [15], would also benefit from using phrase vectors instead of the word vectors.  The extension from word based to phrase based models is relatively simple. First we identify a large number of phrases using a data-driven approach, and then we treat the phrases as individual tokens during the training. To evaluate the quality of the phrase vectors, we developed a test set of analogi- cal reasoning tasks that contains both words and phrases.\n\nfilename: materials_formatted.md\nextension: .md\n\nTo gain further insight into how different the representations learned by different models are, we did inspect manually the nearest neighbours of infrequent phrases using various models. In Table 4, we show a sample of such comparison. Consistently with the previous results, it seems that the best representations of phrases are learned by a model with the hierarchical softmax and subsampling.  ## 5 Additive Compositionality  We demonstrated that the word and phrase representations learned by the Skip-gram model exhibit a linear structure that makes it possible to perform preciseanalogical reasoning using simple vector arithmetics. Interestingly, we found that the Skip-gram representations exhibit another kind of linear structure that makes it possible to meaningfully combine words by an element-wise addition of their vector representations. This phenomenon is illustrated inTable 5.  The additive property of the vectors can be explained by inspecting the training objective. The word vectors are in a linear relationship with the inputs to the softmax nonlinearity. As the word vectors are trained to predict the surrounding words in the sentence, the vectors can be seen as representing the distribution of the context in which a word appears. These values are related logarithmically to the probabilities computed by the output layer, so the sumof two word vectors is related to the product of the two context distributions. The product workshere as the AND function: words that are assigned high probabilities by both word vectors will have high probability, and the other words will have low probability. Thus, if \u201cVolga River\u201d appears frequently in the same sentence together with the words \u201cRussian\u201d and \u201criver\u201d, the sum of these two word vectors will result in such a feature vector that is close to the vector of \u201cVolga River\u201d.  ## 6 Comparison to Published Word Representations  Many authors who previously worked on the neural network based representations of words have published their resulting models for further use and comparison: amongst the most well known au- thors are Collobert and Weston [2], Turian et al. [17], and Mnih and Hinton [10]. We downloaded their word vectors from the web^3. Mikolov et al. [8] have already evaluated these word representa- tions on the word analogy task, where the Skip-gram models achieved the best performance with a huge margin.\n\nfilename: materials_formatted.md\nextension: .md\n\nA typical analogy pair from our test set is \u201cMontreal\u201d:\u201cMontreal Canadiens\u201d::\u201cToronto\u201d:\u201cToronto Maple Leafs\u201d. It is considered to have been answered correctly if the nearest representation to vec(\u201cMontreal Canadiens\u201d) - vec(\u201cMontreal\u201d) + vec(\u201cToronto\u201d) is vec(\u201cToronto Maple Leafs\u201d).  Finally, we describe another interesting property of the Skip-gram model. We found that simple vector addition can often produce meaningful results. For example, vec(\u201cRussia\u201d) + vec(\u201criver\u201d) is close to vec(\u201cVolga River\u201d), and vec(\u201cGermany\u201d) + vec(\u201ccapital\u201d) is close to vec(\u201cBerlin\u201d). This compositionality suggests that a non-obvious degree of language understanding can be obtained by using basic mathematical operations on the word vector representations.  ## 2 The Skip-gram Model  The training objective of the Skip-gram model is to find word representations that are useful for predicting the surrounding words in a sentence or a document. More formally, given a sequence of training wordsw 1 , w 2 , w 3 ,... , wT, the objective of the Skip-gram model is to maximize the average log probability  wherecis the size of the training context (which can be a function ofthe center wordwt). Larger cresults in more training examples and thus can lead to a higher accuracy, at the expense of the  training time. The basic Skip-gram formulation definesp(wt+j|wt)using the softmax function:  wherevwandv\u2032ware the \u201cinput\u201d and \u201coutput\u201d vector representations ofw, andW is the num- ber of words in the vocabulary. This formulation is impractical because the cost of computing \u2207logp(wO|wI)is proportional toW, which is often large ( 105 \u2013 107 terms).  2.1 Hierarchical Softmax  A computationally efficient approximation of the full softmax is the hierarchical softmax. In the context of neural network language models, it was first introduced by Morin and Bengio [12]. The main advantage is that instead of evaluatingW output nodes in the neural network to obtain the probability distribution, it is needed to evaluate only aboutlog 2 (W)nodes.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat does the paper claim about the vectors learned through the Skip-gram model?[/INST] \nAnswer: The paper claims that the vectors learned through the Skip-gram model are efficient and interesting. They encode many linguistic regularities and patterns, some of which can be represented as linear translations. The model is capable of predicting nearby words. The paper also presents extensions of the Skip-gram model that result in faster training and better vector representations. Furthermore, it discusses the model's ability to represent idiomatic phrases and the transition from word-based to phrase-based models. The paper also highlights the additive property of the vectors, which allows meaningful combination of words by element-wise addition of their vector representations."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nThis presents a more realistic scenario for practical applications.  Current approaches require a blackbox informa- tion retrieval (IR) system to do much of the heavy lifting, even though it cannot be \ufb01ne-tuned on the downstream task. In the strongly supervised set- ting popularized by DrQA (Chen et al., 2017), they also assume a reading comprehension model trained on question-answer-evidence triples, such as SQuAD (Rajpurkar et al., 2016). The IR sys- tem is used at test time to generate evidence candi- dates in place of the gold evidence. In the weakly supervised setting, proposed by TriviaQA (Joshi  et al., 2017), SearchQA (Dunn et al., 2017), and Quasar (Dhingra et al., 2017), the dependency on strong supervision is removed by assuming that the IR system provides noisy gold evidence.  These approaches rely on the IR system to mas- sively reduce the search space and/or reduce spu- rious ambiguity. However, QA is fundamentally different from IR (Singh, 2012). Whereas IR is concerned with lexical and semantic matching, questions are by de\ufb01nition under-speci\ufb01ed and re- quire more language understanding, since users are explicitly looking for unknown information. Instead of being subject to the recall ceiling from blackbox IR systems, we should directly learn to retrieve using question-answering data.  In this work, we introduce the \ufb01rst Open- Retrieval Question Answering system (ORQA). ORQA learns to retrieve evidence from an open corpus, and is supervised only by question- answer string pairs. While recent work on im- proving evidence retrieval has made signi\ufb01cant progress (Wang et al., 2018; Kratzwald and Feuer- riegel, 2018; Lee et al., 2018; Das et al., 2019), they still only rerank a closed evidence set. The main challenge to fully end-to-end learning is that retrieval over the open corpus must be considered a latent variable that would be impractical to train from scratch. IR systems offer a reasonable but potentially suboptimal starting point.\n\nfilename: materials_formatted.md\nextension: .md\n\nIR systems offer a reasonable but potentially suboptimal starting point.  The key insight of this work is that end-to- end learning is possible if we pre-train the re- triever with an unsupervised Inverse Cloze Task (ICT). In ICT, a sentence is treated as a pseudo- question, and its context is treated as pseudo- evidence. Given a pseudo-question, ICT requires selecting the corresponding pseudo-evidence out of the candidates in a batch. ICT pre-training provides a suf\ufb01ciently strong initialization such that ORQA, a joint retriever and reader model, can be \ufb01ne-tuned end-to-end by simply optimiz-              Task  Training  Evaluation  Evidence  Answer  Evidence  Answer  Example  Reading Comprehension Open-domain QA  Unsupervised QA Strongly Supervised QA Weakly Supervised QA Closed Retrieval QA Open Retrieval QA  given  none given  heuristic learned  span  none span  string string  given  string  SQuAD (Rajpurkar et al., 2016)  none heuristic  heuristic learned  string string  string string  GPT-2 (Radford et al., 2019) DrQA (Chen et al., 2017)  TriviaQA (Joshi et al., 2017) ORQA (this work)  Table 1: Comparison of assumptions made by related tasks, along with references to examples. Heuristic evidence refers to the typical strategy of considering only a closed set of evidence documents from a traditional IR system, which sets a strict upper-bound on task performance. In this work (ORQA), only question-answer string pairs are observed during training, and evidence retrieval is learned in a completely end-to-end manner.  ing the marginal log-likelihood of correct answers that were found.  We evaluate ORQA on open versions of \ufb01ve ex- isting QA datasets. On datasets where the question writers already know the answer\u2014SQuAD (Ra- jpurkar et al., 2016) and TriviaQA (Joshi et al., 2017)\u2014the retrieval problem resembles tradi- tional IR, and BM25 (Robertson et al., 2009) provides state-of-the-art retrieval.\n\nfilename: materials_formatted.md\nextension: .md\n\n[SEP]...ZIPs for other operating systems may be preceded by...[SEP]  M L P  M  L  P  M L  P  M L P  M  L  P  M L  P  Sread (0, \u201cThe term\u201d, q)  Sread (0, \u201cZone Improvement Plan\u201d, q)  Sread (0, ..., q)  Sread (2, \u201cZIPs\u201d, q)  Sread (2, \u201coperating systems\u201d, q)  Sread (2, ..., q)  Figure 1: Overview of ORQA. A subset of all possible answer derivations given a question q is shown here. Retrieval scores Sretr (q, b) are computed via inner products between BERT-based encoders. Top-scoring evidence blocks are jointly encoded with the question, and span representations are scored with a multi-layer perceptron (MLP) to compute Sread (q, b, s). The \ufb01nal joint model score is Sretr (q, b) + Sread (q, b, s). Unlike previous work using IR systems for candidate proposal, we learn to retrieve from all of Wikipedia directly.  Sread (b, s, q) is learned from gold answer deriva- tions, typically from the SQuAD (Rajpurkar et al., 2016) dataset, where the evidence text is given.  In work that is more closely related to our ap- proach, the reader is learned entirely from weak supervision (Joshi et al., 2017; Dhingra et al., 2017; Dunn et al., 2017). Spurious ambiguities (see Table 2) are heuristically removed by the re- trieval system, and the cleaned results are treated as gold derivations.  3 Open-Retrieval Question Answering  (ORQA)  We propose an end-to-end model where the re- triever and reader components are jointly learned, which we refer to as the Open-Retrieval Question Answering (ORQA) model. An important aspect of ORQA is its expressivity\u2014it is capable of re- trieving any text in an open corpus, rather than be- ing limited to the closed set returned by a black- box IR system.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nHow does ORQA differ from traditional IR systems in handling evidence retrieval?[/INST] \nAnswer: ORQA, or Open-Retrieval Question Answering system, differs from traditional Information Retrieval (IR) systems in its approach to evidence retrieval. Traditional IR systems are often used at test time to generate evidence candidates, and they are also assumed to provide noisy gold evidence in weakly supervised settings. However, these systems are limited by their inability to be fine-tuned on the downstream task and their dependency on strong supervision. They also tend to reduce the search space and decrease spurious ambiguity, but they are fundamentally different from QA systems as they focus on lexical and semantic matching. \n\nOn the other hand, ORQA learns to retrieve evidence from an open corpus and is supervised only by question-answer string pairs. It does not rely on a blackbox IR system, and instead, it directly learns to retrieve using question-answering data. ORQA is capable of retrieving any text in an open corpus, as opposed to being limited to the closed set returned by a traditional IR system. It is also fine-tuned end-to-end by optimizing the marginal log-likelihood of correct answers that were found."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nWhile NCE can be shown to approximately maximize the log probability of the softmax, the Skip- gram model is only concerned with learning high-quality vector representations, so we are free to simplify NCE as long as the vector representations retain their quality. We define Negative sampling (NEG) by the objective  Figure 2:Two-dimensional PCA projection of the 1000-dimensional Skip-gram vectors of countries and their capital cities. The figure illustrates ability of the model to automatically organize concepts and learn implicitly the relationships between them, as during the training we did not provide any supervised information about what a capital city means.  which is used to replace everylogP(wO|wI)term in the Skip-gram objective. Thus the task is to distinguish the target wordwOfrom draws from the noise distributionPn(w)using logistic regres- sion, where there areknegative samples for each data sample. Our experiments indicate that values ofkin the range 5\u201320 are useful for small training datasets, while for large datasets thekcan be as small as 2\u20135. The main difference between the Negative sampling and NCE is that NCE needs both samples and the numerical probabilities of the noise distribution, while Negative sampling uses only samples. And while NCE approximately maximizes the log probability of the softmax, this property is not important for our application.  Both NCE and NEG have the noise distributionPn(w)as a free parameter. We investigated a number of choices forPn(w)and found that the unigram distributionU(w)raised to the 3 / 4 rd power (i.e., U(w)^3 /^4 /Z) outperformed significantly the unigram and the uniform distributions, for both NCE and NEG on every task we tried including language modeling (not reported here).  2.3 Subsampling of Frequent Words  In very large corpora, the most frequent words can easily occur hundreds of millions of times (e.g., \u201cin\u201d, \u201cthe\u201d, and \u201ca\u201d). Such words usually provide less information value than the rare words. For example, while the Skip-gram model benefits from observing the co-occurrences of \u201cFrance\u201d and \u201cParis\u201d, it benefits much less from observing the frequent co-occurrences of \u201cFrance\u201d and \u201cthe\u201d, as nearly every word co-occurs frequently within a sentence with \u201cthe\u201d.\n\nfilename: materials_formatted.md\nextension: .md\n\nHowever, the actual skip-gram with negative sampling (SGNS) objective ends up being a bit more different; we\u2019ll write it here:   log \u03c3 (u\u22a4ovc) +   k   \u2211   \u2113= 1   h log \u03c3 (\u2212u\u22a4\u2113vc)   i ( 15 )   Affinity of wordcfor contexto   Push down everything else in expectation (replacement for partition fn)  where, where _\u03c3_ is the logistic function, andu\u2113\u223c pneg, which meansu\u2113is drawn from a distribution we haven\u2019t defined, called pneg, Think of this for now like the uniform distribution overV. What is this objective doing? It has two terms, just like how we\u2019ve described the orginal skipgram. The first term encouragesvcanduo to be more like each other, and the second term encouragesvcandu\u2113 forkrandom samples from the vocabulary to be less like each other. The intuition here is that if we randomly push down a few words at each step, then on average, things will work out sort of as if we always pushed down every word.    deep learning 13  ## A Extra notes  ### A. 1 Continuous Bag-of-Words  ### A. 2 Singular Value Decomposition  ## References  [Batsuren et al. 2022 ] Batsuren, K. Goldman, O. Khalifa, S. Habash, N. Kiera \u0301s, W. Bella, G. Leonard, B. Nicolai, G. Gorman, K. Ate, Y. G. Ryskina, M. Mielke, S. Budianskaya, E. El-Khaissi, C. Pimentel, T. Gasser, M. Lane, W. A. Raj, M. Coler, M. Samame, J. R. M. Camaiteri, D. S. Rojas, E. Z. L\u00f3pez Francis, D. Oncevay, A. L\u00f3pez Bautista, J. Villegas, G. C. S. Hennigen, L. T. Ek, A. Guriel, D. Dirix, P. Bernardy, J.-P. Scherbakov, A. Bayyr-ool, A. Anastasopoulos, A.\n\nfilename: materials_formatted.md\nextension: .md\n\nThis specific example is considered to have been answered correctly ifxis \u201cParis\u201d. The task has two broad categories: the syntactic analogies (such as \u201cquick\u201d: \u201cquickly\u201d :: \u201cslow\u201d : \u201cslowly\u201d) and the semantic analogies, such as the country to capital city relationship.  For training the Skip-gram models, we have used a large dataset consisting of various news articles (an internal Google dataset with one billion words). We discarded from the vocabulary all words that occurred less than 5 times in the training data, which resulted in a vocabulary of size 692K. The performance of various Skip-gram models on the word analogy test set is reported in Table 1. The table shows that Negative Sampling outperforms the Hierarchical Softmax on the analogical reasoning task, and has even slightly better performance than the Noise Contrastive Estimation. The subsampling of the frequent words improves the training speed several times and makes the word representations significantly more accurate.  It can be argued that the linearity of the skip-gram model makes its vectors more suitable for such linear analogical reasoning, but the results of Mikolov et al. [8] also show that the vectors learned by the standard sigmoidal recurrent neural networks (whichare highly non-linear) improve on this task significantly as the amount of the training data increases, suggesting that non-linear models also have a preference for a linear structure of the word representations.  ## 4 Learning Phrases  As discussed earlier, many phrases have a meaning that is nota simple composition of the mean- ings of its individual words. To learn vector representation for phrases, we first find words that appear frequently together, and infrequently in other contexts. For example, \u201cNew York Times\u201d and \u201cToronto Maple Leafs\u201d are replaced by unique tokens in the training data, while a bigram \u201cthis is\u201d will remain unchanged.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat is the skip-gram with negative-sampling training method popularized by?[/INST] \nAnswer: The skip-gram with negative-sampling training method was popularized by the Skip-gram models."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nWe also provide a brief rubric for giving scores of 1, 4, and 7, along with several Reddit summaries annotated with our own judgments of quality along each of these axes (with explanations).  What makes for a good summary? Roughly speaking, a good summary is a shorter piece of text that has the essence of the original \u2013 tries to accomplish the same purpose and conveys the same information as the original post. We would like you to consider these different dimensions of summaries:  Essence: is the summary a good representation of the post?  Clarity: is the summary reader-friendly? Does it express ideas clearly?  Accuracy: does the summary contain the same information as the longer post?  Purpose: does the summary serve the same purpose as the original post?  Concise: is the summary short and to-the-point?  Style: is the summary written in the same style as the original post?  Generally speaking, we give higher weight to the dimensions at the top of the list. Things are complicated though \u2013 none of these dimensions are simple yes/no matters, and there aren\u2019t hard and fast rules for trading off different dimensions. This is something you\u2019ll pick up through practice and feedback on our website.  Table 6: An excerpt from the instructions we gave to labelers for doing comparisons.  Finally, we provide a FAQ section that answers common questions raised by the small initial set of labelers we assigned to this task.  For CNN/DM, we provide the same set of instructions, except we add some additional clari\ufb01cations for how to judge news articles. We speci\ufb01cally ask labelers to place less emphasis on \ufb02uidity of sentences (because the reference summaries were originally written in bullet-point form, and we didn\u2019t want labelers to penalize this), and to place less emphasis on the summary matching the intent of the article (which was important for Reddit summaries).\n\nfilename: materials_formatted.md\nextension: .md\n\nIf it\u2019s hard to \ufb01nd ways to make the summary better, give the summary a high score. If there are lots of different ways the summary can be made better, give the summary a low score.  Rubric: Score of 1: The summary is terrible. Score of 4: The summary is an okay representation of the post, but could be signi\ufb01cantly improved. Score of 7: The summary is an excellent representation of the post.  Table 7: Instructions given to labelers for evaluating summaries along four different axes of quality.  Supervised policy name sup1 sup2 sup3 sup3_6b sup4 sup4_6b \nParameters 750M 1.3B 1.3B 6.7B 1.3B 6.7B  Reward model name rm1 rm2 rm3 rm3_6b rm4 rm4_6b \nParameters 1.3B 6.7B 1.3B 6.7B 1.3B 6.7B  Table 8: Left: supervised baselines. sup4 and sup4_6b are the \ufb01nal supervised baselines used throughout the paper. Right: reward models. rm4 and rm4_6b are the \ufb01nal reward models used throughout the paper.  # Parameters Objective  RL policy name sup3 ppo rm1 sup4 ppo rm3 1 sup4 ppo rm3 2 sup4 ppo rm3 3 sup4 ppo rm4 sup4_6b ppo rm4_6b  rm1 rm3 rm3 rm3 rm4 rm4_6b Table 9: PPO policies. sup4 ppo rm4 and sup4_6b ppo rm4_6b are the \ufb01nal policies used throughout the paper.\n\nfilename: materials_formatted.md\nextension: .md\n\nGenerally, it\u2019s more important that the summary is understandable than it being free of grammar errors.  Rubric: Score of 1: The summary is impossible to understand. Score of 4: The summary has mistakes or confusing phrasing that make it a bit hard to understand. Score of 7: The summary is perfectly clear.  Accuracy For this axis, answer the question \u201cdoes the factual information in the summary accurately match the post?\u201d A summary is accurate if it doesn\u2019t say things that aren\u2019t in the article, it doesn\u2019t mix up people, and generally is not misleading. If the summary says anything at all that is not mentioned in the post or contradicts something in the post, it should be given a maximum score of 5. (If you are confused about how to use \u20186\u2019, see the FAQ!)  Rubric: Score of 1: The summary is completely wrong, made up, or exactly contradicts what is written in the post. Score of 4: The summary says at least one substantial thing that is not mentioned in the post, or that contradicts something in the post. (Score of 5: The summary says anything, no matter how small, that is not mentioned in the post, or that contradicts something in the post.) Score of 7: The summary has no incorrect statements or misleading implications.  Coverage For this axis, answer the question \u201chow well does the summary cover the important information in the post?\u201d A summary has good coverage if it mentions the main information from the post that\u2019s important to understand the situation described in the post. A summary has poor coverage if someone reading only the summary would be missing several important pieces of information about the situation in the post. A summary with good coverage should also match the purpose of the original post (e.g. to ask for advice).  Rubric: Score of 1: The summary contains no information relevant to the post. Score of 4: The summary is missing at least 1 important piece of information required to under- stand the situation. Score of 7: The summary covers all of the important information required to understand the situation.  Overall quality For this axis, answer the question \u201chow good is the summary overall at representing the post?\u201d This can encompass all of the above axes of quality, as well as others you feel are important.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat is the main goal of the paper?[/INST] \nAnswer: The main goal of the paper is not explicitly mentioned in the provided context."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nAlthough oversimplified, this is a classic case demonstrating the need for non-linear decision boundaries. In the next set of notes, we study neural networks as a class of non-linear models that have performed particularly well in deep learning applications.   Figure 10 : Here, we see that the non- linear decision boundary allows for much better classification of datapoints.           \n\n# SEP 775 Lecture 03:  # Natural Language Processing with Deep Learning Course  ## SEP 775 Lecture Notes: Part III  ## Neural Networks, Backpropagation  Keyphrases: Neural networks. Forward computation. Backward propagation. Neuron Units. Max-margin Loss. Gradient checks. Xavier parameter initialization. Learning rates. Adagrad. This set of notes introduces single and multilayer neural networks, and how they can be used for classification purposes. We then dis- cuss how they can be trained using a distributed gradient descent technique known as backpropagation. We will see how the chain rule can be used to make parameter updates sequentially. After a rigorous mathematical discussion of neural networks, we will discuss some practical tips and tricks in training neural networks involving: neuron units (non-linearities), gradient checks, Xavier parameter ini- tialization, learning rates, Adagrad, etc. Lastly, we will motivate the use of recurrent neural networks as a language model.  ## 1 Neural Networks: Foundations   Figure 1 : We see here how a non-linear decision boundary separates the data very well. This is the prowess of neural networks. Fun Fact: Neural networks are biologically in- spired classifiers which is why they are often called \"artificial neural networks\" to distinguish them from the organic kind. However, in reality human neural networks are so much more capable and complex from artificial neural net- works that it is usually better to not draw too many parallels between the two.  We established in our previous discussions the need for non-linear classifiers since most data are not linearly separable and thus, our classification performance on them is limited. Neural networks are a family of classifiers with non-linear decision boundary as seen in Figure 1. Now that we know the sort of decision boundaries neural networks create, let us see how they manage doing so.  ### 1. 1 A Neuron  A neuron is a generic computational unit that takesninputs and produces a single output.\n\nfilename: materials_formatted.md\nextension: .md\n\nSuch non-linear de- cisions can often not be captured by inputs fed directly to a Softmax function but instead require the scoring of the intermediate layer discussed in Section 1. 2. We can thus use another matrixU\u2208Rm\u00d7^1 to generate an unnormalized score for a classification task from the activations: s=UTa=UTf(W x+b)  wherefis the activation function.   Figure 4 : This image captures how a simple feed-forward network might compute its output.  Analysis of Dimensions: If we represent each word using a 4- dimensional word vector and we use a 5-word window as input (as in the above example), then the inputx\u2208 R^20. If we use 8 sigmoid units in the hidden layer and generate 1 score output from the activa- tions, thenW\u2208 R^8 \u00d7^20 ,b\u2208 R^8 ,U\u2208 R^8 \u00d7^1 ,s\u2208 R.  ### 1. 4 Maximum Margin Objective Function  Like most machine learning models, neural networks also need an optimization objective, a measure of error or goodness which we want to minimize or maximize respectively. Here, we will discuss a popular error metric known as the maximum margin objective. The idea behind using this objective is to ensure that the score computed for \"true\" labeled data points is higher than the score computed for \"false\" labeled data points. Using the previous example, if we call the score computed for the \"true\" labeled window\"Museums in Paris are amazing\"assand the   ## neural networks,backpropagation 4  score computed for the \"false\" labeled window\"Not all museums in Paris\"assc(subscripted ascto signify that the window is \"corrupt\"). Then, our objective function would be to maximize(s\u2212sc)or to minimize(sc\u2212s). However, we modify our objective to ensure that error is only computed ifsc > s\u21d2 (sc\u2212s) >0. The intuition behind doing this is that we only care the the \"true\" data point have a higher score than the \"false\" data point and that the rest does not matter. Thus, we want our error to be(sc\u2212s)ifsc>selse 0.\n\nfilename: materials_formatted.md\nextension: .md\n\nThus, we must typically divide the outputs of each neuron during testing by a certain value \u2013 it is left as an exercise to the reader to determine what this value should be in order for the expected outputs during training and testing to be equivalent.  ### 2. 4 Neuron Units  So far we have discussed neural networks that contain sigmoidal neurons to introduce nonlinearities; however in many applications better networks can be designed using other activation functions. Some common choices are listed here with their function and gra- dient definitions and these can be substituted with the sigmoidal functions discussed above.  Sigmoid: This is the default choice we have discussed; the activation function _\u03c3_ is given by:   Figure 9 : The response of a sigmoid nonlinearity   \u03c3 (z) =  #### 1   1 +exp(\u2212z)   ## neural networks,backpropagation 13   where \u03c3 (z)\u2208(0, 1)  The gradient of _\u03c3_ (z)is:   \u03c3 \u2032(z) = \u2212exp(\u2212z) 1 +exp(\u2212z) = \u03c3 (z)( 1 \u2212 \u03c3 (z))  Tanh: The tanh function is an alternative to the sigmoid function that is often found to converge faster in practice. The primary differ- ence between tanh and sigmoid is that tanh output ranges from\u22121 to 1 while the sigmoid ranges from 0 to 1.   Figure 10 : The response of a tanh nonlinearity   tanh(z) =exp(z)\u2212exp(\u2212z) exp(z) +exp(\u2212z) = 2 \u03c3 ( 2 z)\u2212 1   where tanh(z)\u2208(\u22121, 1)  The gradient of tanh(z)is:   tanh\u2032(z) = 1 \u2212   (   exp(z)\u2212exp(\u2212z) exp(z) +exp(\u2212z)   ) 2   = 1 \u2212tanh^2 (z)  Hard tanh: The hard tanh function is sometimes preferred over the tanh function since it is computationally cheaper. It does however saturate for magnitudes ofzgreater than 1.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat is the purpose of using non-linear activation functions in neural networks?[/INST] \nAnswer: Non-linear activation functions are used in neural networks to introduce non-linearities, which allows the model to learn from the complex patterns in the data. This is particularly important because most real-world data are not linearly separable. Non-linear activation functions, such as the sigmoid or tanh, enable the neural network to create non-linear decision boundaries, leading to better classification performance."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nIn this paper, we combine the strengths of these two ideas in a way that avoids their limitations. Speci\ufb01cally, we explore the ability of language models to perform few-shot prompting for reasoning tasks, given a prompt that consists of triples: (cid:104)input, chain of thought, output(cid:105). A chain of thought is a series of intermediate natural language reasoning steps that lead to the \ufb01nal output, and we refer to this approach as chain-of-thought prompting. An example prompt is shown in Figure 1.  We present empirical evaluations on arithmetic, commonsense, and symbolic reasoning benchmarks, showing that chain-of-thought prompting outperforms standard prompting, sometimes to a striking degree. Figure 2 illustrates one such result\u2014on the GSM8K benchmark of math word problems (Cobbe et al., 2021), chain-of-thought prompting with PaLM 540B outperforms standard prompting by a large margin and achieves new state-of-the-art performance. A prompting only approach is important because it does not require a large training dataset and because a single model checkpoint can perform many tasks without loss of generality. This work underscores how large language models can learn via a few examples with natural language data about the task (c.f. automatically learning the patterns underlying inputs and outputs via a large training dataset).  2 Chain-of-Thought Prompting  Consider one\u2019s own thought process when solving a complicated reasoning task such as a multi-step math word problem. It is typical to decompose the problem into intermediate steps and solve each before giving the \ufb01nal answer: \u201cAfter Jane gives 2 \ufb02owers to her mom she has 10 . . . then after she gives 3 to her dad she will have 7 . . . so the answer is 7.\u201d The goal of this paper is to endow language models with the ability to generate a similar chain of thought\u2014a coherent series of intermediate reasoning steps that lead to the \ufb01nal answer for a problem. We will show that suf\ufb01ciently large  2  language models can generate chains of thought if demonstrations of chain-of-thought reasoning are provided in the exemplars for few-shot prompting.  Figure 1 shows an example of a model producing a chain of thought to solve a math word problem that it would have otherwise gotten incorrect.\n\nfilename: materials_formatted.md\nextension: .md\n\nNo language models were \ufb01netuned in the process of writing this paper.  The emergence of chain-of-thought reasoning as a result of model scale has been a prevailing theme (Wei et al., 2022b). For many reasoning tasks where standard prompting has a \ufb02at scaling curve, chain- of-thought prompting leads to dramatically increasing scaling curves. Chain-of-thought prompting appears to expand the set of tasks that large language models can perform successfully\u2014in other words, our work underscores that standard prompting only provides a lower bound on the capabilities of large language models. This observation likely raises more questions than it answers\u2014for instance, how much more can we expect reasoning ability to improve with a further increase in model scale? What other prompting methods might expand the range of tasks that language models can solve?  As for limitations, we \ufb01rst qualify that although chain of thought emulates the thought processes of human reasoners, this does not answer whether the neural network is actually \u201creasoning,\u201d which we leave as an open question. Second, although the cost of manually augmenting exemplars with chains of thought is minimal in the few-shot setting, such annotation costs could be prohibitive for \ufb01netuning (though this could potentially be surmounted with synthetic data generation, or zero-shot generalization). Third, there is no guarantee of correct reasoning paths, which can lead to both correct and incorrect answers; improving factual generations of language models is an open direction for future work (Rashkin et al., 2021; Ye and Durrett, 2022; Wiegreffe et al., 2022, inter alia). Finally, the emergence of chain-of-thought reasoning only at large model scales makes it costly to serve in real-world applications; further research could explore how to induce reasoning in smaller models.  7 Related Work  This work is inspired by many research areas, which we detail in an extended related work section (Appendix C). Here we describe two directions and associated papers that are perhaps most relevant.  The \ufb01rst relevant direction is using intermediate steps to solve reasoning problems. Ling et al. (2017) pioneer the idea of using natural language rationales to solve math word problems through a series of intermediate steps.\n\nfilename: materials_formatted.md\nextension: .md\n\nOur experimental setup uses the same methods and models as in the prior two sections. We again manually compose chains of thought for the few-shot exemplars for each task, which are given in Figure 3.  Results. The results of these in-domain and OOD evaluations are shown in Figure 8 for PaLM, with results for LaMDA shown in Appendix Table 5. With PaLM 540B, chain-of-thought prompting leads to almost 100% solve rates (note that standard prompting already solves coin \ufb02ip with PaLM 540, though not for LaMDA 137B). Note that these in-domain evaluations are \u201ctoy tasks\u201d in the sense that perfect solution structures are already provided by the chains of thought in the few-shot exemplars; all the model has to do is repeat the same steps with the new symbols in the test-time example. And yet, small models still fail\u2014the ability to perform abstract manipulations on unseen symbols for these three tasks only arises at the scale of 100B model parameters.  As for the OOD evaluations, standard prompting fails for both tasks. With chain-of-thought prompting, language models achieve upward scaling curves (though performance is lower than in the in-domain setting). Hence, chain-of-thought prompting facilitates length generalization beyond seen chains of thought for language models of suf\ufb01cient scale.  6 Discussion  We have explored chain-of-thought prompting as a simple mechanism for eliciting multi-step rea- soning behavior in large language models. We \ufb01rst saw that chain-of-thought prompting improves performance by a large margin on arithmetic reasoning, yielding improvements that are much stronger than ablations and robust to different annotators, exemplars, and language models (Section 3). Next,  3We tested 10 common names using GPT-3 davinci and it got all but one correct. 4For names of length longer than 2 words, we concatenate multiple \ufb01rst and last names together.  8  experiments on commonsense reasoning underscored how the linguistic nature of chain-of-thought reasoning makes it generally applicable (Section 4). Finally, we showed that for symbolic reasoning, chain-of-thought prompting facilitates OOD generalization to longer sequence lengths (Section 5). In all experiments, chain-of-thought reasoning is elicited simply by prompting an off-the-shelf language model.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat are the two key limitations of rationale-augmented training and traditional few-shot prompting methods addressed by chain-of-thought prompting?[/INST] \nAnswer: The two key limitations addressed by chain-of-thought prompting are the requirement of a large training dataset in traditional few-shot prompting methods and the inability to perform multiple tasks without loss of generality. Chain-of-thought prompting does not require a large training dataset and a single model checkpoint can perform many tasks without loss of generality."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\n# Evaluation methods for unsupervised word embeddings  ## Tobias Schnabel  ## Cornell University  ## Ithaca, NY, 14853  ## tbs49@cornell.edu  ## Igor Labutov  ## Cornell University  ## Ithaca, NY, 14853  ## iil4@cornell.edu  ## David Mimno,  ## Thorsten Joachims  ## Cornell University  ## mimno@cornell.edu,  ## tj@cs.cornell.edu  ## Abstract   We present a comprehensive study of eval- uation methods for unsupervised embed- ding techniques that obtain meaningful representations of words from text. Differ- ent evaluations result in different orderings of embedding methods, calling into ques- tion the common assumption that there is one single optimal vector representation. We present new evaluation techniques that directly compare embeddings with respect to specific queries. These methods re- duce bias, provide greater insight, and allow us to solicit data-driven relevance judgments rapidly and accurately through crowdsourcing.  ## 1 Introduction  Neural word embeddings represent meaning via geometry. A good embedding provides vector rep- resentations of words such that the relationship be- tween two vectors mirrors the linguistic relation- ship between the two words. Despite the growing interest in vector representations of semantic in- formation, there has been relatively little work on direct evaluations of these models. In this work, we explore several approaches to measuring the quality of neural word embeddings. In particu- lar, we perform a comprehensive analysis of eval- uation methods and introduce novel methods that can be implemented through crowdsourcing, pro- viding better insights into the relative strengths of different embeddings. Existing schemes fall into two major categories: extrinsic and intrinsic evaluation. In extrinsic eval- uation, we use word embeddings as input features to a downstream task and measure changes in per- formance metrics specific to that task. Examples include part-of-speech tagging and named-entity recognition (Pennington et al., 2014). Extrinsic   evaluation only provides one way to specify the goodness of an embedding, and it is not clear how it connects to other measures. Intrinsic evaluations directly test for syntactic or semantic relationships between words (Mikolov et al., 2013a; Baroni et al., 2014).\n\nfilename: materials_formatted.md\nextension: .md\n\nTo facilitate systematic evaluation and compar- ison of new embedding models, we release a new frequency-calibrated query inventory along with all user judgments athttp://www.cs. cornell.edu/ \u0303schnabts/eval/. Finally, we observe that word embeddings en- code a surprising degree of information about word frequency. We found this was true even in models that explicitly reserve parameters to com- pensate for frequency effects. This finding may explain some of the variability across embeddings and across evaluation methods. It also casts doubt on the common practice of using the vanilla co- sine similarity as a similarity measure in the em- bedding space. It is important to note that this work is a survey ofevaluationmethods not a survey ofembedding methods. The specific example embeddings pre- sented here were chosen as representative samples only, and may not be optimal.  ## 2 Word embeddings  We refer to a word embedding as a mappingV\u2192 RD:w7\u2192~wthat maps a wordwfrom a vocabu- laryV to a real-valued vector~win an embedding space of dimensionalityD. Following previous work (Collobert et al., 2011; Mikolov et al., 2013a) we use the com- monly employed cosine similarity, defined as similarity(w 1 ,w 2 ) = \u2016~w~w 11 \u2016\u2016\u00b7~w~w^22 \u2016, for all similar- ity computations in the embedding space. The list of nearest neighbors of a wordware all words v \u2208 V \\ {w}, sorted in descending order by similarity(w,v). We will denotewas thequery wordin the remainder of this paper. All experiments in this paper are carried out on six popular unsupervised embedding meth- ods. These embeddings form a representative but incomplete subset; and since we are study-   ing evaluation methods and not embeddings them- selves, no attempt has been made to optimize these embeddings. The first two embedding models, the CBOW model of word2vec (Mikolov et al., 2013a) and C&W embeddings (Collobert et al., 2011) both are motivated by a probabilistic predic- tion approach.\n\nfilename: materials_formatted.md\nextension: .md\n\nIntrinsic evaluations measure the quality of word vectors by directly measuring correlation between semantic relatedness and ge- ometric relatedness, usually through inventories of query terms. Focusing on intrinsic measures, Baroni et al. (2014) compare word embeddings against distributional word vectors on a variety of query inventories and tasks. Faruqui and Dyer (2014) provide a website that allows the automatic evaluation of embeddings on a number of query inventories. Gao et al. (2014) publish an improved query inventory for the analogical reasoning task. Finally, Tsvetkov et al. (2015) propose a new in- trinsic measure that better correlates with extrinsic performance. However, all these evaluations are done on precollected inventories and mostly lim- ited to local metrics like relatedness. Extrinsic evaluations use embeddings as fea- tures in models for other tasks, such as semantic role labeling or part-of-speech tagging (Collobert et al., 2011), and improve the performance of ex- isting systems (Turian et al., 2010). However, they have been less successful at other tasks such as parsing (Andreas and Klein, 2014). More work has been done in unsupervised se- mantic modeling in the context of topic models. One example is theword intrusiontask (Chang et al., 2009), in which annotators are asked to iden- tify a random word inserted into the set of high probability words for a given topic. Word embed- dings do not produce interpretable dimensions, so we cannot directly use this method, but we present a related task based on nearest neighbors. Manual evaluation is expensive and time-consuming, but other work establishes that automated evaluations can closely model human intuitions (Newman et al., 2010).  ## 8 Conclusions  There are many factors that affect word embed- ding quality. Standard aggregate evaluations, while useful, do not present a complete or con- sistent picture. Factors such as word frequency play a significant and previously unacknowledged   role. Word frequency also interferes with the commonly-used cosine similarity measure.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat is the main contribution of the paper regarding evaluation methods for unsupervised word embeddings?[/INST] \nAnswer: The paper presents a comprehensive study of evaluation methods for unsupervised word embeddings, which are techniques that derive meaningful representations of words from text. The authors introduce new evaluation techniques that directly compare embeddings with respect to specific queries, reducing bias and providing greater insight. They also introduce methods that can be implemented through crowdsourcing, allowing for rapid and accurate data-driven relevance judgments. Additionally, they release a new frequency-calibrated query inventory for systematic evaluation and comparison of new embedding models. The paper also highlights that word embeddings encode a significant amount of information about word frequency, which may explain variability across embeddings and evaluation methods."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nTo train the retriever and generator end-to-end, we treat the retrieved document as a latent variable. We propose two models that marginalize over the latent documents in different ways to produce a distribution over generated text. In one approach,RAG-Sequence, the model uses the same document to predict each target token. The second approach,RAG-Token, can predict each target token based on a different document. In the following, we formally introduce both models and then describe the p\u03b7andp\u03b8components, as well as the training and decoding procedure.  2.1 Models  RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate the completesequence. Technically, it treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probabilityp(y|x)via a top-K approximation. Concretely, the top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized, RAG-Token Model In the RAG-Token model we can draw a different latent document for each targettokenand marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. Concretely, the top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token, Formally, we define:   pRAG-Token(y|x)\u2248  #### \u220fN   i   \u2211   z\u2208top-k(p(\u00b7|x))   p\u03b7(z|x)p\u03b8(yi|x,z,y1:i\u2212 1 )  Finally, we note that RAG can be used for sequence classification tasks by considering the target class as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent.  2.2 Retriever: DPR  The retrieval componentp\u03b7(z|x)is based on DPR [26].\n\nfilename: materials_formatted.md\nextension: .md\n\nWe com- pare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge- intensive NLP tasks and set the state of the art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.  ### 1 Introduction   Pre-trained neural language models have been shown to learn a substantial amount of in-depth knowl- edge from data [ 47 ]. They can do so without any access to an external memory, as a parameterized implicit knowledge base [ 51 , 52 ]. While this development is exciting, such models do have down- sides: They cannot easily expand or revise their memory, can\u2019t straightforwardly provide insight into their predictions, and may produce \u201challucinations\u201d [ 38 ]. Hybrid models that combine parametric memory with non-parametric (i.e., retrieval-based) memories [ 20 , 26 , 48 ] can address some of these issues because knowledge can be directly revised and expanded, and accessed knowledge can be inspected and interpreted. REALM [ 20 ] and ORQA [ 31 ], two recently introduced models that combine masked language models [ 8 ] with a differentiable retriever, have shown promising results,  # arXiv:2005.11401v4 [cs.CL] 12 Apr 2021    The\tDivine Comedy\t (x) q   Query Encoder q(x)   MIPS p\u03b8   Generator p\u03b8 (Parametric)   Margin- alize This\t14th\tcentury\twork is\tdivided\tinto\t sections:\t\"Inferno\", \"Purgatorio\"\t& \"Paradiso\"\t\t\t\t\t\t\t\t\t (y)   End-to-End Backprop through q and p\u03b8   Barack\tObama\twas born\tin\tHawaii. (x) Fact Verification: Fact Query   supports (y)   Question Generation   Fact Verification: Label Generation   Document Index   Define\t\"middle\tear\" (x) Question Answering: Question Query   The\tmiddle\tear\tincludes the\ttympanic\tcavity\tand the\tthree\tossicles.\n\nfilename: materials_formatted.md\nextension: .md\n\nThese approaches have proved successful in a number of domains including Machine Translation [ 18 , 22 ] and Semantic Parsing [ 21 ]. Our approach does have several differences, including less of emphasis on lightly editing a retrieved item, but on aggregating content from several pieces of retrieved content, as well as learning latent retrieval, and retrieving evidence documents rather than related training pairs. This said, RAG techniques may work well in these settings, and could represent promising future work.  ### 6 Discussion  In this work, we presented hybrid generation models with access to parametric and non-parametric memory. We showed that our RAG models obtain state of the art results on open-domain QA. We found that people prefer RAG\u2019s generation over purely parametric BART, finding RAG more factual and specific. We conducted an thorough investigation of the learned retrieval component, validating its effectiveness, and we illustrated how the retrieval index can be hot-swapped to update the model without requiring any retraining. In future work, it may be fruitful to investigate if the two components can be jointly pre-trained from scratch, either with a denoising objective similar to BART or some another objective. Our work opens up new research directions on how parametric and non-parametric memories interact and how to most effectively combine them, showing promise in being applied to a wide variety of NLP tasks.   ### Broader Impact  This work offers several positive societal benefits over previous work: the fact that it is more strongly grounded in real factual knowledge (in this case Wikipedia) makes it \u201challucinate\u201d less with generations that are more factual, and offers more control and interpretability. RAG could be employed in a wide variety of scenarios with direct benefit to society, for example by endowing it with a medical index and asking it open-domain questions on that topic, or by helping people be more effective at their jobs.  With these advantages also come potential downsides: Wikipedia, or any potential external knowledge source, will probably never be entirely factual and completely devoid of bias. Since RAG can be employed as a language model, similar concerns as for GPT-2 [ 50 ] are valid here, although arguably to a lesser extent, including that it might be used to generate abuse, faked or misleading content in the news or on social media; to impersonate others; or to automate the production of spam/phishing content [ 54 ].\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat are the two formulations of RAG models described in the paper?[/INST] \nAnswer: The two formulations of RAG models described in the paper are RAG-Sequence and RAG-Token. The RAG-Sequence model uses the same retrieved document to generate the complete sequence, treating the retrieved document as a single latent variable. On the other hand, the RAG-Token model can draw a different latent document for each target token, allowing the generator to choose content from several documents when producing an answer."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nSome common techniques are outlined below.  Mean Subtraction Given a set of input dataX, it is customary to zero-center the data by subtracting the mean feature vector ofXfromX. An important point is that in practice, the mean is calculated only across the training set, and this mean is subtracted from the training, validation, and testing sets.  Normalization Another frequently used technique (though perhaps less so than mean subtraction) is to scale every input feature dimension to have similar ranges of magnitudes. This is useful since input features are often measured in different \u201cunits\u201d, but we often want to initially consider all features as equally important. The way we accomplish this is by simply dividing the features by their respective standard deviation calculated across the training set.   ## neural networks,backpropagation 15  Whitening Not as commonly used as mean-subtraction + normalization, whiten- ing essentially converts the data to a have an identity covariance matrix \u2013 that is, features become uncorrelated and have a variance of 1. This is done by first mean-subtracting the data, as usual, to get X\u2032. We can then take the Singular Value Decomposition (SVD) ofX\u2032 to get matricesU,S,V. We then computeUX\u2032to projectX\u2032into the basis defined by the columns ofU. We finally divide each dimension of the result by the corresponding singular value inSto scale our data appropriately (if a singular value is zero, we can just divide by a small number instead).  ### 2. 6 Parameter Initialization  A key step towards achieving superlative performance with a neu- ral network is initializing the parameters in a reasonable way. A good starting strategy is to initialize the weights to small random numbers normally distributed around 0 \u2013 and in practice, this often words acceptably well. However, in Understanding the dif- ficulty of training deep feedforward neural networks ( 2010 ), Xavier et alstudy the effect of different weight and bias initialization schemes on training dynamics.\n\nfilename: materials_formatted.md\nextension: .md\n\n1  Introduction  Deep neural networks trained with some version of Stochastic Gradient Descent have been shown to substantially outperform previous approaches on various supervised learning tasks in computer vision [Krizhevsky et al., 2012] and speech processing [Hinton et al., 2012]. But state-of-the-art deep neural networks often require many days of training. It is possible to speed-up the learning by computing gradients for different subsets of the training cases on different machines or splitting the neural network itself over many machines [Dean et al., 2012], but this can require a lot of com- munication and complex software. It also tends to lead to rapidly diminishing returns as the degree of parallelization increases. An orthogonal approach is to modify the computations performed in the forward pass of the neural net to make learning easier. Recently, batch normalization [Ioffe and Szegedy, 2015] has been proposed to reduce training time by including additional normalization stages in deep neural networks. The normalization standardizes each summed input using its mean and its standard deviation across the training data. Feedforward neural networks trained using batch normalization converge faster even with simple SGD. In addition to training time improvement, the stochasticity from the batch statistics serves as a regularizer during training.  Despite its simplicity, batch normalization requires running averages of the summed input statis- tics. In feed-forward networks with \ufb01xed depth, it is straightforward to store the statistics separately for each hidden layer. However, the summed inputs to the recurrent neurons in a recurrent neu- ral network (RNN) often vary with the length of the sequence so applying batch normalization to RNNs appears to require different statistics for different time-steps. Furthermore, batch normaliza-              tion cannot be applied to online learning tasks or to extremely large distributed models where the minibatches have to be small.  This paper introduces layer normalization, a simple normalization method to improve the training speed for various neural network models. Unlike batch normalization, the proposed method directly estimates the normalization statistics from the summed inputs to the neurons within a hidden layer so the normalization does not introduce any new dependencies between training cases. We show that layer normalization works well for RNNs and improves both the training time and the generalization performance of several existing RNN models.\n\nfilename: materials_formatted.md\nextension: .md\n\nThe collapse could be due to a less-explicit requirement for factual knowledge in some tasks, or the longer target sequences, which could result in less informative gradients for the retriever. Perez et al.[46]also found spurious retrieval results when optimizing a retrieval component in order to improve performance on downstream tasks.  ### I Number of instances per dataset  The number of training, development and test datapoints in each of our datasets is shown in Table 7.      \n\n# Layer Normalization:  Layer Normalization  Jimmy Lei Ba University of Toronto jimmy@psi.toronto.edu  Jamie Ryan Kiros University of Toronto rkiros@cs.toronto.edu  Geoffrey E. Hinton University of Toronto and Google Inc. hinton@cs.toronto.edu  Abstract  Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This signi\ufb01cantly reduces the training time in feed- forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural net- works. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empiri- cally, we show that layer normalization can substantially reduce the training time compared with previously published techniques.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhy is data preprocessing like mean subtraction and normalization important for neural network training?[/INST] \nAnswer: Data preprocessing such as mean subtraction and normalization are important for neural network training for several reasons. Mean subtraction is used to zero-center the data, which involves subtracting the mean feature vector of the input data from the data itself. This process is important because it is done only across the training set, and this mean is subtracted from the training, validation, and testing sets. \n\nNormalization, on the other hand, is used to scale every input feature dimension to have similar ranges of magnitudes. This is beneficial since input features are often measured in different units, but we often want to initially consider all features as equally important. This is achieved by dividing the features by their respective standard deviation calculated across the training set. \n\nThese preprocessing steps are crucial in ensuring that the data fed into the neural network is standardized and uniform, thereby improving the efficiency and effectiveness of the training process."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\n# Language Models are Few-Shot Learners:  Language Models are Few-Shot Learners  Tom B. Brown\u2217  Benjamin Mann\u2217  Nick Ryder\u2217  Melanie Subbiah\u2217  Jared Kaplan\u2020  Prafulla Dhariwal  Arvind Neelakantan  Pranav Shyam  Girish Sastry  Amanda Askell  Sandhini Agarwal  Ariel Herbert-Voss  Gretchen Krueger  Tom Henighan  Rewon Child  Aditya Ramesh  Daniel M. Ziegler  Jeffrey Wu  Clemens Winter  Christopher Hesse  Mark Chen  Eric Sigler  Mateusz Litwin  Scott Gray  Benjamin Chess  Jack Clark  Christopher Berner  Sam McCandlish  Alec Radford  Ilya Sutskever  Dario Amodei  OpenAI  Abstract  Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by \ufb01ne-tuning on a speci\ufb01c task. While typically task-agnostic in architecture, this method still requires task-speci\ufb01c \ufb01ne-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions \u2013 something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art \ufb01ne- tuning approaches. Speci\ufb01cally, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or \ufb01ne-tuning, with tasks and few-shot demonstrations speci\ufb01ed purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-\ufb02y reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic.\n\nfilename: materials_formatted.md\nextension: .md\n\nThough the results in this case are particularly striking, the general trends with both model size and number of examples in-context hold for most tasks we study. We emphasize that these \u201clearning\u201d curves involve no gradient updates or \ufb01ne-tuning, just increasing numbers of demonstrations given as conditioning.  Broadly, on NLP tasks GPT-3 achieves promising results in the zero-shot and one-shot settings, and in the the few-shot setting is sometimes competitive with or even occasionally surpasses state-of-the-art (despite state-of-the-art being held by \ufb01ne-tuned models). For example, GPT-3 achieves 81.5 F1 on CoQA in the zero-shot setting, 84.0 F1 on CoQA in the one-shot setting, 85.0 F1 in the few-shot setting. Similarly, GPT-3 achieves 64.3% accuracy on TriviaQA in the zero-shot setting, 68.0% in the one-shot setting, and 71.2% in the few-shot setting, the last of which is state-of-the-art relative to \ufb01ne-tuned models operating in the same closed-book setting.  GPT-3 also displays one-shot and few-shot pro\ufb01ciency at tasks designed to test rapid adaption or on-the-\ufb02y reasoning, which include unscrambling words, performing arithmetic, and using novel words in a sentence after seeing them de\ufb01ned only once. We also show that in the few-shot setting, GPT-3 can generate synthetic news articles which human evaluators have dif\ufb01culty distinguishing from human-generated articles.  At the same time, we also \ufb01nd some tasks on which few-shot performance struggles, even at the scale of GPT-3. This includes natural language inference tasks like the ANLI dataset, and some reading comprehension datasets like RACE or QuAC. By presenting a broad characterization of GPT-3\u2019s strengths and weaknesses, including these limitations, we hope to stimulate study of few-shot learning in language models and draw attention to where progress is most needed.  A heuristic sense of the overall results can be seen in Figure 1.3, which aggregates the various tasks (though it should not be seen as a rigorous or meaningful benchmark in itself).\n\nfilename: materials_formatted.md\nextension: .md\n\nBLEU+case.mixed+numrefs.1+smooth.exp+tok.intl+version.1.2.20]  Figure 3.4: Few-shot translation performance on 6 language pairs as model capacity increases. There is a consistent trend of improvement across all datasets as the model scales, and as well as tendency for translation into English to be stronger than translation from English.  15  Setting  Winograd Winogrande (XL)  Fine-tuned SOTA GPT-3 Zero-Shot GPT-3 One-Shot GPT-3 Few-Shot  90.1a 88.3* 89.7* 88.6*  84.6b 70.2 73.2 77.7  Table 3.5: Results on the WSC273 version of Winograd schemas and the adversarial Winogrande dataset. See Section 4 for details on potential contamination of the Winograd test set. a[SBBC19] b[LYN+20]  Figure 3.5: Zero-, one-, and few-shot performance on the adversarial Winogrande dataset as model capacity scales. Scaling is relatively smooth with the gains to few-shot learning increasing with model size, and few-shot GPT-3 175B is competitive with a \ufb01ne-tuned RoBERTA-large.  each translation task improves performance by over 7 BLEU and nears competitive performance with prior work. GPT-3 in the full few-shot setting further improves another 4 BLEU resulting in similar average performance to prior unsupervised NMT work. GPT-3 has a noticeable skew in its performance depending on language direction. For the three input languages studied, GPT-3 signi\ufb01cantly outperforms prior unsupervised NMT work when translating into English but underperforms when translating in the other direction. Performance on En-Ro is a noticeable outlier at over 10 BLEU worse than prior unsupervised NMT work. This could be a weakness due to reusing the byte-level BPE tokenizer of GPT-2 which was developed for an almost entirely English training dataset.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat is the main advantage of scaling up language models for few-shot learning, as demonstrated by GPT-3?[/INST] \nAnswer: The main advantage of scaling up language models for few-shot learning, as demonstrated by GPT-3, is that it greatly improves task-agnostic performance. This sometimes even reaches competitiveness with prior state-of-the-art fine-tuning approaches. GPT-3, with its 175 billion parameters, is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. It achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nOur reward modeling approach was developed in prior work on learning to rank [40], which has been applied to ranking search results using either explicit feedback [2, 18] or implicit feedback in the form of click-through data [29, 30]. In a related line of research, human feedback has been used to train agents in simulated environments [10, 24]. There is also a rich literature on using RL to optimize automatic metrics for NLP tasks, such as ROUGE for summarization [50, 65, 45, 15, 19], BLEU for translation [50, 66, 1, 43], and other domains [61, 27, 26]. Finally, there has been extensive research on modifying architectures [22, 59] and pre-training procedures [70, 36, 49, 60, 53, 14] for improving summarization performance.  3 Method and experiment details  3.1 High-level methodology  Our approach is similar to the one outlined in [73], adapted to the batch setting. We start with an initial policy that is \ufb01ne-tuned via supervised learning on the desired dataset (in our case, the Reddit TL;DR summarization dataset). The process (illustrated in Figure 2) then consists of three steps that can be repeated iteratively.  Step 1: Collect samples from existing policies and send comparisons to humans. For each Reddit post, we sample summaries from several sources including the current policy, initial policy, original reference summaries and various baselines. We send a batch of pairs of summaries to our human evaluators, who are tasked with selecting the best summary of a given Reddit post.  Step 2: Learn a reward model from human comparisons. Given a post and a candidate summary, we train a reward model to predict the log odds that this summary is the better one, as judged by our labelers.  Step 3: Optimize a policy against the reward model. We treat the logit output of the reward model as a reward that we optimize using reinforcement learning, speci\ufb01cally with the PPO algorithm [58].  Figure 2: Diagram of our human feedback, reward model training, and policy training procedure.  We provide a more thorough description of our procedure, including details of the reward model and policy training and our quality control process, in the following sections.\n\nfilename: materials_formatted.md\nextension: .md\n\nWe train this model to predict which summary y \u2208 {y0, y1} is better as judged by a human, given a post x. If the summary preferred by the human is yi, we can write the RM loss as:  loss(r\u03b8) = \u2212E(x,y0,y1,i)\u223cD[log(\u03c3(r\u03b8(x, yi) \u2212 r\u03b8(x, y1\u2212i)))]  where r\u03b8(x, y) is the scalar output of the reward model for post x and summary y with parameters \u03b8, and D is the dataset of human judgments. At the end of training, we normalize the reward model outputs such that the reference summaries from our dataset achieve a mean score of 0.  Human feedback policies. We want to use the reward model trained above to train a policy that generates higher-quality outputs as judged by humans. We primarily do this using reinforcement learning, by treating the output of the reward model as a reward for the entire summary that we maximize with the PPO algorithm [58], where each time step is a BPE token.8 We initialize our policy to be the model \ufb01ne-tuned on Reddit TL;DR. Importantly, we include a term in the reward that penalizes the KL divergence between the learned RL policy \u03c0RL \u03c6 with parameters \u03c6 and this original supervised model \u03c0SFT, as previously done in [25]. The full reward R can be written as:  R(x, y) = r\u03b8(x, y) \u2212 \u03b2 log[\u03c0RL  \u03c6 (y|x)/\u03c0SFT(y|x)]  This KL term serves two purposes. First, it acts as an entropy bonus, encouraging the policy to explore and deterring it from collapsing to a single mode. Second, it ensures the policy doesn\u2019t learn to produce outputs that are too different from those that the reward model has seen during training.  For the PPO value function, we use a Transformer with completely separate parameters from the policy. This prevents updates to the value function from partially destroying the pretrained policy early in training (see ablation in Appendix G.1). We initialize the value function to the parameters of the reward model. In our experiments, the reward model, policy, and value function are the same size.\n\nfilename: materials_formatted.md\nextension: .md\n\nSee D for more discussion. We leave this ablation to future work. Nevertheless, we believe reward modeling is more likely to scale to tasks where it is extremely skill-intensive or time-consuming to provide good demonstrations.  Future directions. The methods in this paper could be applied to any task where humans can compare samples, including dialogue, machine translation, question answering, speech synthesis, and music generation. We expect this method to be particularly important for generating long samples, where the distributional shift and degeneracy of maximum likelihood samples can be problematic. It may be possible to improve sample ef\ufb01ciency by training to predict feedback across many tasks [42].  We are particularly interested in scaling human feedback to tasks where humans can\u2019t easily evaluate the quality of model outputs. In this setting, it is particularly challenging to identify whether an ML system is aligned with the human designer\u2019s intentions. One approach is to train ML systems to help humans perform the evaluation task quickly and accurately [9].  There is also a rich landscape of human feedback methods beyond binary comparisons that could be explored for training models [28, 17, 44, 64]. For example, we could solicit high-quality demonstra- tions from labelers, have labelers edit model outputs to make them better, or have labelers provide explanations for why they preferred one model output over another. All of this feedback could be leveraged as a signal to train more capable reward models and policies.  Broader impacts. The techniques we explore in this paper are generic techniques that could be used in a wide variety of machine learning applications, for any task where it is feasible for humans to evaluate the quality of model outputs. Thus, the potential implications are quite broad.  Our research is primarily motivated by the potential positive effects of aligning machine learning algorithms with the designer\u2019s preferences. Many machine learning applications optimize simple metrics which are only rough proxies for what the designer intends. This can lead to problems, such as Youtube recommendations promoting click-bait [11]. In the short term, improving techniques for learning from and optimizing human preferences directly may enable these applications to be more aligned with human well-being.  In the long term, as machine learning systems become more capable it will likely become increasingly dif\ufb01cult to ensure that they are behaving safely: the mistakes they make might be more dif\ufb01cult to spot, and the consequences will be more severe.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat is the role of a reward model in reinforcement learning from human feedback?[/INST] \nAnswer: The role of a reward model in reinforcement learning from human feedback is to predict which summary is better as judged by a human. It is trained to predict the log odds that a given summary is the better one, as judged by human evaluators. The output of the reward model is then treated as a reward that is optimized using reinforcement learning, specifically with the PPO algorithm. The reward model is also used to train a policy that generates higher-quality outputs as judged by humans. The output of the reward model is treated as a reward for the entire summary that is maximized with the PPO algorithm."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\n- Adherence to Guidelines:Submissions should follow the provided guidelines, including     format, labeling, and adherence to the deadline.   \n\n# SEP 775 Assignment 3:   McMaster University SEP 775  # Assignment 3  # TANDA Approach for QA System Enhancement  ## Objectives:  Explore TANDA (Transfer And Adapt) methodology to improve Question-Answering (QA) systems using pre-trained Transformer models, focusing on sequential fine-tuning te- chniques.  1. Introduction and Theory (15%)     - Summarize the TANDA methodology\u2019s principles, focusing on its novelty and the        rationale behind sequential fine-tuning.     - Discuss how Transformers\u2019 architecture benefits the TANDA approach. 2. Preparation and Dataset Understanding (15%)     - Select the ASNQ dataset for the transfer learning step and either WikiQA or TREC-        QA for domain-specific adaptation.     - Conduct an exploratory data analysis (EDA) on your chosen datasets to understand        their structure, content, and challenges. 3. Model Implementation (30%)     - Implement a baseline Transformer model for comparison.     - Apply the TANDA two-step fine-tuning process:        - First, fine-tune on the ASNQ dataset.        - Then, adapt the model to your chosen domain-specific dataset.     - Document the modifications made to the original Transformer architecture, if any,        and justify your choices. 4. Experimental Setup and Evaluation (20%)     - Describe your experimental setup, including training details (e.g., hyperparameters,        loss functions, optimization strategy).     - Evaluate both the baseline and TANDA-enhanced models using appropriate metrics        (accuracy, F1 score, etc.).     - Analyze the performance impact of the TANDA approach compared to the baseline        model. 5. Discussion and Conclusion (20%)     - Discuss any challenges encountered during implementation and how they were ad-        dressed.     - Reflect on the effectiveness of the TANDA approach for QA systems and potential        areas for improvement.     - Propose future research directions or applications of the TANDA methodology in        other NLP tasks.  McMaster University 1    McMaster University SEP 775  # Submission Guidelines:  - Submit all code in a ZIP file, including Jupyter Notebooks, and a detailed PDF report     with written explanations and visualizations.\n\nfilename: materials_formatted.md\nextension: .md\n\n- Clearly label each part and question in your submissions. - Deadline: March 13, 2024  # Evaluation Criteria:  - Depth and clarity of theoretical understanding and literature review. - Comprehensive analysis and insightful observations in EDA. - Correctness, creativity, and efficiency in model implementation and adaptation of TANDA. - Thoroughness in experimental evaluation, including a detailed comparison of models. - Quality of discussion on findings, challenges, and future work.  # References:  - TANDA original paper:https://arxiv.org/abs/1911. - TANDA code repository:https://github.com/amazon-science/wqa_tanda - ASNQ dataset: https://huggingface.co/datasets/asnq - WikiQA dataset:https://huggingface.co/datasets/wiki_qa - Trec-QA dataset: https://huggingface.co/datasets/lucadiliello/trecqa  McMaster University 2   \n\n# SEP 775 Assignment 4:   McMaster University SEP 775 # Assignment 4  # Fine-tuning a Pretrained Model Using LoRA  # Objectives:  Leverage LoRA: Low-Rank Adaptation to fine-tune a pretrained language model for a programming-related Question-Answering (QA) system on the\"flytech/python-codes-25k\" dataset.  ## Understanding LoRA (20%):  1. Review the concept, benefits, and mechanism of Low-Rank Adaptation (LoRA) for adap-     ting pretrained models. 2. Discuss the suitability of pretrained language models for code-related QA tasks and the     advantages of using LoRA for fine-tuning.  ## Dataset Preparation (20%):  1. Provide an overview of the \"flytech/python-codes-25k\"dataset, focusing on its structure     and relevance for a QA system. 2. Describe necessary preprocessing steps, including tokenization and encoding strategies for     code snippets.  ## Model Fine-Tuning with LoRA (30%):  1. Select a suitable pretrained language model and justify the choice based on its architecture     and expected performance on code-related QA tasks. 2. Detail the integration of LoRA, specifying the adaptation process and adjustments made     to the model for the QA task.  ## Training and Evaluation (30%):  1.\n\nfilename: materials_formatted.md\nextension: .md\n\n4. Model Optimization and Training:The effectiveness of model optimization techni-     ques, including hyperparameter tuning, regularization methods, and training procedures. 5. Analysis and Interpretation:Depth of analysis on the model\u2019s performance, including     error analysis, limitations of the model, and interpretation of the results. Insightful     observations and potential improvements should be discussed. 6. Project Report: The quality of the project report, including clarity of writing, or-     ganization, and thoroughness in covering the project\u2019s scope, methodology, results, and     conclusions. The report should also include a clear abstract and introduction to the     problem. 7. Innovation and Creativity:The uniqueness of the approach and creativity in solving     the problem. Innovative use of NLP techniques or development of new methods will be     highly regarded.  ## Submission Requirements  1. Code: Submit all code files used for the project in a single compressed folder (.zip or     .tar.gz). Include a README file with instructions on how to run the code and any     dependencies required. 2. Data:If the dataset used is publicly available, provide the link to the dataset. If the da-     taset is created or modified for the project, include it in the submission with a description     in the README file. 3. Report:A project report in PDF format, not exceeding 10 pages (excluding appendices     and references), formatted in a clear and readable style. The report should include sections     on introduction, methodology, results, analysis, and conclusions. Include any relevant     figures, tables, and references. 4. Presentation: Prepare a short presentation (10-15 slides) summarizing the project\u2019s     objective, methodology, key results, and conclusions.   5. Additional Materials:Any additional materials used or created for the project, such as     scripts for data preprocessing, trained models, or Jupyter notebooks, should be included     in the submission.  Submissions failing to meet these requirements may be penalized. Please ensure that all submitted materials are well-organized and clearly labeled for easy navigation.      \n\n# Attention Is All You Need:  Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat methodology is explored in SEP775 Assignment 3 to improve QA systems?[/INST] \nAnswer: The methodology explored in SEP775 Assignment 3 to improve QA systems is the TANDA (Transfer And Adapt) approach. This involves using pre-trained Transformer models and focusing on sequential fine-tuning techniques."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\n#### DALL-E2 on Winoground I   #### DALL-E2 on Winoground II   STILL NOT SOLVED   #### Outline  1. Early models 2. Features and fusion 3. Contrastive models 4. Multimodal foundation models 5. Evaluation 6. Beyond images: Other modalities 7. Where to next?   #### Speech / audio  Can EASILY do another full lecture just on this topic.  Recent cool example: Whisper, trained on 680,000 hours of multilingual multitask data.  We can also just treat audio as vision ;) Radford et al., 2022  Kiela et al., (^2017) Bapna et al., 2022   #### Video and text and audio  MERLOT (Zellers et al., 2021) MERLOT Reserve (idem, 2022)   #### Grounded language learning in simulated environments   Hermann, Hill, et al. 2017 Das et al., 2018   #### Text to 3D  POINT-E (Nichol, Jun, et al., 2022)   #### Outline  1. Early models 2. Multimodal fusion 3. Contrastive models 4. Multimodal foundation models 5. Other modalities 6. Evaluation 7. Where to next?   #### One foundation model to rule them all  There will modality-agnostic foundation models that can read and generate many modalities.  These models can be bigger and be trained on vastly more data. Parameters will be shared in interesting ways.  Automatic alignment from unpaired unimodal data will become a big topic.   #### Multimodal scaling laws  We are just beginning to understand multimodal scaling laws, lots of interesting work to do here in understanding trade-offs.   #### Retrieval augmented generative multimodal models   Lewis et al., 2021   These can be multimodal   This can be multimodal This^ can^ be^ multimodal   #### Better evaluation and benchmarking   Barbosa-Silva et al. 2022   Kiela et al. 2020  We need better measurement.   # Thank you!\n\nfilename: materials_formatted.md\nextension: .md\n\nMultimodality is one of the main frontiers of the new foundation model revolution.   #### Multimodal brains  McGurk effect (McGurk & MacDonald, 1976)  https://www.youtube.com/watch?v=2k8fHR9jKVM   #### Multimodal applications  Let\u2019s say we\u2019re dealing with two modalities \u2013 text, and images:   \u25cf Retrieval (image <> text) \u25cf Captioning (image - > text) \u25cf Generation (text - > image) \u25cf Visual question answering (image+text - > text) \u25cf Multimodal classification (image+text - > label) \u25cf Better understanding/generation (image+text - > label/text)   #### Multimodal is hot right now  .. and/but has been \u201cthe next big thing\u201d for almost a decade!   #### Outline  1. Early models 2. Features and fusion 3. Contrastive models 4. Multimodal foundation models 5. Evaluation 6. Beyond images: Other modalities 7. Where to next?   #### Cross-modal \u201cVisual-Semantic Embeddings\u201d  WSABI (Weston et al 2010), DeVise (Frome et al 2013), Cross-Modal Transfer (Socher et al 2013)   Frome et al. 2013 Socher et al. 2013   #### Multimodal distributional semantics (Bruni et al., 2014)  Algorithm:   \u25cf Obtain visual \u201cword vector\u201d via BOVW: \u25cb Identify keypoints and get their descriptors \u25cb Cluster these and map to counts \u25cf Concatenate with textual word vector \u25cf Apply SVD to \u201cfuse\u201d information  This approach was shown to lead to better word representations on human similarity judgment datasets. Bag^ of^ visual^ words^ (BOVW)^ illustration.^ Bruni^ et^ al.,^ 2014.\n\nfilename: materials_formatted.md\nextension: .md\n\nIt is possible to encode all the phrases (60 billion phrases in Wikipedia) using dense vectors and only do nearest neighbor search without a BERT model at inference time!   51   Seoet al.,2019. Real-Time Open-DomainQuestion Answering withDense-SparsePhrase Index Lee et al.,2020. LearningDense Representations of Phrases at Scale   #### Large language model-based  #### QA (with web search!)   Problems with large language model-based QA   53  Seems unreasonable!  (1) It could not search  G-Scholar, and  (2) the follow up   suggestions make no sense. Yikes!      \n\n# SEP 775 Lecture 09:   Hamidreza Mahyar mahyarh@mcmaster.ca  ## Computational Natural Language Processing  ##### Multimodal Models   #### What is multimodality?  In our case, focusing on NLP: text + one or more other _modality_ (images, speech, audio, olfaction, others). We\u2019ll mostly focus on images as the other modality.   #### Why does multimodality matter?  A range of very good reasons:   \u25cf Faithfulness: Human experience is multimodal \u25cf Practical: The internet & many applications are multimodal \u25cf Data efficiency and availability: \u25cb Efficiency: Multimodal data is rich and \u201chigh bandwidth\u201d (compared to language; quoting LeCun, \u201can imperfect, incomplete, and low-bandwidth serialization protocol for the internal data structures we call thoughts\u201d), so better for learning? \u25cb Scaling: More data is better, and we\u2019re running out of high quality text data.  Multimodality is one of the main frontiers of the new foundation model revolution.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhy is the study of multimodal models considered crucial?[/INST] \nAnswer: The study of multimodal models is considered crucial for several reasons. Firstly, it provides faithfulness to the human experience, which is inherently multimodal. Secondly, it is practical as the internet and many applications are multimodal, dealing with different types of data such as text, images, audio, and more. Lastly, it contributes to data efficiency and availability. Multimodal data is rich and \"high bandwidth\", potentially leading to better learning. Furthermore, as we are running out of high-quality text data, multimodality provides more data for model training. It is one of the main frontiers of the new foundation model revolution."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\n2013) are publicly available toolkits for building n-gram language models. Modern language modeling is more commonly done withneural networklan- guage models, which solve the major problems with n-grams: the number of param- eters increases exponentially as the n-gram order increases, and n-grams have no way to generalize from training to test set. Neural language models instead project words into acontinuousspace in which words with similar contexts have similar representations. We\u2019ll introduce bothfeedforwardlanguage models (Bengio et al. 2006, Schwenk 2007) in Chapter 7, andrecurrentlanguage models (Mikolov, 2012) in Chapter 9.        \n\n# Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks:   ## Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks   Patrick Lewis\u2020\u2021, Ethan Perez?,   Aleksandra Piktus\u2020, Fabio Petroni\u2020, Vladimir Karpukhin\u2020, Naman Goyal\u2020, Heinrich K\u00fcttler\u2020,   Mike Lewis\u2020, Wen-tau Yih\u2020, Tim Rockt\u00e4schel\u2020\u2021, Sebastian Riedel\u2020\u2021, Douwe Kiela\u2020   \u2020Facebook AI Research;\u2021University College London;?New York University;   plewis@fb.com  ### Abstract   Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on down- stream NLP tasks. However, their ability to access and precisely manipulate knowl- edge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre- trained models with a differentiable access mechanism to explicit non-parametric memory have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) \u2014 models which combine pre-trained parametric and non-parametric mem- ory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever.\n\nfilename: materials_formatted.md\nextension: .md\n\nThese arguments led many linguists and computational linguists to ignore work in statistical modeling for decades. The resurgence of n-gram models came from Jelinek and colleagues at the IBM Thomas J. Watson Research Center, who were influenced by Shannon, and Baker at CMU, who was influenced by the work of Baum and colleagues. Independently these two labs successfully used n-grams in their speech recognition systems (Baker 1975b, Jelinek 1976, Baker 1975a, Bahl et al. 1983, Jelinek 1990). Add-one smoothing derives from Laplace\u2019s 1812 law of succession and was first applied as an engineering solution to the zero frequency problem by Jeffreys (1948) based on an earlier Add-K suggestion by Johnson (1932). Problems with the add- one algorithm are summarized in Gale and Church (1994). A wide variety of different language modeling and smoothing techniques were proposed in the 80s and 90s, including Good-Turing discounting\u2014first applied to the n-gram smoothing at IBM by Katz (N \u0301adas 1984, Church and Gale 1991)\u2014 Witten-Bell discounting (Witten and Bell, 1991), and varieties ofclass-based n- class-basedn-gram grammodels that used information about word classes. Starting in the late 1990s, Chen and Goodman performed a number of carefully controlled experiments comparing different discounting algorithms, cache models, class-based models, and other language model parameters (Chen and Goodman 1999, Goodman 2006, inter alia). They showed the advantages ofModified In- terpolated Kneser-Ney, which became the standard baseline for n-gram language modeling, especially because they showed that caches and class-based models pro- vided only minor additional improvement. These papers are recommended for any reader with further interest in n-gram language modeling. SRILM (Stolcke, 2002) and KenLM (Heafield 2011, Heafield et al. 2013) are publicly available toolkits for building n-gram language models.\n\nfilename: materials_formatted.md\nextension: .md\n\nWe do submit to the test server on a small number of datasets (SuperGLUE, TriviaQA, PiQa) where we were able to make submission work, and we submit only the 200B few-shot results, and report development set results for everything else.  3 Results  In Figure 3.1 we display training curves for the 8 models described in Section 2. For this graph we also include 6 additional extra-small models with as few as 100,000 parameters. As observed in [KMH+20], language modeling performance follows a power-law when making ef\ufb01cient use of training compute. After extending this trend by two more orders of magnitude, we observe only a slight (if any) departure from the power-law. One might worry that these improvements in cross-entropy loss come only from modeling spurious details of our training corpus. However, we will see in the following sections that improvements in cross-entropy loss lead to consistent performance gains across a broad spectrum of natural language tasks.  Below, we evaluate the 8 models described in Section 2 (the 175 billion parameter parameter GPT-3 and 7 smaller models) on a wide range of datasets. We group the datasets into 9 categories representing roughly similar tasks.  In Section 3.1 we evaluate on traditional language modeling tasks and tasks that are similar to language modeling, such as Cloze tasks and sentence/paragraph completion tasks. In Section 3.2 we evaluate on \u201cclosed book\u201d question answering tasks: tasks which require using the information stored in the model\u2019s parameters to answer general knowledge questions. In Section 3.3 we evaluate the model\u2019s ability to translate between languages (especially one-shot and few-shot). In Section 3.4 we evaluate the model\u2019s performance on Winograd Schema-like tasks. In Section 3.5 we evaluate on datasets that involve commonsense reasoning or question answering. In Section 3.6 we evaluate on reading comprehension tasks, in Section 3.7 we evaluate on the SuperGLUE benchmark suite, and in 3.8 we brie\ufb02y explore NLI. Finally, in Section 3.9, we invent some additional tasks designed especially to probe in-context learning abilities \u2013 these tasks focus on on-the-\ufb02y reasoning, adaptation skills, or open-ended text synthesis.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat are the two main issues with n-gram language models?[/INST] \nAnswer: The two main issues with n-gram language models are that the number of parameters increases exponentially as the n-gram order increases, and n-grams have no way to generalize from training to test set."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nBecause most of the weights are randomly sam- pled, is not clear what size of models one would need to solve complex real world tasks. We would make a final note about the approach pro- posed by Tomas Mikolov in his PhD thesis (Mikolov, 2012)(and implicitly used in the state of the art re- sults on language modelling (Mikolovet al., 2011)). It involves clipping the gradient\u2019s temporal compo- nents element-wise (clipping an entry when it exceeds in absolute value a fixed threshold). Clipping has been shown to do well in practice and it forms the backbone of our approach.   3.2. Scaling down the gradients As suggested in section 2.3, one simple mechanism to deal with a sudden increase in the norm of the gradi- ents is to rescale them whenever they go over a thresh- old (see algorithm 1).   Algorithm 1Pseudo-code for norm clipping the gra- dients whenever they explode g\u02c6\u2190\u2202\u2202\u03b8E if \u2016\u02c6g\u2016\u2265thresholdthen g\u02c6\u2190threshold\u2016\u02c6g\u2016 \u02c6g end if   This algorithm is very similar to the one proposed by Tomas Mikolov and we only diverged from the original proposal in an attempt to provide a better theoretical foundation (ensuring that we always move in a de- scent direction with respect to the current mini-batch), though in practice both variants behave similarly. The proposed clipping is simple to implement and computationally efficient, but it does however in- troduce an additional hyper-parameter, namely the threshold. One good heuristic for setting this thresh- old is to look at statistics on the average norm over a sufficiently large number of updates. In our ex- periments we have noticed that for a given task and model size, training is not very sensitive to this hyper- parameter and the algorithm behaves well even for rather small thresholds. The algorithm can also be thought of as adapting the learning rate based on the norm of the gradient. Compared to other learning rate adaptation strate- gies, which focus on improving convergence by col- lecting statistics on the gradient (as for example in   Duchiet al.(2011), or Moreira and Fiesler (1995) for an overview), we rely on theinstantaneousgradient.\n\nfilename: materials_formatted.md\nextension: .md\n\nNaN) which is easily detectable at runtime; this issue is called theGradient Explosion Problem. When the gradient value goes to zero, however, it can go undetected while dras- tically reducing the learning quality of the model for far-away words in the corpus; this issue is called theVanishing Gradient Problem. Due to vanishing gradients, we don\u2019t know whether there is no dependency between stepstandt+nin the data, or we just cannot capture the true dependency due to this issue. To gain practical intuition about the vanishing gradient problem, you may visit the followingexample website.  #### 2. 4 Solution to the Exploding & Vanishing Gradients  Now that we gained intuition about the nature of the vanishing gradi- ents problem and how it manifests itself in deep neural networks, let us focus on a simple and practical heuristic to solve these problems. To solve the problem of exploding gradients, Thomas Mikolov first introduced a simple heuristic solution thatclipsgradients to a small number whenever they explode. That is, whenever they reach a cer- tain threshold, they are set back to a small number as shown in Algo- rithm 1. Algorithm 1 : Pseudo-code for norm clip-   ping in the gradients whenever they ex- g\u02c6\u2190 \u2202 E plode \u2202 W if \u2016g\u02c6\u2016\u2265threshold then g\u02c6\u2190threshold \u2016g\u02c6\u2016 g\u02c6 end if  Figure 7 visualizes the effect of gradient clipping. It shows the de- cision surface of a small recurrent neural network with respect to its Wmatrix and its bias terms,b. The model consists of a single unit of recurrent neural network running through a small number of time- steps; the solid arrows illustrate the training progress on each gradient descent step. When the gradient descent model hits the high error wall in the objective function, the gradient is pushed off to a far-away loca- tion on the decision surface. The clipping model produces the dashed line where it instead pulls back the error gradient to somewhere close to the original gradient landscape.   On the diculty of training Recurrent Neural Networks   Figure 6.We plot the error surface of a single hidden unit recurrent network, highlighting the existence of high cur- vature walls. The solid lines depicts standard trajectories that gradient descent might follow.\n\nfilename: materials_formatted.md\nextension: .md\n\nThis means that we can handle very abrupt changes in norm, while the other methods would not be able to do so.  3.3. Vanishing gradient regularization  We opt to address the vanishing gradients problem us- ing a regularization term that represents a preference for parameter values such that back-propagated gra- dients neither increase or decrease too much in mag- nitude. Our intuition is that increasing the norm of \u2202xt \u2202xkmeans the error at timetis more sensitive to all inputsut,..,uk(\u2202\u2202xxtkis a factor in \u2202\u2202uEtk). In practice some of these inputs will be irrelevant for the predic- tion at timetand will behave like noise that the net- work needs to learn to ignore. The network can not learn to ignore these irrelevant inputs unless there is an error signal. These two issues can not be solved in parallel, and it seems natural to expect that we need to force the network to increase the norm of\u2202\u2202xxtkat the expense of larger errors (caused by the irrelevant input entries) andthenwait for it to learn to ignore these irrelevant input entries. This suggest that moving to- wards increasing the norm of \u2202\u2202xxtkcan not be always done while following a descent direction of the errorE (which is, for e.g., what a second order method would try to do), and therefore we need to enforce it via a regularization term.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat is the purpose of the gradient norm clipping strategy proposed in the paper?[/INST] \nAnswer: The purpose of the gradient norm clipping strategy proposed in the paper is to deal with the problem of exploding gradients in deep neural networks. When the gradient value increases significantly and reaches a certain threshold, this strategy rescales them back to a manageable size. This is done to prevent abrupt changes in the norm, which other methods might not handle effectively. The strategy is computationally efficient and simple to implement. It also introduces an additional hyper-parameter, the threshold, which can be set by looking at statistics on the average norm over a large number of updates."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nFor a \ufb01xed k, a retriever can be evaluated in isolation on top-k retrieval accuracy, which is the fraction of ques- tions for which CF contains a span that answers the question.  3 Dense Passage Retriever (DPR)  We focus our research in this work on improv- ing the retrieval component in open-domain QA. Given a collection of M text passages, the goal of our dense passage retriever (DPR) is to index all the passages in a low-dimensional and continuous space, such that it can retrieve ef\ufb01ciently the top k passages relevant to the input question for the reader at run-time. Note that M can be very large (e.g., 21 million passages in our experiments, de- scribed in Section 4.1) and k is usually small, such as 20\u2013100.  3.1 Overview  Our dense passage retriever (DPR) uses a dense encoder EP (\u00b7) which maps any text passage to a d- dimensional real-valued vectors and builds an index for all the M passages that we will use for retrieval.  3The ideal size and boundary of a text passage are func- tions of both the retriever and reader. We also experimented with natural paragraphs in our preliminary trials and found that using \ufb01xed-length passages performs better in both retrieval and \ufb01nal QA accuracy, as observed by Wang et al. (2019).  4Exceptions include (Seo et al., 2019) and (Roberts et al., 2020), which retrieves and generates the answers, respectively.  At run-time, DPR applies a different encoder EQ(\u00b7) that maps the input question to a d-dimensional vector, and retrieves k passages of which vectors are the closest to the question vector. We de\ufb01ne the similarity between the question and the passage using the dot product of their vectors:  sim(q, p) = EQ(q)(cid:124)EP (p).  (1)  Although more expressive model forms for measur- ing the similarity between a question and a passage do exist, such as networks consisting of multiple layers of cross attentions, the similarity function needs to be decomposable so that the represen- tations of the collection of passages can be pre- computed. Most decomposable similarity functions are some transformations of Euclidean distance (L2).\n\nfilename: materials_formatted.md\nextension: .md\n\nSingle and Multi denote that our Dense Passage Retriever (DPR) was trained using individial or combined training datasets (all the datasets excluding SQuAD). See text for more details.  traditional retrieval methods, the effects of different training schemes and the run-time ef\ufb01ciency.  The DPR model used in our main experiments is trained using the in-batch negative setting (Sec- tion 3.2) with a batch size of 128 and one additional BM25 negative passage per question. We trained the question and passage encoders for up to 40 epochs for large datasets (NQ, TriviaQA, SQuAD) and 100 epochs for small datasets (TREC, WQ), with a learning rate of 10\u22125 using Adam, linear scheduling with warm-up and dropout rate 0.1.  While it is good to have the \ufb02exibility to adapt the retriever to each dataset, it would also be de- sirable to obtain a single retriever that works well across the board. To this end, we train a multi- dataset encoder by combining training data from all datasets excluding SQuAD.8 In addition to DPR, we also present the results of BM25, the traditional retrieval method9 and BM25+DPR, using a linear combination of their scores as the new ranking function. Speci\ufb01cally, we obtain two initial sets of top-2000 passages based on BM25 and DPR, respectively, and rerank the union of them using BM25(q,p) + \u03bb \u00b7 sim(q, p) as the ranking function. We used \u03bb = 1.1 based on the retrieval accuracy in the development set.  5.1 Main Results  Table 2 compares different passage retrieval sys- tems on \ufb01ve QA datasets, using the top-k accuracy (k \u2208 {20, 100}). With the exception of SQuAD, DPR performs consistently better than BM25 on all datasets. The gap is especially large when k is small (e.g., 78.4% vs. 59.1% for top-20 accuracy on Natural Questions). When training with mul-  8SQuAD is limited to a small set of Wikipedia documents and thus introduces unwanted bias. We will discuss this issue more in Section 5.1.  9Lucene implementation.\n\nfilename: materials_formatted.md\nextension: .md\n\nOur experiments show that using triplet loss does not affect the results much. More details can be found in Appendix B.  Cross-dataset generalization One interesting question regarding DPR\u2019s discriminative training is how much performance degradation it may suf- fer from a non-iid setting. In other words, can it still generalize well when directly applied to a different dataset without additional \ufb01ne-tuning? To test the cross-dataset generalization, we train DPR on Natural Questions only and test it directly on the smaller WebQuestions and CuratedTREC datasets. We \ufb01nd that DPR generalizes well, with 3-5 points loss from the best performing \ufb01ne-tuned model in top-20 retrieval accuracy (69.9/86.3 vs. 75.0/89.1 for WebQuestions and TREC, respec- tively), while still greatly outperforming the BM25 baseline (55.0/70.9).  5.3 Qualitative Analysis  Although DPR performs better than BM25 in gen- eral, passages retrieved by these two methods dif- fer qualitatively. Term-matching methods like BM25 are sensitive to highly selective keywords and phrases, while DPR captures lexical variations or semantic relationships better. See Appendix C for examples and more discussion.  5.4 Run-time Ef\ufb01ciency  The main reason that we require a retrieval compo- nent for open-domain QA is to reduce the number of candidate passages that the reader needs to con- sider, which is crucial for answering user\u2019s ques- tions in real-time. We pro\ufb01led the passage retrieval speed on a server with Intel Xeon CPU E5-2698 v4 @ 2.20GHz and 512GB memory. With the help of FAISS in-memory index for real-valued vectors10, DPR can be made incredibly ef\ufb01cient, processing 995.0 questions per second, returning top 100 pas- sages per question. In contrast, BM25/Lucene (im- plemented in Java, using \ufb01le index) processes 23.7 questions per second per CPU thread.  On the other hand, the time required for building an index for dense vectors is much longer.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat is the Dense Passage Retriever (DPR) optimized for?[/INST] \nAnswer: The Dense Passage Retriever (DPR) is optimized for improving the retrieval component in open-domain question answering (QA). It is designed to index all passages in a low-dimensional and continuous space, enabling it to efficiently retrieve the top k passages relevant to an input question at run-time."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\n( 1988 ). Neurocomputing: Foundations of research. chapter Learning Representations by Back-propagating Errors, pages 696 \u2013 699. MIT Press, Cambridge, MA, USA.      \n\n# SEP 775 Lecture 02:  # Natural Language Processing with Deep Learning  ## SEP 775 Lecture Notes: Part II   ## Word Vectors II: GloVe, Evaluation and Training  Keyphrases: Global Vectors for Word Representation (GloVe). In- trinsic and extrinsic evaluations. Effect of hyperparameters on anal- ogy evaluation tasks. Correlation of human judgment with word vector distances. Dealing with ambiguity in word using contexts. Window classification. This set of notes first introduces the GloVe model for training word vectors. Then it extends our discussion of word vectors (in- terchangeably called word embeddings) by seeing how they can be evaluated intrinsically and extrinsically. As we proceed, we discuss the example of word analogies as an intrinsic evaluation technique and how it can be used to tune word embedding techniques. We then discuss training model weights/parameters and word vectors for ex- trinsic tasks. Lastly we motivate artificial neural networks as a class of models for natural language processing tasks.  ## 1 Global Vectors for Word Representation (GloVe)^33 This section is based on the GloVe  ### 1. 1 Comparison with Previous Methods  So far, we have looked at two main classes of methods to find word embeddings. The first set are count-based and rely on matrix factor- ization (e.g. LSA, HAL). While these methods effectively leverage global statistical information, they are primarily used to capture word similarities and do poorly on tasks such as word analogy, indi- cating a sub-optimal vector space structure. The other set of methods are shallow window-based (e.g. the skip-gram and the CBOW mod- els), which learn word embeddings by making predictions in local context windows. These models demonstrate the capacity to capture complex linguistic patterns beyond word similarity, but fail to make use of the global co-occurrence statistics.\n\nfilename: materials_formatted.md\nextension: .md\n\nSVD-L 300 42B 38.4 58.2 49. GloVe 300 42B 81.9 69.3 75.   Table 5 : Here we compare the perfor- mance of different models under the use of different hyperparameters and datasets   ## word vectors ii:glove,evaluation and training 8   Inspecting the above table, we can make 3 primary observations:  - Performance is heavily dependent on the model used for word     embedding:     This is an expected result since different methods try embedding     words to vectors using fundamentally different properties (such as     co-occurrence count, singular vectors, etc.) Implementation Tip: A window size        of 8 around each center word typically        works well for GloVe embeddings   Figure 2 : Here we see how training time improves training performance and helps squeeze the last few perfor- mance.  - Performance increases with larger corpus sizes:     This happens because of the experience an embedding technique     gains with more examples it sees. For instance, an analogy com-     pletion example will produce incorrect results if it has not encoun-     tered the test words previously. - Performance is lower for extremely low dimensional word vec-     tors:     Lower dimensional word vectors are not able to capture the dif-     ferent meanings of the different words in the corpus. This can be     viewed as a high bias problem where our model complexity is too     low. For instance, let us consider the words \"king\", \"queen\", \"man\",     \"woman\". Intuitively, we would need to use two dimensions such     as \"gender\" and \"leadership\" to encode these into 2 -bit word vec-     tors. Any lower would fail to capture semantic differences between     the four words.        Extremely high dimensional vectors:        Intuitively, it would seem that these        vectors would capture noise in the        corpus that doesn\u2019t allow generaliza-        tion, i.e. resulting in high variance, but        Yin et al. show in On theDimen-        sionality ofWordEmbeddingthat        skip-gram and GloVe are robust to this        over-fitting.  Figure 3 demonstrates how accuracy has been shown to improve with larger corpus.\n\nfilename: materials_formatted.md\nextension: .md\n\n(2014) proposed explicit word embed- dings based on a PPMI metric. In the skip-gram and ivLBL models, the objec- tive is to predict a word\u2019s context given the word itself, whereas the objective in the CBOW and vLBL models is to predict a word given its con- text. Through evaluation on a word analogy task, these models demonstrated the capacity to learn linguistic patterns as linear relationships between the word vectors. Unlike the matrix factorization methods, the shallow window-based methods suffer from the disadvantage that they do not operate directly on the co-occurrence statistics of the corpus. Instead, these models scan context windows across the en- tire corpus, which fails to take advantage of the vast amount of repetition in the data.  ## 3 The GloVe Model   The statistics of word occurrences in a corpus is the primary source of information available to all unsupervised methods for learning word represen- tations, and although many such methods now ex- ist, the question still remains as to how meaning is generated from these statistics, and how the re- sulting word vectors might represent that meaning. In this section, we shed some light on this ques- tion. We use our insights to construct a new model for word representation which we call GloVe, for Global Vectors, because the global corpus statis- tics are captured directly by the model. First we establish some notation. Let the matrix of word-word co-occurrence counts be denoted by X, whose entriesXi jtabulate the number of times word\u2211 joccurs in the context of wordi. LetXi= kXikbe the number of times any word appears in the context of wordi. Finally, letPi j=P(j|i)= Xi j/Xibe the probability that wordjappear in the   Table 1: Co-occurrence probabilities for target wordsiceandsteamwith selected context words from a 6 billion token corpus. Only in the ratio does noise from non-discriminative words likewaterandfashion cancel out, so that large values (much greater than 1) correlate well with properties specific to ice, and small values (much less than 1) correlate well with properties specific of steam.   Probability and Ratio k=solid k=gas k=water k=fashion P(k|ice) 1.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nHow does GloVe differ from previous word embedding methods?[/INST] \nAnswer: GloVe, or Global Vectors, is a model for word representation that directly captures the global corpus statistics, which is a contrast to previous methods. Prior to GloVe, there were two main classes of methods to find word embeddings. The first set are count-based methods that rely on matrix factorization. These methods effectively leverage global statistical information, but primarily capture word similarities and perform poorly on tasks such as word analogy. The second set of methods are shallow window-based models, which learn word embeddings by making predictions in local context windows. These models can capture complex linguistic patterns beyond word similarity, but fail to make use of the global co-occurrence statistics. GloVe, on the other hand, combines these approaches by using word-word co-occurrence counts to generate word vectors that encapsulate meaning derived from the entire corpus."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nFigure 1: The Transformer - model architecture.  The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.  3.1 Encoder and Decoder Stacks  Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position- wise fully connected feed-forward network. We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512.  Decoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.  3.2 Attention  An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum  Scaled Dot-Product Attention  Multi-Head Attention  Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.  of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.  3.2.1 Scaled Dot-Product Attention  We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2).\n\nfilename: materials_formatted.md\nextension: .md\n\nMulti-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.  MultiHead(Q, K, V ) = Concat(head1, ..., headh)W O  where headi = Attention(QW Q  i , KW K i  , V W V  i )  Where the projections are parameter matrices W Q and W O \u2208 Rhdv\u00d7dmodel.  i \u2208 Rdmodel\u00d7dk , W K  i \u2208 Rdmodel\u00d7dk , W V  i \u2208 Rdmodel\u00d7dv  In this work we employ h = 8 parallel attention layers, or heads. For each of these we use dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.  3.2.3 Applications of Attention in our Model  The Transformer uses multi-head attention in three different ways:  \u2022 In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9].  \u2022 The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.  \u2022 Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to \u2212\u221e) all values in the input of the softmax which correspond to illegal connections. See Figure 2.  3.3 Position-wise Feed-Forward Networks  In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically.\n\nfilename: materials_formatted.md\nextension: .md\n\nIntuitively, these are the biggest components to understand. How- ever, as of 2023 , by far the most-used architecture in NLP is called theTransformer, introduced by [Vaswani et al., 2017 ], and it contains a number of components that end up being quite important. So now we\u2019ll get into the details of that architecture.  #### 3 The Transformer  The Transformer is an architecture based on self-attention that con- sists of stackedBlocks, each of which contains self-attention and feed- forward layers, and a few other components we\u2019ll discuss. If you\u2019d like to take a peek for intuition, we have a diagram of a Transformer language model architecture in Figure 4. The components we haven\u2019t gone over are multi-head self-attention, layer normalization , resid- ual connections , and attention scaling \u2014and of course, we\u2019ll discuss how these components are combined to form the Transformer.   Embeddings   Add Position Embeddings   Masked Multi- Head Attention   Add & Norm   Feed-Forward   Add & Norm   Block   Linear   Softmax   Repeat for number of   encoder blocks   Probabilities   Decoder Inputs  #### Transformer Decoder   Figure 4 : Diagram of the Transformer Decoder (without corresponding Encoder, and so no cross-attention.  ##### 3. 1 Multi-head Self-Attention  Intuitively, a single call of self-attention is best at picking out asingle value (on average) from the input value set. It does so softly, by averaging over all of the values, but it requires a balancing game in the key-query dot products in order to carefully average two or more things. What we\u2019ll present now, multi-head self-attention , intuitively applies self-attention multiple times at once, each with different key, query, and value transformations of the same input, and then combines the outputs. For an integer number of headsk, we define matricesK(\u2113),Q(\u2113),V(\u2113)\u2208 R d\u00d7d/kfor\u2113in{1,... ,k}. (We\u2019ll see why we have the dimensionality reduction tod/ksoon.) These our the key, query, and value matrices for each head.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nHow does multi-head self-attention enhance the Transformer's performance?[/INST] \nAnswer: Multi-head self-attention enhances the Transformer's performance by allowing the model to attend to information from different representation subspaces at different positions simultaneously. With a single attention head, the model can only average the information, which can limit its effectiveness. By using multiple attention heads, the model can capture various aspects of the input data simultaneously. Each head operates on different key, query, and value transformations of the same input, and their outputs are then combined. This approach not only increases the capacity of the model without additional computational cost but also allows it to handle more complex patterns in the data."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nOur work show that Transformers not only achieve state-of-the-art perplexity on modeling these complex expressive piano performances, but can also generate them at the scale of 60s (~2000 tokens) with remarkable internal consistency. Our relative attention mechanism is essential to the model\u2019s quality. In listening tests (see Section 4.2.3), samples from models with relative self-attention were perceived as more coherent than the baseline Transformer model from Vaswani et al. (2017). Relative attention not only enables Transformers to generate continuations that elaborate on a given motif, but also to generalize and generate in consistent fashion beyond the length it was trained on (see Section 4.2.1). In a seq2seq setup, Transformers can generate accompaniments conditioned on melodies, enabling users to interact with the model.  Algorithmic contributions The space complexity of the relative self attention mechanism in its original formulation (Shaw et al., 2018) made it infeasible to train on sequences of suf\ufb01cient length to capture long-range structure in longer musical compositions. Addressing this we present a crucial algorithmic improvement to the relative self attention mechanism, dramatically reducing its memory requirements from O(L2D) to O(LD). For example, we reduce the memory consumption per layer from 8.5 GB to 4.2 MB (per head from 1.1 GB to 0.52 MB) for a sequence of length L = 2048 and hidden-state size D = 512 (per head Dh = D H = 64, where number of heads is H = 8) (see Table 1), allowing us to use GPUs to train the relative self-attention Transformer on long sequences.  2 RELATED WORK  Sequence models have been the canonical choice for modeling music, from Hidden Markov Models to RNNs and Long Short Term Memory networks (e.g., Eck & Schmidhuber, 2002; Liang, 2016; Oore et al., 2018), to bidirectional LSTMs (e.g., Hadjeres et al., 2017). Successful application of sequential models to polyphonic music often requires serializing the musical score or performance  into a single sequence, for example by interleaving different instruments or voices.\n\nfilename: materials_formatted.md\nextension: .md\n\nTable 4: Validation conditional NLL given groundtruth melody from Piano-e-Competition.  Model variation  Baseline Transformer Relative Transformer (ours)  NLL  2.066 1.786  4.2.3 HUMAN EVALUATIONS  To compare the perceived sample quality of models trained on the Piano-e-Competition dataset, and their ability to generate a continuation for a priming sequence, we carried out a listening test study comparing the baseline Transformer, our Transformer with relative-attention, PerformanceRNN (LSTM), and the validation set. Participants were presented with two musical excerpts (from two different models that were given the same priming sequence) and asked to rate which one is more musical on a Likert scale. For each model, we generated 10 samples each with a different prime, and compared them to three other models, resulting in 60 pairwise comparisons. Each pair was rated by 3 different participants, yielding a total of 180 comparisons.  Figure 5 shows the number of comparisons in which an excerpt from each model was selected as more musical. The improvement in sample quality from using relative attention over the baseline Transformer model was statistically signi\ufb01cant (see Appendix B for the analysis), both in aggregate and between the pair. Even though in aggregate LSTMs performed better in the study than the Transformer, despite having higher perplexity, but when compared against each other head to head, the results were not statistically signi\ufb01cant (see Table 5 in Appendix B).  Figure 5: Number of wins for each model. Error bars show standard deviations of mean.  5 CONCLUSION  In this work we demonstrated that the Transformer equipped with relative attention is very well-suited for generative modeling of symbolic music. The compelling long-term structure in the samples from our model leaves us enthusiastic about this direction of research. Moreover, the ability to expand upon a primer, in particular, suggests potential applications as creative tool.  The signi\ufb01cant improvement from relative attention highlights a shortcoming of the original Trans- former that might also limit its performance in other domains. Improving the Transformer\u2019s ability to capture periodicity at various time scales, for instance, or relations between scalar features akin to pitch could improve time-series models.\n\nfilename: materials_formatted.md\nextension: .md\n\nEach pair was rated by 3 different participants, yielding a total of 180 comparisons.  B.2 ANALYSIS  A Kruskal-Wallis H test of the ratings showed that there was a statistically signi\ufb01cant difference between the models: \u03c72(2) = 63.84, p = 8.86e-14< 0.01. Table 5 show a post-hoc analysis on the comparisons within each pair, using the Wilcoxon signed-rank test for matched samples. Table 6 shows a post-hoc analysis of how well each model performed when compared to all pairs, and compares each model\u2019s aggregate against each other, using the Mann\u2013Whitney U test for independent samples. We use a Bonferroni correction on both to correct for multiple comparisons. The win and loss counts bucket scores 4, 5 and scores 1, 2 respectively, while the tieing score is 3.  Both within pairs and between aggregates, participants rated samples from our relative Transformer as more musical than the baseline Transformer with p < 0.01/6.  For within pairs, we did not observe a consistent statistically signi\ufb01cant difference between the other model pairs, baseline transformer versus LSTM and LSTM versus relative Transformer.  When comparing between aggregates, LSTM was overall perceived as more musical than baseline Transformer. Relative Transformer came a bit close to outperforming LSTM with p = 0.018. When we listen to the samples from the two, they do sound qualitatively different. Relative Transformer often exhibits much more structure (as shown in Figure 4), but the effects were probably less pronounced in the listening test because we used samples around 10s to 15s, which is half the length of those shown in Figure 4 to prevent the baseline Transformer from deteriorating. This weakens the comparison on long-term structure.  When compared to real music from the validation set, we see that in aggregates, real music was better than LSTM and baseline Transformer. There was no statistical signi\ufb01cant difference between real music and relative Transformer. This is probably again due to the samples being too short as real music is de\ufb01nitely still better.  Table 5: A post-hoc comparison of each pair on their pairwise comparisons with each other, using the Wilcoxon signed-rank test for matched samples.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nHow does the Music Transformer perform compared to LSTM-based models in generating music?[/INST] \nAnswer: The Music Transformer equipped with relative attention performed quite well compared to LSTM-based models. In a listening test study, LSTM models performed better in aggregate than the Transformer. However, when compared head-to-head, the difference was not statistically significant. The relative Transformer often exhibited more structure, but this was less pronounced in the listening test due to the shorter sample length used. When compared to real music from the validation set, there was no statistically significant difference between real music and the relative Transformer."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\n4.2 Abstractive Question Answering  As shown in Table 2, RAG-Sequence outperforms BART on Open MS-MARCO NLG by 2.6 Bleu points and 2.6 Rouge-L points. RAG approaches state-of-the-art model performance, which is impressive given that (i) those models access gold passages with specific information required to generate the reference answer , (ii) many questions are unanswerable without the gold passages, and (iii) not all questions are answerable from Wikipedia alone. Table 3 shows some generated answers from our models. Qualitatively, we find that RAG models hallucinate less and generate factually correct text more often than BART. Later, we also show that RAG generations are more diverse than BART generations (see \u00a74.5).  4.3 Jeopardy Question Generation  Table 2 shows that RAG-Token performs better than RAG-Sequence on Jeopardy question generation, with both models outperforming BART on Q-BLEU-1. 4 shows human evaluation results, over 452 pairs of generations from BART and RAG-Token. Evaluators indicated that BART was more factual than RAG in only 7.1% of cases, while RAG was more factual in 42.7% of cases, and both RAG and BART were factual in a further 17% of cases, clearly demonstrating the effectiveness of RAG on the task over a state-of-the-art generation model. Evaluators also find RAG generations to be more specific by a large margin. Table 3 shows typical generations from each model.  Jeopardy questions often contain two separate pieces of information, and RAG-Token may perform best because it can generate responses that combine content from several documents. Figure 2 shows an example. When generating \u201cSun\u201d, the posterior is high for document 2 which mentions \u201cThe Sun Also Rises\u201d. Similarly, document 1 dominates the posterior when \u201cA Farewell to Arms\u201d is generated. Intriguingly, after the first token of each book is generated, the document posterior flattens. This observation suggests that the generator can complete the titles without depending on specific documents. In other words, the model\u2019s parametric knowledge is sufficient to complete the titles.\n\nfilename: materials_formatted.md\nextension: .md\n\nThese approaches have proved successful in a number of domains including Machine Translation [ 18 , 22 ] and Semantic Parsing [ 21 ]. Our approach does have several differences, including less of emphasis on lightly editing a retrieved item, but on aggregating content from several pieces of retrieved content, as well as learning latent retrieval, and retrieving evidence documents rather than related training pairs. This said, RAG techniques may work well in these settings, and could represent promising future work.  ### 6 Discussion  In this work, we presented hybrid generation models with access to parametric and non-parametric memory. We showed that our RAG models obtain state of the art results on open-domain QA. We found that people prefer RAG\u2019s generation over purely parametric BART, finding RAG more factual and specific. We conducted an thorough investigation of the learned retrieval component, validating its effectiveness, and we illustrated how the retrieval index can be hot-swapped to update the model without requiring any retraining. In future work, it may be fruitful to investigate if the two components can be jointly pre-trained from scratch, either with a denoising objective similar to BART or some another objective. Our work opens up new research directions on how parametric and non-parametric memories interact and how to most effectively combine them, showing promise in being applied to a wide variety of NLP tasks.   ### Broader Impact  This work offers several positive societal benefits over previous work: the fact that it is more strongly grounded in real factual knowledge (in this case Wikipedia) makes it \u201challucinate\u201d less with generations that are more factual, and offers more control and interpretability. RAG could be employed in a wide variety of scenarios with direct benefit to society, for example by endowing it with a medical index and asking it open-domain questions on that topic, or by helping people be more effective at their jobs.  With these advantages also come potential downsides: Wikipedia, or any potential external knowledge source, will probably never be entirely factual and completely devoid of bias. Since RAG can be employed as a language model, similar concerns as for GPT-2 [ 50 ] are valid here, although arguably to a lesser extent, including that it might be used to generate abuse, faked or misleading content in the news or on social media; to impersonate others; or to automate the production of spam/phishing content [ 54 ].\n\nfilename: materials_formatted.md\nextension: .md\n\nFor Open MS-MARCO, where useful retrieved documents cannot always be retrieved, we observe that the model learns to always retrieve a particular set of documents for questions that are less likely to benefit from retrieval, suggesting that null document mechanisms may not be necessary for RAG.  ### G Parameters  Our RAG models contain the trainable parameters for the BERT-base query and document encoder of DPR, with 110M parameters each (although we do not train the document encoder ourselves) and 406M trainable parameters from BART-large, 406M parameters, making a total of 626M trainable   Table 7: Number of instances in the datasets used. *A hidden subset of this data is used for evaluation   Task Train Development Test Natural Questions 79169 8758 3611 TriviaQA 78786 8838 11314 WebQuestions 3418 362 2033 CuratedTrec 635 134 635 Jeopardy Question Generation 97392 13714 26849 MS-MARCO 153726 12468 101093* FEVER-3-way 145450 10000 10000 FEVER-2-way 96966 6666 6666  parameters. The best performing \"closed-book\" (parametric only) open-domain QA model is T5-11B with 11 Billion trainable parameters. The T5 model with the closest number of parameters to our models is T5-large (770M parameters), which achieves a score of 28.9 EM on Natural Questions [ 52 ], substantially below the 44.5 that RAG-Sequence achieves, indicating that hybrid parametric/non- parametric models require far fewer trainable parameters for strong open-domain QA performance. The non-parametric memory index does not consist of trainable parameters, but does consists of 21M 728 dimensional vectors, consisting of 15.3B values. These can be easily be stored at 8-bit floating point precision to manage memory and disk footprints.  ### H Retrieval Collapse  In preliminary experiments, we observed that for some tasks such as story generation [ 11 ], the retrieval component would \u201ccollapse\u201d and learn to retrieve the same documents regardless of the input. In these cases, once retrieval had collapsed, the generator would learn to ignore the documents, and the RAG model would perform equivalently to BART.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nHow does the paper demonstrate the effectiveness of RAG models?[/INST] \nAnswer: The paper demonstrates the effectiveness of RAG models through a series of experiments and comparisons. RAG-Sequence outperforms BART on Open MS-MARCO NLG by 2.6 Bleu points and 2.6 Rouge-L points. RAG models are found to hallucinate less and generate more factually correct text than BART. In Jeopardy question generation, RAG-Token performs better than RAG-Sequence and both models outperform BART on Q-BLEU-1. Human evaluation results show that RAG was more factual than BART in 42.7% of cases. The paper also presents that RAG models require far fewer trainable parameters for strong open-domain QA performance compared to T5 models. These findings clearly demonstrate the effectiveness of RAG models."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nBy applying a modern reader model to the top retrieved passages, we achieve compara- ble or better results on multiple QA datasets in the open-retrieval setting, compared to several, much complicated systems.  2 Background  The problem of open-domain QA studied in this paper can be described as follows. Given a factoid question, such as \u201cWho \ufb01rst voiced Meg on Family Guy?\u201d or \u201cWhere was the 8th Dalai Lama born?\u201d, a system is required to answer it using a large corpus of diversi\ufb01ed topics. More speci\ufb01cally, we assume  e  the extractive QA setting, in which the answer is restricted to a span appearing in one or more pas- sages in the corpus. Assume that our collection contains D documents, d1, d2, \u00b7 \u00b7 \u00b7 , dD. We \ufb01rst split each of the documents into text passages of equal lengths as the basic retrieval units3 and get M total passages in our corpus C = {p1, p2, . . . , pM }, where each passage pi can be viewed as a sequence 2 , \u00b7 \u00b7 \u00b7 , w(i) 1 , w(i) of tokens w(i) |pi|. Given a question q, the task is to \ufb01nd a span w(i) s+1, \u00b7 \u00b7 \u00b7 , w(i) s , w(i) from one of the passages pi that can answer the question. Notice that to cover a wide variety of domains, the corpus size can easily range from millions of docu- ments (e.g., Wikipedia) to billions (e.g., the Web). As a result, any open-domain QA system needs to include an ef\ufb01cient retriever component that can se- lect a small set of relevant texts, before applying the reader to extract the answer (Chen et al., 2017).4 Formally speaking, a retriever R : (q, C) \u2192 CF is a function that takes as input a question q and a corpus C and returns a much smaller \ufb01lter set of texts CF \u2282 C, where |CF | = k (cid:28) |C|.\n\nfilename: materials_formatted.md\nextension: .md\n\nAl- though it is still behind cross-attention models, the gap has been greatly reduced and serves as a strong starting point for the open-domain QA model.  Open-domain QA. Experimental results on open-domain QA are summarized in Table 3. With- out any sparse representations, DensePhrases out- performs previous phrase retrieval models by a large margin and achieves a 15%\u201325% absolute improvement on all datasets except SQuAD. Train- ing the model of Lee et al. (2020) on Cphrase = {NQ, SQuAD} only increases the result from 14.5% to 16.5% on NQ, demonstrating that it does not suf\ufb01ce to simply add more datasets for train- ing phrase representations. Our performance is also competitive with recent retriever-reader mod- els (Karpukhin et al., 2020), while running much faster during inference (Table 1).  Model  Retriever-reader  Cretr: (Pre-)Training  NQ WQ TREC TQA SQuAD  DrQA (Chen et al., 2017) BERT + BM25 (Lee et al., 2019) ORQA (Lee et al., 2019) REALMNews (Guu et al., 2020) DPR-multi (Karpukhin et al., 2020)  - - {Wiki.\n\nfilename: materials_formatted.md\nextension: .md\n\nArXiv, abs/1909.08593, 2019.  75     \n\n# Dense Passage Retrieval for Open-Domain Question Answering:  Dense Passage Retrieval for Open-Domain Question Answering  Vladimir Karpukhin\u2217, Barlas O\u02d8guz\u2217, Sewon Min\u2020, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen\u2021, Wen-tau Yih  Facebook AI  \u2020University of Washington  \u2021Princeton University  {vladk, barlaso, plewis, ledell, edunov, scottyih}@fb.com sewon@cs.washington.edu danqic@cs.princeton.edu  Abstract  Open-domain question answering relies on ef- \ufb01cient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented us- ing dense representations alone, where em- beddings are learned from a small number of questions and passages by a simple dual- encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene- BM25 system greatly by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.1  1  Introduction  Open-domain question answering (QA) (Voorhees, 1999) is a task that answers factoid questions us- ing a large collection of documents. While early QA systems are often complicated and consist of multiple components (Ferrucci (2012); Moldovan et al. (2003), inter alia), the advances of reading comprehension models suggest a much simpli\ufb01ed two-stage framework: (1) a context retriever \ufb01rst selects a small subset of passages where some of them contain the answer to the question, and then (2) a machine reader can thoroughly exam- ine the retrieved contexts and identify the correct answer (Chen et al., 2017). Although reducing open-domain QA to machine reading is a very rea- sonable strategy, a huge performance degradation is often observed in practice2, indicating the needs of improving retrieval.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat are the limitations of relying solely on large language models for open-domain QA?[/INST] \nAnswer: The limitations of relying solely on large language models for open-domain QA include the need for an efficient retriever component to select a small set of relevant texts from a large corpus before applying the reader to extract the answer. This is due to the corpus size that can range from millions to billions of documents. Additionally, simply adding more datasets for training phrase representations does not suffice. There is also a performance degradation observed in practice when reducing open-domain QA to machine reading, indicating the need for improving retrieval."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nThis presents a more realistic scenario for practical applications.  Current approaches require a blackbox informa- tion retrieval (IR) system to do much of the heavy lifting, even though it cannot be \ufb01ne-tuned on the downstream task. In the strongly supervised set- ting popularized by DrQA (Chen et al., 2017), they also assume a reading comprehension model trained on question-answer-evidence triples, such as SQuAD (Rajpurkar et al., 2016). The IR sys- tem is used at test time to generate evidence candi- dates in place of the gold evidence. In the weakly supervised setting, proposed by TriviaQA (Joshi  et al., 2017), SearchQA (Dunn et al., 2017), and Quasar (Dhingra et al., 2017), the dependency on strong supervision is removed by assuming that the IR system provides noisy gold evidence.  These approaches rely on the IR system to mas- sively reduce the search space and/or reduce spu- rious ambiguity. However, QA is fundamentally different from IR (Singh, 2012). Whereas IR is concerned with lexical and semantic matching, questions are by de\ufb01nition under-speci\ufb01ed and re- quire more language understanding, since users are explicitly looking for unknown information. Instead of being subject to the recall ceiling from blackbox IR systems, we should directly learn to retrieve using question-answering data.  In this work, we introduce the \ufb01rst Open- Retrieval Question Answering system (ORQA). ORQA learns to retrieve evidence from an open corpus, and is supervised only by question- answer string pairs. While recent work on im- proving evidence retrieval has made signi\ufb01cant progress (Wang et al., 2018; Kratzwald and Feuer- riegel, 2018; Lee et al., 2018; Das et al., 2019), they still only rerank a closed evidence set. The main challenge to fully end-to-end learning is that retrieval over the open corpus must be considered a latent variable that would be impractical to train from scratch. IR systems offer a reasonable but potentially suboptimal starting point.\n\nfilename: materials_formatted.md\nextension: .md\n\n[SEP]...ZIPs for other operating systems may be preceded by...[SEP]  M L P  M  L  P  M L  P  M L P  M  L  P  M L  P  Sread (0, \u201cThe term\u201d, q)  Sread (0, \u201cZone Improvement Plan\u201d, q)  Sread (0, ..., q)  Sread (2, \u201cZIPs\u201d, q)  Sread (2, \u201coperating systems\u201d, q)  Sread (2, ..., q)  Figure 1: Overview of ORQA. A subset of all possible answer derivations given a question q is shown here. Retrieval scores Sretr (q, b) are computed via inner products between BERT-based encoders. Top-scoring evidence blocks are jointly encoded with the question, and span representations are scored with a multi-layer perceptron (MLP) to compute Sread (q, b, s). The \ufb01nal joint model score is Sretr (q, b) + Sread (q, b, s). Unlike previous work using IR systems for candidate proposal, we learn to retrieve from all of Wikipedia directly.  Sread (b, s, q) is learned from gold answer deriva- tions, typically from the SQuAD (Rajpurkar et al., 2016) dataset, where the evidence text is given.  In work that is more closely related to our ap- proach, the reader is learned entirely from weak supervision (Joshi et al., 2017; Dhingra et al., 2017; Dunn et al., 2017). Spurious ambiguities (see Table 2) are heuristically removed by the re- trieval system, and the cleaned results are treated as gold derivations.  3 Open-Retrieval Question Answering  (ORQA)  We propose an end-to-end model where the re- triever and reader components are jointly learned, which we refer to as the Open-Retrieval Question Answering (ORQA) model. An important aspect of ORQA is its expressivity\u2014it is capable of re- trieving any text in an open corpus, rather than be- ing limited to the closed set returned by a black- box IR system.\n\nfilename: materials_formatted.md\nextension: .md\n\nIR systems offer a reasonable but potentially suboptimal starting point.  The key insight of this work is that end-to- end learning is possible if we pre-train the re- triever with an unsupervised Inverse Cloze Task (ICT). In ICT, a sentence is treated as a pseudo- question, and its context is treated as pseudo- evidence. Given a pseudo-question, ICT requires selecting the corresponding pseudo-evidence out of the candidates in a batch. ICT pre-training provides a suf\ufb01ciently strong initialization such that ORQA, a joint retriever and reader model, can be \ufb01ne-tuned end-to-end by simply optimiz-              Task  Training  Evaluation  Evidence  Answer  Evidence  Answer  Example  Reading Comprehension Open-domain QA  Unsupervised QA Strongly Supervised QA Weakly Supervised QA Closed Retrieval QA Open Retrieval QA  given  none given  heuristic learned  span  none span  string string  given  string  SQuAD (Rajpurkar et al., 2016)  none heuristic  heuristic learned  string string  string string  GPT-2 (Radford et al., 2019) DrQA (Chen et al., 2017)  TriviaQA (Joshi et al., 2017) ORQA (this work)  Table 1: Comparison of assumptions made by related tasks, along with references to examples. Heuristic evidence refers to the typical strategy of considering only a closed set of evidence documents from a traditional IR system, which sets a strict upper-bound on task performance. In this work (ORQA), only question-answer string pairs are observed during training, and evidence retrieval is learned in a completely end-to-end manner.  ing the marginal log-likelihood of correct answers that were found.  We evaluate ORQA on open versions of \ufb01ve ex- isting QA datasets. On datasets where the question writers already know the answer\u2014SQuAD (Ra- jpurkar et al., 2016) and TriviaQA (Joshi et al., 2017)\u2014the retrieval problem resembles tradi- tional IR, and BM25 (Robertson et al., 2009) provides state-of-the-art retrieval.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat is the primary goal of the Open-Retrieval Question Answering system (ORQA)?[/INST] \nAnswer: The primary goal of the Open-Retrieval Question Answering system (ORQA) is to introduce an end-to-end model where the retriever and reader components are jointly learned. This system is designed to retrieve evidence from an open corpus and is supervised only by question-answer string pairs. Unlike traditional Information Retrieval (IR) systems, ORQA is not limited to a closed set of evidence documents but can retrieve any text within the open corpus."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\n#### DALL-E2 on Winoground I   #### DALL-E2 on Winoground II   STILL NOT SOLVED   #### Outline  1. Early models 2. Features and fusion 3. Contrastive models 4. Multimodal foundation models 5. Evaluation 6. Beyond images: Other modalities 7. Where to next?   #### Speech / audio  Can EASILY do another full lecture just on this topic.  Recent cool example: Whisper, trained on 680,000 hours of multilingual multitask data.  We can also just treat audio as vision ;) Radford et al., 2022  Kiela et al., (^2017) Bapna et al., 2022   #### Video and text and audio  MERLOT (Zellers et al., 2021) MERLOT Reserve (idem, 2022)   #### Grounded language learning in simulated environments   Hermann, Hill, et al. 2017 Das et al., 2018   #### Text to 3D  POINT-E (Nichol, Jun, et al., 2022)   #### Outline  1. Early models 2. Multimodal fusion 3. Contrastive models 4. Multimodal foundation models 5. Other modalities 6. Evaluation 7. Where to next?   #### One foundation model to rule them all  There will modality-agnostic foundation models that can read and generate many modalities.  These models can be bigger and be trained on vastly more data. Parameters will be shared in interesting ways.  Automatic alignment from unpaired unimodal data will become a big topic.   #### Multimodal scaling laws  We are just beginning to understand multimodal scaling laws, lots of interesting work to do here in understanding trade-offs.   #### Retrieval augmented generative multimodal models   Lewis et al., 2021   These can be multimodal   This can be multimodal This^ can^ be^ multimodal   #### Better evaluation and benchmarking   Barbosa-Silva et al. 2022   Kiela et al. 2020  We need better measurement.   # Thank you!\n\nfilename: materials_formatted.md\nextension: .md\n\nMultimodality is one of the main frontiers of the new foundation model revolution.   #### Multimodal brains  McGurk effect (McGurk & MacDonald, 1976)  https://www.youtube.com/watch?v=2k8fHR9jKVM   #### Multimodal applications  Let\u2019s say we\u2019re dealing with two modalities \u2013 text, and images:   \u25cf Retrieval (image <> text) \u25cf Captioning (image - > text) \u25cf Generation (text - > image) \u25cf Visual question answering (image+text - > text) \u25cf Multimodal classification (image+text - > label) \u25cf Better understanding/generation (image+text - > label/text)   #### Multimodal is hot right now  .. and/but has been \u201cthe next big thing\u201d for almost a decade!   #### Outline  1. Early models 2. Features and fusion 3. Contrastive models 4. Multimodal foundation models 5. Evaluation 6. Beyond images: Other modalities 7. Where to next?   #### Cross-modal \u201cVisual-Semantic Embeddings\u201d  WSABI (Weston et al 2010), DeVise (Frome et al 2013), Cross-Modal Transfer (Socher et al 2013)   Frome et al. 2013 Socher et al. 2013   #### Multimodal distributional semantics (Bruni et al., 2014)  Algorithm:   \u25cf Obtain visual \u201cword vector\u201d via BOVW: \u25cb Identify keypoints and get their descriptors \u25cb Cluster these and map to counts \u25cf Concatenate with textual word vector \u25cf Apply SVD to \u201cfuse\u201d information  This approach was shown to lead to better word representations on human similarity judgment datasets. Bag^ of^ visual^ words^ (BOVW)^ illustration.^ Bruni^ et^ al.,^ 2014.\n\nfilename: materials_formatted.md\nextension: .md\n\nMultimodal foundation models 5. Evaluation 6. Beyond images: Other modalities 7. Where to next?   #### BERT Refresher  How do we make this multimodal? \u2192   Alammar (2018), Illustrated Bert   #### Visual BERTs: VisualBERT   VisualBERT Li et al. 2019   #### Visual BERTs: ViLBERT   ViLBERT Lu et al. 2019   #### Visual BERTs: LXMERT   LXMERT Tan & Bansal 2019  Learning Cross-Modality Encoder Representations from Transformers   #### Visual BERTs: Supervised Multimodal Bitransformers   MMBT Kiela et al. 2019   #### Visual BERTs: PixelBert   Misnomer: they mean segment embedding PixelBert^ Huang^ et^ al.^2020   #### UNITER   Chen, Yi, Lu, et al. 2020   #### ViLT (Kim et al. 2021)  Feeding data directly to the transformer.   #### So many models   Du et al. 2022 VLP Survey   #### Recommended paper  Bugliarello et al. (2021). _Multimodal Pretraining Unmasked: A Meta-Analysis and a Unified Framework of Vision-and-Language BERTs_. Finding: in the same conditions, models perform _very_ similarly.   #### FLAVA (Singh et al., 2021)  Holistic approach to multimodality.  One foundation model spanning V&L, CV and NLP.  Jointly pretrained on:   \u25cf unimodal text data (CCNews + BookCorpus) \u25cf unimodal image data (ImageNet) \u25cf public paired image-text data (70M)  All data/models are publicly released.   #### The PMD dataset   \u25cf 70M image-text pairs from public sources   #### Problem to solve    43  ### How does FLAVA work?    44  ### How does FLAVA work?    45  ### How does FLAVA work?   #### How well does it work?   \u25cf On average, over 35 tasks, FLAVA obtains impressive performance   #### SimVLM (Wang et al., 2022)  Slowly moving from contrastive/discriminative to generative.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat challenges do researchers face when working with multimodal data?[/INST] \nAnswer: One of the challenges researchers face when working with multimodal data is the need for better evaluation and benchmarking. This is crucial to accurately measure the performance of multimodal models. Additionally, understanding multimodal scaling laws is still in its early stages, making it a complex area to navigate. Automatic alignment from unpaired unimodal data is also a big topic that presents challenges. Furthermore, the integration of different modalities such as text, images, audio, and video into a single model can be complex, requiring innovative approaches and techniques."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nFigure 4 shows how PaLM 540B uses chain-of- thought prompting to achieve new state of the art on GSM8K, SVAMP, and MAWPS (though note that standard prompting already passed the prior best for SVAMP). On the other two datasets, AQuA and ASDiv, PaLM with chain-of-thought prompting reaches within 2% of the state of the art (Appendix Table 2).  Standard prompting Chain-of-thought prompting Prior supervised best  LaMDA  GPT  PaLM  K 8 M S G  )  %  (  e t a r  e v l o s  P M A V S  )  %  (  e t a r  e v l o s  60  40  20  0  80  60  40  20  0  100  )  %  (  75  e t a r  50  S P W A M  8  0  25  e v l o s  62 540  0.4 8 137  0.4 7 175  Model scale (# parameters in billions)  To better understand why chain-of-thought prompting works, we manually examined model- generated chains of thought by LaMDA 137B for GSM8K. Of 50 random examples where the model returned the correct \ufb01nal answer, all of the generated chains of thought were also log- ically and mathematically correct except two that coincidentally arrived at the correct answer (see Appendix D.1, and Table 8 for examples of correct model-generated chains of thought). We also randomly examined 50 random sam- ples for which the model gave the wrong answer. The summary of this analysis is that 46% of the chains of thought were almost correct, barring minor mistakes (calculator error, symbol map- ping error, or one reasoning step missing), and that the other 54% of the chains of thought had major errors in semantic understanding or coherence (see Appendix D.2). To provide a small insight into why scaling improves chain-of-thought reasoning ability, we performed a similar analysis of errors made by PaLM 62B and whether those errors were \ufb01xed by scaling to PaLM 540B.\n\nfilename: materials_formatted.md\nextension: .md\n\nThough most chain of thought annotations outperform standard prompting, there is large variation in many cases. For instance, for the coin \ufb02ip task, the performance varied from 99.6% for Annotator A to 71.4% for Annotator C, though both were above standard prompting = 50.0% (see Table 7). There are even tasks where prompt engineering is a requirement for good performance. In preliminary experiments, we tried using chain of thought to enable language models to reverse the order of a list of 5 items. While two co-authors were not able to write chain of thought prompts that solved the task despite their best attempts, a third co-author was able to write a chain of thought that perfectly solved the task.  How to generate chain of thought annotations in a robust fashion could be an interesting direction for future work. For instance, an idea here could be to use a large language model to automatically generate chains of thought via prompting (and potentially optimize this over a validation set).  A.3 Will chain-of-thought prompting improve performance for my task of interest?  While chain-of-thought prompting is in principle applicable for any text-to-text task, it is more helpful for some tasks than others. Based on the experiments in this paper, our intuition is that chain of thought helps the most when three conditions are met: (1) the task is challenging and requires  18  multi-step reasoning, (2) a large language model is used, and (3) the scaling curve is relatively \ufb02at. Conversely, the bene\ufb01ts are smaller when one or more of these conditions are not met.  These intuitions are perhaps supported by the arithmetic reasoning results. The performance gain from chain-of-thought prompting is largest for PaLM 540B on GSM8K (challenging multi-step problems, \ufb02at scaling curve), which meets these conditions. The performance gain is small for the subsets of MAWPS that only require one or two steps (SingleOP, SingleEq, and AddSub), for which PaLM 540B already achieves performance of 90% or higher (and it is also generally true that there is less headroom for improvement when performance is already strong).\n\nfilename: materials_formatted.md\nextension: .md\n\nThe summary is that scaling PaLM to 540B \ufb01xes a large portion of one-step missing and semantic understanding errors in the 62B model (see Appendix A.1).  Figure 4: Chain-of-thought prompting enables large language models to solve challenging math problems. Notably, chain-of-thought reasoning is an emergent ability of increasing model scale. Prior best numbers are from Cobbe et al. (2021) for GSM8K, Jie et al. (2022) for SVAMP, and Lan et al. (2021) for MAWPS.  3.3 Ablation Study  The observed bene\ufb01ts of using chain-of-thought prompting raises the natural question of whether the same performance improvements can be conferred via other types of prompting. Figure 5 shows an ablation study with three variations of chain of thought described below.  Equation only. One reason for why chain-of-thought prompting might help is that it produces the mathematical equation to be evaluated, and so we test a variation where the model is prompted to output only a mathematical equation before giving the answer. Figure 5 shows that equation only prompting does not help much for GSM8K, which implies that the semantics of the questions in GSM8K are too challenging to directly translate into an equation without the natural language reasoning steps in chain of thought. For datasets of one-step or two-step problems, however, we \ufb01nd that equation only prompting does improve performance, since the equation can be easily derived from the question (see Appendix Table 6).  5  Variable compute only. Another intuition is that chain of thought allows the model to spend more computation (i.e., intermediate tokens) on harder problems. To isolate the effect of variable computation from chain-of-thought reasoning, we test a con\ufb01guration where the model is prompted to output a only sequence of dots (. . .) equal to the number of characters in the equation needed to solve the problem. This variant performs about the same as the baseline, which suggests that variable computation by itself is not the reason for the success of chain- of-thought prompting, and that there appears to be utility from expressing intermediate steps via natural language.  Chain of thought after answer.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nHow does chain-of-thought prompting affect performance on the GSM8K benchmark?[/INST] \nAnswer: Chain-of-thought prompting has been shown to significantly improve performance on the GSM8K benchmark. Specifically, PaLM 540B, using chain-of-thought prompting, achieved a new state of the art on this benchmark. This method also helped large language models solve challenging math problems. However, it was noted that an \"equation only\" variation of chain-of-thought prompting did not significantly improve performance for GSM8K, suggesting that the semantics of the questions in this benchmark are too complex to be directly translated into an equation without the natural language reasoning steps involved in chain-of-thought prompting."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nThis is approaching the performance of a \ufb01ne-tuned RoBERTa baseline (55.9%) from Uni\ufb01edQA [KKS+20]. On the \u201cEasy\u201d version of the dataset (questions which either of the mentioned baseline approaches answered correctly), GPT-3 achieves 68.8%, 71.2%, and 70.1% which slightly exceeds a \ufb01ne-tuned RoBERTa baseline from [KKS+20]. However, both of these results are still much worse than the overall SOTAs achieved by the Uni\ufb01edQA which exceeds GPT-3\u2019s few-shot results by 27% on the challenge set and 22% on the easy set.  On OpenBookQA [MCKS18], GPT-3 improves signi\ufb01cantly from zero to few shot settings but is still over 20 points short of the overall SOTA. GPT-3\u2019s few-shot performance is similar to a \ufb01ne-tuned BERT Large baseline on the leaderboard.  Overall, in-context learning with GPT-3 shows mixed results on commonsense reasoning tasks, with only small and inconsistent gains observed in the one and few-shot learning settings for both PIQA and ARC, but a signi\ufb01cant improvement is observed on OpenBookQA. GPT-3 sets SOTA on the new PIQA dataset in all evaluation settings.  3.6 Reading Comprehension  Next we evaluate GPT-3 on the task of reading comprehension. We use a suite of 5 datasets including abstractive, multiple choice, and span based answer formats in both dialog and single question settings. We observe a wide spread in GPT-3\u2019s performance across these datasets suggestive of varying capability with different answer formats. In general we observe GPT-3 is on par with initial baselines and early results trained using contextual representations on each respective dataset.  GPT-3 performs best (within 3 points of the human baseline) on CoQA [RCM19] a free-form conversational dataset and performs worst (13 F1 below an ELMo baseline) on QuAC [CHI+18] a dataset which requires modeling structured dialog acts and answer span selections of teacher-student interactions.\n\nfilename: materials_formatted.md\nextension: .md\n\nThey denote this more restrictive evaluation setting as \u201cclosed-book\u201d. Their work suggests that even higher-capacity models could perform even better and we test this hypothesis with GPT-3. We evaluate GPT-3 on the 3 datasets in [RRS20]: Natural Questions [KPR+19], WebQuestions [BCFL13], and TriviaQA [JCWZ17], using the same splits. Note that in addition to all results being in the closed-book setting, our use of few-shot, one-shot, and zero-shot evaluations represent an even stricter setting than previous closed-book QA work: in addition to external content not being allowed, \ufb01ne-tuning on the Q&A dataset itself is also not permitted.  The results for GPT-3 are shown in Table 3.3. On TriviaQA, we achieve 64.3% in the zero-shot setting, 68.0% in the one-shot setting, and 71.2% in the few-shot setting. The zero-shot result already outperforms the \ufb01ne-tuned T5-11B by 14.2%, and also outperforms a version with Q&A tailored span prediction during pre-training by 3.8%. The one-shot result improves by 3.7% and matches the SOTA for an open-domain QA system which not only \ufb01ne-tunes but also makes use of a learned retrieval mechanism over a 15.3B parameter dense vector index of 21M documents [LPP+20]. GPT-3\u2019s few-shot result further improves performance another 3.2% beyond this.  On WebQuestions (WebQs), GPT-3 achieves 14.4% in the zero-shot setting, 25.3% in the one-shot setting, and 41.5% in the few-shot setting. This compares to 37.4% for \ufb01ne-tuned T5-11B, and 44.7% for \ufb01ne-tuned T5-11B+SSM, which uses a Q&A-speci\ufb01c pre-training procedure. GPT-3 in the few-shot setting approaches the performance of state-of-the-art \ufb01ne-tuned models.\n\nfilename: materials_formatted.md\nextension: .md\n\nTriviaQA few-shot result is evaluated on the wiki split test server.  One note of caution is that an analysis of test set contamination identi\ufb01ed that a signi\ufb01cant minority of the LAMBADA dataset appears to be present in our training data \u2013 however analysis performed in Section 4 suggests negligible impact on performance.  3.1.3 HellaSwag  The HellaSwag dataset [ZHB+19] involves picking the best ending to a story or set of instructions. The examples were adversarially mined to be dif\ufb01cult for language models while remaining easy for humans (who achieve 95.6% accuracy). GPT-3 achieves 78.1% accuracy in the one-shot setting and 79.3% accuracy in the few-shot setting, outperforming the 75.4% accuracy of a \ufb01ne-tuned 1.5B parameter language model [ZHR+19] but still a fair amount lower than the overall SOTA of 85.6% achieved by the \ufb01ne-tuned multi-task model ALUM.  3.1.4 StoryCloze  We next evaluate GPT-3 on the StoryCloze 2016 dataset [MCH+16], which involves selecting the correct ending sentence for \ufb01ve-sentence long stories. Here GPT-3 achieves 83.2% in the zero-shot setting and 87.7% in the few-shot setting (with K = 70). This is still 4.1% lower than the \ufb01ne-tuned SOTA using a BERT based model [LDL19] but improves over previous zero-shot results by roughly 10%.  3.2 Closed Book Question Answering  In this section we measure GPT-3\u2019s ability to answer questions about broad factual knowledge. Due to the immense amount of possible queries, this task has normally been approached by using an information retrieval system to \ufb01nd relevant text in combination with a model which learns to generate an answer given the question and the retrieved text. Since this setting allows a system to search for and condition on text which potentially contains the answer it is denoted \u201copen-book\u201d. [RRS20] recently demonstrated that a large language model can perform surprisingly well directly answering the questions without conditioning on auxilliary information. They denote this more restrictive evaluation setting as \u201cclosed-book\u201d.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nHow does GPT-3 perform on the task of closed-book question answering?[/INST] \nAnswer: GPT-3 performs quite well on the task of closed-book question answering. It was evaluated on three datasets: Natural Questions, WebQuestions, and TriviaQA. On TriviaQA, GPT-3 achieved 64.3% in the zero-shot setting, 68.0% in the one-shot setting, and 71.2% in the few-shot setting. These results outperformed the fine-tuned T5-11B by 14.2% in the zero-shot setting and matched the state-of-the-art for an open-domain QA system in the one-shot setting. On WebQuestions, GPT-3 achieved 14.4% in the zero-shot setting, 25.3% in the one-shot setting, and 41.5% in the few-shot setting. This performance approaches that of state-of-the-art fine-tuned models."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nNo language models were \ufb01netuned in the process of writing this paper.  The emergence of chain-of-thought reasoning as a result of model scale has been a prevailing theme (Wei et al., 2022b). For many reasoning tasks where standard prompting has a \ufb02at scaling curve, chain- of-thought prompting leads to dramatically increasing scaling curves. Chain-of-thought prompting appears to expand the set of tasks that large language models can perform successfully\u2014in other words, our work underscores that standard prompting only provides a lower bound on the capabilities of large language models. This observation likely raises more questions than it answers\u2014for instance, how much more can we expect reasoning ability to improve with a further increase in model scale? What other prompting methods might expand the range of tasks that language models can solve?  As for limitations, we \ufb01rst qualify that although chain of thought emulates the thought processes of human reasoners, this does not answer whether the neural network is actually \u201creasoning,\u201d which we leave as an open question. Second, although the cost of manually augmenting exemplars with chains of thought is minimal in the few-shot setting, such annotation costs could be prohibitive for \ufb01netuning (though this could potentially be surmounted with synthetic data generation, or zero-shot generalization). Third, there is no guarantee of correct reasoning paths, which can lead to both correct and incorrect answers; improving factual generations of language models is an open direction for future work (Rashkin et al., 2021; Ye and Durrett, 2022; Wiegreffe et al., 2022, inter alia). Finally, the emergence of chain-of-thought reasoning only at large model scales makes it costly to serve in real-world applications; further research could explore how to induce reasoning in smaller models.  7 Related Work  This work is inspired by many research areas, which we detail in an extended related work section (Appendix C). Here we describe two directions and associated papers that are perhaps most relevant.  The \ufb01rst relevant direction is using intermediate steps to solve reasoning problems. Ling et al. (2017) pioneer the idea of using natural language rationales to solve math word problems through a series of intermediate steps.\n\nfilename: materials_formatted.md\nextension: .md\n\nIn this paper, we combine the strengths of these two ideas in a way that avoids their limitations. Speci\ufb01cally, we explore the ability of language models to perform few-shot prompting for reasoning tasks, given a prompt that consists of triples: (cid:104)input, chain of thought, output(cid:105). A chain of thought is a series of intermediate natural language reasoning steps that lead to the \ufb01nal output, and we refer to this approach as chain-of-thought prompting. An example prompt is shown in Figure 1.  We present empirical evaluations on arithmetic, commonsense, and symbolic reasoning benchmarks, showing that chain-of-thought prompting outperforms standard prompting, sometimes to a striking degree. Figure 2 illustrates one such result\u2014on the GSM8K benchmark of math word problems (Cobbe et al., 2021), chain-of-thought prompting with PaLM 540B outperforms standard prompting by a large margin and achieves new state-of-the-art performance. A prompting only approach is important because it does not require a large training dataset and because a single model checkpoint can perform many tasks without loss of generality. This work underscores how large language models can learn via a few examples with natural language data about the task (c.f. automatically learning the patterns underlying inputs and outputs via a large training dataset).  2 Chain-of-Thought Prompting  Consider one\u2019s own thought process when solving a complicated reasoning task such as a multi-step math word problem. It is typical to decompose the problem into intermediate steps and solve each before giving the \ufb01nal answer: \u201cAfter Jane gives 2 \ufb02owers to her mom she has 10 . . . then after she gives 3 to her dad she will have 7 . . . so the answer is 7.\u201d The goal of this paper is to endow language models with the ability to generate a similar chain of thought\u2014a coherent series of intermediate reasoning steps that lead to the \ufb01nal answer for a problem. We will show that suf\ufb01ciently large  2  language models can generate chains of thought if demonstrations of chain-of-thought reasoning are provided in the exemplars for few-shot prompting.  Figure 1 shows an example of a model producing a chain of thought to solve a math word problem that it would have otherwise gotten incorrect.\n\nfilename: materials_formatted.md\nextension: .md\n\nC.1 Prompting  The recent success of large-scale language models has led to growing interest in improving their capability to perform tasks via prompting (Brown et al. (2020), and see Liu et al. (2021) for a survey). This paper falls in the category of general prompting approaches, whereby input prompts are optimized to allow a single large language model to better perform a variety of tasks (Li and Liang, 2021; Lester et al., 2021; Reif et al., 2022, inter alia).  One recent line of work aims to improve the ability of language models to perform a task by providing instructions that describe the task (Raffel et al., 2020; Wei et al., 2022a; Ouyang et al., 2022; Sanh et al., 2022; Wang et al., 2022b). This line of work is related because it also augments input\u2013output pairs with meta-data. But whereas an instruction augments the input to a task (instructions are typically prepended to the inputs), chain-of-thought prompting augments the outputs of language models. Another related direction is sequentially combining the outputs of language models; human\u2013computer interaction (HCI) work (Wu et al., 2022a,b) has shown that combining sequential generations of language models improves task outcomes in a 20-person user study.  C.2 Natural language explanations  Another closely related direction uses natural language explanations (NLEs), often with the goal of improving model interpretability (Zhou et al., 2020; Wiegreffe and Marasovi\u00b4c, 2021, inter alia). That line of work typically focuses on natural language inference (Camburu et al., 2018; Yordanov et al., 2021; Bostrom et al., 2021), and produces explanations either simultaneously to or after the \ufb01nal prediction (Narang et al., 2020; Majumder et al., 2021; Wiegreffe et al., 2021, 2022). By contrast, the chain of thought processing considered in this paper occurs before the \ufb01nal answer.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat is the main improvement chain-of-thought prompting brings to large language models?[/INST] \nAnswer: Chain-of-thought prompting enhances the capabilities of large language models by enabling them to perform a wider range of tasks more successfully. This method involves providing a series of intermediate natural language reasoning steps, or a \"chain of thought\", that leads to the final output. This approach has been shown to significantly outperform standard prompting in various reasoning tasks, including arithmetic, commonsense, and symbolic reasoning benchmarks. Furthermore, chain-of-thought prompting allows large language models to learn from a few examples with natural language data about the task, rather than requiring a large training dataset. However, it's important to note that this method does not confirm whether the neural network is actually \"reasoning\", and there's no guarantee of correct reasoning paths."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nOur experiments show that using triplet loss does not affect the results much. More details can be found in Appendix B.  Cross-dataset generalization One interesting question regarding DPR\u2019s discriminative training is how much performance degradation it may suf- fer from a non-iid setting. In other words, can it still generalize well when directly applied to a different dataset without additional \ufb01ne-tuning? To test the cross-dataset generalization, we train DPR on Natural Questions only and test it directly on the smaller WebQuestions and CuratedTREC datasets. We \ufb01nd that DPR generalizes well, with 3-5 points loss from the best performing \ufb01ne-tuned model in top-20 retrieval accuracy (69.9/86.3 vs. 75.0/89.1 for WebQuestions and TREC, respec- tively), while still greatly outperforming the BM25 baseline (55.0/70.9).  5.3 Qualitative Analysis  Although DPR performs better than BM25 in gen- eral, passages retrieved by these two methods dif- fer qualitatively. Term-matching methods like BM25 are sensitive to highly selective keywords and phrases, while DPR captures lexical variations or semantic relationships better. See Appendix C for examples and more discussion.  5.4 Run-time Ef\ufb01ciency  The main reason that we require a retrieval compo- nent for open-domain QA is to reduce the number of candidate passages that the reader needs to con- sider, which is crucial for answering user\u2019s ques- tions in real-time. We pro\ufb01led the passage retrieval speed on a server with Intel Xeon CPU E5-2698 v4 @ 2.20GHz and 512GB memory. With the help of FAISS in-memory index for real-valued vectors10, DPR can be made incredibly ef\ufb01cient, processing 995.0 questions per second, returning top 100 pas- sages per question. In contrast, BM25/Lucene (im- plemented in Java, using \ufb01le index) processes 23.7 questions per second per CPU thread.  On the other hand, the time required for building an index for dense vectors is much longer.\n\nfilename: materials_formatted.md\nextension: .md\n\nUsing the question and answer to- gether as the query, we run Lucene-BM25 and pick the top passage that contains the answer as the pos- itive passage. Table 5 shows the performance of DPR when trained using the original setting and the distant supervision setting.  B Alternative Similarity Functions &  Triplet Loss  In addition to dot product (DP) and negative log- likelihood based on softmax (NLL), we also exper- iment with Euclidean distance (L2) and the triplet loss. We negate L2 similarity scores before ap- plying softmax and change signs of question-to- positive and question-to-negative similarities when applying the triplet loss on dot product scores. The margin value of the triplet loss is set to 1. Ta- ble 6 summarizes the results. All these additional experiments are conducted using the same hyper- parameters tuned for the baseline (DP, NLL).  Note that the retrieval accuracy for our \u201cbaseline\u201d settings reported in Table 5 (Gold) and Table 6 (DP, NLL) is slightly better than those reported in Table 3. This is due to a better hyper-parameter setting used in these analysis experiments, which is documented in our code release.  C Qualitative Analysis  Although DPR performs better than BM25 in gen- eral, the retrieved passages of these two retrievers actually differ qualitatively. Methods like BM25 are sensitive to highly selective keywords and phrases, but cannot capture lexical variations or se- mantic relationships well. In contrast, DPR excels at semantic representation, but might lack suf\ufb01cient capacity to represent salient phrases which appear rarely. Table 7 illustrates this phenomenon with two examples. In the \ufb01rst example, the top scor- ing passage from BM25 is irrelevant, even though keywords such as England and Ireland appear mul- tiple times. In comparison, DPR is able to return  Top-1 Top-5 Top-20 Top-100  Gold Dist. Sup.  44.9 43.9  66.8 65.3  78.1 77.1  85.0 84.4  Table 5: Retrieval accuracy on the development set of Natural Questions, trained on passages that match the gold context (Gold) or the top BM25 passage that con- tains the answer (Dist. Sup.).\n\nfilename: materials_formatted.md\nextension: .md\n\nStarting from our trained DPR model, they show that the retrieval performance can be further improved. Recent work (Izacard and Grave, 2020; Lewis et al., 2020b) have also shown that DPR can be combined with generation models such as BART (Lewis et al., 2020a) and T5 (Raf- fel et al., 2019), achieving good performance on open-domain QA and other knowledge-intensive tasks.  8 Conclusion  In this work, we demonstrated that dense retrieval can outperform and potentially replace the tradi- tional sparse retrieval component in open-domain question answering. While a simple dual-encoder approach can be made to work surprisingly well, we showed that there are some critical ingredients to training a dense retriever successfully. Moreover, our empirical analysis and ablation studies indicate that more complex model frameworks or similarity functions do not necessarily provide additional val- ues. As a result of improved retrieval performance, we obtained new state-of-the-art results on multiple open-domain question answering benchmarks.  Acknowledgments  We thank the anonymous reviewers for their helpful comments and suggestions.  References  Akari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi, Richard Socher, and Caiming Xiong. 2020. Learn- ing to retrieve reasoning paths over Wikipedia graph for question answering. In International Conference on Learning Representations (ICLR).  Petr Baudi\u02c7s and Jan \u02c7Sediv`y. 2015. Modeling of the question answering task in the yodaqa system. In In- ternational Conference of the Cross-Language Eval- uation Forum for European Languages, pages 222\u2013 228. Springer.  Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on Freebase from  question-answer pairs. In Empirical Methods in Nat- ural Language Processing (EMNLP).  Jane Bromley, Isabelle Guyon, Yann LeCun, Eduard S\u00a8ackinger, and Roopak Shah. 1994. Signature veri\ufb01- cation using a \u201cSiamese\u201d time delay neural network. In NIPS, pages 737\u2013744.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nHow does DPR outperform traditional retrieval methods?[/INST] \nAnswer: DPR outperforms traditional retrieval methods like BM25 by capturing lexical variations or semantic relationships better. While BM25 is sensitive to highly selective keywords and phrases, DPR is able to understand and represent the semantics of the content more effectively. This makes DPR more efficient in retrieving relevant passages, even when the keywords are not exactly the same. Furthermore, DPR can be made incredibly efficient with the help of FAISS in-memory index for real-valued vectors, processing a high number of questions per second."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nInstead, we introduce a more general weighting function, which we are free to take to depend on the context word as well:  #### J\u02c6=   W   \u2211   i= 1   W   \u2211   j= 1   f(Xij)(~uTj~vi\u2212logXij)^2  ### 1. 4 Conclusion  In conclusion, the GloVe model efficiently leverages global statistical information by training only on the nonzero elements in a word- word co-occurrence matrix, and produces a vector space with mean- ingful sub-structure. It consistently outperformsword 2 vecon the word analogy task, given the same corpus, vocabulary, window size, and training time. It achieves better results faster, and also obtains the best results irrespective of speed.  ## 2 Evaluation of Word Vectors  So far, we have discussed methods such as theWord _2_ VecandGloVe methods to train and discover latent vector representations of natural language words in a semantic space. In this section, we discuss how we can quantitatively evaluate the quality of word vectors produced by such techniques.  ### 2. 1 Intrinsic Evaluation  Intrinsic evaluation of word vectors is the evaluation of a set of word vectors generated by an embedding technique (such as Word 2 Vec or GloVe) on specific intermediate subtasks (such as analogy comple- tion). These subtasks are typically simple and fast to compute and thereby allow us to help understand the system used to generate the word vectors. An intrinsic evaluation should typically return to us a number that indicates the performance of those word vectors on the evaluation subtask.   Figure 1 : The left subsystem (red) being expensive to train is modified by substituting with a simpler subsystem (green) for intrinsic evaluation.  Motivation: Let us consider an example where our final goal is to create a question answering system which uses word vectors   ## word vectors ii:glove,evaluation and training 4  as inputs. One approach of doing so would be to train a machine learning system that:  1. Takes words as inputs  2. Converts them to word vectors  3. Uses word vectors as inputs for an elaborate machine learning system  4. Maps the output word vectors by this system back to natural language words  5.\n\nfilename: materials_formatted.md\nextension: .md\n\nMaps the output word vectors by this system back to natural language words  5. Produces words as answers Intrinsic evaluation:  - Evaluation on a specific, intermedi-     ate task - Fast to compute performance - Helps understand subsystem - Needs positive correlation with real     task to determine usefulness  Of course, in the process of making such a state-of-the-art question- answering system, we will need to create optimal word-vector repre- sentations since they are used in downstream subsystems (such as deep neural networks). To do this in practice, we will need to tune many hyperparameters in the Word 2 Vec subsystem (such as the dimension of the word vector representation). While the idealistic approach is to retrain the entire system after any parametric changes in the Word 2 Vec subsystem, this is impractical from an engineering standpoint because the machine learning system (in step 3 ) is typi- cally a deep neural network with millions of parameters that takes very long to train. In such a situation, we would want to come up with a simple intrinsic evaluation technique which can provide a measure of \"goodness\" of the word to word vector subsystem. Ob- viously, a requirement is that the intrinsic evaluation has a positive correlation with the final task performance.  ### 2. 2 Extrinsic Evaluation   Extrinsic evaluation:  - Is the evaluation on a real task - Can be slow to compute perfor-     mance - Unclear if subsystem is the prob-     lem, other subsystems, or internal     interactions - If replacing subsystem improves     performance, the change is likely     good  Extrinsic evaluation of word vectors is the evaluation of a set of word vectors generated by an embedding technique on the real task at hand. These tasks are typically elaborate and slow to compute. Using our example from above, the system which allows for the evalua- tion of answers from questions is the extrinsic evaluation system. Typically, optimizing over an underperforming extrinsic evaluation system does not allow us to determine which specific subsystem is at fault and this motivates the need for intrinsic evaluation.  ### 2. 3 Intrinsic Evaluation Example: Word Vector Analogies  A popular choice for intrinsic evaluation of word vectors is its per- formance in completing word vector analogies.\n\nfilename: materials_formatted.md\nextension: .md\n\nIntrinsic evaluations measure the quality of word vectors by directly measuring correlation between semantic relatedness and ge- ometric relatedness, usually through inventories of query terms. Focusing on intrinsic measures, Baroni et al. (2014) compare word embeddings against distributional word vectors on a variety of query inventories and tasks. Faruqui and Dyer (2014) provide a website that allows the automatic evaluation of embeddings on a number of query inventories. Gao et al. (2014) publish an improved query inventory for the analogical reasoning task. Finally, Tsvetkov et al. (2015) propose a new in- trinsic measure that better correlates with extrinsic performance. However, all these evaluations are done on precollected inventories and mostly lim- ited to local metrics like relatedness. Extrinsic evaluations use embeddings as fea- tures in models for other tasks, such as semantic role labeling or part-of-speech tagging (Collobert et al., 2011), and improve the performance of ex- isting systems (Turian et al., 2010). However, they have been less successful at other tasks such as parsing (Andreas and Klein, 2014). More work has been done in unsupervised se- mantic modeling in the context of topic models. One example is theword intrusiontask (Chang et al., 2009), in which annotators are asked to iden- tify a random word inserted into the set of high probability words for a given topic. Word embed- dings do not produce interpretable dimensions, so we cannot directly use this method, but we present a related task based on nearest neighbors. Manual evaluation is expensive and time-consuming, but other work establishes that automated evaluations can closely model human intuitions (Newman et al., 2010).  ## 8 Conclusions  There are many factors that affect word embed- ding quality. Standard aggregate evaluations, while useful, do not present a complete or con- sistent picture. Factors such as word frequency play a significant and previously unacknowledged   role. Word frequency also interferes with the commonly-used cosine similarity measure.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nHow does intrinsic evaluation benefit the development of word vectors?[/INST] \nAnswer: Intrinsic evaluation benefits the development of word vectors in several ways. It provides a quick and specific intermediate task to compute the performance of the word vectors. This helps in understanding the subsystem used to generate the word vectors. It also provides a numerical indicator of the performance of the word vectors on the evaluation subtask. Intrinsic evaluation is particularly useful when the overall system is complex and time-consuming to train, such as a deep neural network. It allows for the tuning of many hyperparameters in the subsystem, like the dimension of the word vector representation, without having to retrain the entire system. However, it's important that the intrinsic evaluation has a positive correlation with the final task performance."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nFor a given sentence example, we select the words, POS tags and arc labels given the schematic defined above, extract their corresponding dense feature representations produced from the embedding matricesEw,Et, andEl, and concatenate these vectors into our inputs[xw,xt,xl]. At training time we backpropagate into the dense vector representations, as well as the parameters at later layers.   Feedforward Neural Network Model:  The network contains an input layer[xw,xt,xl], a hidden layer, and a final softmax layer with a cross-entropy loss function. We can either define a single weight matrix in the hidden layer, to operate on a concatenation of[xw,xt,xl], or we can use three weight matrices [W 1 w,W 1 t,W 1 l], one for each input type, as shown in Figure 3. We then apply a non-linear function and use one more affine layer[W 2 ]so that there are an equivalent number of softmax probabilities to the number of possible transitions (the output dimension).   ## dependency parsing 5   Input layer :[xw,xt,xl]   Hidden layer : h= (W 1 wxw+W 1 txt+W 1 lxl+b 1 )^3   Softmax layer : p = softmax(W 2 h)   words POS tags arc labels   ROOT has_VBZ   He_PRP   nsubj   has_VBZ good_JJ control_NN ._.   Stack Buffer Configuration   Figure 3 : The neural network archi- tecture for greedy, transition-based Note that in Figure 3 ,f(x) =x^3 is the non-linear function used. dependency parsing.  For a more complete explanation of a greedy transition-based neural dependency parser, refer to \"A Fast and Accurate Dependency Parser using Neural Networks\" under Further Reading.  Further reading: Danqi Chen, and Christopher D. Manning. \"A Fast and Accurate Dependency Parser using Neural Networks.\" EMNLP. 2014. Kuebler, Sandra, Ryan McDonald, and Joakim Nivre. \u201cDepen- dency parsing.\u201d Synthesis Lectures on Human Language Technolo- gies 1. 1 ( 2009 ): 1 - 127.\n\nfilename: materials_formatted.md\nextension: .md\n\n(Pre-condition: buffer has to be non-empty.)  2. Left-Arcr: Add a dependency arc(wj,r,wi)to the arc setA, wherewiis the word second to the top of the stack andwjis the   ## dependency parsing 3   word at the top of the stack. Removewifrom the stack. (Pre- condition: the stack needs to contain at least two items andwi cannot be the ROOT.)  3. Right-Arcr: Add a dependency arc(wi,r,wj)to the arc setA, wherewiis the word second to the top of the stack andwjis the word at the top of the stack. Removewjfrom the stack. (Pre- condition: The stack needs to contain at least two items.)  A more formal definition of these three transitions is presented in Figure 2.  ### 1. 4 Neural Dependency Parsing  While there are many deep models for dependency parsing, this section focuses specifically on greedy, transition-based neural de- pendency parsers. This class of model has demonstrated compara- ble performance and significantly better efficiency than traditional feature-based discriminative dependency parsers. The primary dis- tinction from previous models is the reliance on dense rather than sparse feature representations. The model we will describe employs the arc-standard system for transitions, as presented in section 1. 3. Ultimately, the aim of the model is to predict a transition sequence from some initial con- figurationcto a terminal configuration, in which the dependency parse tree is encoded. As the model is greedy, it attempts to correctly predict one transitionT \u2208 {shift, Left-Arcr, Right-Arcr}at a time, based on features extracted from the current configuration c= ( _\u03c3_ , _\u03b2_ ,A). Recall, _\u03c3_ is the stack, _\u03b2_ the buffer, andAthe set of dependency arcs for a given sentence.   Feature Selection:  Depending on the desired complexity of the model, there is flexi- bility in defining the input to the neural network. The features for a given sentenceSgenerally include some subset of:  1. Sword: Vector representations for some of the words inS(and their dependents) at the top of the stack _\u03c3_ and buffer _\u03b2_.\n\nfilename: materials_formatted.md\nextension: .md\n\nJohn Wiley & Sons Inc., 1968.  D. Heckerman, D. M. Chickering, C. Meek, R. Rounthwaite, and C. Kadie. Dependency networks for inference, collaborative filtering, and data visualization. Journal of Machine Learning Re- search (JMLR), 1:49\u201375, 2001.  G. E. Hinton, S. Osindero, and Y.-W. Teh. A fast learning algorithm for deep belief nets. Neural Computation, 18(7):1527\u20131554, July 2006.    COLLOBERT, WESTON, BOTTOU, KARLEN, KAVUKCUOGLU ANDKUKSA  K. Hollingshead, S. Fisher, and B. Roark. Comparing and combining finite-state and context-free parsers. InConference on Human Language Technology and Empirical Methods in Natural Language Processing (HLT-EMNLP), pages 787\u2013794, 2005.  F. Huang and A. Yates. Distributional representations for handling sparsity in supervised sequence- labeling. InMeeting of the Association for Computational Linguistics (ACL), pages 495\u2013503, 2009.  F. Jelinek. Continuous speech recognition by statistical methods.Proceedings of the IEEE, 64(4): 532\u2013556, 1976.  T. Joachims. Transductive inference for text classification using support vector machines. InInter- national Conference on Machine learning (ICML), 1999.  D. Klein and C. D. Manning. Natural language grammar induction using a constituent-context model. InAdvances in Neural Information Processing Systems (NIPS 14), pages 35\u201342. 2002.  T. Koo, X. Carreras, and M. Collins. Simple semi-supervised dependency parsing. InMeeting of the Association for Computational Linguistics (ACL), pages 595\u2013603, 2008.  P. Koomen, V. Punyakanok, D. Roth, and W. Yih. Generalized inference with multiple semantic role labeling systems (shared task paper). InConference on Computational Natural Language Learning (CoNLL), pages 181\u2013184, 2005.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat types of features are generally included in neural dependency parsers?[/INST] \nAnswer: Neural dependency parsers generally include features such as vector representations for some of the words in the given sentence and their dependents at the top of the stack and buffer. These features are selected based on the desired complexity of the model."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nGloVe:  - Using global statistics to predict the     probability of word j appearing in     the context of word i with a least     squares objective  In comparison, GloVe consists of a weighted least squares model that trains on global word-word co-occurrence counts and thus makes efficient use of statistics. The model produces a word vector space with meaningful sub-structure. It shows state-of-the-art per- formance on the word analogy task, and outperforms other current methods on several word similarity tasks.   ## word vectors ii:glove,evaluation and training 2  ### 1. 2 Co-occurrence Matrix   Co-occurrence Matrix:  - X: word-word co-occurrence matrix - Xij: number of times wordjoccur in     the context of wordi - Xi=\u2211kXik: the number of times     any word k appears in the context of     word i - Pij=P(wj|wi) =XXiji: the probability     of j appearing in the context of word     i  LetXdenote the word-word co-occurrence matrix, whereXijindi- cates the number of times wordjoccur in the context of wordi. Let Xi=\u2211kXikbe the number of times any word k appears in the con-  text of word i. Finally, letPij=P(wj|wi) = Xij Xi be the probability of j appearing in the context of word i. Populating this matrix requires a single pass through the entire corpus to collect the statistics. For large corpora, this pass can be computationally expensive, but it is a one-time up-front cost.  ### 1.\n\nfilename: materials_formatted.md\nextension: .md\n\n( 1988 ). Neurocomputing: Foundations of research. chapter Learning Representations by Back-propagating Errors, pages 696 \u2013 699. MIT Press, Cambridge, MA, USA.      \n\n# SEP 775 Lecture 02:  # Natural Language Processing with Deep Learning  ## SEP 775 Lecture Notes: Part II   ## Word Vectors II: GloVe, Evaluation and Training  Keyphrases: Global Vectors for Word Representation (GloVe). In- trinsic and extrinsic evaluations. Effect of hyperparameters on anal- ogy evaluation tasks. Correlation of human judgment with word vector distances. Dealing with ambiguity in word using contexts. Window classification. This set of notes first introduces the GloVe model for training word vectors. Then it extends our discussion of word vectors (in- terchangeably called word embeddings) by seeing how they can be evaluated intrinsically and extrinsically. As we proceed, we discuss the example of word analogies as an intrinsic evaluation technique and how it can be used to tune word embedding techniques. We then discuss training model weights/parameters and word vectors for ex- trinsic tasks. Lastly we motivate artificial neural networks as a class of models for natural language processing tasks.  ## 1 Global Vectors for Word Representation (GloVe)^33 This section is based on the GloVe  ### 1. 1 Comparison with Previous Methods  So far, we have looked at two main classes of methods to find word embeddings. The first set are count-based and rely on matrix factor- ization (e.g. LSA, HAL). While these methods effectively leverage global statistical information, they are primarily used to capture word similarities and do poorly on tasks such as word analogy, indi- cating a sub-optimal vector space structure. The other set of methods are shallow window-based (e.g. the skip-gram and the CBOW mod- els), which learn word embeddings by making predictions in local context windows. These models demonstrate the capacity to capture complex linguistic patterns beyond word similarity, but fail to make use of the global co-occurrence statistics.\n\nfilename: materials_formatted.md\nextension: .md\n\nInstead, we introduce a more general weighting function, which we are free to take to depend on the context word as well:  #### J\u02c6=   W   \u2211   i= 1   W   \u2211   j= 1   f(Xij)(~uTj~vi\u2212logXij)^2  ### 1. 4 Conclusion  In conclusion, the GloVe model efficiently leverages global statistical information by training only on the nonzero elements in a word- word co-occurrence matrix, and produces a vector space with mean- ingful sub-structure. It consistently outperformsword 2 vecon the word analogy task, given the same corpus, vocabulary, window size, and training time. It achieves better results faster, and also obtains the best results irrespective of speed.  ## 2 Evaluation of Word Vectors  So far, we have discussed methods such as theWord _2_ VecandGloVe methods to train and discover latent vector representations of natural language words in a semantic space. In this section, we discuss how we can quantitatively evaluate the quality of word vectors produced by such techniques.  ### 2. 1 Intrinsic Evaluation  Intrinsic evaluation of word vectors is the evaluation of a set of word vectors generated by an embedding technique (such as Word 2 Vec or GloVe) on specific intermediate subtasks (such as analogy comple- tion). These subtasks are typically simple and fast to compute and thereby allow us to help understand the system used to generate the word vectors. An intrinsic evaluation should typically return to us a number that indicates the performance of those word vectors on the evaluation subtask.   Figure 1 : The left subsystem (red) being expensive to train is modified by substituting with a simpler subsystem (green) for intrinsic evaluation.  Motivation: Let us consider an example where our final goal is to create a question answering system which uses word vectors   ## word vectors ii:glove,evaluation and training 4  as inputs. One approach of doing so would be to train a machine learning system that:  1. Takes words as inputs  2. Converts them to word vectors  3. Uses word vectors as inputs for an elaborate machine learning system  4. Maps the output word vectors by this system back to natural language words  5.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nHow does the GloVe model approach the problem of capturing both global statistics and meaningful linear substructures in word vectors?[/INST] \nAnswer: The GloVe model addresses this problem by using a weighted least squares model that trains on global word-word co-occurrence counts. This approach allows it to efficiently use statistics. The model produces a word vector space with meaningful sub-structure, showing top performance on the word analogy task and outperforming other methods on several word similarity tasks. It also leverages global statistical information by training only on the nonzero elements in a word-word co-occurrence matrix. This results in a vector space with meaningful sub-structure and consistently outperforms other models on the word analogy task."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nThe collapse could be due to a less-explicit requirement for factual knowledge in some tasks, or the longer target sequences, which could result in less informative gradients for the retriever. Perez et al.[46]also found spurious retrieval results when optimizing a retrieval component in order to improve performance on downstream tasks.  ### I Number of instances per dataset  The number of training, development and test datapoints in each of our datasets is shown in Table 7.      \n\n# Layer Normalization:  Layer Normalization  Jimmy Lei Ba University of Toronto jimmy@psi.toronto.edu  Jamie Ryan Kiros University of Toronto rkiros@cs.toronto.edu  Geoffrey E. Hinton University of Toronto and Google Inc. hinton@cs.toronto.edu  Abstract  Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This signi\ufb01cantly reduces the training time in feed- forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural net- works. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empiri- cally, we show that layer normalization can substantially reduce the training time compared with previously published techniques.\n\nfilename: materials_formatted.md\nextension: .md\n\nBut this is unnecessary for the logit outputs where the prediction con\ufb01dence is determined by the scale of the logits. We only apply layer normalization to the fully-connected hidden layers that excludes the last softmax layer.  All the models were trained using 55000 training data points and the Adam [Kingma and Ba, 2014] optimizer. For the smaller batch-size, the variance term for batch normalization is computed using the unbiased estimator. The experimental results from Figure 6 highlight that layer normalization is robust to the batch-sizes and exhibits a faster training convergence comparing to batch normalization that is applied to all layers.  6.7 Convolutional Networks  We have also experimented with convolutional neural networks. In our preliminary experiments, we observed that layer normalization offers a speedup over the baseline model without normalization, but batch normalization outperforms the other methods. With fully connected layers, all the hidden units in a layer tend to make similar contributions to the \ufb01nal prediction and re-centering and re- scaling the summed inputs to a layer works well. However, the assumption of similar contributions is no longer true for convolutional neural networks. The large number of the hidden units whose receptive \ufb01elds lie near the boundary of the image are rarely turned on and thus have very different statistics from the rest of the hidden units within the same layer. We think further research is needed to make layer normalization work well in ConvNets.  7 Conclusion  In this paper, we introduced layer normalization to speed-up the training of neural networks. We provided a theoretical analysis that compared the invariance properties of layer normalization with batch normalization and weight normalization. We showed that layer normalization is invariant to per training-case feature shifting and scaling.  Empirically, we showed that recurrent neural networks bene\ufb01t the most from the proposed method especially for long sequences and small mini-batches.  Acknowledgments  This research was funded by grants from NSERC, CFI, and Google.\n\nfilename: materials_formatted.md\nextension: .md\n\n1  Introduction  Deep neural networks trained with some version of Stochastic Gradient Descent have been shown to substantially outperform previous approaches on various supervised learning tasks in computer vision [Krizhevsky et al., 2012] and speech processing [Hinton et al., 2012]. But state-of-the-art deep neural networks often require many days of training. It is possible to speed-up the learning by computing gradients for different subsets of the training cases on different machines or splitting the neural network itself over many machines [Dean et al., 2012], but this can require a lot of com- munication and complex software. It also tends to lead to rapidly diminishing returns as the degree of parallelization increases. An orthogonal approach is to modify the computations performed in the forward pass of the neural net to make learning easier. Recently, batch normalization [Ioffe and Szegedy, 2015] has been proposed to reduce training time by including additional normalization stages in deep neural networks. The normalization standardizes each summed input using its mean and its standard deviation across the training data. Feedforward neural networks trained using batch normalization converge faster even with simple SGD. In addition to training time improvement, the stochasticity from the batch statistics serves as a regularizer during training.  Despite its simplicity, batch normalization requires running averages of the summed input statis- tics. In feed-forward networks with \ufb01xed depth, it is straightforward to store the statistics separately for each hidden layer. However, the summed inputs to the recurrent neurons in a recurrent neu- ral network (RNN) often vary with the length of the sequence so applying batch normalization to RNNs appears to require different statistics for different time-steps. Furthermore, batch normaliza-              tion cannot be applied to online learning tasks or to extremely large distributed models where the minibatches have to be small.  This paper introduces layer normalization, a simple normalization method to improve the training speed for various neural network models. Unlike batch normalization, the proposed method directly estimates the normalization statistics from the summed inputs to the neurons within a hidden layer so the normalization does not introduce any new dependencies between training cases. We show that layer normalization works well for RNNs and improves both the training time and the generalization performance of several existing RNN models.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat main problem does layer normalization aim to solve in neural networks?[/INST] \nAnswer: Layer normalization aims to solve the problem of slow training speed in various neural network models. It is particularly beneficial for recurrent neural networks (RNNs), improving both their training time and generalization performance. It also addresses the issue of batch normalization, which cannot be applied to online learning tasks or to extremely large distributed models where the minibatches have to be small."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nThe configuration of this model is listed in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.  On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than 1/4 the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate Pdrop = 0.1, instead of 0.3.  For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We used beam search with a beam size of 4 and length penalty \u03b1 = 0.6 [38]. These hyperparameters were chosen after experimentation on the development set. We set the maximum output length during inference to input length + 50, but terminate early when possible [38].  Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature. We estimate the number of floating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each GPU 5.  6.2 Model Variations  To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the  5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.  Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base model. All metrics are on the English-to-German translation development set, newstest2013. Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities.\n\nfilename: materials_formatted.md\nextension: .md\n\n(2016) [8] Transformer (4 layers) Zhu et al. (2013) [40] Huang & Harper (2009) [14] McClosky et al. (2006) [26] Vinyals & Kaiser el al. (2014) [37] Transformer (4 layers) Luong et al. (2015) [23] Dyer et al. (2016) [8]  WSJ 23 F1 88.3 90.4 90.4 91.7 91.3 91.3 91.3 92.1 92.1 92.7 93.0 93.3  increased the maximum output length to input length + 300. We used a beam size of 21 and \u03b1 = 0.3 for both WSJ only and the semi-supervised setting.  Our results in Table 4 show that despite the lack of task-specific tuning our model performs sur- prisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8].  In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley- Parser [29] even when training only on the WSJ training set of 40K sentences.  7 Conclusion  In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.  For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles.  We are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours.  The code we used to train and evaluate our models is available at https://github.com/ tensorflow/tensor2tensor.\n\nfilename: materials_formatted.md\nextension: .md\n\nAttention Is All You Need  Ashish Vaswani\u2217 Google Brain avaswani@google.com  Noam Shazeer\u2217 Google Brain noam@google.com  Niki Parmar\u2217 Google Research nikip@google.com  Jakob Uszkoreit\u2217 Google Research usz@google.com  Llion Jones\u2217 Google Research llion@google.com  Aidan N. Gomez\u2217 \u2020 University of Toronto aidan@cs.toronto.edu  \u0141ukasz Kaiser\u2217 Google Brain lukaszkaiser@google.com  Illia Polosukhin\u2217 \u2021 illia.polosukhin@gmail.com  Abstract  The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English- to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.  \u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nHow does the Transformer model perform on the WMT 2014 English-to-German translation task?[/INST] \nAnswer: The Transformer model achieves a BLEU score of 28.4 on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nInstructor has the right to submit work to software to identify plagiarism.  ## 3. SUB TOPIC(S)   Week 1   Overview: Human language, Word meanings, Word2Vec, and Optimization basics   Week 2 Intro to neural networks: Gradients and backpropagation   Week 3 Recurrent Neural Networks and Language Models   Week 4 Conditioned Generation   Week 5   Part 1: Self attention and transformers   Part 2: Seq2Seq models   Week 6 Midterm Test   Week 7 Midterm Recess (No class)   Week 8 Prompting, Instruction Finetuning, and RLHF   Week 9 Question Answering systems   Week 1 0 Multimodal models   Week 1 1 Project presentation and Q&A   Week 1 2 Project presentation and Q&A   Week 1 3 Project presentation and Q&A   Classes end: April 10 th  ## 4. ASSESSMENT OF LEARNING Weight  4 Assignments 20%  4 Quizzes 10%  Midterm Test 35%  Group Project 35%   TOTAL 100%   Note that the percentage distribution of each module may be adjusted. Percentage grades will be converted to letter grades and grade points per the University calendar.  ## 5. LEARNING OUTCOMES  By the end of this course, the students will know about:  - Introduction to Deep Learning - Word representations - Language models - Pre-trained language models - Text generation - Seq2Seq models - Prompting - Feedback systems - Multimodal models - NLP applications (e.g. Q/A systems)  ## 6. COURSE OUTLINE \u2013 APPROVED ADVISORY STATEMENTS  ## ANTI-DISCRIMINATION  The Faculty of Engineering is concerned with ensuring an environment that is free of all discrimination. If there is a problem, individuals are reminded that they should contact the Associate Director, Graduate Studies, the Sexual Harassment Officer or the Human Rights Consultant, as soon as possible.\n\nfilename: materials_formatted.md\nextension: .md\n\n# Materials  Course Outline:  # Course Outline  ## 1. COURSE INFORMATION  Session Offered Winter 2024 Course Name Introduction to Computational Natural Language Processing Course Code SEP 775 Date(s) and Time(s) of lectures Lectures (In-person): Wednesday 3 : 30 pm \u2013 6 : 30 pm Program Name System and Technology Calendar Description This graduate course introduces fundamental concepts in computational natural language processing (NLP) and their applications to teaching students how to deal with textual data in Artificial Intelligence. This course demonstrates how machines can learn different tasks in natural language, such as language modeling, text generation, machine translation, and language understanding. In this regard, we go over the most promising methods in this literature and the most recent state-of-the-art techniques. Moreover, this course explores different real-world applications of NLP and helps students get hands-on experience in this field. Instructor Hamidreza Mahyar E-Mail: mahyarh@mcmaster.ca Office Hours: upon request  ## 2. COURSE SPECIFICS  Course Description This course covers some fundamental concepts in natural language processing (NLP) and how they apply in real-world use cases.  Instruction Type  *Code Type Hours per term C Classroom instruction 39 L Laboratory, workshop or fieldwork T Tutorial DE Distance education Total Hours 39 Resources ISBN Textbook Title & Edition Author & Publisher Natural Language Processing Jacob Eisenstein   Speech and Language Processing Dan Jurafsky and James H. Martin  Deep Learning Ian Goodfellow, Yoshua Bengio, and Aaron Courville Prerequisite(s) \u2022 Proficiency in Python programming language  - Some Machine Learning and/or Deep Learning knowledge is a plus  Corequisite(s) Antirequisite(s) Course Specific Policies All work must be shown to get full credit.   Specific policy of Projects/Assignments submissions:    Projects/Assignments will be conducted online by using related software. Completed Projects/Assignments should be uploaded to the drop box before midnight of the due date. Projects/Assignments missed due to legitimate reasons must be completed later mutually agreed with the instructor.  Departmental Policies The use of cell phones, iPods, laptops and other personal electronic devices are prohibited from the classroom during the class time, unless the instructor makes an explicit exception. Announcements made in class or placed on Avenue are considered to have been communicated to all students including those individuals that are not in class.\n\nfilename: materials_formatted.md\nextension: .md\n\n. . . . . . . . . . . . . . . . . . .  3 Results  .  .  .  . .  . . . . . . . . . . . . . . . . . . . . . . . . . .  3.1 Language Modeling, Cloze, and Completion Tasks 3.2 Closed Book Question Answering . . . . . 3.3 Translation . . 3.4 Winograd-Style Tasks . . . 3.5 Common Sense Reasoning . . . 3.6 Reading Comprehension . . . . 3.7 SuperGLUE . 3.8 NLI . . . . 3.9 Synthetic and Qualitative Tasks .  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat is the main focus of the course mentioned in the note?[/INST] \nAnswer: The main focus of the course, titled \"Introduction to Computational Natural Language Processing\" (SEP 775), is to introduce fundamental concepts in computational natural language processing (NLP) and their applications. It aims to teach students how to handle textual data in Artificial Intelligence. The course covers tasks in natural language such as language modeling, text generation, machine translation, and language understanding. It also explores various real-world applications of NLP and provides students with hands-on experience in this field."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\n9 Adaptive Optimization Methods  AdaGrad is an implementation of standard stochastic gradient de- scent (SGD) with one key difference: the learning rate can vary for each parameter. The learning rate for each parameter depends on the history of gradient updates of that parameter in a way such that parameters with a scarce history of updates are updated faster using a larger learning rate. In other words, parameters that have not been updated much in the past are likelier to have higher learning rates now. Formally:   \u03b8 t,i= \u03b8 t\u22121,i\u2212 \u03b1 \u221a \u2211t \u03c4 = 1 g^2 \u03c4 ,i   gt,iwheregt,i=  #### \u2202   \u2202\u03b8 it Jt( \u03b8 )  In this technique, we see that if the RMS of the history of gradients is extremely low, the learning rate is very high. A simple implemen- tation of this technique is:   Snippet 2. 3 # Assume the gradient dx and parameter vector x cache += dx 2 x += - learning_rate* dx / np.sqrt(cache + 1e-8)  Other common adaptive methods are RMSProp and Adam, whose update rules are shown below (courtesy of Andrej Karpathy):   Snippet 2. 4 # Update rule for RMS prop cache = decay_rate*cache + (1 - decay_rate) *dx 2 x += - learning_rate* dx / (np.sqrt(cache) + eps)   Snippet 2. 5 # Update rule for Adam   ## neural networks,backpropagation 18   m = beta1*m + (1-beta1)*dx v = beta2*v + (1-beta2)*(dx2) x += - learning_rate* m / (np.sqrt(v) + eps)  RMSProp is a variant of AdaGrad that utilizes a moving average of squared gradients \u2013 in particular, unlike AdaGrad, its updates do not become monotonically smaller. The Adam update rule is in turn a variant of RMSProp, but with the addition of momentum- like updates. We refer the reader to the respective sources of these methods for more detailed analyses of their behavior. \n\n# lecture 04:  # Natural Language Processing with Deep Learning Course  ## SEP 775 Lecture Notes: Part IV  ## Dependency Parsing  Keyphrases: Dependency Parsing.\n\nfilename: materials_formatted.md\nextension: .md\n\nThe empirical findings suggest that for sigmoid and tanh activation units, faster convergence and lower error rates are achieved when the weights of a matrix W\u2208 R n(l+^1 )\u00d7n(l)are initialized randomly with a uniform distribution as follows:  #### W\u223cU  #### [  #### \u2212   \u221a  #### 6   n(l)+n(l+^1 )  #### ,   \u221a  #### 6   n(l)+n(l+^1 )  #### ]  Wheren(l)is the number of input units toW(fan-in) andn(l+^1 ) is the number of output units fromW(fan-out). In this parameter initialization scheme, bias units are initialized to 0. This approach attempts to maintain activation variances as well as backpropagated gradient variances across layers. Without such initialization, the gradient variances (which are a proxy for information) generally decrease with backpropagation across layers.  ### 2. 7 Learning Strategies  The rate/magnitude of model parameter updates during training can be controlled using the learning rate. In the following naive Gradient Descent formulation, _\u03b1_ is the learning rate:   \u03b8 new= \u03b8 old\u2212 \u03b1 \u2207 \u03b8 Jt( \u03b8 )   ## neural networks,backpropagation 16  You might think that for fast convergence rates, we should set _\u03b1_ to larger values \u2013 however faster convergence is not guaranteed with larger convergence rates. In fact, with very large learning rates, we might experience that the loss function actually diverges because the parameters update causes the model to overshoot the convex minima as shown in Figure 15. In non-convex models (most of those we work with), the outcome of a large learning rate is unpredictable, but the chances of diverging loss functions are very high.   Figure 15 : Here we see that updating parameterw 2 with a large learning rate can lead to divergence of the error.  The simple solution to avoiding a diverging loss is to use a very small learning rate so that we carefully scan the parameter space \u2013 of course, if we use too small a learning rate, we might not converge in a reasonable amount of time, or might get caught in local minima. Thus, as with any other hyperparameter, the learning rate must be tuned effectively.\n\nfilename: materials_formatted.md\nextension: .md\n\nNaN) which is easily detectable at runtime; this issue is called theGradient Explosion Problem. When the gradient value goes to zero, however, it can go undetected while dras- tically reducing the learning quality of the model for far-away words in the corpus; this issue is called theVanishing Gradient Problem. Due to vanishing gradients, we don\u2019t know whether there is no dependency between stepstandt+nin the data, or we just cannot capture the true dependency due to this issue. To gain practical intuition about the vanishing gradient problem, you may visit the followingexample website.  #### 2. 4 Solution to the Exploding & Vanishing Gradients  Now that we gained intuition about the nature of the vanishing gradi- ents problem and how it manifests itself in deep neural networks, let us focus on a simple and practical heuristic to solve these problems. To solve the problem of exploding gradients, Thomas Mikolov first introduced a simple heuristic solution thatclipsgradients to a small number whenever they explode. That is, whenever they reach a cer- tain threshold, they are set back to a small number as shown in Algo- rithm 1. Algorithm 1 : Pseudo-code for norm clip-   ping in the gradients whenever they ex- g\u02c6\u2190 \u2202 E plode \u2202 W if \u2016g\u02c6\u2016\u2265threshold then g\u02c6\u2190threshold \u2016g\u02c6\u2016 g\u02c6 end if  Figure 7 visualizes the effect of gradient clipping. It shows the de- cision surface of a small recurrent neural network with respect to its Wmatrix and its bias terms,b. The model consists of a single unit of recurrent neural network running through a small number of time- steps; the solid arrows illustrate the training progress on each gradient descent step. When the gradient descent model hits the high error wall in the objective function, the gradient is pushed off to a far-away loca- tion on the decision surface. The clipping model produces the dashed line where it instead pulls back the error gradient to somewhere close to the original gradient landscape.   On the diculty of training Recurrent Neural Networks   Figure 6.We plot the error surface of a single hidden unit recurrent network, highlighting the existence of high cur- vature walls. The solid lines depicts standard trajectories that gradient descent might follow.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nHow do adaptive optimization methods like AdaGrad, RMSProp, and Adam differ from traditional gradient descent?[/INST] \nAnswer: Adaptive optimization methods such as AdaGrad, RMSProp, and Adam differ from traditional gradient descent in the way they adjust the learning rate. In traditional gradient descent, the learning rate is constant for all parameters. However, in AdaGrad, the learning rate can vary for each parameter, depending on the history of gradient updates for that parameter. Parameters with a scarce history of updates are updated faster using a larger learning rate. RMSProp, a variant of AdaGrad, uses a moving average of squared gradients, ensuring its updates do not become monotonically smaller. Adam, another variant of RMSProp, adds momentum-like updates to the mix. These adaptive methods aim to improve the efficiency and performance of the optimization process."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nWe only input new values to the state when we forget something older.  A slightly more dramatic variation on the LSTM is the Gated Recurrent Unit, or GRU, introduced by Cho, et al. (2014) (http://arxiv.org/pdf/1406.1078v3.pdf). It combines the forget and input gates into a single \u201cupdate gate.\u201d It also merges the cell state and hidden state, and makes some other changes. The resulting model is simpler than standard LSTM models, and has been growing increasingly popular.  These are only a few of the most notable LSTM variants. There are lots of others, like Depth Gated RNNs by Yao, et al. (2015) (http://arxiv.org/pdf/1508.03790v2.pdf). There\u2019s also some completely different approach to tackling long-term dependencies, like Clockwork RNNs by Koutnik, et al. (2014) (http://arxiv.org/pdf/1402.3511v1.pdf).  Which of these variants is best? Do the differences matter? Greff, et al. (2015) (http://arxiv.org/pdf/1503.04069.pdf) do a nice comparison of popular variants, finding that they\u2019re all about the same. Jozefowicz, et al. (2015) (http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf) tested more than ten thousand RNN architectures, finding some that worked better than LSTMs on certain tasks.  ## Conclusion  Earlier, I mentioned the remarkable results people are achieving with RNNs. Essentially all of these are achieved using LSTMs. They really work a lot better for most tasks!  Written down as a set of equations, LSTMs look pretty intimidating. Hopefully, walking through them step by step in this essay has made them a bit more approachable.    Attention and Augmented Recurrent Neural Networks On Distill A Modular PerspectiveConv Nets Neural Networks, Manifolds, and Topology Deep Learning, NLP, and Representations  LSTMs were a big step in what we can accomplish with RNNs. It\u2019s natural to wonder: is there another big step? A common opinion among researchers is: \u201cYes!\n\nfilename: materials_formatted.md\nextension: .md\n\n4. Hidden state: The hidden statehtis finally generated using the past hidden inputht\u2212 1 and the new memory generatedh \u0303twith the advice of the update gate.  Figure 12 : The detailed internals of a It is important to note that to train a GRU, we need to learn all GRU the different parameters:W,U,W(r),U(r),W(z),U(z). These follow the same backpropagation procedure we have seen in the past.   ### language models,rnn,gru and lstm 13  ### 4 Long-Short-Term-Memories  Long-Short-Term-Memories are another type of complex activation unit that differ a little from GRUs. The motivation for using these is similar to those for GRUs however the architecture of such units does differ. Let us first take a look at the mathematical formulation of LSTM units before diving into the intuition behind this design:   it= \u03c3 (W(i)xt+U(i)ht\u2212 1 ) (Input gate) ft= \u03c3 (W(f)xt+U(f)ht\u2212 1 ) (Forget gate) ot= \u03c3 (W(o)xt+U(o)ht\u2212 1 ) (Output/Exposure gate) c \u0303t=tanh(W(c)xt+U(c)ht\u2212 1 ) (New memory cell) ct=ft\u25e6ct\u2212 1 +it\u25e6c \u0303t (Final memory cell) ht=ot\u25e6tanh(ct)   Figure 13 : The detailed internals of a LSTM   ### language models,rnn,gru and lstm 14  We can gain intuition of the structure of an LSTM by thinking of its architecture as the following stages:  1. New memory generation: This stage is analogous to the new mem- ory generation stage we saw in GRUs. We essentially use the input wordxtand the past hidden stateht\u2212 1 to generate a new memory c \u0303twhich includes aspects of the new wordx(t).  2. Input Gate: We see that the new memory generation stage doesn\u2019t check if the new word is even important before generating the new memory \u2013 this is exactly the input gate\u2019s function.\n\nfilename: materials_formatted.md\nextension: .md\n\n(1994) (http://www- dsi.ing.unifi.it/~paolo/ps/tnn-94-gradient.pdf), who found some pretty fundamental reasons why it might be difficult.  Thankfully, LSTMs don\u2019t have this problem!  ## LSTM Networks  Long Short Term Memory networks \u2013 usually just called \u201cLSTMs\u201d \u2013 are a special kind of RNN, capable of learning long-term dependencies. They were introduced by Hochreiter & Schmidhuber (1997) (http://www.bioinf.jku.at/publications/older/2604.pdf), and were refined and popularized by many people in following work. They work tremendously well on a large variety of problems, and are now widely used.  LSTMs are explicitly designed to avoid the long-term dependency problem. Remembering information for long periods of time is practically their default behavior, not something they struggle to learn!   1   All recurrent neural networks have the form of a chain of repeating modules of neural network. In standard RNNs, this repeating module will have a very simple structure, such as a single tanh layer.   The repeating module in a standard RNN contains a single layer.  LSTMs also have this chain like structure, but the repeating module has a different structure. Instead of having a single neural network layer, there are four, interacting in a very special way.   The repeating module in an LSTM contains four interacting layers.  Don\u2019t worry about the details of what\u2019s going on. We\u2019ll walk through the LSTM diagram step by step later. For now, let\u2019s just try to get comfortable with the notation we\u2019ll be using.  In the above diagram, each line carries an entire vector, from the output of one node to the inputs of others. The pink circles represent pointwise operations, like vector addition, while the yellow boxes are learned neural network layers. Lines merging denote concatenation, while a line forking denote its content being copied and the copies going to different locations.  ## The Core Idea Behind LSTMs  The key to LSTMs is the cell state, the horizontal line running through the top of the diagram.  The cell state is kind of like a conveyor belt. It runs straight down the entire chain, with only some minor linear interactions. It\u2019s very easy for information to just flow along it unchanged.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat is the key difference between GRUs and LSTMs?[/INST] \nAnswer: The key difference between GRUs and LSTMs lies in their structure and the way they handle information. GRUs combine the forget and input gates into a single \"update gate\" and also merge the cell state and hidden state. This makes GRUs simpler than LSTMs. On the other hand, LSTMs have a separate forget gate, input gate, and output gate, and they maintain separate cell state and hidden state. This allows LSTMs to control the flow of information more precisely."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nReinforcement Learning from Human Feedback (RLHF)  52  #### SEP 775 Lecture Plan: From Language Models to Assistants  4. What\u2019s next?   #### Optimizing for human preferences  53  - Let\u2019s say we were training a language model on some task (e.g. summarization). - For each LM sample \ud835\udc60, imagine we had a way to obtain a _human reward_ of that     summary: \ud835\udc45 \ud835\udc60 \u2208 \u211d, higher is better. - Now we want to maximize the expected reward of samples from our LM:   SAN FRANCISCO, California (CNN) - - A magnitude 4.2 earthquake shook the San Francisco ... overturn unstable objects.   An earthquake hit San Francisco. There was minor property damage, but no injuries.  #### \ud835\udc60 1  #### \ud835\udc45 \ud835\udc60 1\t = 8.0   The Bay Area has good weather but is prone to earthquakes and wildfires.  #### \ud835\udc60 2  #### \ud835\udc45 \ud835\udc60 2\t = 1.2   Note: for mathematical simplicity we\u2019re assuming only one \u201cprompt\u201d   #### Reinforcement learning to the rescue  - The field of reinforcement learning (RL) has studied these     (and related) problems for many years now     [Williams, 1992; Sutton and Barto, 1998 ] - Circa 2013: resurgence of interest in RL applied to     deep learning, game-playing [Mnih et al., 2013 ] - But the interest in applying RL to modern LMs is an     even newer phenomenon [Ziegler et al., 2019;     Stiennon et al., 2020; Ouyang et al., 2022 ]. Why?        - RL w/ LMs has commonly been viewed as very hard           to get right (still is!)        - Newer advances in RL algorithms that work for           large neural models, including language models           (e.g. PPO; [Schulman et al., 2017 ]) 54   #### Optimizing for human preferences  - How do we actually change our LM parameters \ud835\udf03 to maximize this? - Let\u2019s try doing gradient ascent!\n\nfilename: materials_formatted.md\nextension: .md\n\nInstruction finetuning     + Simple and straightforward, generalize to unseen tasks     - Collecting demonstrations for so many tasks is expensive     - Mismatch between LM objective and human preferences 3. Reinforcement Learning from Human Feedback (RLHF)     + Directly model preferences (cf. language modeling), generalize beyond labeled data     - RL is very tricky to get right     - Human preferences are fallible; _models_ of human preferences even more so 4. What\u2019s next?  76  #### SEP 775 Lecture Plan: From Language Models to Assistants   #### Language models as multitask assistants?  We\u2019ve finally (mostly) answered how we get from this  #### McMaster University is located in  to _this_  77   1. Zero-Shot (ZS) and Few-Shot (FS) In-Context Learning     + No finetuning needed, prompt engineering (e.g. CoT) can improve performance     - Limits to what you can fit in context     - Complex tasks will probably need gradient steps 2. Instruction finetuning     + Simple and straightforward, generalize to unseen tasks     - Collecting demonstrations for so many tasks is expensive     - Mismatch between LM objective and human preferences 3. Reinforcement Learning from Human Feedback (RLHF)     + Directly model preferences (cf. language modeling), generalize beyond labeled data     - RL is very tricky to get right     - Human preferences are fallible; _models_ of human preferences even more so 4. What\u2019s next?  78  #### SEP 775 Lecture Plan: From Language Models to Assistants   #### What\u2019s next?  - RLHF is still a very underexplored and fast-     moving area: by the next lecture ( 2025 )     these slides may look completely different! - RLHF gets you further than instruction     finetuning, but is (still!) data expensive. - Recent work aims to alleviate such data     requirements:  79   #### What\u2019s next?  80  - RLHF is still a very underexplored and fast-     moving area: by the next lecture (2024)     these slides may look completely different! - RLHF gets you further than instruction     finetuning, but is (still!) data expensive.\n\nfilename: materials_formatted.md\nextension: .md\n\nFinally, we train a policy via reinforcement learning (RL) to maximize the score given by the RM; the policy generates a token of text at each \u2018time step\u2019, and is updated using the PPO algorithm [58] based on the RM \u2018reward\u2019 given to the entire generated summary. We can then gather more human data using samples from the resulting policy, and repeat the process. We follow the works of [48, 4] and use large pretrained GPT-3 models with as many as 6.7 billion parameters.  Our main contributions are four-fold.  (1) We show that training with human feedback signi\ufb01cantly outperforms very strong baselines on English summarization. When applying our methods on a version of the Reddit TL;DR dataset [63], we train policies via human feedback that produce better summaries than much larger policies trained via supervised learning. Summaries from our human feedback models are preferred by our labelers to the original human demonstrations in the dataset (see Figure 1).  (2) We show human feedback models generalize much better to new domains than supervised models. Our Reddit-trained human feedback models also generate high-quality summaries of news articles on the CNN/DailyMail (CNN/DM) dataset without any news-speci\ufb01c \ufb01ne-tuning, almost matching the quality of the dataset\u2019s reference summaries. We perform several checks to ensure that these human preferences re\ufb02ect a real quality difference: we consistently monitor agreement rates amongst labelers and researchers, and \ufb01nd researcher-labeler agreement rates are nearly as high as researcher-researcher agreement rates (see Section C.2), and we verify models are not merely optimizing simple metrics like length or amount of copying (see Appendices F and G.7).  4Throughout the paper, error bars represent 1 standard error.  Supervised learningHuman feedbackPretrain onlyReference summaries(3) We conduct extensive empirical analyses of our policy and reward model. We examine the impact of model and data size (Figure 6), study performance as we continue to optimize a given reward model (Section 4.3), and analyze reward model performance using synthetic and human- written perturbations of summaries (Section 4.3).\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nHow does reinforcement learning from human feedback (RLHF) improve language models?[/INST] \nAnswer: Reinforcement Learning from Human Feedback (RLHF) improves language models by directly modeling human preferences, which allows it to generalize beyond labeled data. In the process of RLHF, a language model is trained on a task, and for each sample from the language model, a human reward is obtained. The goal is to maximize the expected reward of samples from the language model. This is done by changing the language model parameters to maximize this reward, typically through gradient ascent. The use of RLHF has shown to significantly outperform strong baselines on tasks like English summarization. It has been found that models trained with human feedback generalize much better to new domains than supervised models."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\n## 1 Dependency Grammar and Dependency Structure  Parse trees in NLP, analogous to those in compilers, are used to ana- lyze the syntactic structure of sentences. There are two main types of structures used - constituency structures and dependency structures. Constituency Grammar uses phrase structure grammar to organize words into nested constituents. This will be covered in more detail in following chapters. We now focus on Dependency Parsing. Dependency structure of sentences shows which words depend on (modify or are arguments of) which other words. These binary asymmetric relations between the words are called dependencies and are depicted as arrows going from the head (or governor, superior, regent) to the dependent (or modifier, inferior, subordinate). Usually these dependencies form a tree structure. They are often typed with the name of grammatical relations (subject, prepositional object, apposition, etc.). An example of such a dependency tree is shown in Figure 1. Sometimes a fake ROOT node is added as the head to the whole tree so that every word is a dependent of exactly one node.   Figure 1 : Dependency tree for the sen- tence \"Bills on ports and immigration were submitted by Senator Brownback, Republican of Kansas\"  ### 1. 1 Dependency Parsing  Dependency parsing is the task of analyzing the syntactic depen- dency structure of a given input sentenceS. The output of a depen- dency parser is a dependency tree where the words of the input sen- tence are connected by typed dependency relations. Formally, the de- pendency parsing problem asks to create a mapping from the input sentence with wordsS=w 0 w 1 ...wn(wherew 0 is the ROOT) to its dependency tree graphG. Many different variations of dependency- based methods have been developed in recent years, including neural network-based methods, which we will describe later. To be precise, there are two subproblems in dependency parsing (adapted from Kuebler et al., chapter 1. 2 ):  1. Learning:Given a training setDof sentences annotated with de- pendency graphs, induce a parsing modelMthat can be used to parse new sentences.   ## dependency parsing 2  2. Parsing:Given a parsing modelMand a sentenceS, derive the optimal dependency graphDforSaccording toM.\n\nfilename: materials_formatted.md\nextension: .md\n\nThe sentence structure is revealed as a side effect of the successive transformations. Sentence transformations can also have a semantic interpretation.    COLLOBERT, WESTON, BOTTOU, KARLEN, KAVUKCUOGLU ANDKUKSA  In the spirit of structural linguistics, Harris describes procedures to discover sentence trans- formation operators by leveraging the statistical regularities of the language. Such procedures are obviously useful for machine learning approaches. In particular, he proposes a test to decide whether two sentences forms are semantically related by a transformation operator. He first defines a ranking criterion (Harris, 1968, Section 4.1):   \u201cStarting for convenience with very short sentence forms, sayABC, we choose a particular word choice for all the classes, sayBqCq, except one, in this caseA; for every pair of membersAi,Ajof that word class we ask how the sentence formed with one of the members, that is,AiBqCqcompares as to acceptability with the sentence formed with the other member, that is,AjBqCq.\u201d  Thesegradingsare then used to compare sentence forms:   \u201cIt now turns out that, given the gradedn-tuples of words for a particular sentence form, we can find other sentences forms of the same word classes in which the same n-tuples of words produce the same grading of sentences.\u201d  This is an indication that these two sentence forms exploit common words with the same syntac- tic function and possibly the same meaning. This observation forms the empiricalbasis for the construction of operator grammars that describe real-world natural languages such as English. Therefore there are solid reasons to believe that the ranking criterion (17) has the conceptual potential to capture strong syntactic and semantic information. On the other hand, the structure of our language models is probably too restrictive for such goals, and our current approach only exploits the word embeddings discovered during training.  #### 5. Multi-Task Learning  It is generally accepted that featurestrainedfor one task can be useful forrelated tasks. This idea was already exploited in the previous section when certain language model features, namely the word embeddings, were used to initialize the supervised networks. Multi-task learning (MTL) leverages this idea in a more systematic way.\n\nfilename: materials_formatted.md\nextension: .md\n\ntion to these basic features, we resolve lexical vari- ation using lexicalized features, and syntactic varia- tion using dependency tree path features.  The multiclass log-likelihood loss is optimized using AdaGrad with an initial learning rate of 0.1. Each update is performed on the batch of all ques- tions in a paragraph for ef\ufb01ciency, since they share the same candidates. L2 regularization is used, with a coef\ufb01cient of 0.1 divided by the number of batches. The model is trained with three passes over the train-  012345678Syntactic divergence0.05.010.015.020.025.030.0PercentageFeature Groups  Description  Matching Word Frequencies  Matching Bigram Frequencies  Root Match  Lengths  Span Word Frequencies Constituent Label  Sum of the TF-IDF of the words that occur in both the question and the sentence containing the candidate answer. Separate features are used for the words to the left, to the right, inside the span, and in the whole sentence. Same as above, but using bigrams. We use the generalization of the TF-IDF described in Shirakawa et al. (2015).  Whether the dependency parse tree roots of the question and sentence match, whether the sentence contains the root of the dependency parse tree of the question, and whether the question contains the root of the dependency parse tree of the sentence. Number of words to the left, to the right, inside the span, and in the whole sentence.  Sum of the TF-IDF of the words in the span, regardless of whether they appear in the question. Constituency parse tree label of the span, optionally combined with the wh-word in the question.  Span POS Tags  Sequence of the part-of-speech tags in the span, optionally combined with the wh-word in the question.  Lexicalized  Dependency Tree Paths  Lemmas of question words combined with the lemmas of words within distance 2 to the span in the sentence based on the dependency parse trees. Separately, question word lemmas combined with answer word lemmas.  For each word that occurs in both the question and sentence, the path in the dependency parse tree from that word in the sentence to the span, optionally combined with the path from the wh-word to the word in the question.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat does a dependency structure show in sentences?[/INST] \nAnswer: A dependency structure in sentences shows which words depend on (modify or are arguments of) which other words. These relations between the words are called dependencies and are typically depicted as arrows going from the head (or governor, superior, regent) to the dependent (or modifier, inferior, subordinate). These dependencies often form a tree structure and are usually typed with the name of grammatical relations such as subject, prepositional object, apposition, etc. Sometimes a fake ROOT node is added as the head to the whole tree so that every word is a dependent of exactly one node."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nInitialization KL coef\ufb01cient KL(ppo, sup) 0.35 0.10 0.07 0.05 0.05 0.05  sup3 sup4 sup4 sup4 sup4 sup4_6b  1.3B 1.3B 1.3B 1.3B 1.3B 6.7B  1.8 3.8 9.4 19.0 18.0 14.0  BoN policy name sup2 bo8 rm1 sup3 bo8 rm1 sup3 bo63 rm2 sup4 bo8 rm3 sup4 bo64 rm3 sup4 bo128 rm3 sup4 bo256 rm3 sup4 bo512 rm3 sup4 bo128 rm3_6b sup4 bo256 rm3_6b  Objective Base policy  rm1 rm2 rm2 rm3 rm3 rm3 rm3 rm3 rm3_6b rm3_6b  sup2 sup3 sup3 sup4 sup4 sup4 sup4 sup4 sup4 sup4  N 8 8 63 8 64 128 256 512 128 256  KL(BoN, sup) 1.2 1.2 3.2 1.2 3.2 3.9 4.5 5.2 3.9 4.5  Table 10: Best-of-N policies. KL divergence is computed analytically as KL(boN, sup) = log N - (N-1)/N.  We also explored a simple alternative to reinforcement learning: Sample N summaries from a supervised baseline at temperature 0.7, score them with a reward model, and take the summary with the highest score. This best-of-N (BoN) procedure is effectively a mildly optimized policy requiring no training. These policies are named in Table 10, and samples from them form part of the training data.  Table 11 lists the source policies for the training data for each reward model.\n\nfilename: materials_formatted.md\nextension: .md\n\nThus, we want our error to be(sc\u2212s)ifsc>selse 0. Thus, our optimization objective is now:   minimizeJ=max(sc\u2212s, 0)  However, the above optimization objective is risky in the sense that it does not attempt to create a margin of safety. We would want the \"true\" labeled data point to score higher than the \"false\" labeled data point by some positive margin\u2206. In other words, we would want error to be calculated if(s\u2212sc<\u2206)and not just when(s\u2212sc< 0 ). Thus, we modify the optimization objective:   minimizeJ=max(\u2206+sc\u2212s, 0)  We can scale this margin such that it is\u2206= 1 and let the other parameters in the optimization problem adapt to this without any change in performance. For more information on this, read about functional and geometric margins - a topic often covered in the study of Support Vector Machines. Finally, we define the following opti- The max-margin objective function is most commonly associated with mization objective which we optimize over all training windows: Support Vector Machines (SVMs)   minimizeJ=max( 1 +sc\u2212s, 0)  In the above formulationsc=UTf(W xc+b)ands=UTf(W x+b).  ### 1. 5 Training with Backpropagation \u2013 Elemental  In this section we discuss how we train the different parameters in the model when the costJdiscussed in Section 1. 4 is positive. No parameter updates are necessary if the cost is 0. Since we typically update parameters using gradient descent (or a variant such as SGD), we typically need the gradient information for any parameter as required in the update equation:   \u03b8 (t+^1 )= \u03b8 (t)\u2212 \u03b1 \u2207 \u03b8 (t)J  Backpropagation is technique that allows us to use the chain rule of differentiation to calculate loss gradients for any parameter used in the feed-forward computation on the model. To understand this further, let us understand the toy network shown in Figure 5 for which we will perform backpropagation.   Figure 5 : This is a 4 - 2 - 1 neural network where neuronjon layerkreceives input z(jk)and produces activation outputa(jk).\n\nfilename: materials_formatted.md\nextension: .md\n\nWe then compute iteratively:   \u2202Clogadd \u2202\u03b4t\u2212 1 (i)   =\u2211   j   \u2202Clogadd \u2202\u03b4t(j)   e\u03b4t\u2212^1 (i)+[A]i,j \u2211ke\u03b4t\u2212^1 (k)+[A]k,j  ##### ,  where at each steptof the recursion we accumulate of the gradients with respect to the inputsf\u03b8, and the transition scores[A]i,j:   \u2202C \u2202  ##### [   f\u03b8  ##### ]   i,t  ##### +=   \u2202Clogadd \u2202\u03b4t(i)   \u2202\u03b4t(i) \u2202  ##### [   f\u03b8  ##### ]   i,t   =   \u2202Clogadd \u2202\u03b4t(i) \u2202C \u2202[A]i,j  ##### +=   \u2202Clogadd \u2202\u03b4t(j)   \u2202\u03b4t(j) \u2202[A]i,j   =   \u2202Clogadd \u2202\u03b4t(j)   e\u03b4t\u2212^1 (i)+[A]i,j \u2211ke\u03b4t\u2212^1 (k)+[A]k,j  ##### .  A.8 Ranking Criterion  We use the ranking criterion (17) for training our language model. In this case, given a \u201cpositive\u201d examplexand a \u201cnegative\u201d examplex(w), we want to minimize:   C(f\u03b8(x),f\u03b8(xw)) =max  ##### {   0 , 1 \u2212f\u03b8(x)+f\u03b8(x(w))  ##### }  ##### .  Ignoring the non-differentiability ofmax( 0 ,\u00b7)in zero, the gradient is simply given by:   ( \u2202C \u2202f\u03b8(x) \u2202C \u2202f\u03b8(xw)   )   =  #####   #####   #####   #####   #####    (   \u2212 1 1   )   if 1\u2212f\u03b8(x)+f\u03b8(x(w))> 0 ( 0 0   )   otherwise  ##### .    COLLOBERT, WESTON, BOTTOU, KARLEN, KAVUKCUOGLU ANDKUKSA  #### References  R. K. Ando and T. Zhang.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhich optimizer is defined in the SEP775 Final Projects coding exercise?[/INST] \nAnswer: I don't know."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nAttention mechanisms in previous works typically have one or more of the following characteristics. First, the computed attention weights are often used to extract the most relevant information from the context for answering the question by sum- marizing the context into a \ufb01xed-size vector. Second, in the text domain, they are often temporally dynamic, whereby the attention weights at the current time step are a function of the attended vector at the previous time step. Third, they are usually uni-directional, wherein the query attends on the context paragraph or the image.  In this paper, we introduce the Bi-Directional Attention Flow (BIDAF) network, a hierarchical multi-stage architecture for modeling the representations of the context paragraph at different levels of granularity (Figure 1). BIDAF includes character-level, word-level, and contextual embeddings, and uses bi-directional attention \ufb02ow to obtain a query-aware context representation. Our attention mechanism offers following improvements to the previously popular attention paradigms. First, our attention layer is not used to summarize the context paragraph into a \ufb01xed-size vector. Instead, the attention is computed for every time step, and the attended vector at each time step, along with the representations from previous layers, is allowed to \ufb02ow through to the subsequent modeling layer. This reduces the information loss caused by early summarization. Second, we use a memory-less attention mechanism. That is, while we iteratively compute attention through time as in Bahdanau et al. (2015), the attention at each time step is a function of only the query and the context para- graph at the current time step and does not directly depend on the attention at the previous time step. We hypothesize that this simpli\ufb01cation leads to the division of labor between the attention layer and the modeling layer. It forces the attention layer to focus on learning the attention between the query and the context, and enables the modeling layer to focus on learning the interaction within the  \u2217The majority of the work was done while the author was interning at the Allen Institute for AI.  1              Published as a conference paper at ICLR 2017  Figure 1: BiDirectional Attention Flow Model (best viewed in color)  query-aware context representation (the output of the attention layer).\n\nfilename: materials_formatted.md\nextension: .md\n\nAAAI Press, 2011.      \n\n# BI-DIRECTIONAL ATTENTION FLOW FOR MACHINE COMPREHENSION:  Published as a conference paper at ICLR 2017  BI-DIRECTIONAL ATTENTION FLOW FOR MACHINE COMPREHENSION  Minjoon Seo1\u2217 University of Washington1, Allen Institute for Arti\ufb01cial Intelligence2 {minjoon,ali,hannaneh}@cs.washington.edu, {anik}@allenai.org  Aniruddha Kembhavi2  Ali Farhadi1,2  Hananneh Hajishirzi1   ABSTRACT  Machine comprehension (MC), answering a query about a given context para- graph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typ- ically these methods use attention to focus on a small portion of the con- text and summarize it with a \ufb01xed-size vector, couple attentions temporally, In this paper we introduce the and/or often form a uni-directional attention. Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical pro- cess that represents the context at different levels of granularity and uses bi- directional attention \ufb02ow mechanism to obtain a query-aware context represen- tation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test.  1  INTRODUCTION  The tasks of machine comprehension (MC) and question answering (QA) have gained signi\ufb01cant popularity over the past few years within the natural language processing and computer vision com- munities. Systems trained end-to-end now achieve promising results on a variety of tasks in the text and image domains. One of the key factors to the advancement has been the use of neural attention mechanism, which enables the system to focus on a targeted area within a context paragraph (for MC) or within an image (for Visual QA), that is most relevant to answer the question (Weston et al., 2015; Antol et al., 2015; Xiong et al., 2016a). Attention mechanisms in previous works typically have one or more of the following characteristics.\n\nfilename: materials_formatted.md\nextension: .md\n\nIt also allows the attention at each time step to be unaffected from incorrect attendances at previous time steps. Our experi- ments show that memory-less attention gives a clear advantage over dynamic attention. Third, we use attention mechanisms in both directions, query-to-context and context-to-query, which provide complimentary information to each other.  Our BIDAF model1 outperforms all previous approaches on the highly-competitive Stanford Ques- tion Answering Dataset (SQuAD) test set leaderboard at the time of submission. With a modi\ufb01cation to only the output layer, BIDAF achieves the state-of-the-art results on the CNN/DailyMail cloze test. We also provide an in-depth ablation study of our model on the SQuAD development set, vi- sualize the intermediate feature spaces in our model, and analyse its performance as compared to a more traditional language model for machine comprehension (Rajpurkar et al., 2016).  2 MODEL  Our machine comprehension model is a hierarchical multi-stage process and consists of six layers (Figure 1):  1. Character Embedding Layer maps each word to a vector space using character-level  CNNs.  2. Word Embedding Layer maps each word to a vector space using a pre-trained word em-  bedding model.  3. Contextual Embedding Layer utilizes contextual cues from surrounding words to re\ufb01ne the embedding of the words. These \ufb01rst three layers are applied to both the query and context.  4. Attention Flow Layer couples the query and context vectors and produces a set of query-  aware feature vectors for each word in the context.  5. Modeling Layer employs a Recurrent Neural Network to scan the context.  6. Output Layer provides an answer to the query.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat unique approach does the Bi-Directional Attention Flow (BIDAF) network introduce?[/INST] \nAnswer: The Bi-Directional Attention Flow (BIDAF) network introduces a unique approach that includes a hierarchical multi-stage architecture for modeling the representations of the context paragraph at different levels of granularity. It includes character-level, word-level, and contextual embeddings, and uses bi-directional attention flow to obtain a query-aware context representation. The attention layer is not used to summarize the context paragraph into a fixed-size vector, but instead, the attention is computed for every time step. The attended vector at each time step, along with the representations from previous layers, is allowed to flow through to the subsequent modeling layer. This reduces the information loss caused by early summarization. The model also uses a memory-less attention mechanism, where the attention at each time step is a function of only the query and the context paragraph at the current time step and does not directly depend on the attention at the previous time step. Additionally, the model uses attention mechanisms in both directions, query-to-context and context-to-query."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nFollowing the classical \u201cback-propagation\u201d derivations (LeCun, 1985;Rumelhart et al., 1986) and the modular approach shown in Bottou (1991), any feed-forward neural network withLlayers, like the ones shown in Figure 1 and Figure 2, can be seen as a composition of functions f\u03b8l(\u00b7), corresponding to each layerl:   f\u03b8(\u00b7) =f\u03b8L(f\u03b8L\u2212^1 (...f\u03b8^1 (\u00b7)...))  Partitioning the parameters of the network with respect to each layers 1\u2264l\u2264L, we write:   \u03b8= (\u03b8^1 , ...,\u03b8l, ...,\u03b8L).  We are now interested in computing the gradients of the cost with respect to each\u03b8l. Applying the chain rule (generalized to vectors) we obtain the classical backpropagation recursion:   \u2202C \u2202\u03b8l   =   \u2202f\u03b8l \u2202\u03b8l  ##### \u2202C   \u2202f\u03b8l   (19)  ##### \u2202C   \u2202f\u03b8l\u2212^1   =   \u2202f\u03b8l \u2202f\u03b8l\u2212^1  ##### \u2202C   \u2202f\u03b8l  ##### . (20)  In other words, we first initialize the recursion by computing the gradient ofthe cost with respect to the last layer output\u2202C/\u2202f\u03b8L. Then each layerlcomputes the gradient respect to its own parameters    NATURALLANGUAGEPROCESSING(ALMOST)FROMSCRATCH  with (19), given the gradient coming from its output\u2202C/\u2202f\u03b8l. To perform the backpropagation, it also computes the gradient with respect to its own inputs, as shown in (20). We now derive the gradients for each layer we used in this paper.  A.1 Lookup Table Layer  Given a matrix of parameters\u03b8^1 =W^1 and word (or discrete feature) indices[w]T 1 , the layer outputs the matrix: f\u03b8l([w]Tl) =   (   \u3008W\u3009^1 [w] 1 \u3008W\u3009^1 [w] 2 ... \u3008W\u3009^1 [w]T   )  ##### .\n\nfilename: materials_formatted.md\nextension: .md\n\n## neural networks,backpropagation 5  Here, we use a neural network with a single hidden layer and a single unit output. Let us establish some notation that will make it easier to generalize this model later:  - xiis an input to the neural network. - sis the output of the neural network. - Each layer (including the input and output layers) has neurons     which receive an input and produce an output. Thej-th neuron     of layerkreceives the scalar inputz(jk)and produces the scalar     activation outputa(jk). - We will call the backpropagated error calculated atz(jk)as _\u03b4_ (jk). - Layer 1 refers to the input layer and not the first hidden layer. For     the input layer,xj=z(j^1 )=a(j^1 ). - W(k)is the transfer matrix that maps the output from thek-th layer     to the input to the(k+ 1 )-th Thus,W(^1 )=WandW(^2 )=Uto put     this new generalized notation in perspective of Section 1. 3.        Backpropagation Notation:           - xiis an input to the neural network.           - sis the output of the neural net-              work.           - Thej-th neuron of layerkreceives              the scalar inputz(jk)and produces              the scalar activation outputa(jk).           - For the input layer,xj=z(j^1 )=a(j^1 ).           - W(k)is the transfer matrix that maps              the output from thek-th layer to              the input to the(k+ 1 )-th. Thus,              W(^1 )=WandW(^2 )=UTusing              notation from Section 1. 3.  Let us begin: Suppose the costJ= ( 1 +sc\u2212s)is positive and we want to perform the update of parameterW 14 (^1 )(in Figure 5 and  Figure 6 ), we must realize thatW 14 (^1 )only contributes toz( 12 )and thusa( 12 ). This fact is crucial to understanding backpropagation \u2013 backpropagated gradients are only affected by values they contribute to.a( 12 )is consequently used in the forward computation of score by  multiplication withW 1 (^2 ).\n\nfilename: materials_formatted.md\nextension: .md\n\nAlthough oversimplified, this is a classic case demonstrating the need for non-linear decision boundaries. In the next set of notes, we study neural networks as a class of non-linear models that have performed particularly well in deep learning applications.   Figure 10 : Here, we see that the non- linear decision boundary allows for much better classification of datapoints.           \n\n# SEP 775 Lecture 03:  # Natural Language Processing with Deep Learning Course  ## SEP 775 Lecture Notes: Part III  ## Neural Networks, Backpropagation  Keyphrases: Neural networks. Forward computation. Backward propagation. Neuron Units. Max-margin Loss. Gradient checks. Xavier parameter initialization. Learning rates. Adagrad. This set of notes introduces single and multilayer neural networks, and how they can be used for classification purposes. We then dis- cuss how they can be trained using a distributed gradient descent technique known as backpropagation. We will see how the chain rule can be used to make parameter updates sequentially. After a rigorous mathematical discussion of neural networks, we will discuss some practical tips and tricks in training neural networks involving: neuron units (non-linearities), gradient checks, Xavier parameter ini- tialization, learning rates, Adagrad, etc. Lastly, we will motivate the use of recurrent neural networks as a language model.  ## 1 Neural Networks: Foundations   Figure 1 : We see here how a non-linear decision boundary separates the data very well. This is the prowess of neural networks. Fun Fact: Neural networks are biologically in- spired classifiers which is why they are often called \"artificial neural networks\" to distinguish them from the organic kind. However, in reality human neural networks are so much more capable and complex from artificial neural net- works that it is usually better to not draw too many parallels between the two.  We established in our previous discussions the need for non-linear classifiers since most data are not linearly separable and thus, our classification performance on them is limited. Neural networks are a family of classifiers with non-linear decision boundary as seen in Figure 1. Now that we know the sort of decision boundaries neural networks create, let us see how they manage doing so.  ### 1. 1 A Neuron  A neuron is a generic computational unit that takesninputs and produces a single output.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat does backpropagation enable for neural networks?[/INST] \nAnswer: Backpropagation enables the computation of gradients in a neural network. It is a method used to train neural networks by calculating the gradient of the cost function with respect to each parameter in the network. This is done by applying the chain rule to recursively compute the gradients from the output layer to the input layer. These gradients are then used to update the parameters of the network in a way that minimizes the cost function."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nFLAN\u2019s zero-shot also outperforms 175B-parameter GPT-3\u2019s zero-shot on 20 of 25 datasets that we evaluate, and even outperforms GPT-3\u2019s few-shot by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. In ablation studies, we \ufb01nd that increasing the number of task clusters in instruction tuning improves performance on unseen tasks and that the bene\ufb01ts of instruction tuning emerge only with suf\ufb01cient model scale.  Instruction tuning is a simple method that, as depicted in Figure 2, combines appealing aspects of both the pretrain\u2013\ufb01netune and prompting paradigms by using supervision via \ufb01netuning to improve language model\u2019s responses to inference-time text interactions. Our empirical results demonstrate promising abilities of language models to perform tasks described purely via instructions. Source code for loading the instruction tuning dataset used for FLAN is publicly available at https://github.com/google-research/flan.  Figure 2: Comparing instruction tuning with pretrain\u2013\ufb01netune and prompting.  2 FLAN: INSTRUCTION TUNING IMPROVES ZERO-SHOT LEARNING  The motivation of instruction tuning is to improve the ability of language models to respond to NLP instructions. The idea is that by using supervision to teach an LM to perform tasks described via instructions, the LM will learn to follow instructions and do so even for unseen tasks. To evaluate performance on unseen tasks, we group datasets into clusters by task type and hold out each task cluster for evaluation while instruction tuning on all remaining clusters.\n\nfilename: materials_formatted.md\nextension: .md\n\n. . \u2295  7  0.4B2B8B68B137BModel Size (# parameters)3040506070Instruction tuningUntuned modelAverage zero-shot accuracy  on 13 held-out tasks (%) Performance on held-out tasks2030405060FT: no instruction Eval: instructionFT: dataset name Eval: instructionFT: dataset name Eval: dataset nameFT: instruction Eval: instruction (FLAN)37.346.647.055.2Zero-shot performance  (4 task cluster avg.) Published as a conference paper at ICLR 2022  instruct(xk)\u2295yk \u2295instruct(x)\u201d, where \u2295 denotes string concatenation with a delimiter token inserted in between. At both training and inference time, exemplars are randomly drawn from the training set, and the number of exemplars is capped at 16 and such that the total sequence length is less than 960 tokens. Our experiment uses the same task splits and evaluation procedure as \u00a73, such that few-shot exemplars for an unseen task are only used at inference time.  As shown in Figure 9, few-shot exemplars improve the performance on all task clusters, compared with zero-shot FLAN. Exemplars are especially effective for tasks with large/complex output spaces, such as struct to text, translation, and closed-book QA, potentially because exemplars help the model better understand the output format. In addition, for all task clusters, standard deviation among templates is lower for few-shot FLAN, indicating reduced sensitivity to prompt engineering.  Figure 9: Adding few-shot exemplars to FLAN is a complementary method for improving the performance of instruction-tuned models. The orange bars indicate standard deviation among templates, averaged at the dataset level for each task cluster.  4.5  INSTRUCTION TUNING FACILITATES PROMPT TUNING  As we\u2019ve seen that instruction tuning improves the ability of a model to respond to instructions, it follows that, if FLAN is indeed more amenable to performing NLP tasks, then it should also achieve better performance when performing inference using soft prompts, represented by prepended continuous variables optimized via prompt tuning (Li & Liang, 2021; Lester et al., 2021).\n\nfilename: materials_formatted.md\nextension: .md\n\nFinally, the scale of FLAN 137B makes it costly to serve. Future work on instruction tuning could include gathering/generating even more task clusters for \ufb01netuning, cross-lingual experiments, using FLAN to generate data for training downstream classi\ufb01ers, and using \ufb01netuning to improve model behavior with respect to bias and fairness (Solaiman & Dennison, 2021).  7 CONCLUSIONS  This paper has explored a simple method for improving the ability of language models at scale to perform zero-shot tasks based purely on instructions. Our instruction-tuned model, FLAN, compares favorably against GPT-3 and signals the potential ability for language models at scale to follow instructions. We hope that our paper will spur further research on instructions-based NLP, zero-shot learning, and using labeled data to improve large language models.  9  Published as a conference paper at ICLR 2022  ETHICAL CONSIDERATIONS  This work uses language models, for which the risks and potential harms are discussed in Bender & Koller (2020), Brown et al. (2020), Bender et al. (2021), Patterson et al., (2021), and others. As our contribution in this paper is not a pretrained language model itself but rather an empirical study of how instruction tuning affects the zero-shot performance of a language model on unseen tasks, we additionally highlight two relevant ethical considerations. First, labeled datasets such as those we use for \ufb01netuning can contain undesirable biases, and these biases can be propagated into zero-shot applications of the model on downstream tasks. And second, instruction-tuned models can potentially require less data and expertise to use; such lower barriers to access could increase both the bene\ufb01ts and associated risks of such models.  ENVIRONMENTAL CONSIDERATIONS  We use the same pretrained language models as Austin et al. (2021). The energy cost and carbon footprint for the pretrained models were 451 MWh and 26 tCO2e, respectively. The additional instruction tuning gradient-steps for \ufb01netuning FLAN is less than 2% of the number of pretraining steps, and so the estimated additional energy cost is comparatively smaller.  AUTHOR CONTRIBUTIONS  Maarten Bosma conceived the original idea and implemented the \ufb01rst version of FLAN.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat is the instruction tuning strategy employed for FLAN?[/INST] \nAnswer: The instruction tuning strategy employed for FLAN is a method that combines aspects of both the pretrain\u2013finetune and prompting paradigms. It uses supervision via finetuning to improve a language model\u2019s responses to inference-time text interactions. The aim of instruction tuning is to enhance the ability of language models to respond to NLP instructions. By using supervision to teach a language model to perform tasks described via instructions, the model learns to follow instructions, including for unseen tasks. Performance on unseen tasks is evaluated by grouping datasets into clusters by task type and holding out each task cluster for evaluation while instruction tuning on all remaining clusters."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nSuch non-linear de- cisions can often not be captured by inputs fed directly to a Softmax function but instead require the scoring of the intermediate layer discussed in Section 1. 2. We can thus use another matrixU\u2208Rm\u00d7^1 to generate an unnormalized score for a classification task from the activations: s=UTa=UTf(W x+b)  wherefis the activation function.   Figure 4 : This image captures how a simple feed-forward network might compute its output.  Analysis of Dimensions: If we represent each word using a 4- dimensional word vector and we use a 5-word window as input (as in the above example), then the inputx\u2208 R^20. If we use 8 sigmoid units in the hidden layer and generate 1 score output from the activa- tions, thenW\u2208 R^8 \u00d7^20 ,b\u2208 R^8 ,U\u2208 R^8 \u00d7^1 ,s\u2208 R.  ### 1. 4 Maximum Margin Objective Function  Like most machine learning models, neural networks also need an optimization objective, a measure of error or goodness which we want to minimize or maximize respectively. Here, we will discuss a popular error metric known as the maximum margin objective. The idea behind using this objective is to ensure that the score computed for \"true\" labeled data points is higher than the score computed for \"false\" labeled data points. Using the previous example, if we call the score computed for the \"true\" labeled window\"Museums in Paris are amazing\"assand the   ## neural networks,backpropagation 4  score computed for the \"false\" labeled window\"Not all museums in Paris\"assc(subscripted ascto signify that the window is \"corrupt\"). Then, our objective function would be to maximize(s\u2212sc)or to minimize(sc\u2212s). However, we modify our objective to ensure that error is only computed ifsc > s\u21d2 (sc\u2212s) >0. The intuition behind doing this is that we only care the the \"true\" data point have a higher score than the \"false\" data point and that the rest does not matter. Thus, we want our error to be(sc\u2212s)ifsc>selse 0.\n\nfilename: materials_formatted.md\nextension: .md\n\nThus, we want our error to be(sc\u2212s)ifsc>selse 0. Thus, our optimization objective is now:   minimizeJ=max(sc\u2212s, 0)  However, the above optimization objective is risky in the sense that it does not attempt to create a margin of safety. We would want the \"true\" labeled data point to score higher than the \"false\" labeled data point by some positive margin\u2206. In other words, we would want error to be calculated if(s\u2212sc<\u2206)and not just when(s\u2212sc< 0 ). Thus, we modify the optimization objective:   minimizeJ=max(\u2206+sc\u2212s, 0)  We can scale this margin such that it is\u2206= 1 and let the other parameters in the optimization problem adapt to this without any change in performance. For more information on this, read about functional and geometric margins - a topic often covered in the study of Support Vector Machines. Finally, we define the following opti- The max-margin objective function is most commonly associated with mization objective which we optimize over all training windows: Support Vector Machines (SVMs)   minimizeJ=max( 1 +sc\u2212s, 0)  In the above formulationsc=UTf(W xc+b)ands=UTf(W x+b).  ### 1. 5 Training with Backpropagation \u2013 Elemental  In this section we discuss how we train the different parameters in the model when the costJdiscussed in Section 1. 4 is positive. No parameter updates are necessary if the cost is 0. Since we typically update parameters using gradient descent (or a variant such as SGD), we typically need the gradient information for any parameter as required in the update equation:   \u03b8 (t+^1 )= \u03b8 (t)\u2212 \u03b1 \u2207 \u03b8 (t)J  Backpropagation is technique that allows us to use the chain rule of differentiation to calculate loss gradients for any parameter used in the feed-forward computation on the model. To understand this further, let us understand the toy network shown in Figure 5 for which we will perform backpropagation.   Figure 5 : This is a 4 - 2 - 1 neural network where neuronjon layerkreceives input z(jk)and produces activation outputa(jk).\n\nfilename: materials_formatted.md\nextension: .md\n\nAlthough oversimplified, this is a classic case demonstrating the need for non-linear decision boundaries. In the next set of notes, we study neural networks as a class of non-linear models that have performed particularly well in deep learning applications.   Figure 10 : Here, we see that the non- linear decision boundary allows for much better classification of datapoints.           \n\n# SEP 775 Lecture 03:  # Natural Language Processing with Deep Learning Course  ## SEP 775 Lecture Notes: Part III  ## Neural Networks, Backpropagation  Keyphrases: Neural networks. Forward computation. Backward propagation. Neuron Units. Max-margin Loss. Gradient checks. Xavier parameter initialization. Learning rates. Adagrad. This set of notes introduces single and multilayer neural networks, and how they can be used for classification purposes. We then dis- cuss how they can be trained using a distributed gradient descent technique known as backpropagation. We will see how the chain rule can be used to make parameter updates sequentially. After a rigorous mathematical discussion of neural networks, we will discuss some practical tips and tricks in training neural networks involving: neuron units (non-linearities), gradient checks, Xavier parameter ini- tialization, learning rates, Adagrad, etc. Lastly, we will motivate the use of recurrent neural networks as a language model.  ## 1 Neural Networks: Foundations   Figure 1 : We see here how a non-linear decision boundary separates the data very well. This is the prowess of neural networks. Fun Fact: Neural networks are biologically in- spired classifiers which is why they are often called \"artificial neural networks\" to distinguish them from the organic kind. However, in reality human neural networks are so much more capable and complex from artificial neural net- works that it is usually better to not draw too many parallels between the two.  We established in our previous discussions the need for non-linear classifiers since most data are not linearly separable and thus, our classification performance on them is limited. Neural networks are a family of classifiers with non-linear decision boundary as seen in Figure 1. Now that we know the sort of decision boundaries neural networks create, let us see how they manage doing so.  ### 1. 1 A Neuron  A neuron is a generic computational unit that takesninputs and produces a single output.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat challenge does the maximum margin objective function address in neural networks?[/INST] \nAnswer: The maximum margin objective function addresses the challenge of ensuring that the score computed for \"true\" labeled data points is higher than the score computed for \"false\" labeled data points in neural networks. It aims to create a safety margin, meaning the \"true\" labeled data point should score higher than the \"false\" labeled data point by some positive margin. This function is modified to calculate error only if the \"true\" data point does not have a higher score than the \"false\" data point by at least this margin."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nGloVe:  - Using global statistics to predict the     probability of word j appearing in     the context of word i with a least     squares objective  In comparison, GloVe consists of a weighted least squares model that trains on global word-word co-occurrence counts and thus makes efficient use of statistics. The model produces a word vector space with meaningful sub-structure. It shows state-of-the-art per- formance on the word analogy task, and outperforms other current methods on several word similarity tasks.   ## word vectors ii:glove,evaluation and training 2  ### 1. 2 Co-occurrence Matrix   Co-occurrence Matrix:  - X: word-word co-occurrence matrix - Xij: number of times wordjoccur in     the context of wordi - Xi=\u2211kXik: the number of times     any word k appears in the context of     word i - Pij=P(wj|wi) =XXiji: the probability     of j appearing in the context of word     i  LetXdenote the word-word co-occurrence matrix, whereXijindi- cates the number of times wordjoccur in the context of wordi. Let Xi=\u2211kXikbe the number of times any word k appears in the con-  text of word i. Finally, letPij=P(wj|wi) = Xij Xi be the probability of j appearing in the context of word i. Populating this matrix requires a single pass through the entire corpus to collect the statistics. For large corpora, this pass can be computationally expensive, but it is a one-time up-front cost.  ### 1.\n\nfilename: materials_formatted.md\nextension: .md\n\n(2014) proposed explicit word embed- dings based on a PPMI metric. In the skip-gram and ivLBL models, the objec- tive is to predict a word\u2019s context given the word itself, whereas the objective in the CBOW and vLBL models is to predict a word given its con- text. Through evaluation on a word analogy task, these models demonstrated the capacity to learn linguistic patterns as linear relationships between the word vectors. Unlike the matrix factorization methods, the shallow window-based methods suffer from the disadvantage that they do not operate directly on the co-occurrence statistics of the corpus. Instead, these models scan context windows across the en- tire corpus, which fails to take advantage of the vast amount of repetition in the data.  ## 3 The GloVe Model   The statistics of word occurrences in a corpus is the primary source of information available to all unsupervised methods for learning word represen- tations, and although many such methods now ex- ist, the question still remains as to how meaning is generated from these statistics, and how the re- sulting word vectors might represent that meaning. In this section, we shed some light on this ques- tion. We use our insights to construct a new model for word representation which we call GloVe, for Global Vectors, because the global corpus statis- tics are captured directly by the model. First we establish some notation. Let the matrix of word-word co-occurrence counts be denoted by X, whose entriesXi jtabulate the number of times word\u2211 joccurs in the context of wordi. LetXi= kXikbe the number of times any word appears in the context of wordi. Finally, letPi j=P(j|i)= Xi j/Xibe the probability that wordjappear in the   Table 1: Co-occurrence probabilities for target wordsiceandsteamwith selected context words from a 6 billion token corpus. Only in the ratio does noise from non-discriminative words likewaterandfashion cancel out, so that large values (much greater than 1) correlate well with properties specific to ice, and small values (much less than 1) correlate well with properties specific of steam.   Probability and Ratio k=solid k=gas k=water k=fashion P(k|ice) 1.\n\nfilename: materials_formatted.md\nextension: .md\n\n( 1988 ). Neurocomputing: Foundations of research. chapter Learning Representations by Back-propagating Errors, pages 696 \u2013 699. MIT Press, Cambridge, MA, USA.      \n\n# SEP 775 Lecture 02:  # Natural Language Processing with Deep Learning  ## SEP 775 Lecture Notes: Part II   ## Word Vectors II: GloVe, Evaluation and Training  Keyphrases: Global Vectors for Word Representation (GloVe). In- trinsic and extrinsic evaluations. Effect of hyperparameters on anal- ogy evaluation tasks. Correlation of human judgment with word vector distances. Dealing with ambiguity in word using contexts. Window classification. This set of notes first introduces the GloVe model for training word vectors. Then it extends our discussion of word vectors (in- terchangeably called word embeddings) by seeing how they can be evaluated intrinsically and extrinsically. As we proceed, we discuss the example of word analogies as an intrinsic evaluation technique and how it can be used to tune word embedding techniques. We then discuss training model weights/parameters and word vectors for ex- trinsic tasks. Lastly we motivate artificial neural networks as a class of models for natural language processing tasks.  ## 1 Global Vectors for Word Representation (GloVe)^33 This section is based on the GloVe  ### 1. 1 Comparison with Previous Methods  So far, we have looked at two main classes of methods to find word embeddings. The first set are count-based and rely on matrix factor- ization (e.g. LSA, HAL). While these methods effectively leverage global statistical information, they are primarily used to capture word similarities and do poorly on tasks such as word analogy, indi- cating a sub-optimal vector space structure. The other set of methods are shallow window-based (e.g. the skip-gram and the CBOW mod- els), which learn word embeddings by making predictions in local context windows. These models demonstrate the capacity to capture complex linguistic patterns beyond word similarity, but fail to make use of the global co-occurrence statistics.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nHow does the GloVe model represent word meanings?[/INST] \nAnswer: The GloVe model represents word meanings by using global statistics to predict the probability of a word appearing in the context of another word. It uses a weighted least squares model that trains on global word-word co-occurrence counts, thus making efficient use of statistics. The model produces a word vector space with meaningful sub-structure. It also captures the global corpus statistics directly. This model shows state-of-the-art performance on the word analogy task, and outperforms other current methods on several word similarity tasks."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nBut they really are pretty amazing.  Essential to these successes is the use of \u201cLSTMs,\u201d a very special kind of recurrent neural network which works, for many tasks, much much better than the standard version. Almost all exciting results based on recurrent neural networks are achieved with them. It\u2019s these LSTMs that this essay will explore.  ## The Problem of Long-Term Dependencies  One of the appeals of RNNs is the idea that they might be able to connect previous information to the present task, such as using previous video frames might inform the understanding of the present frame. If RNNs could do this, they\u2019d be extremely useful. But can they? It depends.  Sometimes, we only need to look at recent information to perform the present task. For example, consider a language model trying to predict the next word based on the previous ones. If we are trying to predict the last word in \u201cthe clouds are in the _sky_ ,\u201d we don\u2019t need any further context \u2013 it\u2019s pretty obvious the next word is going to be sky. In such cases, where the gap between the relevant information and the place that it\u2019s needed is small, RNNs can learn to use the past information.  But there are also cases where we need more context. Consider trying to predict the last word in the text \u201cI grew up in France... I speak fluent _French_ .\u201d Recent information suggests that the next word is probably the name of a language, but if we want to narrow down which language, we need the context of France, from further back. It\u2019s entirely possible for the gap between the relevant information and the point where it is needed to become very large.  Unfortunately, as that gap grows, RNNs become unable to learn to connect the information.  In theory, RNNs are absolutely capable of handling such \u201clong-term dependencies.\u201d A human could carefully pick parameters for them to solve toy problems of this form. Sadly, in practice, RNNs don\u2019t seem to be able to learn them. The problem was explored in depth by Hochreiter (1991) [German] (http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf) and Bengio, et al.\n\nfilename: materials_formatted.md\nextension: .md\n\n### 2 Recurrent Neural Networks (RNN)  Unlike the conventional translation models, where only a finite win- dow of previous words would be considered for conditioning the language model, Recurrent Neural Networks (RNN) are capable of conditioning the model onallprevious words in the corpus.   xt\u2212 1 xt xt +   ht\u2212 1 ht ht + W\" W\"   yt\u2212 1 yt yt+ 1   Figure 3 : A Recurrent Neural Network (RNN). Three time-steps are shown.  Figure 3 introduces the RNN architecture where each vertical rect- angular box is a hidden layer at a time-step,t. Each such layer holds a number of neurons, each of which performs a linear matrix oper- ation on its inputs followed by a non-linear operation (e.g. tanh()). At each time-step, there are two inputs to the hidden layer: the out- put of the previous layerht\u2212 1 , and the input at that timestepxt. The former input is multiplied by a weight matrixW(hh)and the latter by a weight matrixW(hx)to produce output featuresht, which are multiplied with a weight matrixW(S)and run through a softmax over the vocabulary to obtain a prediction outputy\u02c6of the next word (Equations 5 and 6 ). The inputs and outputs of each single neuron are illustrated in Figure 4.   ht= \u03c3 (W(hh)ht\u2212 1 +W(hx)x[t]) ( 5 )   y\u02c6t=so f tmax(W(S)ht) ( 6 )   ### language models,rnn,gru and lstm 4  What is interesting here is that the same weightsW(hh)andW(hx)are applied repeatedly at each timestep. Thus, the number of parameters the model has to learn is less, and most importantly, is independent of the length of the input sequence - thus defeating the curse of di- mensionality!   Figure 4 : The inputs and outputs to a neuron of a RNN  Below are the details associated with each parameter in the net- work:  - x 1 , ...,xt\u2212 1 ,xt,xt+ 1 , ...xT: the word vectors corresponding to a cor-     pus with T words.\n\nfilename: materials_formatted.md\nextension: .md\n\nHere, we discuss the use of a gated activation function thereby modifying the RNN architecture. What motivates this? Well, although RNNs can theoretically capture long-term dependencies, they are very hard to actually train to do this. Gated recurrent units are designed in a manner to have more persistent memory thereby making it easier for RNNs to capture long-term dependencies. Let us see mathematically how a GRU usesht\u2212 1 andxtto generate the next hidden stateht. We will then dive into the intuition of this architecture.   zt= \u03c3 (W(z)xt+U(z)ht\u2212 1 ) (Update gate) rt= \u03c3 (W(r)xt+U(r)ht\u2212 1 ) (Reset gate) \u0303ht=tanh(rt\u25e6Uht\u2212 1 +Wxt) (New memory) ht= ( 1 \u2212zt)\u25e6h \u0303t+zt\u25e6ht\u2212 1 (Hidden state)   ### language models,rnn,gru and lstm 12  The above equations can be thought of a GRU\u2019s four fundamental op- erational stages and they have intuitive interpretations that make this model much more intellectually satisfying (see Figure 12 ):  1. New memory generation: A new memory \u0303htis the consolidation of a new input wordxtwith the past hidden stateht\u2212 1. Anthropomor- phically, this stage is the one who knows the recipe of combining a newly observed word with the past hidden stateht\u2212 1 to summarize this new word in light of the contextual past as the vectorh \u0303t.  2. Reset Gate: The reset signalrtis responsible for determining how importantht\u2212 1 is to the summarizationh \u0303t. The reset gate has the ability to completely diminish past hidden state if it finds thatht\u2212 1 is irrelevant to the computation of the new memory.  3. Update Gate: The update signalztis responsible for determining how much ofht\u2212 1 should be carried forward to the next state. For instance, ifzt \u22481, thenht\u2212 1 is almost entirely copied out toht. Conversely, ifzt\u22480, then mostly the new memoryh \u0303tis forwarded to the next hidden state.  4.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat are the advantages and disadvantages of RNNs?[/INST] \nAnswer: RNNs, or Recurrent Neural Networks, have several advantages. They are capable of conditioning the model on all previous words in a corpus, which is unlike conventional translation models that only consider a finite window of previous words. This allows RNNs to connect previous information to the present task, which can be extremely useful in tasks such as understanding a present frame in a video based on previous frames, or predicting the next word in a sentence based on the previous ones. RNNs also have the advantage of having a fixed number of parameters to learn, regardless of the length of the input sequence, which helps to avoid the curse of dimensionality.\n\nHowever, RNNs also have some disadvantages. While they are theoretically capable of handling long-term dependencies, in practice they often struggle to learn them. This means that as the gap between the relevant information and the point where it is needed grows, RNNs become unable to learn to connect the information. This can make them less effective in situations where more context is needed to perform a task, such as predicting the last word in a text based on information from further back in the text."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\n# 2 Implement Bert Model  ## 2.1 Code structure:  - bert.py\u2013 This file contains the BERT Model whose backbone is the transformer. It     contains three classes:        - BertSelfAttention:The multi-head attention layer of the transformer. This layer           maps a query and a set of key-value pairs to an output. The output is calculated           as the weighted sum of the values, where the weight of each value is computed by a           function that takes the query and the corresponding key. To implement this layer,           you can:              \u2217 linearly project the queries, keys, and values with their corresponding linear                 layers              \u2217 split the vectors for multi-head attention              \u2217 follow the equation to compute the attended output of each head              \u2217 concatenate multi-head attention outputs to recover the original shape   Attention(Q, K, V) =softmax(  ### QKT   \u221a   dk   )V  - BertLayer: This corresponds to one transformer layer which has     \u2217 a multi-head attention layer     \u2217 add-norm layer     \u2217 a feed-forward layer     \u2217 another add-norm layer - BertModel: This is the BertModel that takes the input ids and returns the con-     textualized representation for each word. The structure of the BertModel is:    \u2217 an embedding layer that consists of word embedding word_embedding and po- sitional embeddingpos_embedding. \u2217 bert encoder layer which is a stack of config.num_hidden_layers BertLaye \u2217 a projection layer for [CLS] token which is often used for classification tasks The desired outputs are \u2217 last_hidden_state: the contextualized embedding for each word of the sentence, taken from the last BertLayer (i.e. the output of the bert encoder) \u2217 pooler_output: the [CLS] token embedding  - sanity_check.py\u2013 You can use this file to test your BERT implementation - classifier.py\u2013 This file is the main entrance of training and testing the BERT Model     for text classification task, contains the pipeline to        - call the BERT model to encode the sentences for their contextualized representations        - feed in the encoded representations for the sentence classification task        - fine-tune the Bert model on the downstream tasks (e.g. sentence classification) - base_bert.py- This is the base class for the BertModel.\n\nfilename: materials_formatted.md\nextension: .md\n\nsentence classification) - base_bert.py- This is the base class for the BertModel. It contains functions to     - initialize the weights init_weights, _init_weights     - restore pre-trained weights from_pretrained. Since we are using the weights from        HuggingFace, we are doing a few mappings to match the parameter names You won\u2019t        need to modify this file in this assignment. - tokenizer.py\u2013 This is where BertTokenizer is defined. - optimizer.py\u2013 This is where the AdamW optimizer is defined. - config.py\u2013 This is where the configuration class is defined. - utils.py\u2013 This file contains utility functions for various purposes.  ## 2.2 What you should to implement  Components that require your implementations are comment with # todo. The detailed instructions can be found in their corresponding code blocks You need to complete the following 4 functions in corresponding class:  - bert.BertSelfAttention.attention - bert.BertLayer.add_norm - bert.BertLayer.forward - bert.BertModel.embed  And complete the following class:  - classifier.BertCentClassifier  This class is used to   - encode the sentences using BERT to obtain the pooled output representation of the     sentence. - classify the sentence by applying dropout to the pooled-output and project it using a     linear layer. - adjust the model parameters depending on whether we are pre-training or fine-tuning     BERT.  ATTENTION:you are free to reorganize the functions inside each class, but please don\u2019t change the variable names that correspond to BERT parameters. The change to these variable names will fail to load the pre-trained weights.  # 3 Test your code  - Using sanity_check.py to test your BERT implementation - Run the following command to train and test the BERT model:   python3 classifier.py \u2013option [pretrain/finetune] \u2013epochs NUM_EPOCHS \u2013lr LR \u2013train data/sst-train.txt \u2013dev data/sst-dev.txt \u2013test data/sst-test.txt   \n\n# SEP 775 Lecture 01:  # Note 1 : Introduction and Word 2 Vec Course  Natural Language Processing with Deep Learning    Summary.\n\nfilename: materials_formatted.md\nextension: .md\n\nAn illustration of how ORQA scores answer derivations is presented in Figure 1. Following recent advances in transfer learn- ing, all scoring components are derived from BERT (Devlin et al., 2018), a bidirectional trans- former that has been pre-trained on unsupervised language-modeling data. We refer the reader to the original paper for details of the architecture. In this work, the relevant abstraction can be de- scribed by the following function:  BERT(x1, [x2]) = {CLS : hCLS, 1 : h1, 2 : h2, ...}  The BERT function takes one or two string in- puts (x1 and optionally x2) as arguments. It re- turns vectors corresponding to representations of the CLS pooling token or the input tokens.  Retriever component In order for the retriever to be learnable, we de\ufb01ne the retrieval score as the inner product of dense vector representations of the question q and the evidence block b.  hq = WqBERTQ(q)[CLS] hb = WbBERTB(b)[CLS]  Sretr (b, q) = h(cid:62)  q hb  where Wq and Wb are matrices that project the BERT output into 128-dimensional vectors.  Reader component The reader is a span-based variant of the reading comprehension model pro- posed in Devlin et al. (2018):  hstart = BERTR(q, b)[START(s)] hend = BERTR(q, b)[END(s)]  Sread (b, s, q) = MLP([hstart ; hend ])  Following Lee et al. (2016), a span is represented by the concatenation of its end points, which is scored by a multi-layer perceptron to enable start/end interaction.  Inference & Learning Challenges The model described above is conceptually simple. However, inference and learning are challenging since (1) an  Example  Q: Who is credited with developing the XY coordinate plane? A: Ren\u00b4e Descartes  Q: How many districts are in the state of Alabama?\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat is the purpose of the BertLayer class in the SEP775 Final Projects coding exercise?[/INST] \nAnswer: The BertLayer class in the SEP775 Final Projects coding exercise corresponds to one transformer layer. It contains a multi-head attention layer, an add-norm layer, a feed-forward layer, and another add-norm layer. It is a part of the BERT Model structure and plays a crucial role in the transformation and processing of the input data."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nBLEU+case.mixed+numrefs.1+smooth.exp+tok.intl+version.1.2.20]  Figure 3.4: Few-shot translation performance on 6 language pairs as model capacity increases. There is a consistent trend of improvement across all datasets as the model scales, and as well as tendency for translation into English to be stronger than translation from English.  15  Setting  Winograd Winogrande (XL)  Fine-tuned SOTA GPT-3 Zero-Shot GPT-3 One-Shot GPT-3 Few-Shot  90.1a 88.3* 89.7* 88.6*  84.6b 70.2 73.2 77.7  Table 3.5: Results on the WSC273 version of Winograd schemas and the adversarial Winogrande dataset. See Section 4 for details on potential contamination of the Winograd test set. a[SBBC19] b[LYN+20]  Figure 3.5: Zero-, one-, and few-shot performance on the adversarial Winogrande dataset as model capacity scales. Scaling is relatively smooth with the gains to few-shot learning increasing with model size, and few-shot GPT-3 175B is competitive with a \ufb01ne-tuned RoBERTA-large.  each translation task improves performance by over 7 BLEU and nears competitive performance with prior work. GPT-3 in the full few-shot setting further improves another 4 BLEU resulting in similar average performance to prior unsupervised NMT work. GPT-3 has a noticeable skew in its performance depending on language direction. For the three input languages studied, GPT-3 signi\ufb01cantly outperforms prior unsupervised NMT work when translating into English but underperforms when translating in the other direction. Performance on En-Ro is a noticeable outlier at over 10 BLEU worse than prior unsupervised NMT work. This could be a weakness due to reusing the byte-level BPE tokenizer of GPT-2 which was developed for an almost entirely English training dataset.\n\nfilename: materials_formatted.md\nextension: .md\n\nFor both Fr-En and De-En, few shot GPT-3 outperforms the best supervised result we could \ufb01nd but due to our unfamiliarity with the literature and the appearance that these are un-competitive benchmarks we do not suspect those results represent true state of the art. For Ro-En, few shot GPT-3 performs within 0.5 BLEU of the overall SOTA which is achieved by a combination of unsupervised pretraining, supervised \ufb01netuning on 608K labeled examples, and backtranslation [LHCG19b].  Finally, across all language pairs and across all three settings (zero-, one-, and few-shot), there is a smooth trend of improvement with model capacity. This is shown in Figure 3.4 in the case of few-shot results, and scaling for all three settings is shown in Appendix H.  3.4 Winograd-Style Tasks  The Winograd Schemas Challenge [LDM12] is a classical task in NLP that involves determining which word a pronoun refers to, when the pronoun is grammatically ambiguous but semantically unambiguous to a human. Recently \ufb01ne-tuned language models have achieved near-human performance on the original Winograd dataset, but more dif\ufb01cult versions  16  Setting  PIQA ARC (Easy)  Fine-tuned SOTA 79.4 80.5* GPT-3 Zero-Shot 80.5* GPT-3 One-Shot 82.8* GPT-3 Few-Shot  92.0[KKS+20] 68.8 71.2 70.1  ARC (Challenge) OpenBookQA 87.2[KKS+20] 78.5[KKS+20] 57.6 51.4 58.8 53.2 65.4 51.5  Table 3.6: GPT-3 results on three commonsense reasoning tasks, PIQA, ARC, and OpenBookQA. GPT-3 Few-Shot PIQA result is evaluated on the test server. See Section 4 for details on potential contamination issues on the PIQA test set.  Figure 3.6: GPT-3 results on PIQA in the zero-shot, one-shot, and few-shot settings.\n\nfilename: materials_formatted.md\nextension: .md\n\nNevertheless, one-shot performance is still quite strong, and even zero-shot performance of the full GPT-3 signi\ufb01cantly  22  Setting  GPT-3 Zero-shot GPT-3 One-shot GPT-3 Few-shot  2D+  76.9 99.6 100.0  2D-  58.0 86.4 98.9  3D+  34.2 65.5 80.4  3D-  48.3 78.7 94.2  4D+  4.0 14.0 25.5  4D-  7.5 14.0 26.8  5D+  5D-  0.7 3.5 9.3  0.8 3.8 9.9  2Dx  19.8 27.4 29.2  1DC  9.8 14.3 21.3  Table 3.9: Results on basic arithmetic tasks for GPT-3 175B. {2,3,4,5}D{+,-} is 2, 3, 4, and 5 digit addition or subtraction, 2Dx is 2 digit multiplication. 1DC is 1 digit composite operations. Results become progressively stronger moving from the zero-shot to one-shot to few-shot setting, but even the zero-shot shows signi\ufb01cant arithmetic abilities.  Setting  GPT-3 Zero-shot GPT-3 One-shot GPT-3 Few-shot  CL  3.66 21.7 37.9  A1  2.28 8.62 15.1  A2  8.91 25.9 39.7  RI  8.26 45.4 67.2  RW  0.09 0.48 0.44  Table 3.10: GPT-3 175B performance on various word unscrambling and word manipulation tasks, in zero-, one-, and few-shot settings.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat method did GPT-3 use to improve its translation performance in few-shot settings?[/INST] \nAnswer: GPT-3 improved its translation performance in few-shot settings by increasing its model capacity. There was a consistent trend of improvement across all datasets as the model scaled. This was particularly noticeable when translating into English."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nGloVe:  - Using global statistics to predict the     probability of word j appearing in     the context of word i with a least     squares objective  In comparison, GloVe consists of a weighted least squares model that trains on global word-word co-occurrence counts and thus makes efficient use of statistics. The model produces a word vector space with meaningful sub-structure. It shows state-of-the-art per- formance on the word analogy task, and outperforms other current methods on several word similarity tasks.   ## word vectors ii:glove,evaluation and training 2  ### 1. 2 Co-occurrence Matrix   Co-occurrence Matrix:  - X: word-word co-occurrence matrix - Xij: number of times wordjoccur in     the context of wordi - Xi=\u2211kXik: the number of times     any word k appears in the context of     word i - Pij=P(wj|wi) =XXiji: the probability     of j appearing in the context of word     i  LetXdenote the word-word co-occurrence matrix, whereXijindi- cates the number of times wordjoccur in the context of wordi. Let Xi=\u2211kXikbe the number of times any word k appears in the con-  text of word i. Finally, letPij=P(wj|wi) = Xij Xi be the probability of j appearing in the context of word i. Populating this matrix requires a single pass through the entire corpus to collect the statistics. For large corpora, this pass can be computationally expensive, but it is a one-time up-front cost.  ### 1.\n\nfilename: materials_formatted.md\nextension: .md\n\nStudents should be aware that their voice and/or image may be recorded by others during the class. Please speak with the instructor if this is a concern for you.  ## EXTREME CIRCUMSTANCES  The University reserves the right to change the dates and deadlines for any or all courses in extreme circumstances (e.g., severe weather, labour disruptions, etc.). Changes will be communicated through regular McMaster communication channels, such as McMaster Daily News, A2L and/or McMaster email.   \n\n# SEP 775 Assignment 1:   McMaster University SEP 775  # Assignment 1  # In-Depth Word Vectors Analysis (Total: 50 Points)  ## Objectives:  This assignment focuses on a comprehensive understanding of word vector technologies, specifically Word2Vec and GloVe. You will explore their applications, visualize the results, and analyze the semantic and syntactic relationships they capture.  1. Building and Analyzing Word Vectors with Word2Vec (20 Points)     - Task:Create word vectors using the Word2Vec model on a selected corpus.     - Visualization:Utilize PCA or t-SNE for visualizing these vectors in 2D.     - Analysis:Discuss the word relationships and clusters formed. 2. GloVe Vectors Advanced Analysis (20 Points)     - Task:Analyze word relationships using pre-trained GloVe vectors.     - Activity:Conduct an analogy task (e.g., king - man + woman = ?) with examples.     - Explanation:Explain the results of the analogy tasks. 3. Semantic and Syntactic Word Relationships (10 Points)     - Comparison:Evaluate Word2Vec and GloVe for capturing semantic and syntactic        relationships.     - Illustration:Use specific word pairs or groups to demonstrate differences in repre-        sentation by each model.  McMaster University 1    McMaster University SEP 775  ## Resources for Word2Vec and GloVe:  - Word2Vec: Pre-trained Word2Vec embeddings can be found on repositories such as     HuggingFace or directly using Gensim. - GloVe: Pre-trained GloVe embeddings are available at the GloVe website or through     HuggingFace.  # Submission Guidelines:  - Submit all code in a ZIP file, including Jupyter Notebooks, and a detailed PDF report     with written explanations and visualizations.\n\nfilename: materials_formatted.md\nextension: .md\n\nGloVe outperforms it while using a corpus less than half the size. Table 4 shows results on the NER task with the CRF-based model. The L-BFGS training termi- nates when no improvement has been achieved on the dev set for 25 iterations. Otherwise all config- urations are identical to those used by Wang and Manning (2013). The model labeledDiscreteis the baseline using a comprehensive set of discrete features that comes with the standard distribution of the Stanford NER model, but with no word vec- tor features. In addition to the HPCA and SVD models discussed previously, we also compare to the models of Huang et al. (2012) (HSMN) and Collobert and Weston (2008) (CW). We trained the CBOW model using the word2vectool^8. The GloVe model outperforms all other methods on all evaluation metrics, except for the CoNLL test set, on which the HPCA method does slightly better. We conclude that the GloVe vectors are useful in downstream NLP tasks, as was first (^8) We use the same parameters as above, except in this case we found 5 negative samples to work slightly better than 10.   Table 4: F1 score on NER task with 50d vectors. Discreteis the baseline without word vectors. We use publicly-available vectors for HPCA, HSMN, and CW. See text for details. Model Dev Test ACE MUC Discrete 91.0 85.4 77.4 73. SVD 90.8 85.7 77.3 73. SVD-S 91.0 85.5 77.6 74. SVD-L 90.5 84.8 73.6 71. HPCA 92.6 88.7 81.7 80. HSMN 90.5 85.7 78.7 74. CW 92.2 87.4 81.7 80. CBOW 93.1 88.2 82.2 81.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat are the main objectives of the GloVe model?[/INST] \nAnswer: The main objectives of the GloVe model are to use global statistics to predict the probability of a word appearing in the context of another word. This is achieved by training a weighted least squares model on global word-word co-occurrence counts, which allows for efficient use of statistics. The model aims to produce a word vector space with meaningful sub-structure, demonstrating high performance on the word analogy task and outperforming other methods on several word similarity tasks."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nArXiv, abs/1909.08593, 2019.  75     \n\n# Dense Passage Retrieval for Open-Domain Question Answering:  Dense Passage Retrieval for Open-Domain Question Answering  Vladimir Karpukhin\u2217, Barlas O\u02d8guz\u2217, Sewon Min\u2020, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen\u2021, Wen-tau Yih  Facebook AI  \u2020University of Washington  \u2021Princeton University  {vladk, barlaso, plewis, ledell, edunov, scottyih}@fb.com sewon@cs.washington.edu danqic@cs.princeton.edu  Abstract  Open-domain question answering relies on ef- \ufb01cient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented us- ing dense representations alone, where em- beddings are learned from a small number of questions and passages by a simple dual- encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene- BM25 system greatly by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.1  1  Introduction  Open-domain question answering (QA) (Voorhees, 1999) is a task that answers factoid questions us- ing a large collection of documents. While early QA systems are often complicated and consist of multiple components (Ferrucci (2012); Moldovan et al. (2003), inter alia), the advances of reading comprehension models suggest a much simpli\ufb01ed two-stage framework: (1) a context retriever \ufb01rst selects a small subset of passages where some of them contain the answer to the question, and then (2) a machine reader can thoroughly exam- ine the retrieved contexts and identify the correct answer (Chen et al., 2017). Although reducing open-domain QA to machine reading is a very rea- sonable strategy, a huge performance degradation is often observed in practice2, indicating the needs of improving retrieval.\n\nfilename: materials_formatted.md\nextension: .md\n\nThe dense representation alone, however, is typically inferior to the sparse one. While not the focus of this work, dense representations from pre- trained models, along with cross-attention mecha- nisms, have also been shown effective in passage or dialogue re-ranking tasks (Nogueira and Cho, 2019; Humeau et al., 2020). Finally, a concurrent work (Khattab and Zaharia, 2020) demonstrates the feasibility of full dense retrieval in IR tasks. Instead of employing the dual-encoder framework, they introduced a late-interaction operator on top of the BERT encoders.  Dense retrieval for open-domain QA has been explored by Das et al. (2019), who propose to re- trieve relevant passages iteratively using reformu- lated question vectors. As an alternative approach that skips passage retrieval, Seo et al. (2019) pro- pose to encode candidate answer phrases as vectors and directly retrieve the answers to the input ques- tions ef\ufb01ciently. Using additional pretraining with the objective that matches surrogates of questions and relevant passages, Lee et al. (2019) jointly train the question encoder and reader. Their approach outperforms the BM25 plus reader paradigm on multiple open-domain QA datasets in QA accuracy, and is further extended by REALM (Guu et al., 2020), which includes tuning the passage encoder asynchronously by re-indexing the passages dur- ing training. The pretraining objective has also recently been improved by Xiong et al. (2020b). In contrast, our model provides a simple and yet  effective solution that shows stronger empirical per- formance, without relying on additional pretraining or complex joint training schemes.  DPR has also been used as an important mod- ule in very recent work. For instance, extending the idea of leveraging hard negatives, Xiong et al. (2020a) use the retrieval model trained in the pre- vious iteration to discover new negatives and con- struct a different set of examples in each training iteration. Starting from our trained DPR model, they show that the retrieval performance can be further improved.\n\nfilename: materials_formatted.md\nextension: .md\n\nThe exact impact on throughput is harder to mea- sure: ORQA uses 2-3x longer passages compared to DPR (288 word pieces compared to our 100 tokens) and the computational complexity is super- linear in passage length. We also note that we found k = 50 to be optimal for NQ, and k = 10 leads to only marginal loss in exact match accu- racy (40.8 vs. 41.5 EM on NQ), which should be roughly comparable to ORQA\u2019s 5-passage setup.  7 Related Work  Passage retrieval has been an important compo- nent for open-domain QA (Voorhees, 1999). It not only effectively reduces the search space for answer extraction, but also identi\ufb01es the support context for users to verify the answer. Strong sparse vector space models like TF-IDF or BM25 have  been used as the standard method applied broadly to various QA tasks (e.g., Chen et al., 2017; Yang et al., 2019a,b; Nie et al., 2019; Min et al., 2019a; Wolfson et al., 2020). Augmenting text-based re- trieval with external structured information, such as knowledge graph and Wikipedia hyperlinks, has also been explored recently (Min et al., 2019b; Asai et al., 2020).  The use of dense vector representations for re- trieval has a long history since Latent Semantic Analysis (Deerwester et al., 1990). Using labeled pairs of queries and documents, discriminatively trained dense encoders have become popular re- cently (Yih et al., 2011; Huang et al., 2013; Gillick et al., 2019), with applications to cross-lingual document retrieval, ad relevance prediction, Web search and entity retrieval. Such approaches com- plement the sparse vector methods as they can po- tentially give high similarity scores to semantically relevant text pairs, even without exact token match- ing. The dense representation alone, however, is typically inferior to the sparse one.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat is the main advantage of using dense representations for passage retrieval in open-domain QA, as opposed to traditional sparse vector space models?[/INST] \nAnswer: The main advantage of using dense representations for passage retrieval in open-domain QA is that they can potentially give high similarity scores to semantically relevant text pairs, even without exact token matching. This allows for more efficient and accurate retrieval of relevant contexts for answering questions. Dense representations can be learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a range of open-domain QA datasets, dense retrievers have been found to outperform traditional sparse vector space models, such as TF-IDF or BM25, by 9%-19% in terms of top-20 passage retrieval accuracy."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\n#### DALL-E2 on Winoground I   #### DALL-E2 on Winoground II   STILL NOT SOLVED   #### Outline  1. Early models 2. Features and fusion 3. Contrastive models 4. Multimodal foundation models 5. Evaluation 6. Beyond images: Other modalities 7. Where to next?   #### Speech / audio  Can EASILY do another full lecture just on this topic.  Recent cool example: Whisper, trained on 680,000 hours of multilingual multitask data.  We can also just treat audio as vision ;) Radford et al., 2022  Kiela et al., (^2017) Bapna et al., 2022   #### Video and text and audio  MERLOT (Zellers et al., 2021) MERLOT Reserve (idem, 2022)   #### Grounded language learning in simulated environments   Hermann, Hill, et al. 2017 Das et al., 2018   #### Text to 3D  POINT-E (Nichol, Jun, et al., 2022)   #### Outline  1. Early models 2. Multimodal fusion 3. Contrastive models 4. Multimodal foundation models 5. Other modalities 6. Evaluation 7. Where to next?   #### One foundation model to rule them all  There will modality-agnostic foundation models that can read and generate many modalities.  These models can be bigger and be trained on vastly more data. Parameters will be shared in interesting ways.  Automatic alignment from unpaired unimodal data will become a big topic.   #### Multimodal scaling laws  We are just beginning to understand multimodal scaling laws, lots of interesting work to do here in understanding trade-offs.   #### Retrieval augmented generative multimodal models   Lewis et al., 2021   These can be multimodal   This can be multimodal This^ can^ be^ multimodal   #### Better evaluation and benchmarking   Barbosa-Silva et al. 2022   Kiela et al. 2020  We need better measurement.   # Thank you!\n\nfilename: materials_formatted.md\nextension: .md\n\nMultimodality is one of the main frontiers of the new foundation model revolution.   #### Multimodal brains  McGurk effect (McGurk & MacDonald, 1976)  https://www.youtube.com/watch?v=2k8fHR9jKVM   #### Multimodal applications  Let\u2019s say we\u2019re dealing with two modalities \u2013 text, and images:   \u25cf Retrieval (image <> text) \u25cf Captioning (image - > text) \u25cf Generation (text - > image) \u25cf Visual question answering (image+text - > text) \u25cf Multimodal classification (image+text - > label) \u25cf Better understanding/generation (image+text - > label/text)   #### Multimodal is hot right now  .. and/but has been \u201cthe next big thing\u201d for almost a decade!   #### Outline  1. Early models 2. Features and fusion 3. Contrastive models 4. Multimodal foundation models 5. Evaluation 6. Beyond images: Other modalities 7. Where to next?   #### Cross-modal \u201cVisual-Semantic Embeddings\u201d  WSABI (Weston et al 2010), DeVise (Frome et al 2013), Cross-Modal Transfer (Socher et al 2013)   Frome et al. 2013 Socher et al. 2013   #### Multimodal distributional semantics (Bruni et al., 2014)  Algorithm:   \u25cf Obtain visual \u201cword vector\u201d via BOVW: \u25cb Identify keypoints and get their descriptors \u25cb Cluster these and map to counts \u25cf Concatenate with textual word vector \u25cf Apply SVD to \u201cfuse\u201d information  This approach was shown to lead to better word representations on human similarity judgment datasets. Bag^ of^ visual^ words^ (BOVW)^ illustration.^ Bruni^ et^ al.,^ 2014.\n\nfilename: materials_formatted.md\nextension: .md\n\nMultimodal foundation models 5. Evaluation 6. Beyond images: Other modalities 7. Where to next?   #### BERT Refresher  How do we make this multimodal? \u2192   Alammar (2018), Illustrated Bert   #### Visual BERTs: VisualBERT   VisualBERT Li et al. 2019   #### Visual BERTs: ViLBERT   ViLBERT Lu et al. 2019   #### Visual BERTs: LXMERT   LXMERT Tan & Bansal 2019  Learning Cross-Modality Encoder Representations from Transformers   #### Visual BERTs: Supervised Multimodal Bitransformers   MMBT Kiela et al. 2019   #### Visual BERTs: PixelBert   Misnomer: they mean segment embedding PixelBert^ Huang^ et^ al.^2020   #### UNITER   Chen, Yi, Lu, et al. 2020   #### ViLT (Kim et al. 2021)  Feeding data directly to the transformer.   #### So many models   Du et al. 2022 VLP Survey   #### Recommended paper  Bugliarello et al. (2021). _Multimodal Pretraining Unmasked: A Meta-Analysis and a Unified Framework of Vision-and-Language BERTs_. Finding: in the same conditions, models perform _very_ similarly.   #### FLAVA (Singh et al., 2021)  Holistic approach to multimodality.  One foundation model spanning V&L, CV and NLP.  Jointly pretrained on:   \u25cf unimodal text data (CCNews + BookCorpus) \u25cf unimodal image data (ImageNet) \u25cf public paired image-text data (70M)  All data/models are publicly released.   #### The PMD dataset   \u25cf 70M image-text pairs from public sources   #### Problem to solve    43  ### How does FLAVA work?    44  ### How does FLAVA work?    45  ### How does FLAVA work?   #### How well does it work?   \u25cf On average, over 35 tasks, FLAVA obtains impressive performance   #### SimVLM (Wang et al., 2022)  Slowly moving from contrastive/discriminative to generative.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat future directions do researchers see for multimodal models and foundation models?[/INST] \nAnswer: Researchers envision several future directions for multimodal models and foundation models. They foresee the development of modality-agnostic foundation models that can read and generate many modalities. These models are expected to be larger and trained on significantly more data, with parameters shared in interesting ways. The automatic alignment from unpaired unimodal data is predicted to become a significant topic. Additionally, the understanding of multimodal scaling laws is just beginning, indicating a lot of potential work in understanding trade-offs. Furthermore, the field is moving towards better evaluation and benchmarking, indicating a need for improved measurement methods."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nThis note introduces the field of Natural Language Pro- cessing (NLP) briefly, and then discusses word 2 vec and the funda- mental, beautiful idea of representing words as low-dimensional real-valued vectors learned from distributional signal.  ## 1 Introduction to Natural Language Processing   Natural language processing is a field of science and engineering focused on the development and study of automatic systems that understand and generate natural (that is, human,) languages.  ### 1. 1 Humans and language   Human languages are communicative devices enabling the effi- cient sharing and storage of complex ideas, facts, and intents. As [Manning, 2022 ] argues, the complexity of communication enabled by language is a uniquely human intelligence among species. As scientists and engineers interested in the creation and study of intel- ligent systems, human language is to us both a fascinating object of study\u2014after all, it hasevolved to be learnable and useful\u2014and a great enabler for interacting with humans even in contexts where other modalities (e.g., vision) are also of interest.  ### 1. 2 Language and machines   Human children, interacting with a rich multi-modality world and various forms of feedback, acquire language with exceptional sample efficiency (not observing that much language) and compute efficiency (brains are efficient computing machines!) With all the (impressive!) advances in NLP in the last decades, we are still nowhere close to developing learning machines that have a fraction of acquisition ability of children. One fundamental (and still quite open) problem in building language-learning machines is the question of representa- tion ; how should we represent language in a computer such that the computer can robustly process and/or generate it? This is where this course focuses on the tools provided by deep learning , a highly effec- tive toolkit for representing both the wild variety of natural language    deep learning 2  and some of the rules and structures it sometimes adheres to. Much of this course will be dedicated to this question of representation, and the rest of this note will talk about a basic subquestion: how do we represent words? Before that, though, let\u2019s briefly discuss some of the applications you can hope to build after learning modern NLP techniques.  ### 1.\n\nfilename: materials_formatted.md\nextension: .md\n\nBy contrast pre-training at large enough scale appears to offer a \u201cnatural\u201d broad distribution of tasks implicitly contained in predicting the text itself. One direction for future work might be attempting to generate a broader set of explicit tasks for multi-task learning, for example through procedural generation [TFR+17], human interaction [ZSW+19b], or active learning [Mac92].  Algorithmic innovation in language models over the last two years has been enormous, including denoising-based bidirectionality [DCLT18], pre\ufb01xLM [DL15] and encoder-decoder architectures [LLG+19, RSR+19], random permu- tations during training [YDY+19], architectures that improve the ef\ufb01ciency of sampling [DYY+19], improvements in data and training procedures [LOG+19], and ef\ufb01ciency increases in the embedding parameters [LCG+19]. Many of these techniques provide signi\ufb01cant gains on downstream tasks. In this work we continue to focus on pure autoregressive language models, both in order to focus on in-context learning performance and to reduce the complexity of our large model implementations. However, it is very likely that incorporating these algorithmic advances could improve GPT-3\u2019s performance on downstream tasks, especially in the \ufb01ne-tuning setting, and combining GPT-3\u2019s scale with these algorithmic techniques is a promising direction for future work.  8 Conclusion  We presented a 175 billion parameter language model which shows strong performance on many NLP tasks and benchmarks in the zero-shot, one-shot, and few-shot settings, in some cases nearly matching the performance of  40  state-of-the-art \ufb01ne-tuned systems, as well as generating high-quality samples and strong qualitative performance at tasks de\ufb01ned on-the-\ufb02y. We documented roughly predictable trends of scaling in performance without using \ufb01ne-tuning. We also discussed the social impacts of this class of model. Despite many limitations and weaknesses, these results suggest that very large language models may be an important ingredient in the development of adaptable, general language systems.  Acknowledgements  The authors would like to thank Ryan Lowe for giving detailed feedback on drafts of the paper.\n\nfilename: materials_formatted.md\nextension: .md\n\nThe core argument of this work is that the bi-directionality and the two pre- training tasks presented in Section 3.1 account for the majority of the empirical improvements, but we do note that there are several other differences between how BERT and GPT were trained:  \u2022 GPT is trained on the BooksCorpus (800M words); BERT is trained on the BooksCor- pus (800M words) and Wikipedia (2,500M words).  \u2022 GPT uses a sentence separator ([SEP]) and classi\ufb01er token ([CLS]) which are only in- troduced at \ufb01ne-tuning time; BERT learns [SEP], [CLS] and sentence A/B embed- dings during pre-training.  \u2022 GPT was trained for 1M steps with a batch size of 32,000 words; BERT was trained for 1M steps with a batch size of 128,000 words.  \u2022 GPT used the same learning rate of 5e-5 for all \ufb01ne-tuning experiments; BERT chooses a task-speci\ufb01c \ufb01ne-tuning learning rate which performs the best on the development set.  To isolate the effect of these differences, we per- form ablation experiments in Section 5.1 which demonstrate that the majority of the improvements are in fact coming from the two pre-training tasks and the bidirectionality they enable.  A.5  Illustrations of Fine-tuning on Different Tasks  The illustration of \ufb01ne-tuning BERT on different tasks can be seen in Figure 4. Our task-speci\ufb01c models are formed by incorporating BERT with one additional output layer, so a minimal num- ber of parameters need to be learned from scratch. Among the tasks, (a) and (b) are sequence-level tasks while (c) and (d) are token-level tasks. In the \ufb01gure, E represents the input embedding, Ti represents the contextual representation of token i, [CLS] is the special symbol for classi\ufb01cation out- put, and [SEP] is the special symbol to separate non-consecutive token sequences.  B Detailed Experimental Setup  B.1 Detailed Descriptions for the GLUE  Benchmark Experiments.  Our GLUE results in Table1 are obtained https://gluebenchmark.com/ from leaderboard https://blog.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWho are the authors of 'Speech and Language Processing'?[/INST] \nAnswer: I don't know."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nThese approaches have proved successful in a number of domains including Machine Translation [ 18 , 22 ] and Semantic Parsing [ 21 ]. Our approach does have several differences, including less of emphasis on lightly editing a retrieved item, but on aggregating content from several pieces of retrieved content, as well as learning latent retrieval, and retrieving evidence documents rather than related training pairs. This said, RAG techniques may work well in these settings, and could represent promising future work.  ### 6 Discussion  In this work, we presented hybrid generation models with access to parametric and non-parametric memory. We showed that our RAG models obtain state of the art results on open-domain QA. We found that people prefer RAG\u2019s generation over purely parametric BART, finding RAG more factual and specific. We conducted an thorough investigation of the learned retrieval component, validating its effectiveness, and we illustrated how the retrieval index can be hot-swapped to update the model without requiring any retraining. In future work, it may be fruitful to investigate if the two components can be jointly pre-trained from scratch, either with a denoising objective similar to BART or some another objective. Our work opens up new research directions on how parametric and non-parametric memories interact and how to most effectively combine them, showing promise in being applied to a wide variety of NLP tasks.   ### Broader Impact  This work offers several positive societal benefits over previous work: the fact that it is more strongly grounded in real factual knowledge (in this case Wikipedia) makes it \u201challucinate\u201d less with generations that are more factual, and offers more control and interpretability. RAG could be employed in a wide variety of scenarios with direct benefit to society, for example by endowing it with a medical index and asking it open-domain questions on that topic, or by helping people be more effective at their jobs.  With these advantages also come potential downsides: Wikipedia, or any potential external knowledge source, will probably never be entirely factual and completely devoid of bias. Since RAG can be employed as a language model, similar concerns as for GPT-2 [ 50 ] are valid here, although arguably to a lesser extent, including that it might be used to generate abuse, faked or misleading content in the news or on social media; to impersonate others; or to automate the production of spam/phishing content [ 54 ].\n\nfilename: materials_formatted.md\nextension: .md\n\n2013) are publicly available toolkits for building n-gram language models. Modern language modeling is more commonly done withneural networklan- guage models, which solve the major problems with n-grams: the number of param- eters increases exponentially as the n-gram order increases, and n-grams have no way to generalize from training to test set. Neural language models instead project words into acontinuousspace in which words with similar contexts have similar representations. We\u2019ll introduce bothfeedforwardlanguage models (Bengio et al. 2006, Schwenk 2007) in Chapter 7, andrecurrentlanguage models (Mikolov, 2012) in Chapter 9.        \n\n# Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks:   ## Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks   Patrick Lewis\u2020\u2021, Ethan Perez?,   Aleksandra Piktus\u2020, Fabio Petroni\u2020, Vladimir Karpukhin\u2020, Naman Goyal\u2020, Heinrich K\u00fcttler\u2020,   Mike Lewis\u2020, Wen-tau Yih\u2020, Tim Rockt\u00e4schel\u2020\u2021, Sebastian Riedel\u2020\u2021, Douwe Kiela\u2020   \u2020Facebook AI Research;\u2021University College London;?New York University;   plewis@fb.com  ### Abstract   Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on down- stream NLP tasks. However, their ability to access and precisely manipulate knowl- edge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre- trained models with a differentiable access mechanism to explicit non-parametric memory have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) \u2014 models which combine pre-trained parametric and non-parametric mem- ory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever.\n\nfilename: materials_formatted.md\nextension: .md\n\nIn contrast, we explore a setting where both parametric and non-parametric memory components are pre-trained and pre-loaded with extensive knowledge. Crucially, by using pre-trained access mechanisms, the ability to access knowledge is present without additional training.  Our results highlight the benefits of combining parametric and non-parametric memory with genera- tion forknowledge-intensive tasks\u2014tasks that humans could not reasonably be expected to perform without access to an external knowledge source. Our RAG models achieve state-of-the-art results on open Natural Questions [ 29 ], WebQuestions [ 3 ] and CuratedTrec [ 2 ] and strongly outperform recent approaches that use specialised pre-training objectives on TriviaQA [ 24 ]. Despite these being extractive tasks, we find that unconstrained generation outperforms previous extractive approaches. For knowledge-intensive generation, we experiment with MS-MARCO [ 1 ] and Jeopardy question generation, and we find that our models generate responses that are more factual, specific, and diverse than a BART baseline. For FEVER [ 56 ] fact verification, we achieve results within 4.3% of state-of-the-art pipeline models which use strong retrieval supervision. Finally, we demonstrate that the non-parametric memory can be replaced to update the models\u2019 knowledge as the world changes.^1  ### 2 Methods  We explore RAG models, which use the input sequencexto retrieve text documentszand use them as additional context when generating the target sequencey. As shown in Figure 1, our models leverage two components: (i) a retrieverp\u03b7(z|x)with parameters\u03b7that returns (top-K truncated) distributions over text passages given a queryxand (ii) a generatorp\u03b8(yi|x,z,y1:i\u2212 1 )parametrized  (^1) Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform- ers Library [ 66 ] and can be found athttps://github.com/huggingface/transformers/blob/master/ examples/rag/. An interactive demo of RAG models can be found athttps://huggingface.co/rag/   by\u03b8that generates a current token based on a context of the previousi\u2212 1 tokensy1:i\u2212 1 , the original inputxand a retrieved passagez.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat is the main advantage of retrieval-augmented generation (RAG) models for NLP tasks?[/INST] \nAnswer: The main advantage of retrieval-augmented generation (RAG) models for NLP tasks is that they combine pre-trained parametric and non-parametric memory for language generation. This combination allows the models to access and precisely manipulate knowledge, which is particularly beneficial for knowledge-intensive tasks. RAG models can retrieve and aggregate content from several pieces of retrieved content, making them more factual and specific. They have been found to outperform task-specific architectures and other pre-training objectives, achieving state-of-the-art results on various tasks. Furthermore, the retrieval index in RAG models can be updated without requiring any retraining, allowing the models to adapt to new information."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\n1  Introduction  Deep neural networks trained with some version of Stochastic Gradient Descent have been shown to substantially outperform previous approaches on various supervised learning tasks in computer vision [Krizhevsky et al., 2012] and speech processing [Hinton et al., 2012]. But state-of-the-art deep neural networks often require many days of training. It is possible to speed-up the learning by computing gradients for different subsets of the training cases on different machines or splitting the neural network itself over many machines [Dean et al., 2012], but this can require a lot of com- munication and complex software. It also tends to lead to rapidly diminishing returns as the degree of parallelization increases. An orthogonal approach is to modify the computations performed in the forward pass of the neural net to make learning easier. Recently, batch normalization [Ioffe and Szegedy, 2015] has been proposed to reduce training time by including additional normalization stages in deep neural networks. The normalization standardizes each summed input using its mean and its standard deviation across the training data. Feedforward neural networks trained using batch normalization converge faster even with simple SGD. In addition to training time improvement, the stochasticity from the batch statistics serves as a regularizer during training.  Despite its simplicity, batch normalization requires running averages of the summed input statis- tics. In feed-forward networks with \ufb01xed depth, it is straightforward to store the statistics separately for each hidden layer. However, the summed inputs to the recurrent neurons in a recurrent neu- ral network (RNN) often vary with the length of the sequence so applying batch normalization to RNNs appears to require different statistics for different time-steps. Furthermore, batch normaliza-              tion cannot be applied to online learning tasks or to extremely large distributed models where the minibatches have to be small.  This paper introduces layer normalization, a simple normalization method to improve the training speed for various neural network models. Unlike batch normalization, the proposed method directly estimates the normalization statistics from the summed inputs to the neurons within a hidden layer so the normalization does not introduce any new dependencies between training cases. We show that layer normalization works well for RNNs and improves both the training time and the generalization performance of several existing RNN models.\n\nfilename: materials_formatted.md\nextension: .md\n\nBoth models use Adam [Kingma and Ba, 2014] with the same initial hyperparameters and both models are trained using the same architectural choices as used in Vendrov et al. [2016]. We refer the reader to the appendix for a description of how layer normalization is applied to GRU.  Figure 1 illustrates the validation curves of the models, with and without layer normalization. We plot R@1, R@5 and R@10 for the image retrieval task. We observe that layer normalization offers a per-iteration speedup across all metrics and converges to its best validation model in 60% of the time it takes the baseline model to do so. In Table 2, the test set results are reported from which we observe that layer normalization also results in improved generalization over the original model. The results we report are state-of-the-art for RNN embedding models, with only the structure-preserving model of Wang et al. [2016] reporting better results on this task. However, they evaluate under different conditions (1 test set instead of the mean over 5) and are thus not directly comparable.  6.2 Teaching machines to read and comprehend  In order to compare layer normalization to the recently proposed recurrent batch normalization [Cooijmans et al., 2016], we train an unidirectional attentive reader model on the CNN corpus both introduced by Hermann et al. [2015]. This is a question-answering task where a query description about a passage must be answered by \ufb01lling in a blank. The data is anonymized such that entities are given randomized tokens to prevent degenerate solutions, which are consistently permuted dur- ing training and evaluation. We follow the same experimental protocol as Cooijmans et al. [2016] and modify their public code to incorporate layer normalization 2 which uses Theano [Team et al., 2016]. We obtained the pre-processed dataset used by Cooijmans et al. [2016] which differs from the original experiments of Hermann et al. [2015] in that each passage is limited to 4 sentences. In Cooijmans et al. [2016], two variants of recurrent batch normalization are used: one where BN is only applied to the LSTM while the other applies BN everywhere throughout the model.\n\nfilename: materials_formatted.md\nextension: .md\n\nBut this is unnecessary for the logit outputs where the prediction con\ufb01dence is determined by the scale of the logits. We only apply layer normalization to the fully-connected hidden layers that excludes the last softmax layer.  All the models were trained using 55000 training data points and the Adam [Kingma and Ba, 2014] optimizer. For the smaller batch-size, the variance term for batch normalization is computed using the unbiased estimator. The experimental results from Figure 6 highlight that layer normalization is robust to the batch-sizes and exhibits a faster training convergence comparing to batch normalization that is applied to all layers.  6.7 Convolutional Networks  We have also experimented with convolutional neural networks. In our preliminary experiments, we observed that layer normalization offers a speedup over the baseline model without normalization, but batch normalization outperforms the other methods. With fully connected layers, all the hidden units in a layer tend to make similar contributions to the \ufb01nal prediction and re-centering and re- scaling the summed inputs to a layer works well. However, the assumption of similar contributions is no longer true for convolutional neural networks. The large number of the hidden units whose receptive \ufb01elds lie near the boundary of the image are rarely turned on and thus have very different statistics from the rest of the hidden units within the same layer. We think further research is needed to make layer normalization work well in ConvNets.  7 Conclusion  In this paper, we introduced layer normalization to speed-up the training of neural networks. We provided a theoretical analysis that compared the invariance properties of layer normalization with batch normalization and weight normalization. We showed that layer normalization is invariant to per training-case feature shifting and scaling.  Empirically, we showed that recurrent neural networks bene\ufb01t the most from the proposed method especially for long sequences and small mini-batches.  Acknowledgments  This research was funded by grants from NSERC, CFI, and Google.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nHow does layer normalization address the challenge of applying normalization techniques to RNNs?[/INST] \nAnswer: Layer normalization addresses the challenge by directly estimating the normalization statistics from the summed inputs to the neurons within a hidden layer. This means the normalization does not introduce any new dependencies between training cases. It works well for RNNs and improves both the training time and the generalization performance of several existing RNN models. It is particularly beneficial for long sequences and small mini-batches."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nHere, we discuss the use of a gated activation function thereby modifying the RNN architecture. What motivates this? Well, although RNNs can theoretically capture long-term dependencies, they are very hard to actually train to do this. Gated recurrent units are designed in a manner to have more persistent memory thereby making it easier for RNNs to capture long-term dependencies. Let us see mathematically how a GRU usesht\u2212 1 andxtto generate the next hidden stateht. We will then dive into the intuition of this architecture.   zt= \u03c3 (W(z)xt+U(z)ht\u2212 1 ) (Update gate) rt= \u03c3 (W(r)xt+U(r)ht\u2212 1 ) (Reset gate) \u0303ht=tanh(rt\u25e6Uht\u2212 1 +Wxt) (New memory) ht= ( 1 \u2212zt)\u25e6h \u0303t+zt\u25e6ht\u2212 1 (Hidden state)   ### language models,rnn,gru and lstm 12  The above equations can be thought of a GRU\u2019s four fundamental op- erational stages and they have intuitive interpretations that make this model much more intellectually satisfying (see Figure 12 ):  1. New memory generation: A new memory \u0303htis the consolidation of a new input wordxtwith the past hidden stateht\u2212 1. Anthropomor- phically, this stage is the one who knows the recipe of combining a newly observed word with the past hidden stateht\u2212 1 to summarize this new word in light of the contextual past as the vectorh \u0303t.  2. Reset Gate: The reset signalrtis responsible for determining how importantht\u2212 1 is to the summarizationh \u0303t. The reset gate has the ability to completely diminish past hidden state if it finds thatht\u2212 1 is irrelevant to the computation of the new memory.  3. Update Gate: The update signalztis responsible for determining how much ofht\u2212 1 should be carried forward to the next state. For instance, ifzt \u22481, thenht\u2212 1 is almost entirely copied out toht. Conversely, ifzt\u22480, then mostly the new memoryh \u0303tis forwarded to the next hidden state.  4.\n\nfilename: materials_formatted.md\nextension: .md\n\nWe only input new values to the state when we forget something older.  A slightly more dramatic variation on the LSTM is the Gated Recurrent Unit, or GRU, introduced by Cho, et al. (2014) (http://arxiv.org/pdf/1406.1078v3.pdf). It combines the forget and input gates into a single \u201cupdate gate.\u201d It also merges the cell state and hidden state, and makes some other changes. The resulting model is simpler than standard LSTM models, and has been growing increasingly popular.  These are only a few of the most notable LSTM variants. There are lots of others, like Depth Gated RNNs by Yao, et al. (2015) (http://arxiv.org/pdf/1508.03790v2.pdf). There\u2019s also some completely different approach to tackling long-term dependencies, like Clockwork RNNs by Koutnik, et al. (2014) (http://arxiv.org/pdf/1402.3511v1.pdf).  Which of these variants is best? Do the differences matter? Greff, et al. (2015) (http://arxiv.org/pdf/1503.04069.pdf) do a nice comparison of popular variants, finding that they\u2019re all about the same. Jozefowicz, et al. (2015) (http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf) tested more than ten thousand RNN architectures, finding some that worked better than LSTMs on certain tasks.  ## Conclusion  Earlier, I mentioned the remarkable results people are achieving with RNNs. Essentially all of these are achieved using LSTMs. They really work a lot better for most tasks!  Written down as a set of equations, LSTMs look pretty intimidating. Hopefully, walking through them step by step in this essay has made them a bit more approachable.    Attention and Augmented Recurrent Neural Networks On Distill A Modular PerspectiveConv Nets Neural Networks, Manifolds, and Topology Deep Learning, NLP, and Representations  LSTMs were a big step in what we can accomplish with RNNs. It\u2019s natural to wonder: is there another big step? A common opinion among researchers is: \u201cYes!\n\nfilename: materials_formatted.md\nextension: .md\n\n4. Hidden state: The hidden statehtis finally generated using the past hidden inputht\u2212 1 and the new memory generatedh \u0303twith the advice of the update gate.  Figure 12 : The detailed internals of a It is important to note that to train a GRU, we need to learn all GRU the different parameters:W,U,W(r),U(r),W(z),U(z). These follow the same backpropagation procedure we have seen in the past.   ### language models,rnn,gru and lstm 13  ### 4 Long-Short-Term-Memories  Long-Short-Term-Memories are another type of complex activation unit that differ a little from GRUs. The motivation for using these is similar to those for GRUs however the architecture of such units does differ. Let us first take a look at the mathematical formulation of LSTM units before diving into the intuition behind this design:   it= \u03c3 (W(i)xt+U(i)ht\u2212 1 ) (Input gate) ft= \u03c3 (W(f)xt+U(f)ht\u2212 1 ) (Forget gate) ot= \u03c3 (W(o)xt+U(o)ht\u2212 1 ) (Output/Exposure gate) c \u0303t=tanh(W(c)xt+U(c)ht\u2212 1 ) (New memory cell) ct=ft\u25e6ct\u2212 1 +it\u25e6c \u0303t (Final memory cell) ht=ot\u25e6tanh(ct)   Figure 13 : The detailed internals of a LSTM   ### language models,rnn,gru and lstm 14  We can gain intuition of the structure of an LSTM by thinking of its architecture as the following stages:  1. New memory generation: This stage is analogous to the new mem- ory generation stage we saw in GRUs. We essentially use the input wordxtand the past hidden stateht\u2212 1 to generate a new memory c \u0303twhich includes aspects of the new wordx(t).  2. Input Gate: We see that the new memory generation stage doesn\u2019t check if the new word is even important before generating the new memory \u2013 this is exactly the input gate\u2019s function.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat are Gated Recurrent Units (GRUs) and why are they used?[/INST] \nAnswer: Gated Recurrent Units (GRUs) are a modification of the Recurrent Neural Network (RNN) architecture that use a gated activation function. The motivation behind GRUs is that while RNNs can theoretically capture long-term dependencies, they are difficult to train to do so. GRUs are designed to have more persistent memory, making it easier for RNNs to capture these long-term dependencies. They operate through four fundamental stages: new memory generation, reset gate, update gate, and hidden state. The new memory generation stage combines a new input with the past hidden state. The reset gate determines the importance of the past hidden state to the new memory. The update gate determines how much of the past hidden state should be carried forward to the next state. Finally, the hidden state is generated using the past hidden input and the new memory. GRUs are simpler than standard LSTM models and have been growing increasingly popular."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nInstead, we introduce a more general weighting function, which we are free to take to depend on the context word as well:  #### J\u02c6=   W   \u2211   i= 1   W   \u2211   j= 1   f(Xij)(~uTj~vi\u2212logXij)^2  ### 1. 4 Conclusion  In conclusion, the GloVe model efficiently leverages global statistical information by training only on the nonzero elements in a word- word co-occurrence matrix, and produces a vector space with mean- ingful sub-structure. It consistently outperformsword 2 vecon the word analogy task, given the same corpus, vocabulary, window size, and training time. It achieves better results faster, and also obtains the best results irrespective of speed.  ## 2 Evaluation of Word Vectors  So far, we have discussed methods such as theWord _2_ VecandGloVe methods to train and discover latent vector representations of natural language words in a semantic space. In this section, we discuss how we can quantitatively evaluate the quality of word vectors produced by such techniques.  ### 2. 1 Intrinsic Evaluation  Intrinsic evaluation of word vectors is the evaluation of a set of word vectors generated by an embedding technique (such as Word 2 Vec or GloVe) on specific intermediate subtasks (such as analogy comple- tion). These subtasks are typically simple and fast to compute and thereby allow us to help understand the system used to generate the word vectors. An intrinsic evaluation should typically return to us a number that indicates the performance of those word vectors on the evaluation subtask.   Figure 1 : The left subsystem (red) being expensive to train is modified by substituting with a simpler subsystem (green) for intrinsic evaluation.  Motivation: Let us consider an example where our final goal is to create a question answering system which uses word vectors   ## word vectors ii:glove,evaluation and training 4  as inputs. One approach of doing so would be to train a machine learning system that:  1. Takes words as inputs  2. Converts them to word vectors  3. Uses word vectors as inputs for an elaborate machine learning system  4. Maps the output word vectors by this system back to natural language words  5.\n\nfilename: materials_formatted.md\nextension: .md\n\n( 1988 ). Neurocomputing: Foundations of research. chapter Learning Representations by Back-propagating Errors, pages 696 \u2013 699. MIT Press, Cambridge, MA, USA.      \n\n# SEP 775 Lecture 02:  # Natural Language Processing with Deep Learning  ## SEP 775 Lecture Notes: Part II   ## Word Vectors II: GloVe, Evaluation and Training  Keyphrases: Global Vectors for Word Representation (GloVe). In- trinsic and extrinsic evaluations. Effect of hyperparameters on anal- ogy evaluation tasks. Correlation of human judgment with word vector distances. Dealing with ambiguity in word using contexts. Window classification. This set of notes first introduces the GloVe model for training word vectors. Then it extends our discussion of word vectors (in- terchangeably called word embeddings) by seeing how they can be evaluated intrinsically and extrinsically. As we proceed, we discuss the example of word analogies as an intrinsic evaluation technique and how it can be used to tune word embedding techniques. We then discuss training model weights/parameters and word vectors for ex- trinsic tasks. Lastly we motivate artificial neural networks as a class of models for natural language processing tasks.  ## 1 Global Vectors for Word Representation (GloVe)^33 This section is based on the GloVe  ### 1. 1 Comparison with Previous Methods  So far, we have looked at two main classes of methods to find word embeddings. The first set are count-based and rely on matrix factor- ization (e.g. LSA, HAL). While these methods effectively leverage global statistical information, they are primarily used to capture word similarities and do poorly on tasks such as word analogy, indi- cating a sub-optimal vector space structure. The other set of methods are shallow window-based (e.g. the skip-gram and the CBOW mod- els), which learn word embeddings by making predictions in local context windows. These models demonstrate the capacity to capture complex linguistic patterns beyond word similarity, but fail to make use of the global co-occurrence statistics.\n\nfilename: materials_formatted.md\nextension: .md\n\n4, we plot the overall performance on the analogy task as a function of training time. The twox-axes at the bottom indicate the corre- sponding number of training iterations for GloVe and negative samples forword2vec. We note thatword2vec\u2019s performance actually decreases if the number of negative samples increases be- yond about 10. Presumably this is because the negative sampling method does not approximate the target probability distribution well.^9 For the same corpus, vocabulary, window size, and training time, GloVe consistently outperforms word2vec. It achieves better results faster, and also obtains the best results irrespective of speed.  ## 5 Conclusion  Recently, considerable attention has been focused on the question of whether distributional word representations are best learned from count-based  (^9) In contrast, noise-contrastive estimation is an approxi- mation which improves with more negative samples. In Ta- ble 1 of (Mnih et al., 2013), accuracy on the analogy task is a non-decreasing function of the number of negative samples. methods or from prediction-based methods. Cur- rently, prediction-based models garner substantial support; for example, Baroni et al. (2014) argue that these models perform better across a range of tasks. In this work we argue that the two classes of methods are not dramatically different at a fun- damental level since they both probe the under- lying co-occurrence statistics of the corpus, but the efficiency with which the count-based meth- ods capture global statistics can be advantageous. We construct a model that utilizes this main ben- efit of count data while simultaneously capturing the meaningful linear substructures prevalent in recent log-bilinear prediction-based methods like word2vec. The result, GloVe, is a new global log-bilinear regression model for the unsupervised learning of word representations that outperforms other models on word analogy, word similarity, and named entity recognition tasks.  ## Acknowledgments   We thank the anonymous reviewers for their valu- able comments. Stanford University gratefully acknowledges the support of the Defense Threat Reduction Agency (DTRA) under Air Force Re- search Laboratory (AFRL) contract no.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat is the significance of the word analogy task in evaluating word vector models according to the GloVe model paper?[/INST] \nAnswer: The word analogy task is significant in evaluating word vector models as it helps in assessing the performance of these models. According to the GloVe model paper, this task is used to compare the efficiency of different models. For instance, GloVe consistently outperforms word2vec on the word analogy task, given the same corpus, vocabulary, window size, and training time. This task is also used to tune word embedding techniques. The paper suggests that GloVe achieves better results faster on this task, and also obtains the best results irrespective of speed."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nArXiv, abs/1909.08593, 2019.  75     \n\n# Dense Passage Retrieval for Open-Domain Question Answering:  Dense Passage Retrieval for Open-Domain Question Answering  Vladimir Karpukhin\u2217, Barlas O\u02d8guz\u2217, Sewon Min\u2020, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen\u2021, Wen-tau Yih  Facebook AI  \u2020University of Washington  \u2021Princeton University  {vladk, barlaso, plewis, ledell, edunov, scottyih}@fb.com sewon@cs.washington.edu danqic@cs.princeton.edu  Abstract  Open-domain question answering relies on ef- \ufb01cient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented us- ing dense representations alone, where em- beddings are learned from a small number of questions and passages by a simple dual- encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene- BM25 system greatly by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.1  1  Introduction  Open-domain question answering (QA) (Voorhees, 1999) is a task that answers factoid questions us- ing a large collection of documents. While early QA systems are often complicated and consist of multiple components (Ferrucci (2012); Moldovan et al. (2003), inter alia), the advances of reading comprehension models suggest a much simpli\ufb01ed two-stage framework: (1) a context retriever \ufb01rst selects a small subset of passages where some of them contain the answer to the question, and then (2) a machine reader can thoroughly exam- ine the retrieved contexts and identify the correct answer (Chen et al., 2017). Although reducing open-domain QA to machine reading is a very rea- sonable strategy, a huge performance degradation is often observed in practice2, indicating the needs of improving retrieval.\n\nfilename: materials_formatted.md\nextension: .md\n\n. . .  Table 7: Examples of passages returned from BM25 and DPR. Correct answers are written in blue and the content words in the question are written in bold.  from the top 5 positive and top 30 negative passages (from the set of top 100 passages retrieved from the same question). The question encoder\u2019s initial state is taken from a DPR model previously trained on the NQ dataset. The reader\u2019s initial state is a BERT-base model. In terms of the end-to-end QA results, our joint-training scheme does not provide better results compared to the usual retriever/reader training pipeline, resulting in the same 39.8 exact match score on NQ dev as in our regular reader model training.     \n\n# Latent Retrieval for Weakly Supervised Open Domain Question Answering:  Latent Retrieval for Weakly Supervised Open Domain Question Answering  Kenton Lee Ming-Wei Chang Kristina Toutanova Google Research Seattle, WA {kentonl,mingweichang,kristout}@google.com  Abstract  Recent work on open domain question answer- ing (QA) assumes strong supervision of the supporting evidence and/or assumes a black- box information retrieval (IR) system to re- trieve evidence candidates. We argue that both are suboptimal, since gold evidence is not al- ways available, and QA is fundamentally dif- ferent from IR. We show for the \ufb01rst time that it is possible to jointly learn the retriever and reader from question-answer string pairs and without any IR system. In this setting, evi- dence retrieval from all of Wikipedia is treated as a latent variable. Since this is impracti- cal to learn from scratch, we pre-train the re- triever with an Inverse Cloze Task. We evalu- ate on open versions of \ufb01ve QA datasets. On datasets where the questioner already knows the answer, a traditional IR system such as BM25 is suf\ufb01cient. On datasets where a user is genuinely seeking an answer, we show that learned retrieval is crucial, outperforming BM25 by up to 19 points in exact match.  1  Introduction  Due to recent advances in reading comprehension systems, there has been a revival of interest in open domain question answering (QA), where the evidence must be retrieved from an open corpus, rather than being given as input.\n\nfilename: materials_formatted.md\nextension: .md\n\nBy applying a modern reader model to the top retrieved passages, we achieve compara- ble or better results on multiple QA datasets in the open-retrieval setting, compared to several, much complicated systems.  2 Background  The problem of open-domain QA studied in this paper can be described as follows. Given a factoid question, such as \u201cWho \ufb01rst voiced Meg on Family Guy?\u201d or \u201cWhere was the 8th Dalai Lama born?\u201d, a system is required to answer it using a large corpus of diversi\ufb01ed topics. More speci\ufb01cally, we assume  e  the extractive QA setting, in which the answer is restricted to a span appearing in one or more pas- sages in the corpus. Assume that our collection contains D documents, d1, d2, \u00b7 \u00b7 \u00b7 , dD. We \ufb01rst split each of the documents into text passages of equal lengths as the basic retrieval units3 and get M total passages in our corpus C = {p1, p2, . . . , pM }, where each passage pi can be viewed as a sequence 2 , \u00b7 \u00b7 \u00b7 , w(i) 1 , w(i) of tokens w(i) |pi|. Given a question q, the task is to \ufb01nd a span w(i) s+1, \u00b7 \u00b7 \u00b7 , w(i) s , w(i) from one of the passages pi that can answer the question. Notice that to cover a wide variety of domains, the corpus size can easily range from millions of docu- ments (e.g., Wikipedia) to billions (e.g., the Web). As a result, any open-domain QA system needs to include an ef\ufb01cient retriever component that can se- lect a small set of relevant texts, before applying the reader to extract the answer (Chen et al., 2017).4 Formally speaking, a retriever R : (q, C) \u2192 CF is a function that takes as input a question q and a corpus C and returns a much smaller \ufb01lter set of texts CF \u2282 C, where |CF | = k (cid:28) |C|.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nHow does the retriever-reader framework approach the challenge of open-domain question answering?[/INST] \nAnswer: The retriever-reader framework for open-domain question answering operates in two stages. First, a context retriever selects a small subset of passages that may contain the answer to the question. Then, a machine reader thoroughly examines the retrieved contexts and identifies the correct answer. This approach simplifies the process by reducing open-domain question answering to machine reading. However, there is often a significant performance degradation in practice, indicating the need for improved retrieval methods."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nUnlike recent language repre- sentation models (Peters et al., 2018a; Rad- ford et al., 2018), BERT is designed to pre- train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a re- sult, the pre-trained BERT model can be \ufb01ne- tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task- speci\ufb01c architecture modi\ufb01cations.  BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art re- sults on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answer- ing Test F1 to 93.2 (1.5 point absolute im- provement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).  1  Introduction  Language model pre-training has been shown to be effective for improving many natural language processing tasks (Dai and Le, 2015; Peters et al., 2018a; Radford et al., 2018; Howard and Ruder, 2018). These include sentence-level tasks such as natural language inference (Bowman et al., 2015; Williams et al., 2018) and paraphrasing (Dolan and Brockett, 2005), which aim to predict the re- lationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce \ufb01ne-grained output at the token level (Tjong Kim Sang and De Meulder, 2003; Rajpurkar et al., 2016).  There are two existing strategies for apply- ing pre-trained language representations to down- stream tasks: feature-based and \ufb01ne-tuning. The feature-based approach, such as ELMo (Peters et al., 2018a), uses task-speci\ufb01c architectures that include the pre-trained representations as addi- tional features.\n\nfilename: materials_formatted.md\nextension: .md\n\nUn- like Radford et al. (2018), which uses unidirec- tional language models for pre-training, BERT uses masked language models to enable pre- trained deep bidirectional representations. This is also in contrast to Peters et al. (2018a), which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs.  \u2022 We show that pre-trained representations reduce the need for many heavily-engineered task- speci\ufb01c architectures. BERT is the \ufb01rst \ufb01ne- tuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outper- forming many task-speci\ufb01c architectures.  \u2022 BERT advances the state of the art for eleven NLP tasks. The code and pre-trained mod- els are available at https://github.com/ google-research/bert.  2 Related Work  There is a long history of pre-training general lan- guage representations, and we brie\ufb02y review the most widely-used approaches in this section.  2.1 Unsupervised Feature-based Approaches  Learning widely applicable representations of words has been an active area of research for decades, including non-neural (Brown et al., 1992; Ando and Zhang, 2005; Blitzer et al., 2006) and neural (Mikolov et al., 2013; Pennington et al., 2014) methods. Pre-trained word embeddings are an integral part of modern NLP systems, of- fering signi\ufb01cant improvements over embeddings learned from scratch (Turian et al., 2010). To pre- train word embedding vectors, left-to-right lan- guage modeling objectives have been used (Mnih and Hinton, 2009), as well as objectives to dis- criminate correct from incorrect words in left and right context (Mikolov et al., 2013).  These approaches have been generalized to coarser granularities, such as sentence embed- dings (Kiros et al., 2015; Logeswaran and Lee, 2018) or paragraph embeddings (Le and Mikolov, 2014).\n\nfilename: materials_formatted.md\nextension: .md\n\nThe core argument of this work is that the bi-directionality and the two pre- training tasks presented in Section 3.1 account for the majority of the empirical improvements, but we do note that there are several other differences between how BERT and GPT were trained:  \u2022 GPT is trained on the BooksCorpus (800M words); BERT is trained on the BooksCor- pus (800M words) and Wikipedia (2,500M words).  \u2022 GPT uses a sentence separator ([SEP]) and classi\ufb01er token ([CLS]) which are only in- troduced at \ufb01ne-tuning time; BERT learns [SEP], [CLS] and sentence A/B embed- dings during pre-training.  \u2022 GPT was trained for 1M steps with a batch size of 32,000 words; BERT was trained for 1M steps with a batch size of 128,000 words.  \u2022 GPT used the same learning rate of 5e-5 for all \ufb01ne-tuning experiments; BERT chooses a task-speci\ufb01c \ufb01ne-tuning learning rate which performs the best on the development set.  To isolate the effect of these differences, we per- form ablation experiments in Section 5.1 which demonstrate that the majority of the improvements are in fact coming from the two pre-training tasks and the bidirectionality they enable.  A.5  Illustrations of Fine-tuning on Different Tasks  The illustration of \ufb01ne-tuning BERT on different tasks can be seen in Figure 4. Our task-speci\ufb01c models are formed by incorporating BERT with one additional output layer, so a minimal num- ber of parameters need to be learned from scratch. Among the tasks, (a) and (b) are sequence-level tasks while (c) and (d) are token-level tasks. In the \ufb01gure, E represents the input embedding, Ti represents the contextual representation of token i, [CLS] is the special symbol for classi\ufb01cation out- put, and [SEP] is the special symbol to separate non-consecutive token sequences.  B Detailed Experimental Setup  B.1 Detailed Descriptions for the GLUE  Benchmark Experiments.  Our GLUE results in Table1 are obtained https://gluebenchmark.com/ from leaderboard https://blog.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat makes BERT different from previous language representation models?[/INST] \nAnswer: BERT differs from previous language representation models in several ways. Firstly, it is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. This is in contrast to models like Radford et al. (2018) which uses unidirectional models for pre-training, and Peters et al. (2018a) which uses a shallow concatenation of independently trained left-to-right and right-to-left language models. Secondly, BERT can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, reducing the need for many heavily-engineered task-specific architectures. Lastly, BERT's pre-training tasks and bidirectionality account for the majority of its empirical improvements."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\n### 1. 2 Transition-Based Dependency Parsing  Transition-based dependency parsing relies on a state machine which defines the possible transitions to create the mapping from the input sentence to the dependency tree. Thelearning problemis to induce a model which can predict the next transition in the state machine based on the transition history. Theparsing problemis to construct the optimal sequence of transitions for the input sentence, given the previously induced model. Most transition-based systems do not make use of a formal grammar.  ### 1. 3 Greedy Deterministic Transition-Based Parsing  This system was introduced by Nivre in 2003 and was radically dif- ferent from other methods in use at that time. This transition system is a state machine, which consists ofstates andtransitionsbetween those states. The model induces a sequence of transitions from someinitialstate to one of severalterminalstates. States: For any sentenceS=w 0 w 1 ...wn, a state can be described with a triplec= ( _\u03c3_ , _\u03b2_ ,A):  1. a stack _\u03c3_ of wordswifromS,  2. a buffer _\u03b2_ of wordswifromS,  3. a set of dependency arcsAof the form(wi,r,wj), wherewi,wjare fromS, andrdescribes a dependency relation.  It follows that for any sentenceS=w 0 w 1 ...wn,  1. aninitialstatec 0 is of the form([w 0 ] _\u03c3_ ,[w 1 , ...,wn] _\u03b2_ ,\u2205)(only the ROOT is on the stack _\u03c3_ , all other words are in the buffer _\u03b2_ and no actions have been chosen yet),  2. a terminal state has the form( _\u03c3_ ,[] _\u03b2_ ,A).   Transitions: Figure 2 : Transitions for Dependency Parsing.   There are three types of transitions between states:  1. Shift: Remove the first word in the buffer and push it on top of the stack. (Pre-condition: buffer has to be non-empty.)  2.\n\nfilename: materials_formatted.md\nextension: .md\n\nFor a given sentence example, we select the words, POS tags and arc labels given the schematic defined above, extract their corresponding dense feature representations produced from the embedding matricesEw,Et, andEl, and concatenate these vectors into our inputs[xw,xt,xl]. At training time we backpropagate into the dense vector representations, as well as the parameters at later layers.   Feedforward Neural Network Model:  The network contains an input layer[xw,xt,xl], a hidden layer, and a final softmax layer with a cross-entropy loss function. We can either define a single weight matrix in the hidden layer, to operate on a concatenation of[xw,xt,xl], or we can use three weight matrices [W 1 w,W 1 t,W 1 l], one for each input type, as shown in Figure 3. We then apply a non-linear function and use one more affine layer[W 2 ]so that there are an equivalent number of softmax probabilities to the number of possible transitions (the output dimension).   ## dependency parsing 5   Input layer :[xw,xt,xl]   Hidden layer : h= (W 1 wxw+W 1 txt+W 1 lxl+b 1 )^3   Softmax layer : p = softmax(W 2 h)   words POS tags arc labels   ROOT has_VBZ   He_PRP   nsubj   has_VBZ good_JJ control_NN ._.   Stack Buffer Configuration   Figure 3 : The neural network archi- tecture for greedy, transition-based Note that in Figure 3 ,f(x) =x^3 is the non-linear function used. dependency parsing.  For a more complete explanation of a greedy transition-based neural dependency parser, refer to \"A Fast and Accurate Dependency Parser using Neural Networks\" under Further Reading.  Further reading: Danqi Chen, and Christopher D. Manning. \"A Fast and Accurate Dependency Parser using Neural Networks.\" EMNLP. 2014. Kuebler, Sandra, Ryan McDonald, and Joakim Nivre. \u201cDepen- dency parsing.\u201d Synthesis Lectures on Human Language Technolo- gies 1. 1 ( 2009 ): 1 - 127.\n\nfilename: materials_formatted.md\nextension: .md\n\n(Pre-condition: buffer has to be non-empty.)  2. Left-Arcr: Add a dependency arc(wj,r,wi)to the arc setA, wherewiis the word second to the top of the stack andwjis the   ## dependency parsing 3   word at the top of the stack. Removewifrom the stack. (Pre- condition: the stack needs to contain at least two items andwi cannot be the ROOT.)  3. Right-Arcr: Add a dependency arc(wi,r,wj)to the arc setA, wherewiis the word second to the top of the stack andwjis the word at the top of the stack. Removewjfrom the stack. (Pre- condition: The stack needs to contain at least two items.)  A more formal definition of these three transitions is presented in Figure 2.  ### 1. 4 Neural Dependency Parsing  While there are many deep models for dependency parsing, this section focuses specifically on greedy, transition-based neural de- pendency parsers. This class of model has demonstrated compara- ble performance and significantly better efficiency than traditional feature-based discriminative dependency parsers. The primary dis- tinction from previous models is the reliance on dense rather than sparse feature representations. The model we will describe employs the arc-standard system for transitions, as presented in section 1. 3. Ultimately, the aim of the model is to predict a transition sequence from some initial con- figurationcto a terminal configuration, in which the dependency parse tree is encoded. As the model is greedy, it attempts to correctly predict one transitionT \u2208 {shift, Left-Arcr, Right-Arcr}at a time, based on features extracted from the current configuration c= ( _\u03c3_ , _\u03b2_ ,A). Recall, _\u03c3_ is the stack, _\u03b2_ the buffer, andAthe set of dependency arcs for a given sentence.   Feature Selection:  Depending on the desired complexity of the model, there is flexi- bility in defining the input to the neural network. The features for a given sentenceSgenerally include some subset of:  1. Sword: Vector representations for some of the words inS(and their dependents) at the top of the stack _\u03c3_ and buffer _\u03b2_.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat are the key components of the state used in greedy deterministic transition-based parsing?[/INST] \nAnswer: The key components of the state used in greedy deterministic transition-based parsing are a stack of words from the sentence, a buffer of words from the sentence, and a set of dependency arcs. The stack and buffer contain words from the sentence, while the set of dependency arcs is of the form (wi,r,wj), where wi and wj are words from the sentence and r describes a dependency relation."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nTo facilitate systematic evaluation and compar- ison of new embedding models, we release a new frequency-calibrated query inventory along with all user judgments athttp://www.cs. cornell.edu/ \u0303schnabts/eval/. Finally, we observe that word embeddings en- code a surprising degree of information about word frequency. We found this was true even in models that explicitly reserve parameters to com- pensate for frequency effects. This finding may explain some of the variability across embeddings and across evaluation methods. It also casts doubt on the common practice of using the vanilla co- sine similarity as a similarity measure in the em- bedding space. It is important to note that this work is a survey ofevaluationmethods not a survey ofembedding methods. The specific example embeddings pre- sented here were chosen as representative samples only, and may not be optimal.  ## 2 Word embeddings  We refer to a word embedding as a mappingV\u2192 RD:w7\u2192~wthat maps a wordwfrom a vocabu- laryV to a real-valued vector~win an embedding space of dimensionalityD. Following previous work (Collobert et al., 2011; Mikolov et al., 2013a) we use the com- monly employed cosine similarity, defined as similarity(w 1 ,w 2 ) = \u2016~w~w 11 \u2016\u2016\u00b7~w~w^22 \u2016, for all similar- ity computations in the embedding space. The list of nearest neighbors of a wordware all words v \u2208 V \\ {w}, sorted in descending order by similarity(w,v). We will denotewas thequery wordin the remainder of this paper. All experiments in this paper are carried out on six popular unsupervised embedding meth- ods. These embeddings form a representative but incomplete subset; and since we are study-   ing evaluation methods and not embeddings them- selves, no attempt has been made to optimize these embeddings. The first two embedding models, the CBOW model of word2vec (Mikolov et al., 2013a) and C&W embeddings (Collobert et al., 2011) both are motivated by a probabilistic predic- tion approach.\n\nfilename: materials_formatted.md\nextension: .md\n\nFor example, we find that embeddings encode differing degrees of information about word frequency, even after length normalization. This result is surprising for two reasons. First, many algorithms reserve dis- tinct \u201cintercept\u201d parameters to absorb frequency- based effects. Second, we expect that the ge- ometry of the embedding space will be primar-    102 103 104 105 Word frequency   0.   0.   0.   0.   0.   1.   AccuracyCCA C&W CBOW GloVe Rand. Proj H-PCA  Figure 3: Embeddings can accurately predict whether a word is frequent or rare.  ily driven by semantics: the relatively small num- ber of frequent words should be evenly distributed through the space, while large numbers of rare, specific words should cluster around related, but more frequent, words. We trained a logistic regression model to predict word frequency categories based on word vectors. The linear classifier was trained to put words ei- ther in a frequent or rare category, with thresholds varying from 100 to 50,000. At each threshold fre- quency, we sampled the training sets to ensure a consistent balance of the label distribution across all frequencies. We used length-normalized em- beddings, as rare words might have shorter vec- tors resulting from fewer updates during training (Turian et al., 2010). We report the mean accuracy and standard deviation (1\u03c3) using five-fold cross- validation at each threshold frequency in Figure 3. All word embeddings do better than random, suggesting that they contain some frequency in- formation. GloVe and TSCCA achieve nearly 100% accuracy on thresholds up to 1000. Unlike all other embeddings, accuracy for C&W embed- dings increases for larger threshold values. Fur- ther investigation revealed that the weight vector direction changes gradually with the threshold fre- quency \u2014 indicating that frequency seems to be encoded in a smooth way in the embedding space. Although GloVe and CBOW are the two best performing embeddings on the intrinsic tasks, they differ vastly in the amount of frequency informa- tion they encode. As a consequence, we can con- clude that most of the differences in frequency pre- diction are not due to intrinsic properties of natu- ral language: it is not the case that frequent words naturally have only frequent neighbors.\n\nfilename: materials_formatted.md\nextension: .md\n\nWord frequency also interferes with the commonly-used cosine similarity measure. We present a novel evaluation framework based on di- rect comparisons between embeddings that pro- vides more fine-grained analysis and supports sim- ple, crowdsourced relevance judgments. We also present a novelCoherencetask that measures our intuition that neighborhoods in the embedding space should be semantically or syntactically re- lated. We find that extrinsic evaluations, although useful for highlighting specific aspects of embed- ding performance, should not be used as a proxy for generic quality.   Acknowledgments This research was funded in part through NSF Award IIS-1513692. We would like to thank Alexandra Schofield, Adith Swaminathan and all other members of the NLP seminar for their help- ful feedback.  ## References   Jacob Andreas and Dan Klein. 2014. How much do word embeddings encode about syntax? InACL: Short Papers, pages 822\u2013827. Javed Aslam and Mark Montague. 2001. Models for metasearch. InSIGIR, pages 276\u2013284. Marco Baroni, Georgiana Dinu, and German \u0301 Kruszewski. 2014. Don\u2019t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors. InACL, pages 238\u2013247. Elia Bruni, Nam-Khanh Tran, and Marco Baroni.  2014. Multimodal distributional semantics. JAIR, 49:1\u201347. James McKeen Cattell. 1886. The time taken up by cerebral operations.Mind,, (42):220\u2013242. Jonathan Chang, Jordan Boyd-Graber, Sean Gerrish, Chong Wang, and David Blei. 2009. Reading tea leaves: How humans interpret topic models. In NIPS, pages 288\u2013296. Yanqing Chen, Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2013. The expressive power of word embeddings.arXiv preprint: 1408.3456. Ronan Collobert, Jason Weston, L \u0301eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat significant findings about word embeddings are reported in the paper?[/INST] \nAnswer: The paper reports several significant findings about word embeddings. Firstly, it was found that word embeddings encode a surprising degree of information about word frequency. This was true even in models that explicitly reserve parameters to compensate for frequency effects. Secondly, the paper found that embeddings encode differing degrees of information about word frequency, even after length normalization. This was surprising as many algorithms reserve distinct \u201cintercept\u201d parameters to absorb frequency-based effects. Lastly, the paper found that word frequency interferes with the commonly-used cosine similarity measure. The authors also introduced a novel evaluation framework for a more fine-grained analysis and a new Coherence task to measure semantic or syntactic relations in the embedding space."}
{"prompt_response_text": "Context information is below.\n---------------------\nfilename: materials_formatted.md\nextension: .md\n\nSVD-L 300 42B 38.4 58.2 49. GloVe 300 42B 81.9 69.3 75.   Table 5 : Here we compare the perfor- mance of different models under the use of different hyperparameters and datasets   ## word vectors ii:glove,evaluation and training 8   Inspecting the above table, we can make 3 primary observations:  - Performance is heavily dependent on the model used for word     embedding:     This is an expected result since different methods try embedding     words to vectors using fundamentally different properties (such as     co-occurrence count, singular vectors, etc.) Implementation Tip: A window size        of 8 around each center word typically        works well for GloVe embeddings   Figure 2 : Here we see how training time improves training performance and helps squeeze the last few perfor- mance.  - Performance increases with larger corpus sizes:     This happens because of the experience an embedding technique     gains with more examples it sees. For instance, an analogy com-     pletion example will produce incorrect results if it has not encoun-     tered the test words previously. - Performance is lower for extremely low dimensional word vec-     tors:     Lower dimensional word vectors are not able to capture the dif-     ferent meanings of the different words in the corpus. This can be     viewed as a high bias problem where our model complexity is too     low. For instance, let us consider the words \"king\", \"queen\", \"man\",     \"woman\". Intuitively, we would need to use two dimensions such     as \"gender\" and \"leadership\" to encode these into 2 -bit word vec-     tors. Any lower would fail to capture semantic differences between     the four words.        Extremely high dimensional vectors:        Intuitively, it would seem that these        vectors would capture noise in the        corpus that doesn\u2019t allow generaliza-        tion, i.e. resulting in high variance, but        Yin et al. show in On theDimen-        sionality ofWordEmbeddingthat        skip-gram and GloVe are robust to this        over-fitting.  Figure 3 demonstrates how accuracy has been shown to improve with larger corpus.\n\nfilename: materials_formatted.md\nextension: .md\n\nCBOW 93.1 88.2 82.2 81. GloVe 93.2 88.3 82.9 82.  shown for neural vectors in (Turian et al., 2010).  4.4 Model Analysis: Vector Length and Context Size  In Fig. 2, we show the results of experiments that vary vector length and context window. A context window that extends to the left and right of a tar- get word will be called symmetric, and one which extends only to the left will be called asymmet- ric. In (a), we observe diminishing returns for vec- tors larger than about 200 dimensions. In (b) and (c), we examine the effect of varying the window size for symmetric and asymmetric context win- dows. Performance is better on the syntactic sub- task for small and asymmetric context windows, which aligns with the intuition that syntactic infor- mation is mostly drawn from the immediate con- text and can depend strongly on word order. Se- mantic information, on the other hand, is more fre- quently non-local, and more of it is captured with larger window sizes.  4.5 Model Analysis: Corpus Size  In Fig. 3, we show performance on the word anal- ogy task for 300-dimensional vectors trained on different corpora. On the syntactic subtask, there is a monotonic increase in performance as the cor- pus size increases. This is to be expected since larger corpora typically produce better statistics. Interestingly, the same trend is not true for the se- mantic subtask, where the models trained on the smaller Wikipedia corpora do better than those trained on the larger Gigaword corpus. This is likely due to the large number of city- and country- based analogies in the analogy dataset and the fact that Wikipedia has fairly comprehensive articles for most such locations.\n\nfilename: materials_formatted.md\nextension: .md\n\nAnother way to improve accuracy is to provide more than one example of the relationship. By using ten examples instead of one to form the relationship vector (we average the individual vectors together), we have observed improvement of accuracy of our best models by about 10% absolutely on the semantic-syntactic test.  It is also possible to apply the vector operations to solve different tasks. For example, we have observed good accuracy for selecting out-of-the-list words, by computing average vector for a list of words, and finding the most distant word vector. This is a popular type of problems in certain human intelligence tests. Clearly, there is still a lot of discoveries to be made using these techniques.  ## 6 Conclusion  In this paper we studied the quality of vector representations of words derived by various models on a collection of syntactic and semantic language tasks. We observed that it is possible to train high quality word vectors using very simple model architectures, compared to the popular neural network models (both feedforward and recurrent). Because of the much lower computational complexity, it is possible to compute very accurate high dimensional word vectors from a much larger data set. Using the DistBelief distributed framework, it should be possible to train the CBOW and Skip-gram models even on corpora with one trillion words, for basically unlimited size of the vocabulary. That is several orders of magnitude larger than the best previously published results for similar models.  An interesting task where the word vectors have recently been shown to significantly outperform the previous state of the art is the SemEval-2012 Task 2 [11]. The publicly available RNN vectors were used together with other techniques to achieve over 50% increase in Spearman\u2019s rank correlation over the previous best result [31]. The neural network based word vectors were previously applied to many other NLP tasks, for example sentiment analysis [12] and paraphrase detection [28]. It can be expected that these applications can benefit from the model architectures described in this paper.  Our ongoing work shows that the word vectors can be successfully applied to automatic extension of facts in Knowledge Bases, and also for verification of correctness of existing facts. Results from machine translation experiments also look very promising. In the future, it would be also interesting to compare our techniques to Latent Relational Analysis [30] and others.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: [INST]<<SYS>>\n\nYou are an AI teaching Assistant for the course SEP 775. \nYou will provide an interactive platform for students to ask questions and receive guidance on course materials.\nYour goal is to answer questions as accurately as possible based on the instructions and context provided.\nIf you do not know the answer, response with \"I don't know.\"\n<</SYS>>\n\nWhat role does corpus size play in the performance of word vector models?[/INST] \nAnswer: The performance of word vector models tends to increase with larger corpus sizes. This is because the embedding technique gains more experience with more examples it sees. For instance, an analogy completion example will produce incorrect results if it has not encountered the test words previously. However, this trend is not always true for the semantic subtask, where models trained on smaller corpora may perform better than those trained on larger ones. This could be due to the specific nature of the data in the smaller corpus. For example, Wikipedia has comprehensive articles for most city- and country-based analogies, which may not be as prevalent in a larger corpus like Gigaword."}
