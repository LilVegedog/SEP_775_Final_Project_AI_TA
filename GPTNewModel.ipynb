{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.evaluation import EmbeddingQAFinetuneDataset\n",
    "from llama_index.core.schema import MetadataMode\n",
    "from llama_index.finetuning import generate_qa_embedding_pairs, SentenceTransformersFinetuneEngine\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "import os\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we have to set up open ai key to use open ai service from llama_index\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-\"\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files: 100%|██████████| 1/1 [00:00<00:00,  1.92file/s]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# This part is using SimpleDirectoryReader to process the data into chunks of sentences\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=['materials_formatted.md']\n",
    ").load_data(show_progress=True)\n",
    "\n",
    "# Shuffle the documents\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "#we create chromadb as our database \n",
    "# we do not have to redo the data process and just reload the data from db \n",
    "\n",
    "db = chromadb.PersistentClient(path=\"./trainDataBase\")\n",
    "chroma_collection = db.get_or_create_collection(\"FinlaProjectTrainData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex,StorageContext\n",
    "#then this the index part for data we can get the response and know where the response come from \n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "db_index = VectorStoreIndex.from_documents(documents, storage_context=storage_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageContext(docstore=<llama_index.core.storage.docstore.simple_docstore.SimpleDocumentStore object at 0x29c9c2450>, index_store=<llama_index.core.storage.index_store.simple_index_store.SimpleIndexStore object at 0x29f590090>, vector_stores={'default': ChromaVectorStore(stores_text=True, is_embedding_query=True, flat_metadata=True, collection_name=None, host=None, port=None, ssl=False, headers=None, persist_dir=None, collection_kwargs={}), 'image': <llama_index.core.vector_stores.simple.SimpleVectorStore object at 0x29f7fa190>}, graph_store=<llama_index.core.graph_stores.simple.SimpleGraphStore object at 0x29ca69a90>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_index.storage_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Content</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the main focus of the course mentioned...</td>\n",
       "      <td>The main focus of the course is on the tools p...</td>\n",
       "      <td>Lecture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is one of the most successful application...</td>\n",
       "      <td>Machine translation is one of the earliest and...</td>\n",
       "      <td>Lecture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What fundamental problem does the note identif...</td>\n",
       "      <td>One fundamental problem in building language-l...</td>\n",
       "      <td>Lecture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What does the 'distributional hypothesis' sugg...</td>\n",
       "      <td>The distributional hypothesis suggests that th...</td>\n",
       "      <td>Lecture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>According to the note, what major challenge do...</td>\n",
       "      <td>Most existing tools work for precious few (usu...</td>\n",
       "      <td>Lecture</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  \\\n",
       "0  What is the main focus of the course mentioned...   \n",
       "1  What is one of the most successful application...   \n",
       "2  What fundamental problem does the note identif...   \n",
       "3  What does the 'distributional hypothesis' sugg...   \n",
       "4  According to the note, what major challenge do...   \n",
       "\n",
       "                                             Content     Type  \n",
       "0  The main focus of the course is on the tools p...  Lecture  \n",
       "1  Machine translation is one of the earliest and...  Lecture  \n",
       "2  One fundamental problem in building language-l...  Lecture  \n",
       "3  The distributional hypothesis suggests that th...  Lecture  \n",
       "4  Most existing tools work for precious few (usu...  Lecture  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# the question and answer pair is genrate by GPT 4\n",
    "# since the auto generate service for open ai from llama-index is no longer used. \n",
    "# we store all the question to csv file\n",
    "\n",
    "questions_context_df = pd.read_csv('question_context.csv')\n",
    "questions_context_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = questions_context_df['Question'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_engine = db_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamidreza Mahyar teaches Natural Language Processing (NLP).\n"
     ]
    }
   ],
   "source": [
    "response = db_engine.query(\"who teach NLP?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What is the main focus of the course mentioned in the note?'\n",
      " 'What is one of the most successful applications of Natural Language Processing?'\n",
      " 'What fundamental problem does the note identify in building language-learning machines?'\n",
      " \"What does the 'distributional hypothesis' suggest about word meanings?\"\n",
      " 'According to the note, what major challenge do current NLP tools face?'\n",
      " \"What does the term 'signifier' refer to in the context of the note?\"\n",
      " 'What does the note say about the ability of children to acquire language?'\n",
      " \"What kind of model does the 'word 2 vec' algorithm represent each word as?\"\n",
      " 'What is a major limitation of human-annotated resources for word representation, according to the note?'\n",
      " 'What significant insight about word vectors did the GloVe algorithm introduce?'\n",
      " 'What model is introduced for training word vectors in the notes?'\n",
      " 'What are the two main classes of methods for finding word embeddings mentioned?'\n",
      " 'How does GloVe differ from previous word embedding methods?'\n",
      " 'What is the significance of the co-occurrence matrix in GloVe?'\n",
      " 'What kind of evaluation methods are discussed for word vectors?'\n",
      " 'What example is given for intrinsic evaluation of word vectors?'\n",
      " 'How does intrinsic evaluation benefit the development of word vectors?'\n",
      " 'What are some hyperparameters that might be tuned for word embedding techniques?'\n",
      " 'What does the notes mention about the effect of window size on word vector performance?'\n",
      " 'What role does corpus size play in the performance of word vector models?'\n",
      " 'What does backpropagation enable for neural networks?'\n",
      " 'What is the purpose of using non-linear activation functions in neural networks?'\n",
      " 'What are some of the practical tips discussed for training neural networks?'\n",
      " 'What mathematical tool is crucial for backpropagation?'\n",
      " 'What is the significance of Xavier parameter initialization?'\n",
      " 'How do adaptive optimization methods like AdaGrad, RMSProp, and Adam differ from traditional gradient descent?'\n",
      " 'What is dropout, and how does it help in training neural networks?'\n",
      " 'Why is gradient checking important in training neural networks?'\n",
      " 'What challenge does the maximum margin objective function address in neural networks?'\n",
      " 'Why is data preprocessing like mean subtraction and normalization important for neural network training?'\n",
      " 'What are the two main types of structures used to analyze the syntactic structure of sentences in NLP?'\n",
      " 'What does a dependency structure show in sentences?'\n",
      " 'What is the primary difference between transition-based and graph-based approaches in dependency parsing?'\n",
      " 'What are the key components of the state used in greedy deterministic transition-based parsing?'\n",
      " 'What are the three types of transitions between states in dependency parsing?'\n",
      " 'How does neural dependency parsing differ from previous models?'\n",
      " 'What types of features are generally included in neural dependency parsers?'\n",
      " 'How are the dense vector representations produced in neural dependency parsing?'\n",
      " 'What does the feedforward neural network model contain in neural dependency parsing?'\n",
      " 'What is the purpose of the non-linear function applied in the hidden layer of the neural network model for dependency parsing?'\n",
      " 'What is the main objective of language models in NLP?'\n",
      " 'What is an n-gram language model and how does it work?'\n",
      " 'What are the two main issues with n-gram language models?'\n",
      " 'How do Recurrent Neural Networks (RNNs) differ from conventional translation models?'\n",
      " 'What are the advantages and disadvantages of RNNs?'\n",
      " 'What is the vanishing gradient problem in RNNs and how does it affect training?'\n",
      " 'What are Gated Recurrent Units (GRUs) and why are they used?'\n",
      " 'What is the key difference between GRUs and LSTMs?'\n",
      " 'How do Deep Bidirectional RNNs enhance the capability of standard RNNs?'\n",
      " 'What role does perplexity play in evaluating language models?'\n",
      " 'What is the main advantage of self-attention mechanisms over recurrent neural networks in NLP?'\n",
      " 'What are the key components of the Transformer architecture?'\n",
      " 'Why is layer normalization important in Transformers?'\n",
      " \"How does multi-head self-attention enhance the Transformer's performance?\"\n",
      " 'What role do positional embeddings play in the Transformer model?'\n",
      " \"How do residual connections contribute to the Transformer's architecture?\"\n",
      " \"What is the purpose of future masking in the Transformer's decoder?\"\n",
      " 'How does attention logit scaling improve the Transformer model?'\n",
      " 'What differentiates the Transformer Encoder from the Decoder?'\n",
      " 'Explain the concept of cross-attention in the Transformer Encoder-Decoder architecture.'\n",
      " 'What is instruction finetuning in the context of language models?'\n",
      " 'How does reinforcement learning from human feedback (RLHF) improve language models?'\n",
      " 'What are the limitations of instruction finetuning?'\n",
      " 'Why are reinforcement learning techniques considered tricky to get right in language models?'\n",
      " 'What is the role of a reward model in reinforcement learning from human feedback?'\n",
      " 'What are the potential risks associated with reward hacking in reinforcement learning models?'\n",
      " 'How can language models become multitask assistants?'\n",
      " 'What is chain-of-thought prompting and its significance?'\n",
      " 'What future directions are suggested for improving reinforcement learning from human feedback in language models?'\n",
      " 'Why is modeling human preferences challenging in the context of language models?'\n",
      " 'What distinguishes open-domain question answering from reading comprehension?'\n",
      " 'What is the function of a retriever in the retriever-reader framework for open-domain QA?'\n",
      " 'How does the Stanford question answering dataset (SQuAD) evaluate answers?'\n",
      " 'What led to the surpassing of human performance on SQuAD, and does it imply that reading comprehension is solved?'\n",
      " 'What advancements did SpanBERT introduce to improve upon BERT for QA tasks?'\n",
      " 'How does dense passage retrieval (DPR) enhance open-domain QA?'\n",
      " 'What are the limitations of relying solely on large language models for open-domain QA?'\n",
      " 'In what way does the Fusion-in-decoder (FID) model combine dense retrieval with generative models for QA?'\n",
      " 'Why is reading comprehension considered an essential testbed for evaluating natural language understanding by computer systems?'\n",
      " 'How does the retriever-reader framework approach the challenge of open-domain question answering?'\n",
      " 'What defines multimodality in the context of natural language processing?'\n",
      " 'Why is the study of multimodal models considered crucial?'\n",
      " 'What are some key applications of multimodal models?'\n",
      " 'What challenges do researchers face when working with multimodal data?'\n",
      " 'How does the CLIP model by OpenAI demonstrate robustness in multimodal learning?'\n",
      " 'What was the motivation behind creating the Hateful Memes dataset, and what does it aim to test?'\n",
      " 'How does the FLAVA model approach multimodality, and what datasets does it use for pretraining?'\n",
      " 'What are the main tasks associated with the COCO dataset in multimodal research?'\n",
      " 'Why is the Visual Question Answering (VQA) task considered dominant in vision and language research?'\n",
      " 'What future directions do researchers see for multimodal models and foundation models?'\n",
      " 'What is the deadline for SEP775 Assignment 1?'\n",
      " 'Which dataset is used in SEP775 Assignment 4 for the programming-related QA system?'\n",
      " 'What methodology is explored in SEP775 Assignment 3 to improve QA systems?'\n",
      " 'What is the objective of the text generation task in SEP775 Assignment 2?'\n",
      " 'What are the two datasets mentioned for sentence classification in the SEP775 Final Projects document?'\n",
      " 'Which optimizer is defined in the SEP775 Final Projects coding exercise?'\n",
      " 'What is the purpose of the BertLayer class in the SEP775 Final Projects coding exercise?'\n",
      " 'What architectural enhancement is suggested for the RNN model in SEP775 Assignment 2?'\n",
      " 'In SEP775 Assignment 3, which datasets are candidates for the domain-specific adaptation step?'\n",
      " 'For SEP775 Assignment 4, where can the LoRA code repository be found?'\n",
      " 'What does the Transformer model architecture replace in traditional sequence transduction models?'\n",
      " 'What are the main benefits of the Transformer model mentioned in the paper?'\n",
      " 'How does the Transformer model perform on the WMT 2014 English-to-German translation task?'\n",
      " \"What are the key components of the Transformer's encoder and decoder layers?\"\n",
      " 'What is the role of positional encodings in the Transformer model?'\n",
      " 'What does BERT stand for?'\n",
      " 'What are the two steps in the BERT framework?'\n",
      " 'What makes BERT different from previous language representation models?'\n",
      " 'How many natural language processing tasks did BERT advance the state-of-the-art results for, as mentioned in the paper?'\n",
      " \"What is the 'masked language model' (MLM) pre-training objective in BERT?\"\n",
      " 'What is the primary challenge in machine comprehension (MC) as discussed in the paper?'\n",
      " 'What unique approach does the Bi-Directional Attention Flow (BIDAF) network introduce?'\n",
      " 'On which datasets did the BIDAF model achieve state-of-the-art results?'\n",
      " 'What are the key layers of the BIDAF model?'\n",
      " 'How does the Attention Flow Layer in BIDAF work?'\n",
      " 'What does BLEU stand for?'\n",
      " 'What are the two key ingredients required for the BLEU MT evaluation system?'\n",
      " 'What is the primary programming task for a BLEU implementor?'\n",
      " 'How does BLEU calculate its final score?'\n",
      " \"What is the modified n-gram precision's role in BLEU's evaluation?\"\n",
      " 'What is the main improvement chain-of-thought prompting brings to large language models?'\n",
      " 'How does chain-of-thought prompting affect performance on the GSM8K benchmark?'\n",
      " 'What are the two key limitations of rationale-augmented training and traditional few-shot prompting methods addressed by chain-of-thought prompting?'\n",
      " 'What datasets were used to evaluate the chain-of-thought prompting method?'\n",
      " 'What unique feature does chain-of-thought prompting add to the exemplars used in few-shot prompting?'\n",
      " 'What is the main purpose of the introduction to contextual word representations?'\n",
      " 'What are the two ways words are discussed in this document?'\n",
      " 'What method introduced by ELMo improved word vector representations?'\n",
      " 'What challenge does representing word types independent of context present?'\n",
      " 'What are contextual word vectors, and why are they significant?'\n",
      " 'What is the main advantage of using dense representations for passage retrieval in open-domain QA, as opposed to traditional sparse vector space models?'\n",
      " 'What is the Dense Passage Retriever (DPR) optimized for?'\n",
      " 'How does DPR outperform traditional retrieval methods?'\n",
      " 'What are the key components of the training data used for DPR?'\n",
      " 'How does DPR achieve efficient retrieval at run-time?'\n",
      " 'Who are the authors of the paper on Distributed Representations of Words and Phrases and their Compositionality?'\n",
      " 'What model does the paper introduce for learning high-quality vector representations of words?'\n",
      " 'What does the paper claim about the vectors learned through the Skip-gram model?'\n",
      " 'How does subsampling of frequent words improve the Skip-gram model according to the paper?'\n",
      " 'What is an example of vector addition producing meaningful results as mentioned in the paper?'\n",
      " 'Who are the authors of the paper on Efficient Estimation of Word Representations in Vector Space?'\n",
      " 'What is the main goal of the paper?'\n",
      " 'What novel model architectures does the paper propose for computing continuous vector representations of words?'\n",
      " 'How does the paper measure the quality of the word vector representations?'\n",
      " 'What significant improvements do the proposed models offer?'\n",
      " 'Who are the authors of the paper on Evaluation methods for unsupervised word embeddings?'\n",
      " 'What is the main contribution of the paper regarding evaluation methods for unsupervised word embeddings?'\n",
      " 'What are the two major categories of evaluation schemes discussed in the paper?'\n",
      " 'What novel approach does the paper introduce for evaluating word embeddings?'\n",
      " 'What significant findings about word embeddings are reported in the paper?'\n",
      " \"Who are the authors of the paper 'Natural Language Processing (Almost) from Scratch'?\"\n",
      " \"What is the main contribution of the 'Natural Language Processing (Almost) from Scratch' paper?\"\n",
      " 'Which NLP tasks does the paper focus on for benchmarking the proposed architecture?'\n",
      " 'How does the proposed architecture in the paper deal with different levels of granularity in the text?'\n",
      " 'What datasets were used to evaluate the performance of the proposed system in the paper?'\n",
      " \"Who are the authors of 'Speech and Language Processing'?\"\n",
      " 'What is the goal of predicting the next few words someone is going to say, as discussed in the paper?'\n",
      " 'How is the probability of a word given some history calculated in n-gram models?'\n",
      " 'What are the two major categories of evaluation schemes for language models as discussed in the paper?'\n",
      " 'What does the paper mention as a simple example to explain the concept of perplexity in language models?'\n",
      " \"Who are the authors of 'On the difficulty of training Recurrent Neural Networks'?\"\n",
      " 'What are the two widely known issues with training Recurrent Neural Networks (RNNs)?'\n",
      " 'What strategies does the paper propose to address these training issues?'\n",
      " 'What is the purpose of the gradient norm clipping strategy proposed in the paper?'\n",
      " 'How does the paper suggest addressing the vanishing gradients problem?'\n",
      " \"Who are the authors of 'Reading Wikipedia to Answer Open-Domain Questions'?\"\n",
      " 'What does the paper propose for answering open-domain questions?'\n",
      " 'What are the two components of the proposed system in the paper?'\n",
      " 'What datasets were used to evaluate the proposed system for question answering from Wikipedia?'\n",
      " 'How does the proposed Document Retriever perform compared to the built-in Wikipedia search engine?'\n",
      " \"Who are the authors of 'Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks'?\"\n",
      " 'What is the main advantage of retrieval-augmented generation (RAG) models for NLP tasks?'\n",
      " 'How do RAG models incorporate external knowledge into the generation process?'\n",
      " 'What are the two formulations of RAG models described in the paper?'\n",
      " 'How does the paper demonstrate the effectiveness of RAG models?'\n",
      " \"Who are the authors of 'SQuAD: 100,000+ Questions for Machine Comprehension of Text'?\"\n",
      " 'What is the main contribution of the SQuAD dataset to the field of machine learning?'\n",
      " 'What unique challenge does SQuAD provide compared to previous datasets for machine comprehension?'\n",
      " 'What are the key components of the logistic regression model built to assess the difficulty of SQuAD?'\n",
      " \"How does human performance compare to the logistic regression model's performance on the SQuAD dataset?\"\n",
      " \"Who wrote the 'Understanding LSTM Networks' article?\"\n",
      " 'What is the primary purpose of Recurrent Neural Networks (RNNs)?'\n",
      " 'What major issue do Long Short Term Memory (LSTM) networks address in RNNs?'\n",
      " 'What are the key structural differences between standard RNNs and LSTMs?'\n",
      " 'How do LSTMs decide what information to store or forget?'\n",
      " 'What is the primary objective of instruction tuning in language models?'\n",
      " 'How does FLAN compare with GPT-3 in terms of zero-shot learning performance?'\n",
      " 'What key factors contribute to the success of instruction tuning according to the ablation studies?'\n",
      " 'What is the instruction tuning strategy employed for FLAN?'\n",
      " \"How does FLAN's few-shot performance compare with its zero-shot performance?\"\n",
      " 'What is the primary goal of the Image Transformer model?'\n",
      " 'How does the Image Transformer model differ in its approach from traditional convolutional neural networks?'\n",
      " 'What were the main results achieved by the Image Transformer on the ImageNet dataset?'\n",
      " 'What method does the Image Transformer use for conditional image generation?'\n",
      " 'What improvements does the Image Transformer introduce for image super-resolution tasks?'\n",
      " 'What are the main objectives of the GloVe model?'\n",
      " 'How does the GloVe model represent word meanings?'\n",
      " 'What is the significance of the word analogy task in evaluating word vector models according to the GloVe model paper?'\n",
      " 'How does the GloVe model approach the problem of capturing both global statistics and meaningful linear substructures in word vectors?'\n",
      " \"What are the main findings of the experiments conducted to evaluate the GloVe model's performance?\"\n",
      " 'What do recent trends suggest about the performance of neural-network-inspired word embedding models compared to traditional count-based distributional models?'\n",
      " 'What is the skip-gram with negative-sampling training method popularized by?'\n",
      " 'How can the performance of traditional count-based distributional models be improved according to the document?'\n",
      " 'What is the significance of context distribution smoothing in word embeddings according to the document?'\n",
      " 'According to the document, what result challenges the claim that embeddings are superior to count-based methods?'\n",
      " 'What is the main goal of learning dense representations of phrases according to the document?'\n",
      " 'How does the document propose to overcome the limitations of current phrase retrieval models?'\n",
      " 'What significant improvements do dense phrase representations provide in open-domain QA according to the experiments?'\n",
      " 'What is the role of the Inverse Cloze Task (ICT) in learning phrase representations?'\n",
      " 'How does the document envision using DensePhrases beyond open-domain QA tasks?'\n",
      " 'What is the main advantage of scaling up language models for few-shot learning, as demonstrated by GPT-3?'\n",
      " 'How does GPT-3 perform tasks without any gradient updates or fine-tuning?'\n",
      " 'What method did GPT-3 use to improve its translation performance in few-shot settings?'\n",
      " 'How does GPT-3 perform on the task of closed-book question answering?'\n",
      " 'What challenges do GPT-3 face according to its performance on Winograd-style tasks and common sense reasoning?'\n",
      " 'What main problem does layer normalization aim to solve in neural networks?'\n",
      " 'How does layer normalization differ from batch normalization in its application?'\n",
      " 'What are the key benefits of using layer normalization in recurrent neural networks?'\n",
      " 'What invariant properties does layer normalization have compared to batch and weight normalization?'\n",
      " 'How does layer normalization address the challenge of applying normalization techniques to RNNs?'\n",
      " 'What unique property does the Music Transformer bring to generating music with long-term structure?'\n",
      " 'How does the Music Transformer handle the challenge of relative timing in musical composition?'\n",
      " 'What are the benefits of the memory-efficient implementation of relative position-based attention in the Music Transformer?'\n",
      " 'How does the Music Transformer perform compared to LSTM-based models in generating music?'\n",
      " 'In what ways does the Music Transformer generalize beyond its training data?'\n",
      " 'What is the primary goal of the Open-Retrieval Question Answering system (ORQA)?'\n",
      " 'How does ORQA differ from traditional IR systems in handling evidence retrieval?'\n",
      " 'What is the Inverse Cloze Task (ICT) and how does it relate to ORQA?'\n",
      " 'What challenges does ORQA address in open-domain question answering?'\n",
      " 'How does the performance of ORQA compare to traditional IR systems like BM25 on datasets where questioners genuinely seek answers?']\n"
     ]
    }
   ],
   "source": [
    "print(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lp/swtmbhmn3m1_mxsjr83wqhpm0000gn/T/ipykernel_52917/4283721461.py:9: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  gpt_4_context = ServiceContext.from_defaults(\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.finetuning.callbacks import OpenAIFineTuningHandler\n",
    "from llama_index.core.callbacks import CallbackManager\n",
    "from llama_index.core import ServiceContext\n",
    "\n",
    "# we set up a callback fucntion \n",
    "# this is because we want record all questions and answers when we send query to GPT 4 \n",
    "# then it will generat jsonl file for use to finetune the model \n",
    "finetuning_handler = OpenAIFineTuningHandler()\n",
    "callback_manager = CallbackManager([finetuning_handler])\n",
    "\n",
    "gpt_4_context = ServiceContext.from_defaults(\n",
    "    llm=OpenAI(model=\"gpt-4\", temperature=0.3),\n",
    "    context_window=2048, \n",
    "    callback_manager=callback_manager,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "# we reload the index data from VectorStoreIndex \n",
    "\n",
    "index = VectorStoreIndex.from_vector_store(vector_store, service_context=gpt_4_context)\n",
    "\n",
    "query_engine = index.as_query_engine(similarity_top_k=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we use question as a query and send it to GPT 4\n",
    "# the callback manager will record all data \n",
    "for question in questions:\n",
    "    response = query_engine.query(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 730 examples to finetuning_qa_pairs.jsonl\n"
     ]
    }
   ],
   "source": [
    "# callback manager write the record to the file\n",
    "finetuning_handler.save_finetuning_events(\"finetuning_qa_pairs.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.finetuning import OpenAIFinetuneEngine\n",
    "# this is the pretrained model we use to finetune\n",
    "finetune_engine = OpenAIFinetuneEngine(\n",
    "    \"gpt-3.5-turbo\",\n",
    "    \"finetuning_events.jsonl\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples: 730\n",
      "First example:\n",
      "{'role': 'system', 'content': 'You are an AI Teaching Assistant designed to help students with their educational queries. Your goal is to provide accurate, clear, and helpful responses to questions related to coursework, study materials, and educational concepts. Always provide responses based on the information presented in the query without assuming prior knowledge. Please adhere to the following guidelines: 1. Provide direct answers to questions based on the provided context or known information. 2. Do not reference external sources unless specified in the query. 3. Maintain a supportive and educational tone in all interactions. 4. Avoid speculation and ensure your responses are grounded in factual or well-understood educational principles.'}\n",
      "{'role': 'user', 'content': \"Question: How does Hamidreza Mahyarmahyarh's research in Computational Natural Language Processing contribute to the field?\"}\n",
      "{'role': 'assistant', 'content': \"Hamidreza Mahyarmahyarh's research in Computational Natural Language Processing contributes to the field by focusing on language models, recurrent neural networks, sequence-to-sequence models, and neural machine translation.\"}\n",
      "No errors found\n",
      "Num examples missing system message: 0\n",
      "Num examples missing user message: 0\n",
      "#### Distribution of num_messages_per_example:\n",
      "min / max: 3, 3\n",
      "mean / median: 3.0, 3.0\n",
      "p5 / p95: 3.0, 3.0\n",
      "#### Distribution of num_total_tokens_per_example:\n",
      "min / max: 157, 584\n",
      "mean / median: 251.64520547945204, 251.0\n",
      "p5 / p95: 198.9, 299.1\n",
      "#### Distribution of num_assistant_tokens_per_example:\n",
      "min / max: 3, 242\n",
      "mean / median: 77.84520547945205, 79.0\n",
      "p5 / p95: 30.900000000000006, 119.0\n",
      "0 examples may be over the 4096 token limit, they will be truncated during fine-tuning\n",
      "Dataset has ~183701 tokens that will be charged for during training\n",
      "By default, you'll train for 3 epochs on this dataset\n",
      "By default, you'll be charged for ~551103 tokens\n",
      "As of August 22, 2023, fine-tuning gpt-3.5-turbo is $0.008 / 1K Tokens.\n",
      "This means your total cost for training will be $1.469608 per epoch.\n"
     ]
    }
   ],
   "source": [
    "# the fine tune process\n",
    "finetune_engine.finetune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FineTuningJob(id='ftjob-A6jPhmnPgVlYK7K1bKTdxLTx', created_at=1711508321, error=Error(code=None, message=None, param=None, error=None), fine_tuned_model='ft:gpt-3.5-turbo-0125:personal::97FTe9Ci', finished_at=1711514817, hyperparameters=Hyperparameters(n_epochs=3, batch_size=1, learning_rate_multiplier=2), model='gpt-3.5-turbo-0125', object='fine_tuning.job', organization_id='org-BwtNVnRGHyHMfKEqmdxuL8eJ', result_files=['file-fhztKMzB8jYJBPo570a2Kho3'], status='succeeded', trained_tokens=546723, training_file='file-RhnAlFF9kqkrkZJGyYXXAwZ7', validation_file=None, user_provided_suffix=None)\n"
     ]
    }
   ],
   "source": [
    "finetune_engine.get_current_job()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model_name = \"ft:gpt-3.5-turbo-0125:personal::97FTe9Ci\"\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "ft_llm = OpenAI(model=ft_model_name, temperature=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_vector_store(vector_store)\n",
    "\n",
    "query_engine = index.as_query_engine(similarity_top_k=3, llm=ft_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The professor's name for SEP 775 is Hamidreza Mahyar.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What is the professor's name for SEP 775?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
