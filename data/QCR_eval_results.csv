,question,answer,contexts,embedding_model,chunk_size,chunk_overlap,sim_top_k,sys_prompt,Answer Relevancy,Faithfulness,Answer Semantics Similarity,Context Relevancy
0,What is the instructor's name for this course?,"The instructor's name for the course ""Introduction to Computational Natural Language Processing"" is Hamidreza Mahyar. 

Source: NLP Winter 2024 Course Outline","['file name: NLP Winter 2024 Course Outline. Course Outline 1. COURSE INFORMATION Session Offered Winter 2024 Course Name Introduction to Computational Natural Language Processing Course Code SEP 775 Date s and Time s of lectures Lectures In person Wednesday 3 30 pm 6 30 pm Program Name System and Technology Calendar Description This graduate course introduces fundamental concepts in computational natural language processing NLP and their applications to teaching students how to deal with textual data in Artificial Intelligence. This course demonstrates how machines can learn different tasks in natural language such as language modeling text generation machine translation and language understanding. In this regard we go over the most promising methods in this literature and the most recent state of the art techniques. Moreover this course explores different real world applications of NLP and helps students get hands on experience in this field. Instructor Hamidreza Mahyar E Mail mahyarh@mcmaster.ca Office Hours upon request 2. COURSE SPECIFICS Course Description This course covers some fundamental concepts in natural language processing NLP and how they apply in real world use cases. Instruction Type Code Type Hours per term C Classroom instruction 39 L Laboratory workshop or fieldwork T Tutorial DE Distance education Total Hours 39 Resources ISBN Textbook Title Edition Author Publisher Natural Language Processing Jacob Eisenstein Speech and Language Processing Dan Jurafsky and James H. Martin Deep Learning Ian Goodfellow Yoshua Bengio and Aaron Courville Prerequisite s Proficiency in Python programming language Some Machine Learning and or Deep Learning knowledge is a plus Corequisite s Antirequisite s Course Specific Policies All work must be shown to get full credit. Specific policy of Projects Assignments submissionsProjects Assignments will be conducted online by using related software. Completed Projects Assignments should be uploaded to the drop box before midnight of the due date. Projects Assignments missed due to legitimate reasons must be completed later mutually agreed with the instructor. Departmental Policies The use of cell phones iPods laptops and other personal electronic devices are prohibited from the classroom during the class time unless the instructor makes an explicit exception. Announcements made in class or placed on Avenue are considered to have been communicated to all students including those individuals that are not in class. Instructor has the right to submit work to software to identify plagiarism.', 'Instructor has the right to submit work to software to identify plagiarism. 3. SUB TOPIC S Week 1 Overview Human language Word meanings Word2Vec and Optimization basics Week 2 Intro to neural networks Gradients and backpropagation Week 3 Recurrent Neural Networks and Language Models Week 4 Conditioned Generation Week 5 Part 1 Self attention and transformers Part 2 Seq2Seq models Week 6 Midterm Test Week 7 Midterm Recess No class Week 8 Prompting Instruction Finetuning and RLHF Week 9 Question Answering systems Week 10 Multimodal models Week 11 Project presentation and Q A Week 12 Project presentation and Q A Week 13 Project presentation and Q A Classes end April 10th 4. ASSESSMENT OF LEARNING Weight 4 Assignments 20 4 Quizzes 10 Midterm Test 35 Group Project 35 TOTAL 100Note that the percentage distribution of each module may be adjusted. Percentage grades will be converted to letter grades and grade points per the University calendar. 5. LEARNING OUTCOMES By the end of this course the students will know about Introduction to Deep Learning Word representations Language models Pre trained language models Text generation Seq2Seq models Prompting Feedback systems Multimodal models NLP applications e.g. Q A systems 6. COURSE OUTLINE APPROVED ADVISORY STATEMENTS ANTI DISCRIMINATION The Faculty of Engineering is concerned with ensuring an environment that is free of all discrimination. If there is a problem individuals are reminded that they should contact the Associate Director Graduate Studies the Sexual Harassment Officer or the Human Rights Consultant as soon as possible. http www.mcmaster.ca policy General HR Discrimination_Harassment_Sexual_Harassment Prevention Response.pdf ACADEMIC INTEGRITY You are expected to exhibit honesty and use ethical behaviour in all aspects of the learning process. Academic credentials you earn are rooted in principles of honesty and academic integrity. It is your responsibility to understand what constitutes academic dishonesty. Academic dishonesty is to knowingly act or fail to act in a way that results or could result in unearned academic credit or advantage. This behaviour can result in serious consequences e.g.']",text-embedding-3-small,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9161228991125252,1.0,0.9057902290172654,0.0869565217391304
1,What is the email address of the instructor for this course?,"The email address of the instructor for the course ""Introduction to Computational Natural Language Processing"" (SEP 775) is mahyarh@mcmaster.ca.

Source: NLP Winter 2024 Course Outline.","['file name: NLP Winter 2024 Course Outline. Course Outline 1. COURSE INFORMATION Session Offered Winter 2024 Course Name Introduction to Computational Natural Language Processing Course Code SEP 775 Date s and Time s of lectures Lectures In person Wednesday 3 30 pm 6 30 pm Program Name System and Technology Calendar Description This graduate course introduces fundamental concepts in computational natural language processing NLP and their applications to teaching students how to deal with textual data in Artificial Intelligence. This course demonstrates how machines can learn different tasks in natural language such as language modeling text generation machine translation and language understanding. In this regard we go over the most promising methods in this literature and the most recent state of the art techniques. Moreover this course explores different real world applications of NLP and helps students get hands on experience in this field. Instructor Hamidreza Mahyar E Mail mahyarh@mcmaster.ca Office Hours upon request 2. COURSE SPECIFICS Course Description This course covers some fundamental concepts in natural language processing NLP and how they apply in real world use cases. Instruction Type Code Type Hours per term C Classroom instruction 39 L Laboratory workshop or fieldwork T Tutorial DE Distance education Total Hours 39 Resources ISBN Textbook Title Edition Author Publisher Natural Language Processing Jacob Eisenstein Speech and Language Processing Dan Jurafsky and James H. Martin Deep Learning Ian Goodfellow Yoshua Bengio and Aaron Courville Prerequisite s Proficiency in Python programming language Some Machine Learning and or Deep Learning knowledge is a plus Corequisite s Antirequisite s Course Specific Policies All work must be shown to get full credit. Specific policy of Projects Assignments submissionsProjects Assignments will be conducted online by using related software. Completed Projects Assignments should be uploaded to the drop box before midnight of the due date. Projects Assignments missed due to legitimate reasons must be completed later mutually agreed with the instructor. Departmental Policies The use of cell phones iPods laptops and other personal electronic devices are prohibited from the classroom during the class time unless the instructor makes an explicit exception. Announcements made in class or placed on Avenue are considered to have been communicated to all students including those individuals that are not in class. Instructor has the right to submit work to software to identify plagiarism.', 'If you have any questions or concerns about such disclosure please discuss this with the course instructor. ONLINE PROCTORING Some courses may use online proctoring software for tests and exams. This software may require students to turn on their video camera present identification monitor and record their computer activities and or lock restrict their browser or other applications software during tests or exams. This software may be required to be installed before the test exam begins. COMMUNICATIONS It is the student s responsibility to Maintain current contact information with the University including address phone numbers and emergency contact information. Use the University provided e mail address or maintain a valid forwarding e mail address. Regularly check the official University communications channels. Official University communications are considered received if sent by postal mail by fax or by e mail to the student s designated primary e mail account via their @mcmaster.ca alias. Accept that forwarded e mails may be lost and that e mail is considered received if sent via the student s @mcmaster.ca alias. Check the McMaster Avenue email and course websites on a regular basis during the term. CONDUCT EXPECTATIONS As a McMaster student you have the right to experience and the responsibility to demonstrate respectful and dignified interactions within all of our living learning and working communities. These expectations are described in the Code of Student Rights Responsibilities the Code . All students share the responsibility of maintaining a positive environment for the academic and personal growth of all McMaster community members whether in person or online. It is essential that students be mindful of their interactions online as the Code remains in effect in virtual learning environments. The Code applies to any interactions that adversely affect disrupt or interfere with reasonable participation in University activities. Student disruptions or behaviours that interfere with university functions on online platforms e.g. use of Avenue 2 Learn WebEx or Zoom for delivery will be taken very seriously and will be investigated. Outcomes may include restriction or removal of the involved students access to these platforms. ACADEMIC ACCOMMODATION OF STUDENTS WITH DISABILITIES Students with disabilities who require academic accommodation must contact Student Accessibility Services SAS at 905 525 9140 ext. 28652 or sas@mcmaster.ca to make arrangements with a Program Coordinator.']",text-embedding-3-small,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.8907449601778002,1.0,0.8973413321539809,0.0
2,When is this course held every week?,"The course ""Introduction to Computational Natural Language Processing"" (Course Code: SEP 775) is held in person on Wednesdays from 3:30 pm to 6:30 pm every week during the Winter 2024 session.","['file name: NLP Winter 2024 Course Outline. Course Outline 1. COURSE INFORMATION Session Offered Winter 2024 Course Name Introduction to Computational Natural Language Processing Course Code SEP 775 Date s and Time s of lectures Lectures In person Wednesday 3 30 pm 6 30 pm Program Name System and Technology Calendar Description This graduate course introduces fundamental concepts in computational natural language processing NLP and their applications to teaching students how to deal with textual data in Artificial Intelligence. This course demonstrates how machines can learn different tasks in natural language such as language modeling text generation machine translation and language understanding. In this regard we go over the most promising methods in this literature and the most recent state of the art techniques. Moreover this course explores different real world applications of NLP and helps students get hands on experience in this field. Instructor Hamidreza Mahyar E Mail mahyarh@mcmaster.ca Office Hours upon request 2. COURSE SPECIFICS Course Description This course covers some fundamental concepts in natural language processing NLP and how they apply in real world use cases. Instruction Type Code Type Hours per term C Classroom instruction 39 L Laboratory workshop or fieldwork T Tutorial DE Distance education Total Hours 39 Resources ISBN Textbook Title Edition Author Publisher Natural Language Processing Jacob Eisenstein Speech and Language Processing Dan Jurafsky and James H. Martin Deep Learning Ian Goodfellow Yoshua Bengio and Aaron Courville Prerequisite s Proficiency in Python programming language Some Machine Learning and or Deep Learning knowledge is a plus Corequisite s Antirequisite s Course Specific Policies All work must be shown to get full credit. Specific policy of Projects Assignments submissionsProjects Assignments will be conducted online by using related software. Completed Projects Assignments should be uploaded to the drop box before midnight of the due date. Projects Assignments missed due to legitimate reasons must be completed later mutually agreed with the instructor. Departmental Policies The use of cell phones iPods laptops and other personal electronic devices are prohibited from the classroom during the class time unless the instructor makes an explicit exception. Announcements made in class or placed on Avenue are considered to have been communicated to all students including those individuals that are not in class. Instructor has the right to submit work to software to identify plagiarism.', 'Instructor has the right to submit work to software to identify plagiarism. 3. SUB TOPIC S Week 1 Overview Human language Word meanings Word2Vec and Optimization basics Week 2 Intro to neural networks Gradients and backpropagation Week 3 Recurrent Neural Networks and Language Models Week 4 Conditioned Generation Week 5 Part 1 Self attention and transformers Part 2 Seq2Seq models Week 6 Midterm Test Week 7 Midterm Recess No class Week 8 Prompting Instruction Finetuning and RLHF Week 9 Question Answering systems Week 10 Multimodal models Week 11 Project presentation and Q A Week 12 Project presentation and Q A Week 13 Project presentation and Q A Classes end April 10th 4. ASSESSMENT OF LEARNING Weight 4 Assignments 20 4 Quizzes 10 Midterm Test 35 Group Project 35 TOTAL 100Note that the percentage distribution of each module may be adjusted. Percentage grades will be converted to letter grades and grade points per the University calendar. 5. LEARNING OUTCOMES By the end of this course the students will know about Introduction to Deep Learning Word representations Language models Pre trained language models Text generation Seq2Seq models Prompting Feedback systems Multimodal models NLP applications e.g. Q A systems 6. COURSE OUTLINE APPROVED ADVISORY STATEMENTS ANTI DISCRIMINATION The Faculty of Engineering is concerned with ensuring an environment that is free of all discrimination. If there is a problem individuals are reminded that they should contact the Associate Director Graduate Studies the Sexual Harassment Officer or the Human Rights Consultant as soon as possible. http www.mcmaster.ca policy General HR Discrimination_Harassment_Sexual_Harassment Prevention Response.pdf ACADEMIC INTEGRITY You are expected to exhibit honesty and use ethical behaviour in all aspects of the learning process. Academic credentials you earn are rooted in principles of honesty and academic integrity. It is your responsibility to understand what constitutes academic dishonesty. Academic dishonesty is to knowingly act or fail to act in a way that results or could result in unearned academic credit or advantage. This behaviour can result in serious consequences e.g.']",text-embedding-3-small,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.8454684632075044,1.0,0.8535941478108241,0.0869565217391304
3,What is the name of the TA for this course?,"The name of the instructor for the course is Hamidreza Mahyar. Unfortunately, the name of the TA is not provided in the context information. If you need more specific details, you may want to reach out to the instructor directly or refer to any official course documents.","['file name: NLP Winter 2024 Course Outline. Course Outline 1. COURSE INFORMATION Session Offered Winter 2024 Course Name Introduction to Computational Natural Language Processing Course Code SEP 775 Date s and Time s of lectures Lectures In person Wednesday 3 30 pm 6 30 pm Program Name System and Technology Calendar Description This graduate course introduces fundamental concepts in computational natural language processing NLP and their applications to teaching students how to deal with textual data in Artificial Intelligence. This course demonstrates how machines can learn different tasks in natural language such as language modeling text generation machine translation and language understanding. In this regard we go over the most promising methods in this literature and the most recent state of the art techniques. Moreover this course explores different real world applications of NLP and helps students get hands on experience in this field. Instructor Hamidreza Mahyar E Mail mahyarh@mcmaster.ca Office Hours upon request 2. COURSE SPECIFICS Course Description This course covers some fundamental concepts in natural language processing NLP and how they apply in real world use cases. Instruction Type Code Type Hours per term C Classroom instruction 39 L Laboratory workshop or fieldwork T Tutorial DE Distance education Total Hours 39 Resources ISBN Textbook Title Edition Author Publisher Natural Language Processing Jacob Eisenstein Speech and Language Processing Dan Jurafsky and James H. Martin Deep Learning Ian Goodfellow Yoshua Bengio and Aaron Courville Prerequisite s Proficiency in Python programming language Some Machine Learning and or Deep Learning knowledge is a plus Corequisite s Antirequisite s Course Specific Policies All work must be shown to get full credit. Specific policy of Projects Assignments submissionsProjects Assignments will be conducted online by using related software. Completed Projects Assignments should be uploaded to the drop box before midnight of the due date. Projects Assignments missed due to legitimate reasons must be completed later mutually agreed with the instructor. Departmental Policies The use of cell phones iPods laptops and other personal electronic devices are prohibited from the classroom during the class time unless the instructor makes an explicit exception. Announcements made in class or placed on Avenue are considered to have been communicated to all students including those individuals that are not in class. Instructor has the right to submit work to software to identify plagiarism.', 'Instructor has the right to submit work to software to identify plagiarism. 3. SUB TOPIC S Week 1 Overview Human language Word meanings Word2Vec and Optimization basics Week 2 Intro to neural networks Gradients and backpropagation Week 3 Recurrent Neural Networks and Language Models Week 4 Conditioned Generation Week 5 Part 1 Self attention and transformers Part 2 Seq2Seq models Week 6 Midterm Test Week 7 Midterm Recess No class Week 8 Prompting Instruction Finetuning and RLHF Week 9 Question Answering systems Week 10 Multimodal models Week 11 Project presentation and Q A Week 12 Project presentation and Q A Week 13 Project presentation and Q A Classes end April 10th 4. ASSESSMENT OF LEARNING Weight 4 Assignments 20 4 Quizzes 10 Midterm Test 35 Group Project 35 TOTAL 100Note that the percentage distribution of each module may be adjusted. Percentage grades will be converted to letter grades and grade points per the University calendar. 5. LEARNING OUTCOMES By the end of this course the students will know about Introduction to Deep Learning Word representations Language models Pre trained language models Text generation Seq2Seq models Prompting Feedback systems Multimodal models NLP applications e.g. Q A systems 6. COURSE OUTLINE APPROVED ADVISORY STATEMENTS ANTI DISCRIMINATION The Faculty of Engineering is concerned with ensuring an environment that is free of all discrimination. If there is a problem individuals are reminded that they should contact the Associate Director Graduate Studies the Sexual Harassment Officer or the Human Rights Consultant as soon as possible. http www.mcmaster.ca policy General HR Discrimination_Harassment_Sexual_Harassment Prevention Response.pdf ACADEMIC INTEGRITY You are expected to exhibit honesty and use ethical behaviour in all aspects of the learning process. Academic credentials you earn are rooted in principles of honesty and academic integrity. It is your responsibility to understand what constitutes academic dishonesty. Academic dishonesty is to knowingly act or fail to act in a way that results or could result in unearned academic credit or advantage. This behaviour can result in serious consequences e.g.']",text-embedding-3-small,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9175611607980152,0.6666666666666666,0.8959027819140706,0.0
4,What is Word2vec?,"Word2vec is a framework for learning word vectors where every word in a fixed vocabulary is represented by a vector. It involves going through a large corpus of text, considering a center word and context outside words, and using the similarity of word vectors to calculate the probability of outside words given the center word or vice versa. The objective is to adjust word vectors to maximize this probability. Essentially, Word2vec is a method for creating word embeddings by capturing semantic relationships between words based on their usage in context.

Source: Lecture01_wordvecs1.txt","['Word2vec Overview 21 Word2vec Mikolov et al. 2013 is a framework for learning word vectors Idea We have a large corpus body of text a long list of words Every word in a fixed vocabulary is represented by a vector Go through each position t in the text which has a center word c and context outside words o Use the similarity of the word vectors for c and o to calculate the probability of o given c or vice versa Keep adjusting the word vectors to maximize this probabilityWord2Vec Overview Example windows and process for computing ùëÉùë§ùë° ùëó ùë§ùë° crises banking into turning problems as center word at position t outside context words in window of size 2 outside context words in window of size 2 ùëÉùë§ùë° 1 ùë§ùë° 22 ùëÉùë§ùë° 2 ùë§ùë° ùëÉùë§ùë° 1 ùë§ùë° ùëÉùë§ùë° 2 ùë§ùë°Word2Vec Overview Example windows and process for computing ùëÉùë§ùë° ùëó ùë§ùë° crises banking into turning problems as center word at position t outside context words in window of size 2 outside context words in window of size 2 ùëÉùë§ùë° 2 ùë§ùë° ùëÉùë§ùë° 1 ùë§ùë° 23 ùëÉùë§ùë° 2 ùë§ùë° ùëÉùë§ùë° 1 ùë§ùë°Word2vec objective function For each position ùë° 1 ùëá predict context words within a window of fixed size m given center word ùë§ùë°. Data likelihood The objective function ùêΩùúÉis the average negative log likelihood Minimizing objective function Maximizing predictive accuracy sometimes called a cost or loss function 24Word2vec objective function We want to minimize the objective function Question How to calculate ùëÉùë§ùë° ùëó ùë§ùë° ùúÉ Answer We will use two vectors per word w', '3.3 Estimating a word2vec model from a corpus How do we train word2vec in practice Specifying the word2vec model is relatively transparent from the math we ve given above one constructs matrices U and V and can write out the math of the probability. However it may not yet be evident how to estimate the parameters 1 how to calculate the expectation in Eqn 5 for a given value of U and V and then 2 how to perform the minimization operation. Let s start with 1. Word2vec empirical loss. Let D be a set of documents d where each document is a sequence of words w d 1 . . . w d m with all w V. Let k N be a positive integer window size. Let s define how our center word random variable C and outside word r.v. O relate to this concrete dataset. O takes on the value of each word wi in each document and for each such wi the outside words are wi k . . . wi 1 wi 1 . . . wi k . So our Eqn 5 objective becomes L U V d D m i 1 k j 1 log pU V w d i j w d i 6 where you ll note we re taking the sum over 1 all documents of the sum over 2 all words in the document of the sum over 3 all words occuring in the window of the likelihood of the outside word given the center word. Now how do we do the minimization Gradient based estimation At a high level we try to find good U and V for the objective we ve specified by starting with a relatively uninformed guess and iteratively moving in the direction that locally best improves the guess. This is done by gradient based methods a full description of which is out of scope for this note.']",text-embedding-3-small,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999999999999994,1.0,0.958160541729254,0.0833333333333333
5,What are the main learning goals of the course?,"The main learning goals of the course include Introduction to Deep Learning, Word representations, Language models, Pre-trained language models, Text generation, Seq2Seq models, Prompting Feedback systems, Multimodal models, and NLP applications such as Q&A systems. Students will also learn about human language, word meanings, Word2Vec, optimization basics, neural networks, gradients, backpropagation, recurrent neural networks, conditioned generation, self-attention, transformers, and question-answering systems. Additionally, the course will cover topics like midterm tests, group projects, and various assessments to ensure understanding and application of the concepts taught.","['Instructor has the right to submit work to software to identify plagiarism. 3. SUB TOPIC S Week 1 Overview Human language Word meanings Word2Vec and Optimization basics Week 2 Intro to neural networks Gradients and backpropagation Week 3 Recurrent Neural Networks and Language Models Week 4 Conditioned Generation Week 5 Part 1 Self attention and transformers Part 2 Seq2Seq models Week 6 Midterm Test Week 7 Midterm Recess No class Week 8 Prompting Instruction Finetuning and RLHF Week 9 Question Answering systems Week 10 Multimodal models Week 11 Project presentation and Q A Week 12 Project presentation and Q A Week 13 Project presentation and Q A Classes end April 10th 4. ASSESSMENT OF LEARNING Weight 4 Assignments 20 4 Quizzes 10 Midterm Test 35 Group Project 35 TOTAL 100Note that the percentage distribution of each module may be adjusted. Percentage grades will be converted to letter grades and grade points per the University calendar. 5. LEARNING OUTCOMES By the end of this course the students will know about Introduction to Deep Learning Word representations Language models Pre trained language models Text generation Seq2Seq models Prompting Feedback systems Multimodal models NLP applications e.g. Q A systems 6. COURSE OUTLINE APPROVED ADVISORY STATEMENTS ANTI DISCRIMINATION The Faculty of Engineering is concerned with ensuring an environment that is free of all discrimination. If there is a problem individuals are reminded that they should contact the Associate Director Graduate Studies the Sexual Harassment Officer or the Human Rights Consultant as soon as possible. http www.mcmaster.ca policy General HR Discrimination_Harassment_Sexual_Harassment Prevention Response.pdf ACADEMIC INTEGRITY You are expected to exhibit honesty and use ethical behaviour in all aspects of the learning process. Academic credentials you earn are rooted in principles of honesty and academic integrity. It is your responsibility to understand what constitutes academic dishonesty. Academic dishonesty is to knowingly act or fail to act in a way that results or could result in unearned academic credit or advantage. This behaviour can result in serious consequences e.g.', 'file name: NLP Winter 2024 Course Outline. Course Outline 1. COURSE INFORMATION Session Offered Winter 2024 Course Name Introduction to Computational Natural Language Processing Course Code SEP 775 Date s and Time s of lectures Lectures In person Wednesday 3 30 pm 6 30 pm Program Name System and Technology Calendar Description This graduate course introduces fundamental concepts in computational natural language processing NLP and their applications to teaching students how to deal with textual data in Artificial Intelligence. This course demonstrates how machines can learn different tasks in natural language such as language modeling text generation machine translation and language understanding. In this regard we go over the most promising methods in this literature and the most recent state of the art techniques. Moreover this course explores different real world applications of NLP and helps students get hands on experience in this field. Instructor Hamidreza Mahyar E Mail mahyarh@mcmaster.ca Office Hours upon request 2. COURSE SPECIFICS Course Description This course covers some fundamental concepts in natural language processing NLP and how they apply in real world use cases. Instruction Type Code Type Hours per term C Classroom instruction 39 L Laboratory workshop or fieldwork T Tutorial DE Distance education Total Hours 39 Resources ISBN Textbook Title Edition Author Publisher Natural Language Processing Jacob Eisenstein Speech and Language Processing Dan Jurafsky and James H. Martin Deep Learning Ian Goodfellow Yoshua Bengio and Aaron Courville Prerequisite s Proficiency in Python programming language Some Machine Learning and or Deep Learning knowledge is a plus Corequisite s Antirequisite s Course Specific Policies All work must be shown to get full credit. Specific policy of Projects Assignments submissionsProjects Assignments will be conducted online by using related software. Completed Projects Assignments should be uploaded to the drop box before midnight of the due date. Projects Assignments missed due to legitimate reasons must be completed later mutually agreed with the instructor. Departmental Policies The use of cell phones iPods laptops and other personal electronic devices are prohibited from the classroom during the class time unless the instructor makes an explicit exception. Announcements made in class or placed on Avenue are considered to have been communicated to all students including those individuals that are not in class. Instructor has the right to submit work to software to identify plagiarism.']",text-embedding-3-small,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999999999999996,1.0,0.9277807187366056,0.1304347826086956
6,How is the course grade distributed?,"The course grade is distributed as follows:
- 4 Assignments: 20%
- 4 Quizzes: 10%
- Midterm Test: 35%
- Group Project: 35%
Total: 100%
(Source: NLP_Winter_2024_Course_Outline.txt)","['Instructor has the right to submit work to software to identify plagiarism. 3. SUB TOPIC S Week 1 Overview Human language Word meanings Word2Vec and Optimization basics Week 2 Intro to neural networks Gradients and backpropagation Week 3 Recurrent Neural Networks and Language Models Week 4 Conditioned Generation Week 5 Part 1 Self attention and transformers Part 2 Seq2Seq models Week 6 Midterm Test Week 7 Midterm Recess No class Week 8 Prompting Instruction Finetuning and RLHF Week 9 Question Answering systems Week 10 Multimodal models Week 11 Project presentation and Q A Week 12 Project presentation and Q A Week 13 Project presentation and Q A Classes end April 10th 4. ASSESSMENT OF LEARNING Weight 4 Assignments 20 4 Quizzes 10 Midterm Test 35 Group Project 35 TOTAL 100Note that the percentage distribution of each module may be adjusted. Percentage grades will be converted to letter grades and grade points per the University calendar. 5. LEARNING OUTCOMES By the end of this course the students will know about Introduction to Deep Learning Word representations Language models Pre trained language models Text generation Seq2Seq models Prompting Feedback systems Multimodal models NLP applications e.g. Q A systems 6. COURSE OUTLINE APPROVED ADVISORY STATEMENTS ANTI DISCRIMINATION The Faculty of Engineering is concerned with ensuring an environment that is free of all discrimination. If there is a problem individuals are reminded that they should contact the Associate Director Graduate Studies the Sexual Harassment Officer or the Human Rights Consultant as soon as possible. http www.mcmaster.ca policy General HR Discrimination_Harassment_Sexual_Harassment Prevention Response.pdf ACADEMIC INTEGRITY You are expected to exhibit honesty and use ethical behaviour in all aspects of the learning process. Academic credentials you earn are rooted in principles of honesty and academic integrity. It is your responsibility to understand what constitutes academic dishonesty. Academic dishonesty is to knowingly act or fail to act in a way that results or could result in unearned academic credit or advantage. This behaviour can result in serious consequences e.g.', 'This behaviour can result in serious consequences e.g. the grade of zero on an assignment loss of credit with a notation on the transcript notation reads Grade of F assigned for academic dishonesty and or suspension or expulsion from the university. For information on the various types of academic dishonesty please refer to the Academic Integrity Policy located at https secretariat.mcmaster.ca university policies procedures guidelines The following illustrates only three forms of academic dishonesty plagiarism e.g. the submission of work that is not one s own or for which other credit has been obtained. improper collaboration in group work. copying or using unauthorized aids in tests and examinations. AUTHENTICITY PLAGIARISM DETECTION Some courses may use a web based service Turnitin.com to reveal authenticity and ownership of student submitted work. For courses using such software students will be expected to submit their work electronically either directly to Turnitin.com or via an online learning platform e.g. A2L etc. using plagiarism detection a service supported by Turnitin.com so it can be checked for academic dishonesty. Students who do not wish their work to be submitted through the plagiarism detection software must inform the Instructor before the assignment is due. No penalty will be assigned to a student who does not submit work to the plagiarism detection software. All submitted work is subject to normal verification that standards of academicintegrity have been upheld e.g. on line search other software etc. . For more details about McMaster s use of Turnitin.com please go to www.mcmaster.ca academicintegrity. COURSES WITH AN ON LINE ELEMENT Some courses may use on line elements e.g. e mail Avenue to Learn A2L LearnLink web pages capa Moodle ThinkingCap etc. . Students should be aware that when they access the electronic components of a course using these elements private information such as first and last names user names for the McMaster e mail accounts and program affiliation may become apparent to all other students in the same course. The available information is dependent on the technology used. Continuation in a course that uses on line elements will be deemed consent to this disclosure. If you have any questions or concerns about such disclosure please discuss this with the course instructor.']",text-embedding-3-small,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999999999999996,1.0,0.9600655619610268,0.0
7,What is the policy for late submissions in this course?,The policy for late submissions in this course is not explicitly mentioned in the provided context information. It is advisable to refer to the course syllabus or contact the course instructor for specific details regarding late submissions.,"['This behaviour can result in serious consequences e.g. the grade of zero on an assignment loss of credit with a notation on the transcript notation reads Grade of F assigned for academic dishonesty and or suspension or expulsion from the university. For information on the various types of academic dishonesty please refer to the Academic Integrity Policy located at https secretariat.mcmaster.ca university policies procedures guidelines The following illustrates only three forms of academic dishonesty plagiarism e.g. the submission of work that is not one s own or for which other credit has been obtained. improper collaboration in group work. copying or using unauthorized aids in tests and examinations. AUTHENTICITY PLAGIARISM DETECTION Some courses may use a web based service Turnitin.com to reveal authenticity and ownership of student submitted work. For courses using such software students will be expected to submit their work electronically either directly to Turnitin.com or via an online learning platform e.g. A2L etc. using plagiarism detection a service supported by Turnitin.com so it can be checked for academic dishonesty. Students who do not wish their work to be submitted through the plagiarism detection software must inform the Instructor before the assignment is due. No penalty will be assigned to a student who does not submit work to the plagiarism detection software. All submitted work is subject to normal verification that standards of academicintegrity have been upheld e.g. on line search other software etc. . For more details about McMaster s use of Turnitin.com please go to www.mcmaster.ca academicintegrity. COURSES WITH AN ON LINE ELEMENT Some courses may use on line elements e.g. e mail Avenue to Learn A2L LearnLink web pages capa Moodle ThinkingCap etc. . Students should be aware that when they access the electronic components of a course using these elements private information such as first and last names user names for the McMaster e mail accounts and program affiliation may become apparent to all other students in the same course. The available information is dependent on the technology used. Continuation in a course that uses on line elements will be deemed consent to this disclosure. If you have any questions or concerns about such disclosure please discuss this with the course instructor.', '28652 or sas@mcmaster.ca to make arrangements with a Program Coordinator. For further information consult McMaster University s Academic Accommodation of Students with Disabilities policy. ACADEMIC ACCOMMODATION FOR RELIGIOUS INDIGENOUS OR SPIRITUAL OBSERVANCES RISOStudents requiring academic accommodation based on religious indigenous or spiritual observances should follow the procedures set out in the RISO policy. Students should submit their request to their Faculty Office normally within 10 working days of the beginning of term in which they anticipate a need for accommodation or to the Registrar s Office prior to their examinations. Students should also contact their instructors as soon as possible to make alternative arrangements for classes assignments and tests. http www.mcmaster.ca policy Students AcademicStudies Studentcode.pdf COPYRIGHT AND RECORDING Students are advised that lectures demonstrations performances and any other course material provided by an instructor include copyright protected works. The Copyright Act and copyright law protect every original literary dramatic musical and artistic work including lectures by University instructors The recording of lectures tutorials or other methods of instruction may occur during a course. Recording may be done by either the instructor for the purpose of authorized distribution or by a student for the purpose of personal study. Students should be aware that their voice and or image may be recorded by others during the class. Please speak with the instructor if this is a concern for you. EXTREME CIRCUMSTANCES The University reserves the right to change the dates and deadlines for any or all courses in extreme circumstances e.g. severe weather labour disruptions etc. . Changes will be communicated through regular McMaster communication channels such as McMaster Daily News A2L and or McMaster email.']",text-embedding-3-small,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.0,1.0,0.8672037335286649,0.0
8,What does the course hope to teach regarding word vectors?,"The course aims to teach about representing word semantics not just as one-hot vectors, but as a collection of features and relationships to linguistic categories and other words. It discusses the wealth of information that can be annotated about a word, including grammatical, derivational, and semantic information. It also mentions existing resources like WordNet and UniMorph for annotated information about words, and the challenges of building word vectors based on human annotated resources. Additionally, the course emphasizes the importance of allowing data to determine aspects of text representations and the promise of learning rich representations from data in the field of NLP.

Source: Lecture01_notes_Introduction.txt","['2.3 Vectors from annotated discrete properties. Should we represent word semantics not as one hot vectors but instead as a collection of features and relationships to linguistic categories and other words For any word say runners there is a wealth of information we can annotate about that word. There is grammatical information like plurality there s derivational information like how the runners is something like the verb to run plus a notion of doer or agent think one who runs. There s also semantic information like how runners might be a hyponym of humans or animals or entities. A hyponym is a member of an is a relationship e.g. a runner is a human. There are substantial existing resources in English and a few other languages for various kinds of annotated information about words. WordNet Miller 1995 annotates for synonyms hyponyms and other semantic relations UniMorph Batsuren et al. 2022 annotates for morphology subword structure information across many lan guages. With such resources one could build word vectors that look something like vtea 0 0 1 . . . 1 plural noun 3rd singular verb hyponym of beverage . . . synonym of chai 3 In 2023 word vectors resulting from these methods are not the norm and they won t be the focus of this course. One main failure is that human annotated resources are always lacking in vocabulary compared to methods that can draw a vocabulary from a naturally occuring text source updating these resources is costly and they re always incomplete. Another failure is a tradeoff between dimension ality and utility of the embedding it takes a very high dimensional vector think much larger than the vocabulary size to represent all of these categories and modern neural methods that tend to operate on dense vectors do not behave well with such vectors. Finally a continual theme we ll see in this course is that human ideas of what the right representations should be for text tend to underperform methods that allow data to determine more aspects at least when one has a lot of data to learn from. 3 Distributional semantics and Word2vec A promise of deep learning is to learn rich representations of com plex objects from data. Increasingly relevant in NLP is the idea that we can unsupervisedly learn rich representations from data.', 'This has been done on various datasets that contain human judgement survey data. Model Size WS353 MC RG SCWS RW SVD 6B 35.3 35.1 42.5 38.3 25.6 SVD S 6B 56.5 71.5 71.0 53.6 34.7 SVD L 6B 65.7 72.7 75.1 56.5 37.0 CBOW 6B 57.2 65.6 68.2 57.0 32.5 SG 6B 62.8 65.2 69.7 58.1 37.2 GloVe 6B 65.8 72.7 77.8 53.9 38.1 SVD L 42B 74.0 76.4 74.1 58.3 39.9 GloVe 42B 75.9 83.6 82.9 59.6 47.8 CBOW 100B 68.4 79.6 75.4 59.4 45.5 Table 6 Here we see the correlations be tween of word vector similarities using different embedding techniques with different human judgment datasets 2.6 Further Reading Dealing With Ambiguity One might wonder how we handle the situation where we want to capture the same word with different vectors for its different uses in natural language. For instance run is both a noun and a verb and is used and interpreted differently based on the context. Improving Word Representations Via Global Context And Multi ple Word Prototypes Huang et al 2012 describes how such cases can also be handled in NLP. The essence of the method is the following 1. Gather Ô¨Åxed size context windows of all occurrences of the word for instance 5 before and 5 after 2. Each context is represented by a weighted average of the context words vectors using idf weighting 3. Apply spherical k means to cluster these context representations. 4. Finally each word occurrence is re labeled to its associated cluster and is used to train the word representation for that cluster.']",text-embedding-3-small,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9025042410701696,1.0,0.897626128418066,0.0454545454545454
9,How does Word2vec represent word meaning?,"Word2vec represents word meaning by assigning each word in a fixed vocabulary a vector. It goes through each position in the text with a center word and context outside words, using the similarity of the word vectors for the center word and context words to calculate the probability of the context words given the center word or vice versa. By adjusting the word vectors to maximize this probability, Word2vec captures the meaning of words based on their context in the text.","['500 000 Means one 1 the rest 0s 16Problem with words as discrete symbols 17 Example in web search if a user searches for Seattle motel we would like to match documents containing Seattle hotel But motel 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 hotel 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 These two vectors are orthogonal There is no natural notion of similarity for one hot vectors Solution Could try to rely on WordNet s list of synonyms to get similarity But it is well known to fail badly incompleteness etc. Instead learn to encode similarity in the vectors themselves Sec. 9.2.2Representing words by their context 21 Distributional semantics A word s meaning is given by the words that frequently appear close by You shall know a word by the company it keeps J. R. Firth 1957 11 One of the most successful ideas of modern statistical NLP When a word w appears in a text its context is the set of words that appear nearby within a fixed size window . We use the many contexts of w to build up a representation of w government debt problems turning into banking crises as happened in 2009 saying that Europe needs unified banking regulation to replace the hodgepodge India has just given its banking system a shot in the arm These context words will represent bankingWord vectors We will build a dense vector for each word chosen so that it is similar to vectors of words that appear in similar contexts measuring similarity as the vector dot scalar product Note word vectors are also called word embeddings or neural word representations They are a distributed representation banking 0.286 0.792 0.177 0.107 0.109 0.542 0.349 0.271 monetary 0.413 0.582 0.007 0.247 0.216 0.718 0.147 0.051 19Word meaning as a neural word vector visualization 0.286 0.792 0.177 0.107 0.109 0.542 0.349 0.271 0.487 expect 203.', 'Word2vec Overview 21 Word2vec Mikolov et al. 2013 is a framework for learning word vectors Idea We have a large corpus body of text a long list of words Every word in a fixed vocabulary is represented by a vector Go through each position t in the text which has a center word c and context outside words o Use the similarity of the word vectors for c and o to calculate the probability of o given c or vice versa Keep adjusting the word vectors to maximize this probabilityWord2Vec Overview Example windows and process for computing ùëÉùë§ùë° ùëó ùë§ùë° crises banking into turning problems as center word at position t outside context words in window of size 2 outside context words in window of size 2 ùëÉùë§ùë° 1 ùë§ùë° 22 ùëÉùë§ùë° 2 ùë§ùë° ùëÉùë§ùë° 1 ùë§ùë° ùëÉùë§ùë° 2 ùë§ùë°Word2Vec Overview Example windows and process for computing ùëÉùë§ùë° ùëó ùë§ùë° crises banking into turning problems as center word at position t outside context words in window of size 2 outside context words in window of size 2 ùëÉùë§ùë° 2 ùë§ùë° ùëÉùë§ùë° 1 ùë§ùë° 23 ùëÉùë§ùë° 2 ùë§ùë° ùëÉùë§ùë° 1 ùë§ùë°Word2vec objective function For each position ùë° 1 ùëá predict context words within a window of fixed size m given center word ùë§ùë°. Data likelihood The objective function ùêΩùúÉis the average negative log likelihood Minimizing objective function Maximizing predictive accuracy sometimes called a cost or loss function 24Word2vec objective function We want to minimize the objective function Question How to calculate ùëÉùë§ùë° ùëó ùë§ùë° ùúÉ Answer We will use two vectors per word w']",text-embedding-3-small,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9994884235900416,1.0,0.9245318263495668,0.2
10,What is the main focus of this lecture on neural networks?,"The main focus of this lecture on neural networks is to introduce single and multilayer neural networks, discuss how they can be used for classification purposes, and explain how they can be trained using backpropagation. Additionally, the lecture covers topics such as neuron units, max margin loss, gradient checks, Xavier parameter initialization, learning rates, and Adagrad. Lastly, the lecture motivates the use of recurrent neural networks as a language model.

Source: SEP 775 - Computational Natural Language Processing, lecture02 notes Backpropagation.","['file name: lecture02 notes Backpropagation.  Neural networks. Forward computation. Backward propagation. Neuron Units. Max margin Loss. Gradient checks. Xavier parameter initialization. Learning rates. Adagrad. This set of notes introduces single and multilayer neural networks and how they can be used for classiÔ¨Åcation purposes. We then dis cuss how they can be trained using a distributed gradient descent technique known as backpropagation. We will see how the chain rule can be used to make parameter updates sequentially. After a rigorous mathematical discussion of neural networks we will discuss some practical tips and tricks in training neural networks involving neuron units non linearities gradient checks Xavier parameter ini tialization learning rates Adagrad etc. Lastly we will motivate the use of recurrent neural networks as a language model. 1 Neural Networks Foundations Figure 1 We see here how a non linear decision boundary separates the data very well. This is the prowess of neural networks. Fun Fact Neural networks are biologically in spired classiÔ¨Åers which is why they are often called artiÔ¨Åcial neural networks to distinguish them from the organic kind. However in reality human neural networks are so much more capable and complex from artiÔ¨Åcial neural net works that it is usually better to not draw too many parallels between the two. We established in our previous discussions the need for non linear classiÔ¨Åers since most data are not linearly separable and thus our classiÔ¨Åcation performance on them is limited. Neural networks are a family of classiÔ¨Åers with non linear decision boundary as seen in Figure 1. Now that we know the sort of decision boundaries neural networks create let us see how they manage doing so. 1.1 A Neuron A neuron is a generic computational unit that takes n inputs and produces a single output. What differentiates the outputs of different neurons is their parameters also referred to as their weights . One of the most popular choices for neurons is the sigmoid or binary lo gistic regression unit.', 'file name: lecture03 RNNs and LLMs. Hamidreza Mahyar mahyarh@mcmaster.ca Computational Natural Language Processing Language Models and Recurrent Neural NetsLecture Plan 1. A bit more about neural networks Language modeling RNNs 2. A new NLP task Language Modeling 4. Problems with RNNs 5. Recap on RNNs LMs Reminders You should have started working on Assignment 1 2 motivates 3. A new family of neural networks Recurrent Neural Networks RNNs This is the most important concept in the class It leads to GPT 3 and ChatGPT Important and used in Ass1 but not the only way to build LMs0 model power Training error Test error Classic view Regularization works to prevent overfitting when we have a lot of features or later a very powerful deep model etc. Now Regularization produces models that generalize well when we have a big model We do not care that our models overfit on the training data even though they are hugely overfit error overfitting We have models with many parameters Regularization 3 A full loss function includes regularization over all parameters ùúÉ e.g. L2 regularizationDropout Srivastava Hinton Krizhevsky Sutskever Salakhutdinov 2012 JMLR 2014 4 Preventing Feature Co adaptation Good Regularization Method Use it everywhere Training time at each instance of evaluation in online SGD training randomly set 50 p of the inputs to each neuron to 0 Test time halve the model weights now twice as many Except usually only drop first layer inputs a little 15 or not at all This prevents feature co adaptation A feature cannot only be useful in the presence of particular other features In a single layer A kind of middle ground between Na√Øve Bayes where all feature weights are set independently and logistic regression models where weights are set in the context of all others Can be thought of as a form of model bagging i.e.']",text-embedding-3-small,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.0000000000000002,1.0,0.925081044920158,0.0357142857142857
11,What are the non-linear functions mentioned for neural networks?,"The non-linear functions mentioned for neural networks are logistic, tanh, ReLU, Swish, Leaky ReLU, Parametric ReLU, and GELU. These functions are used to introduce non-linearities in neural networks for better performance and training efficiency.","['file name: lecture02 Neural Nets. Hamidreza Mahyar mahyarh@mcmaster.ca Computational Natural Language Processing Neural Networks Gradients and BackpropagationBinary classification for center word being location We do supervised training and want high score if it s a location ùêΩùë°ùúÉ ùúéùë† 1 1 ùëí ùë† x xmuseums xin xParis xare xamazing predicted model probability of class f Some element wise non linear function e.g. logistic tanh ReLU 2 R5d Embedding of 1 hot wordsNeural computation Original McCulloch Pitts 1943 threshold unit ùüè ùëäùë• ùúÉ ùüè ùëäùë• ùúÉ 0 This function has no slope so no gradient based learning 3tanh is just a rescaled and shifted sigmoid 2 as steep 1 1 tanh z 2logistic 2z 1 Logistic and tanh are still used e.g. logistic to get a probability However now for deep networks the first thing to try is ReLU it trains quickly and performs well due to good gradient backflow. ReLU has a negative dead zone that recent proposals mitigate GELU is frequently used with Transformers BERT RoBERTa etc. Non linearities old and new logistic sigmoid tanh hard tanh 1 0 1 1 Swish arXiv 1710.05941 swish ùë• ùë•. logistic ùë• Rectified Linear Unit ReLU ReLU ùëß max ùëß 0 Leaky ReLU Parametric ReLU 0 0 0 GELU arXiv 1606.08415 GELU ùë• ùë•. ùëÉùëã ùë• ùëã ùëÅ 0 1 ùë•. logistic 1.702ùë•Non linearities i.e. f on previous slide Why they re needed Neural networks do function approximation e.g.', 'Then during back propagation we only pass gradients through neurons that were kept alive during the forward pass. Finally during testing we compute the forward pass using all of the neurons in the network. However a key sub tlety is that in order for dropout to work effectively the expected output of a neuron during testing should be approximately the same as it was during training else the magnitude of the outputs could be radically different and the behavior of the network is no longer well deÔ¨Åned. Thus we must typically divide the outputs of each neuron during testing by a certain value it is left as an exercise to the reader to determine what this value should be in order for the expected outputs during training and testing to be equivalent. 2.4 Neuron Units So far we have discussed neural networks that contain sigmoidal neurons to introduce nonlinearities however in many applications better networks can be designed using other activation functions. Some common choices are listed here with their function and gra dient deÔ¨Ånitions and these can be substituted with the sigmoidal functions discussed above. Sigmoid This is the default choice we have discussed the activation function œÉ is given by Figure 9 The response of a sigmoid nonlinearity œÉ z 1 1 exp z where œÉ z 0 1 The gradient of œÉ z is œÉ z exp z 1 exp z œÉ z 1 œÉ z Tanh The tanh function is an alternative to the sigmoid function that is often found to converge faster in practice. The primary differ ence between tanh and sigmoid is that tanh output ranges from 1 to 1 while the sigmoid ranges from 0 to 1. Figure 10 The response of a tanh nonlinearity tanh z exp z exp z exp z exp z 2œÉ 2z 1 where tanh z 1 1 The gradient of tanh z is tanh z 1 exp z exp z exp z exp z 2 1 tanh2 z Hard tanh The hard tanh function is sometimes preferred over the tanh function since it is computationally cheaper. It does however saturate for magnitudes of z greater than 1.']",text-embedding-3-small,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.990252641413291,1.0,0.968261532007181,0.0588235294117647
12,What is the purpose of non-linearities in neural networks?,"The purpose of non-linearities in neural networks is to allow deep neural networks to approximate any complex function beyond just a linear transform. Without non-linearities, deep neural networks would be limited to performing only linear transformations, but with the inclusion of non-linearities in extra layers, neural networks can approximate more complex functions effectively.","['file name: lecture02 notes Backpropagation.  Neural networks. Forward computation. Backward propagation. Neuron Units. Max margin Loss. Gradient checks. Xavier parameter initialization. Learning rates. Adagrad. This set of notes introduces single and multilayer neural networks and how they can be used for classiÔ¨Åcation purposes. We then dis cuss how they can be trained using a distributed gradient descent technique known as backpropagation. We will see how the chain rule can be used to make parameter updates sequentially. After a rigorous mathematical discussion of neural networks we will discuss some practical tips and tricks in training neural networks involving neuron units non linearities gradient checks Xavier parameter ini tialization learning rates Adagrad etc. Lastly we will motivate the use of recurrent neural networks as a language model. 1 Neural Networks Foundations Figure 1 We see here how a non linear decision boundary separates the data very well. This is the prowess of neural networks. Fun Fact Neural networks are biologically in spired classiÔ¨Åers which is why they are often called artiÔ¨Åcial neural networks to distinguish them from the organic kind. However in reality human neural networks are so much more capable and complex from artiÔ¨Åcial neural net works that it is usually better to not draw too many parallels between the two. We established in our previous discussions the need for non linear classiÔ¨Åers since most data are not linearly separable and thus our classiÔ¨Åcation performance on them is limited. Neural networks are a family of classiÔ¨Åers with non linear decision boundary as seen in Figure 1. Now that we know the sort of decision boundaries neural networks create let us see how they manage doing so. 1.1 A Neuron A neuron is a generic computational unit that takes n inputs and produces a single output. What differentiates the outputs of different neurons is their parameters also referred to as their weights . One of the most popular choices for neurons is the sigmoid or binary lo gistic regression unit.', 'f on previous slide Why they re needed Neural networks do function approximation e.g. regression or classification Without non linearities deep neural networks can t do anything more than a linear transform Extra layers could just be compiled down into a single linear transform W1 W2 x Wx But with more layers that include non linearities they can approximate any complex function 5Training with cross entropy loss you use this in PyTorch Until now our objective was stated as to maximize the probability of the correct class y or equivalently we can minimize the negative log probability of that class Now restated in terms of cross entropy a concept from information theory Let the true probability distribution be p let our computed model probability be q The cross entropy is Assuming a ground truth or true or gold or target probability distribution that is 1 at the right class and 0 everywhere else p 0 0 1 0 0 then Because of one hot p the only term left is the negative log probability of the true class yi log ùëù ùë¶i ùë•i Cross entropy can be used in other ways with a more interesting p but for now just know that you ll want to use it as the loss in PyTorch 6Remember Stochastic Gradient Descent Update equation i.e. for each parameter In deep learning ùúÉincludes the data representation e.g. word vectors too How can we compute ùúÉùêΩ ùúÉ 1. By hand 2. Algorithmically the backpropagation algorithm ùõº step size or learning rate 7Lecture Plan 8 Gradients 1. Introduction 2. Matrix calculus 3.']",text-embedding-3-small,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.0000000000000004,1.0,0.93362477278922,0.04
13,What is cross entropy loss used for in PyTorch?,"Cross entropy loss is used in PyTorch to minimize the negative log probability of the correct class or target class in a classification task. It is a concept from information theory where the true probability distribution (p) and the computed model probability distribution (q) are compared to calculate the loss. By using cross entropy loss, the model aims to improve its performance by reducing the difference between the predicted probabilities and the actual probabilities of the classes.

Source: Lecture02_Neural_Nets.txt","['f on previous slide Why they re needed Neural networks do function approximation e.g. regression or classification Without non linearities deep neural networks can t do anything more than a linear transform Extra layers could just be compiled down into a single linear transform W1 W2 x Wx But with more layers that include non linearities they can approximate any complex function 5Training with cross entropy loss you use this in PyTorch Until now our objective was stated as to maximize the probability of the correct class y or equivalently we can minimize the negative log probability of that class Now restated in terms of cross entropy a concept from information theory Let the true probability distribution be p let our computed model probability be q The cross entropy is Assuming a ground truth or true or gold or target probability distribution that is 1 at the right class and 0 everywhere else p 0 0 1 0 0 then Because of one hot p the only term left is the negative log probability of the true class yi log ùëù ùë¶i ùë•i Cross entropy can be used in other ways with a more interesting p but for now just know that you ll want to use it as the loss in PyTorch 6Remember Stochastic Gradient Descent Update equation i.e. for each parameter In deep learning ùúÉincludes the data representation e.g. word vectors too How can we compute ùúÉùêΩ ùúÉ 1. By hand 2. Algorithmically the backpropagation algorithm ùõº step size or learning rate 7Lecture Plan 8 Gradients 1. Introduction 2. Matrix calculus 3.', 'The cross entropy can never be lower than the true entropy so a model cannot err by underestimating the true entropy. We are Ô¨Ånally ready to see the relation between perplexity and cross entropy as we saw it in Eq. 3.51. Cross entropy is deÔ¨Åned in the limit as the length of the observed word sequence goes to inÔ¨Ånity. We will need an approximation to cross entropy relying on a sufÔ¨Åciently long sequence of Ô¨Åxed length. This approxima tion to the cross entropy of a model M P wi wi N 1 i 1 on a sequence of words W is H W 1 N logP w1w2 ...wN 3.53 The perplexity of a model P on a sequence of words W is now formally deÔ¨Åned as perplexity 2 raised to the power of this cross entropy3.9 SUMMARY 25 Perplexity W 2H W P w1w2 ...wN 1 N N s 1 P w1w2 ...wN N v u u t N Y i 1 1 P wi w1 ...wi 1 3.54 3.9 Summary This chapter introduced language modeling and the n gram one of the most widely used tools in language processing. Language models offer a way to assign a probability to a sentence or other sequence of words and to predict a word from preceding words. n grams are Markov models that estimate words from a Ô¨Åxed window of pre vious words. n gram probabilities can be estimated by counting in a corpus and normalizing the maximum likelihood estimate . n gram language models are evaluated extrinsically in some task or intrinsi cally using perplexity. The perplexity of a test set according to a language model is the geometric mean of the inverse test set probability computed by the model. Smoothing algorithms provide a more sophisticated way to estimate the prob ability of n grams. Commonly used smoothing algorithms for n grams rely on lower order n gram counts through backoff or interpolation. Both backoff and interpolation require discounting to create a probability dis tribution. Kneser Ney smoothing makes use of the probability of a word being a novel continuation.']",text-embedding-3-small,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999999999999988,1.0,0.9578057944628332,0.0476190476190476
14,What is the significance of the backpropagation algorithm?,"The significance of the backpropagation algorithm is that it allows us to use the chain rule of differentiation to calculate loss gradients for any parameter used in the feedforward computation of a neural network model. It enables updating parameters efficiently using gradient descent or its variants, such as stochastic gradient descent (SGD), by propagating error signals backward through the network to adjust the weights and biases based on the calculated gradients. This iterative process of backpropagation plays a crucial role in training neural networks by optimizing their performance through learning from the errors and updating the model parameters accordingly.","['No parameter updates are necessary if the cost is 0. Since we typically update parameters using gradient descent or a variant such as SGD we typically need the gradient information for any parameter as required in the update equation Œ∏ t 1 Œ∏ t Œ± Œ∏ t J Backpropagation is technique that allows us to use the chain rule of differentiation to calculate loss gradients for any parameter used in the feed forward computation on the model. To understand this further let us understand the toy network shown in Figure 5 for which we will perform backpropagation. Figure 5 This is a 4 2 1 neural network where neuron j on layer k receives input z k j and produces activation output a k j . Here we use a neural network with a single hidden layer and a single unit output. Let us establish some notation that will make it easier to generalize this model later xi is an input to the neural network. s is the output of the neural network. Each layer including the input and output layers has neurons which receive an input and produce an output. The j th neuron of layer k receives the scalar input z k j and produces the scalar activation output a k j . We will call the backpropagated error calculated at z k j as Œ¥ k j . Layer 1 refers to the input layer and not the Ô¨Årst hidden layer. For the input layer xj z 1 j a 1 j . W k is the transfer matrix that maps the output from the k th layer to the input to the k 1 th Thus W 1 W and W 2 U to put this new generalized notation in perspective of Section 1.3. Backpropagation Notation xi is an input to the neural network. s is the output of the neural net work. The j th neuron of layer k receives the scalar input z k j and produces the scalar activation output a k j . For the input layer xj z 1 j a 1 j . W k is the transfer matrix that maps the output from the k th layer to the input to the k 1 th. Thus W 1 W and W 2 UT using notation from Section 1.3.', 'Figure 6 This subnetwork shows the relevant parts of the network required to update W 1 ij Let us discuss the error sharing distribution interpretation of backpropagation better using Figure 6 as an example. Say we were to update W 1 14 1. We start with the an error signal of 1 propagating backwards from a 3 1 . 2. We then multiply this error by the local gradient of the neuron which maps z 3 1 to a 3 1 . This happens to be 1 in this case and thus the error is still 1. This is now known as Œ¥ 3 1 1. 3. At this point the error signal of 1 has reached z 3 1 . We now need to distribute the error signal so that the fair share of the error reaches to a 2 1 . 4. This amount is the error signal at z 3 1 Œ¥ 3 1 W 2 1 W 2 1 . Thus the error at a 2 1 W 2 1 . 5. As we did in step 2 we need to move the error across the neuron which maps z 2 1 to a 2 1 . We do this by multiplying the error signal at a 2 1 by the local gradient of the neuron which happens to be f z 2 1 . 6. Thus the error signal at z 2 1 is f z 2 1 W 2 1 . This is known as Œ¥ 2 1 . 7. Finally we need to distribute the fair share of the error to W 1 14 by simply multiplying it by the input it was responsible for for warding which happens to be a 1 4 . 8. Thus the gradient of the loss with respect to W 1 14 is calculated to be a 1 4 f z 2 1 W 2 1 . Notice that the result we arrive at using this approach is exactly the same as that we arrived at using explicit differentiation earlier.']",text-embedding-3-small,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.000000000000001,1.0,0.9483442996093148,0.03125
15,What does the Jacobian matrix represent in the context of neural networks?,"The Jacobian matrix in the context of neural networks represents an m x n matrix of partial derivatives, where m is the number of outputs and n is the number of inputs of a given function. It generalizes the concept of the gradient when dealing with functions that have multiple outputs and inputs. The Jacobian matrix helps in understanding how each input affects each output in a multivariable function.

Source: lecture02_Neural_Nets.txt","['So if k i everything in the sum is zero and the gradient is zero. Otherwise the only nonzero element of the sum is when l j so we just get xj. Thus we Ô¨Ånd zk Wij xj if k i and 0 if otherwise. Another way of writing this is z Wij 0 . . . 0 xj 0 . . . 0 ith element Now let s compute J Wij J Wij J z z Wij Œ¥ z Wij m X k 1 Œ¥k zk Wij Œ¥ixj the only nonzero term in the sum is Œ¥i zi Wij . To get J W we want a ma trix where entry i j is Œ¥ixj. This matrix is equal to the outer product J W Œ¥T xT 6 Row vector time matrix with respect to the matrix z xW Œ¥ J z what is J W Œ¥ z W A similar computation to 5 shows that J W xT Œ¥ . 7 Cross entropy loss with respect to logits ÀÜ y softmax Œ∏ J CE y ÀÜ y what is J Œ∏ The gradient is J Œ∏ ÀÜ y y or ÀÜ y y T if y is a column vector . 4These identities will be enough to let you quickly compute the gradients for many neural networks. However it s important to know how to compute Jacobians for other functions as well in case they show up. Some examples if you want practice dot product of two vectors elementwise product of two vectors 2 norm of a vector. Feel free to use these identities in the assignments. One option is just to memorize them. Another option is to Ô¨Ågure them out by looking at the dimensions. For example only one ordering orientation of Œ¥ and x will produce the correct shape for J W assuming W is not square . 4 Gradient Layout Jacobean formulation is great for applying the chain rule you just have to mul tiply the Jacobians. However when doing SGD it s more convenient to follow the convention the shape of the gradient equals the shape of the parameter as we did when computing J W . That way subtracting the gradient times the learning rate from the parameters is easy. We expect answers to homework questions to follow this convention.', 'Introduction 2. Matrix calculus 3. BackpropagationComputing Gradients by Hand 9 Matrix calculus Fully vectorized gradients Multivariable calculus is just like single variable calculus if you use matrices Much faster and more useful than non vectorized gradients But doing a non vectorized gradient can be good for intuition recall the first lecture for an exampleGradients Given a function with 1 output and 1 input ùëìùë• ùë•3 It s gradient slope is its derivative ùëëf 3ùë•2 10 ùëëùë• How much will the output change if we change the input a bit At x 1 it changes about 3 times as much 1.013 1.03 At x 4 it changes about 48 times as much 4.013 64.48Gradients Given a function with 1 output and n inputs Its gradient is a vector of partial derivatives with respect to each input 11Jacobian Matrix Generalization of the Gradient Given a function with m outputs and n inputs It s Jacobian is an m x n matrix of partial derivatives 12Chain Rule For composition of one variable functions multiply derivatives For multiple variables functions multiply Jacobians 13Example Jacobian Elementwise activation Function 14Example Jacobian Elementwise activation Function Function has n outputs and n inputs n by n Jacobian 15Example Jacobian Elementwise activation Function 16Example Jacobian Elementwise activation Function 17Example Jacobian Elementwise activation Function 18Other Jacobians Compute these at home for practice Check your answers with the lecture notes 19Other Jacobians Compute these at home for practice Check your answers with the lecture notes 20Other Jacobians Compute these at home for practice Check your answers with the lecture notes Fine print This is the correct Jacobian. Later we discuss the shape convention using it the answer would be h. 21Other Jacobians Compute these at home for practice 22Back to our Neural Net x xmuseums xin xParis xare xamazing 23Back to our Neural Net Let s find Really we care about the gradient of the loss Jt but we will compute the gradient of the score for simplicity x xmuseums xin xParis xare xamazing 241.']",text-embedding-3-small,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999999999999988,1.0,0.938534632876426,0.6086956521739131
16,How is the derivative with respect to a matrix described?,The derivative with respect to a matrix is described as a n x m matrix where each entry represents the derivative of the loss function with respect to a specific entry in the original matrix. This arrangement allows for easier computation and application of gradient descent during optimization.,"['So z x ij zi xj xj f xi f xi if i j 0 if otherwise So we see that the Jacobian z x is a diagonal matrix where the entry at i i is the derivative of f applied to xi. We can write this as z x diag f x . Since multiplication by a diagonal matrix is the same as doing elementwise multiplication by the diagonal we could also write f x when applying the chain rule. 5 Matrix times column vector with respect to the matrix z W x Œ¥ J z what is J W J z z W Œ¥ z W This is a bit more complicated than the other identities. The reason for in cluding J z in the above problem formulation will become clear in a moment. First suppose we have a loss function J a scalar and are computing its gradient with respect to a matrix W Rn m. Then we could think of J as a function of W taking nm inputs the entries of W to a single output J . This means the Jacobian J W would be a 1 nm vector. But in practice this is not a very useful way of arranging the gradient. It would be much nicer if the derivatives were in a n m matrix like this J W J W11 . . . J W1m . . . ... . . . J Wn1 . . . J Wnm Since this matrix has the same shape as W we could just subtract it times the learning rate from W when doing gradient descent. So in a slight abuse of notation let s Ô¨Ånd this matrix as J W instead. This way of arranging the gradients becomes complicated when computing z W . Unlike J z is a vector. So if we are trying to rearrange the gradients like with J W z W would be an n m n tensor Luckily we can avoid the issue by taking the gradient with respect to a single weight Wij instead. 3z Wij is just a vector which is much easier to deal with. We have zk m X l 1 Wklxl zk Wij m X l 1 xl Wij Wkl Note that Wij Wkl 1 if i k and j l and 0 if otherwise. So if k i everything in the sum is zero and the gradient is zero.', 'Using the regular chain rule we can compute the derivative of g as the Jacobian g x xg1 f1 x f2 x xg2 f1 x f2 x g1 f1 f1 x g1 f2 f2 x g2 f1 f1 x g2 f2 f2 x 1And we see this is the same as multiplying the two Jacobians g x g f f x g1 f1 g1 f2 g2 f1 g2 f2 f1 x f2 x 3 Useful Identities This section will now go over how to compute the Jacobian for several simple functions. It will provide some useful identities you can apply when taking neu ral network gradients. 1 Matrix times column vector with respect to the column vector z W x what is z x Suppose W Rn m. Then we can think of z as a function of x taking an m dimensional vector to an n dimensional vector. So its Jacobian will be n m. Note that zi m X k 1 Wikxk So an entry z x ij of the Jacobian will be z x ij zi xj xj m X k 1 Wikxk m X k 1 Wik xj xk Wij because xj xk 1 if k j and 0 if otherwise. So we see that z x W 2 Row vector times matrix with respect to the row vector z xW what is z x A computation similar to 1 shows that z x W T . 3 A vector with itself z x what is z x We have zi xi. So z x ij zi xj xj xi 1 if i j 0 if otherwise So we see that the Jacobian z x is a diagonal matrix where the entry at i i is 1. This is just the identity matrix z x I . When applying the chain 2rule this term will disappear because a matrix or vector multiplied by the identity matrix does not change. 4 An elementwise function applied a vector z f x what is z x Since f is being applied elementwise we have zi f xi .']",text-embedding-3-small,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9768913042834108,0.5,0.8983354856596555,0.0344827586206896
17,What role does automatic differentiation play in modern deep learning frameworks?,"Automatic differentiation plays a crucial role in modern deep learning frameworks as they compute gradients automatically. This feature saves time and effort for developers by handling the complex task of calculating derivatives efficiently, allowing them to focus on designing and training neural network models effectively. Understanding how automatic differentiation works can help developers debug and improve their models when necessary.

Source: Lecture02_Neural_Nets","['do backpropagation for you but mainly leave layer node writer to hand calculate the local derivative 74Backprop Implementations 75Implementation forward backward API 76Implementation forward backward API 77Manual Gradient checking Numeric Gradient For small h 1e 4 Easy to implement correctly But approximate and very slow You have to recompute f for every parameter of our model Useful for checking your implementation In the old days we hand wrote everything doing this everywhere was the key test Now much less needed you can use it to check layers are correctly implemented 78Summary We ve mastered the core technology of neural nets Backpropagation recursively and hence efficiently apply the chain rule along computation graph downstream gradient upstream gradient x local gradient Forward pass compute results of operations and save intermediate values Backward pass apply chain rule to compute gradients 79Why learn all these details about gradients 80 Modern deep learning frameworks compute gradients for you But why take a class on compilers or systems when they are implemented for you Understanding what is going on under the hood is useful Backpropagation doesn t always work perfectly out of the box Understanding why is crucial for debugging and improving models See Karpathy article https medium.com @karpathy yes you should understand backprop e2f06eab496b Example in future lecture exploding and vanishing gradients', 'Introduction 2. Matrix calculus 3. BackpropagationComputing Gradients by Hand 9 Matrix calculus Fully vectorized gradients Multivariable calculus is just like single variable calculus if you use matrices Much faster and more useful than non vectorized gradients But doing a non vectorized gradient can be good for intuition recall the first lecture for an exampleGradients Given a function with 1 output and 1 input ùëìùë• ùë•3 It s gradient slope is its derivative ùëëf 3ùë•2 10 ùëëùë• How much will the output change if we change the input a bit At x 1 it changes about 3 times as much 1.013 1.03 At x 4 it changes about 48 times as much 4.013 64.48Gradients Given a function with 1 output and n inputs Its gradient is a vector of partial derivatives with respect to each input 11Jacobian Matrix Generalization of the Gradient Given a function with m outputs and n inputs It s Jacobian is an m x n matrix of partial derivatives 12Chain Rule For composition of one variable functions multiply derivatives For multiple variables functions multiply Jacobians 13Example Jacobian Elementwise activation Function 14Example Jacobian Elementwise activation Function Function has n outputs and n inputs n by n Jacobian 15Example Jacobian Elementwise activation Function 16Example Jacobian Elementwise activation Function 17Example Jacobian Elementwise activation Function 18Other Jacobians Compute these at home for practice Check your answers with the lecture notes 19Other Jacobians Compute these at home for practice Check your answers with the lecture notes 20Other Jacobians Compute these at home for practice Check your answers with the lecture notes Fine print This is the correct Jacobian. Later we discuss the shape convention using it the answer would be h. 21Other Jacobians Compute these at home for practice 22Back to our Neural Net x xmuseums xin xParis xare xamazing 23Back to our Neural Net Let s find Really we care about the gradient of the loss Jt but we will compute the gradient of the score for simplicity x xmuseums xin xParis xare xamazing 241.']",text-embedding-3-small,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.0000000000000002,0.2,0.9111661137411092,0.1666666666666666
18,What is the purpose of manual gradient checking?,"The purpose of manual gradient checking is to compare analytical gradients with numerical gradients in order to verify the correctness of the analytical gradients. This process helps ensure that the gradients calculated analytically are accurate and close to the numerical gradients, providing a way to validate the gradient computation process in machine learning models. It is a way to catch errors in the gradient calculation and ensure the model is learning effectively.","['We note that this deÔ¨Ånition of the numerical gradient follows very naturally from the deÔ¨Ånition of the derivative where in the scalar case f x f x œµ f x œµ Gradient checks are a great way to compare analytical and numerical gradients. Analytical gradients should be close and numerical gradients can be calculated using f Œ∏ J Œ∏ i J Œ∏ i 2œµ J Œ∏ i and J Œ∏ i can be evalu ated using two forward passes. An implementation of this can be seen in Snippet 2.1. Of course there is a slight difference the deÔ¨Ånition above only perturbs x in the positive direction to compute the gradient. While it would have been perfectly acceptable to deÔ¨Åne the numerical gradi ent in this way in practice it is often more precise and stable to use the centered difference formula where we perturb a parameter in both directions. The intuition is that to get a better approximation of the derivative slope around a point we need to examine the func tion f s behavior both to the left and right of that point. It can also be shown using Taylor s theorem that the centered difference formula has an error proportional to œµ2 which is quite small whereas the derivative deÔ¨Ånition is more error prone. Now a natural question you might ask is if this method is so pre cise why do we not use it to compute all of our network gradients instead of applying back propagation The simple answer as hinted earlier is inefÔ¨Åciency recall that every time we want to compute the gradient with respect to an element we need to make two forward passes through the network which will be computationally expen sive. Furthermore many large scale neural networks can contain millions of parameters and computing two passes per parameter is clearly not optimal. And since in optimization techniques such as SGD we must compute the gradients once per iteration for several thousands of iterations it is obvious that this method quickly grows intractable. This inefÔ¨Åciency is why we only use gradient check to verify the correctness of our analytic gradients which are much quicker to compute.', 'This means that when stochastic gradient descent SGD reaches the wall and does a gradient descent step it will be forced to jump across the valley moving perpen dicular to the steep walls possibly leaving the valley and disrupting the learning process. The dashed arrows in Fig. 6 correspond to ignoring the norm of this large step ensuring that the model stays close to the wall. The key insight is that all the steps taken when the gradient explodes are aligned with v and ignore other descent direction i.e. the model moves perpendicular to the wall . At the wall a small norm step in the direction of the gradient there fore merely pushes us back inside the smoother low curvature region besides the wall whereas a regular gradient step would bring us very far thus slowing or preventing further training. Instead with a bounded step we get back in that smooth region near the wall where SGD is free to explore other descent directions. The important addition in this scenario to the classical high curvature valley is that we assume that the val ley is wide as we have a large region around the wall where if we land we can rely on Ô¨Årst order methods to move towards the local minima. This is why just clipping the gradient might be suÔ¨Écient not requiring the use a second order method. Note that this algo rithm should work even when the rate of growth of the gradient is not the same as the one of the curvature a case for which a second order method would fail as the ratio between the gradient and curvature could still explode . Our hypothesis could also help to understand the re cent success of the Hessian Free approach compared to other second order methods. There are two key dif ferences between Hessian Free and most other second order algorithms. First it uses the full Hessian matrix and hence can deal with exploding directions that are not necessarily axis aligned. Second it computes a new estimate of the Hessian matrix before each up date step and can take into account abrupt changes in curvature such as the ones suggested by our hypothe sis while most other approaches use a smoothness as sumption i.e. averaging 2nd order signals over many steps. 3. Dealing with the exploding and vanishing gradient 3.1.']",text-embedding-3-small,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999994313386336,0.6666666666666666,0.9331793187288586,0.04
19,Why is understanding gradients important despite automatic differentiation?,"Understanding gradients is important despite automatic differentiation because it allows us to debug and improve models when backpropagation doesn't work perfectly out of the box. It helps us comprehend what is happening under the hood, which is crucial for addressing issues such as exploding and vanishing gradients. Source: SEP 775 - Computational Natural Language Processing, lecture02_Neural_Nets.txt","['We note that this deÔ¨Ånition of the numerical gradient follows very naturally from the deÔ¨Ånition of the derivative where in the scalar case f x f x œµ f x œµ Gradient checks are a great way to compare analytical and numerical gradients. Analytical gradients should be close and numerical gradients can be calculated using f Œ∏ J Œ∏ i J Œ∏ i 2œµ J Œ∏ i and J Œ∏ i can be evalu ated using two forward passes. An implementation of this can be seen in Snippet 2.1. Of course there is a slight difference the deÔ¨Ånition above only perturbs x in the positive direction to compute the gradient. While it would have been perfectly acceptable to deÔ¨Åne the numerical gradi ent in this way in practice it is often more precise and stable to use the centered difference formula where we perturb a parameter in both directions. The intuition is that to get a better approximation of the derivative slope around a point we need to examine the func tion f s behavior both to the left and right of that point. It can also be shown using Taylor s theorem that the centered difference formula has an error proportional to œµ2 which is quite small whereas the derivative deÔ¨Ånition is more error prone. Now a natural question you might ask is if this method is so pre cise why do we not use it to compute all of our network gradients instead of applying back propagation The simple answer as hinted earlier is inefÔ¨Åciency recall that every time we want to compute the gradient with respect to an element we need to make two forward passes through the network which will be computationally expen sive. Furthermore many large scale neural networks can contain millions of parameters and computing two passes per parameter is clearly not optimal. And since in optimization techniques such as SGD we must compute the gradients once per iteration for several thousands of iterations it is obvious that this method quickly grows intractable. This inefÔ¨Åciency is why we only use gradient check to verify the correctness of our analytic gradients which are much quicker to compute.', 'do backpropagation for you but mainly leave layer node writer to hand calculate the local derivative 74Backprop Implementations 75Implementation forward backward API 76Implementation forward backward API 77Manual Gradient checking Numeric Gradient For small h 1e 4 Easy to implement correctly But approximate and very slow You have to recompute f for every parameter of our model Useful for checking your implementation In the old days we hand wrote everything doing this everywhere was the key test Now much less needed you can use it to check layers are correctly implemented 78Summary We ve mastered the core technology of neural nets Backpropagation recursively and hence efficiently apply the chain rule along computation graph downstream gradient upstream gradient x local gradient Forward pass compute results of operations and save intermediate values Backward pass apply chain rule to compute gradients 79Why learn all these details about gradients 80 Modern deep learning frameworks compute gradients for you But why take a class on compilers or systems when they are implemented for you Understanding what is going on under the hood is useful Backpropagation doesn t always work perfectly out of the box Understanding why is crucial for debugging and improving models See Karpathy article https medium.com @karpathy yes you should understand backprop e2f06eab496b Example in future lecture exploding and vanishing gradients']",text-embedding-3-small,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.984563332089755,1.0,0.945738058894314,0.0833333333333333
20,What is the core concept introduced in this class that leads to GPT-3 and ChatGPT?,"The core concept introduced in this class that leads to GPT-3 and ChatGPT is the idea of emergent zero-shot learning. This concept refers to the ability of models like GPT-2 and GPT-3 to perform tasks without specific examples or gradient updates, simply by specifying the right sequence prediction problem. This allows the models to excel in various tasks without task-specific fine-tuning, showcasing their versatility and capability in natural language processing tasks.","['2019 Same architecture as GPT just bigger 117M 1.5B But trained on much more data 4GB 40GB of internet text data WebText Scrape links posted on Reddit w at least 3 upvotes rough proxy of human quality GPT 2018 GPT 2 2019 117M 1.5B 15One key emergent ability in GPT 2 is zero shot learning the ability to do many tasks with no examples and no gradient updates by simply Specifying the right sequence prediction problem e.g. question answering Passage Tom Brady... Q Where was Tom Brady born A ... Comparing probabilities of sequences e.g. Winograd Schema Challenge Levesque 2011 16 The cat couldn t fit into the hat because it was too big. Does it the cat or the hat Is P ...because the cat was too big P ...because the hat was too big Emergent zero shot learning Radford et al. 2019Emergent zero shot learning GPT 2 beats SoTA on language modeling benchmarks with no task specific fine tuning LAMBADA language modeling w long discourse dependencies Paperno et al. 2016 Radford et al. 2019 17Emergent zero shot learning 18 You can get interesting zero shot behavior if you re creative enough with how you specify your task Summarization on CNN DailyMail dataset See et al. 2017 SAN FRANCISCO California CNN A magnitude 4.2 earthquake shook the San Francisco ... overturn unstable Radford et al. 2019 2018 SoTA Supervised 287K Too Long Didn t Read Prompting objects. TL DR Select from article ROUGEEmergent abilities of large language models GPT 3 2020 19 GPT 3 175B parameters Brown et al.', 'These possibilities exist on a spectrum ranging from demonstrations in the training set that are drawn from exactly the same distribution as those at test time to recognizing the same task but in a different format to adapting to a speciÔ¨Åc style of a general task such as QA to learning a skill entirely de novo. Where GPT 3 is on this spectrum may also vary from task to task. Synthetic tasks such as wordscrambling or deÔ¨Åning nonsense words seem especially likely to be learned de novo whereas translation clearly must be learned during pretraining although possibly from data that is very different in organization and style than the test data. Ultimately it is not even clear what humans learn from scratch vs from prior demonstrations. Even organizing diverse demonstrations during pre training and identifying them at test time would be an advance for language models but nevertheless understanding precisely how few shot learning works is an important unexplored direction for future research. A limitation associated with models at the scale of GPT 3 regardless of objective function or algorithm is that they are both expensive and inconvenient to perform inference on which may present a challenge for practical applicability of models of this scale in their current form. One possible future direction to address this is distillation HVD15 of large models down to a manageable size for speciÔ¨Åc tasks. Large models such as GPT 3 contain a very wide range of skills most of which are not needed for a speciÔ¨Åc task suggesting that in principle aggressive distillation may be possible. Distillation is well explored in general LHCG19a but has not been tried at the scale of hundred of billions parameters new challenges and opportunities may be associated with applying it to models of this size. Finally GPT 3 shares some limitations common to most deep learning systems its decisions are not easily interpretable it is not necessarily well calibrated in its predictions on novel inputs as observed by the much higher variance in performance than humans on standard benchmarks and it retains the biases of the data it has been trained on. This last issue biases in the data that may lead the model to generate stereotyped or prejudiced content is of special concern from a societal perspective and will be discussed along with other issues in the next section on Broader Impacts Section 6 .']",text-embedding-3-small,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999999999999996,0.0,0.8789638276369943,0.0625
21,What is the purpose of Dropout as mentioned in the document?,"The purpose of Dropout, as mentioned in the document, is to act as a powerful technique for regularization in neural networks. During training, Dropout randomly drops a subset of neurons with a certain probability during each forward and backward pass, or keeps each neuron alive with a certain probability. This process helps prevent overfitting, allows the network to learn more meaningful information from the data, and generally leads to higher performance on the task at hand by effectively training exponentially many smaller networks at once and averaging over their predictions.

Source: SEP775_NLP - lecture02_notes_Backpropagation.txt","['Imposing such a con straint can also be interpreted as the prior Bayesian belief that the optimal weights are close to zero how close depends on the value of Œª. Choosing the right value of Œª is critical and must be chosen via hyperparameter tuning. Too high a value of Œª causes most of the weights to be set too close to 0 and the model does not learn anything meaningful from the training data often obtaining poor ac curacy on training validation and testing sets. Too low a value and we fall into the domain of overÔ¨Åtting once again. It must be noted that the bias terms are not regularized and do not contribute to the cost term above try thinking about why this is the case There are indeed other types of regularization that are sometimes used such as L1 regularization which sums over the absolute values rather than squares of parameter elements however this is less commonly applied in practice since it leads to sparsity of parameter weights. In the next section we discuss dropout which effectively acts as another form of regularization by randomly dropping i.e. setting to zero neurons in the forward pass. 2.3 Dropout Dropout is a powerful technique for regularization Ô¨Årst introduced by Srivastava et al. in Dropout A Simple Way to Prevent Neural Net works from OverÔ¨Åtting. The idea is simple yet effective during train ing we will randomly drop with some probability 1 p a subset of neurons during each forward backward pass or equivalently we will keep alive each neuron with a probability p . Then during testing we will use the full network to compute our predictions. The result is that the network typically learns more meaningful informa tion from the data is less likely to overÔ¨Åt and usually obtains higher performance overall on the task at hand. One intuitive reason why this technique should be so effective is that what dropout is doing is essentially doing is training exponentially many smaller networks at once and averaging over their predictions. Dropout applied to an artiÔ¨Åcial neural network. Image credits to Srivastava et al. In practice the way we introduce dropout is that we take the out put h of each layer of neurons and keep each neuron with prob ability p and else set it to 0. Then during back propagation we only pass gradients through neurons that were kept alive during the forward pass.', 'like an ensemble model Nowadays usually thought of as strong feature dependent regularizer Wager Wang Liang 2013DropoutDropout Srivastava Hinton Krizhevsky Sutskever Salakhutdinov 2012 JMLR 2014 During training For each data point each time Randomly set input to 0 with probability ùëù dropout ratio often p 0.5 except p 0.15 for input layer via dropout mask During testing Multiply all weights by 1 ùëù No other dropout ùë•2 ùë•3 0 ùë§1 ùë§2 ùë§3 ùë§4 ùëè 1 ùë¶ ùë§1ùë•1 ùë§2ùë•2 ùë§3ùë•3 ùë• ùë• 1 1 Train 1 Train 2 Test ùë•3 0 1 ùë¶ ùë§1ùë•1 ùë§3ùë•3 ùëè ùë§1 ùë§2 ùë§3 ùë§4 ùëè ùë•1 0 ùë•2 ùë•3 ùë•4 ùë§1 ùë§2 ùë§3 ùë§4 ùëè 1 ùë¶ 1 ùëù ùë§1ùë•1 ùë§2ùë•2 ùë§3ùë•3 ùë§4ùë•4 6Vectorization E.g. looping over word vectors versus concatenating them all into one large matrix and then multiplying the softmax weights with that matrix for loop 1000 loops best of 3 639 ¬µs per loop Using single a C x N matrix 10000 loops best of 3 53.8 ¬µs per loop Matrices are awesome Always try to use vectors and matrices rather than for loops The speed gain goes from 1 to 2 orders of magnitude with GPUs 7Parameter Initialization You normally must initialize weights to small random values i.e. not zero matrices To avoid symmetries that prevent learning specialization Initialize hidden layer biases to 0 and output or reconstruction biases to optimal value if weights were 0 e.g.']",text-embedding-3-small,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9535660078682912,0.8,0.9138025318595606,0.0625
22,What are some approaches to address the vanishing gradient problem in RNNs?,"Some approaches to address the vanishing gradient problem in RNNs include:
1. Using Long Short Term Memory RNNs (LSTMs) proposed by Hochreiter and Schmidhuber in 1997, which are designed to mitigate the vanishing gradient issue.
2. Implementing RNNs with separate memory components, such as LSTMs, which allow for better information preservation over many timesteps.
3. Creating more direct and linear pass-through connections in the model, such as attention mechanisms and residual connections, to help alleviate the vanishing gradient problem.

Source: On the difficulty of training recurrent neural networks Pascanu et al 2013. http://proceedings.mlr.press/v28/pascanu13.pdf","['Problems with RNNs Vanishing and Exploding Gradients 48Vanishing gradient intuition 49Vanishing gradient intuition chain rule 50Vanishing gradient intuition chain rule 51Vanishing gradient intuition chain rule 52Vanishing gradient intuition What happens if these are small Vanishing gradient problem When these are small the gradient signal gets smaller and smaller as it backpropagates further 53Vanishing gradient proof sketch linear case Recall What if were the identity function 53 Source On the difficulty of training recurrent neural networks Pascanu et al 2013. http proceedings.mlr.press v28 pascanu13.pdf and supplemental materials at http proceedings.mlr.press v28 pascanu13 supp.pdf If Wh is small then this term gets exponentially problematic as becomes large chain rule Consider the gradient of the loss on step with respect to the hidden state on some previous step . Let chain rule value of ONLY READ IF INTERESTEDVanishing gradient proof sketch linear case What s wrong with Consider if the eigenvalues of are all less than 1 We can write as a basis What about nonlinear activations i.e. what we use Pretty much the same thing except the proof requires for some dependent on dimensionality and Source On the difficulty of training recurrent neural networks Pascanu et al 2013. http proceedings.mlr.press v28 pascanu13.pdf and supplemental materials at http proceedings.mlr.press v28 pascanu13 supp.pdf 54 eigenvectors using the eigenvectors of Approaches 0 as grows so gradient vanishes sufficient but not necessary ONLY READ IF INTERESTEDWhy is vanishing gradient a problem Gradient signal from far away is lost because it s much smaller than gradient signal from close by. So model weights are updated only with respect to near effects not long term effects. 56Effect of vanishing gradient on RNN LM 57 LM task When she tried to print her tickets she found that the printer was out of toner. She went to the stationery store to buy more toner. It was very overpriced. After installing the toner into the printer she finally printed her To learn from this training example the RNN LM needs to model the dependency between tickets on the 7th step and the target word tickets at the end.', 'But if the gradient is small the model can t learn this dependency So the model is unable to predict similar long distance dependencies at test timeWhy is exploding gradient a problem If the gradient becomes too big then the SGD update step becomes too big learning rate gradient This can cause bad updates we take too large a step and reach a weird and bad parameter configuration with large loss You think you ve found a hill to climb but suddenly you re in Iowa In the worst case this will result in Inf or NaN in your network then you have to restart training from an earlier checkpoint 58Gradient clipping solution for exploding gradient 58 Gradient clipping if the norm of the gradient is greater than some threshold scale it down before applying SGD update Intuition take a step in the same direction but a smaller step In practice remembering to clip gradients is important but exploding gradients are an easy problem to solve Source On the difficulty of training recurrent neural networks Pascanu et al 2013. http proceedings.mlr.press v28 pascanu13.pdfHow to fix the vanishing gradient problem The main problem is that it s too difficult for the RNN to learn to preserve information over many timesteps. In a vanilla RNN the hidden state is constantly being rewritten First off next time How about an RNN with separate memory which is added to LSTMs And then Creating more direct and linear pass through connections in model Attention residual connections etc. 60Long Short Term Memory RNNs LSTMs A type of RNN proposed by Hochreiter and Schmidhuber in 1997 as a solution to the problem of vanishing gradients Everyone cites that paper but really a crucial part of the modern LSTM is from Gers et al. 2000 Only started to be recognized as promising through the work of S s student Alex Graves c. 2006 Work in which he also invented CTC connectionist temporal classification for speech recognition But only really became well known after Hinton brought it to Google in 2013 Following Graves having been a postdoc with Hinton 61 Hochreiter and Schmidhuber 1997. Long short term memory. https www.bioinf.jku.at publications older 2604.pdf Gers Schmidhuber and Cummins 2000. Learning to Forget Continual Prediction with LSTM.']",text-embedding-3-small,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.0000000000000009,1.0,0.872486515081448,0.0625
23,"What is perplexity, and how is it used in evaluating language models?","Perplexity is a measurement used in evaluating language models. It is the geometric mean of the probabilities assigned by the model to each word in a test set. A lower perplexity indicates that the model is more certain about its predictions and is better at predicting the test set. In other words, minimizing perplexity is equivalent to maximizing the test set probability according to the language model. Perplexity can be seen as a way to measure how well a language model predicts a given text or sequence of words. It is used to compare different language models, with lower perplexity values indicating better performance. However, it is important to note that perplexity alone does not guarantee improved performance in tasks like speech recognition or machine translation. It is commonly used as a quick check on the effectiveness of a language model, but any improvements in perplexity should be confirmed through end-to-end evaluations on real tasks before drawing conclusions about the model's overall performance.

Source: Lecture notes on N-gram Language Models.","['Here s the perplexity of W with a unigram language model just the geometric mean of the unigram probabilities perplexity W N v u u t N Y i 1 1 P wi 3.16 The perplexity of W computed with a bigram language model is still a geometric mean but now of the bigram probabilities perplexity W N v u u t N Y i 1 1 P wi wi 1 3.17 Note that because of the inverse in Eq. 3.15 the higher the conditional probabil ity of the word sequence the lower the perplexity. Thus minimizing perplexity is equivalent to maximizing the test set probability according to the language model. What we generally use for word sequence in Eq. 3.15 or Eq. 3.17 is the entire se quence of words in some test set. Since this sequence will cross many sentence boundaries we need to include the begin and end sentence markers s and s in the probability computation. We also need to include the end of sentence marker s but not the beginning of sentence marker s in the total count of word to kens N. There is another way to think about perplexity as the weighted average branch ing factor of a language. The branching factor of a language is the number of possi ble next words that can follow any word. Consider the task of recognizing the digits3.2 EVALUATING LANGUAGE MODELS 9 in English zero one two ... nine given that both in some training set and in some test set each of the 10 digits occurs with equal probability P 1 10. The perplexity of this mini language is in fact 10. To see that imagine a test string of digits of length N and assume that in the training set all the digits occurred with equal probability. By Eq. 3.15 the perplexity will be perplexity W P w1w2 ...wN 1 N 1 10 N 1 N 1 10 1 10 3.18 But suppose that the number zero is really frequent and occurs far more often than other numbers. Let s say that 0 occur 91 times in the training set and each of the other digits occurred 1 time each.', 'Any kind of knowledge of the test set can cause the perplexity to be artiÔ¨Åcially low. The perplexity of two language models is only comparable if they use identical vocabularies. An intrinsic improvement in perplexity does not guarantee an extrinsic im provement in the performance of a language processing task like speech recognition10 CHAPTER 3 N GRAM LANGUAGE MODELS or machine translation. Nonetheless because perplexity often correlates with such improvements it is commonly used as a quick check on an algorithm. But a model s improvement in perplexity should always be conÔ¨Årmed by an end to end evaluation of a real task before concluding the evaluation of the model. 3.3 Sampling sentences from a language model One important way to visualize what kind of knowledge a language model embodies is to sample from it. Sampling from a distribution means to choose random points sampling according to their likelihood. Thus sampling from a language model which rep resents a distribution over sentences means to generate some sentences choosing each sentence according to its likelihood as deÔ¨Åned by the model. Thus we are more likely to generate sentences that the model thinks have a high probability and less likely to generate sentences that the model thinks have a low probability. This technique of visualizing a language model by sampling was Ô¨Årst suggested very early on by Shannon 1951 and Miller and Selfridge 1950 . It s simplest to visualize how this works for the unigram case. Imagine all the words of the English language covering the probability space between 0 and 1 each word covering an interval proportional to its frequency. Fig. 3.3 shows a visualization using a unigram LM computed from the text of this book. We choose a random value between 0 and 1 Ô¨Ånd that point on the probability line and print the word whose interval includes this chosen value. We continue choosing random numbers and generating words until we randomly generate the sentence Ô¨Ånal token s .']",text-embedding-3-small,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9377451467611764,1.0,0.9463004537836776,0.0333333333333333
24,How does LSTM solve the vanishing gradient problem?,"LSTMs (Long Short Term Memory networks) were proposed as a solution to the vanishing gradient problem in RNNs. They address this issue by introducing a mechanism called ""gates"" that regulate the flow of information in and out of the memory cell. These gates consist of the input gate, forget gate, and output gate, which control the information to be stored, forgotten, or outputted respectively. By selectively retaining or updating information over time, LSTMs can better preserve long-term dependencies and mitigate the vanishing gradient problem compared to traditional RNNs.","['But if the gradient is small the model can t learn this dependency So the model is unable to predict similar long distance dependencies at test timeWhy is exploding gradient a problem If the gradient becomes too big then the SGD update step becomes too big learning rate gradient This can cause bad updates we take too large a step and reach a weird and bad parameter configuration with large loss You think you ve found a hill to climb but suddenly you re in Iowa In the worst case this will result in Inf or NaN in your network then you have to restart training from an earlier checkpoint 58Gradient clipping solution for exploding gradient 58 Gradient clipping if the norm of the gradient is greater than some threshold scale it down before applying SGD update Intuition take a step in the same direction but a smaller step In practice remembering to clip gradients is important but exploding gradients are an easy problem to solve Source On the difficulty of training recurrent neural networks Pascanu et al 2013. http proceedings.mlr.press v28 pascanu13.pdfHow to fix the vanishing gradient problem The main problem is that it s too difficult for the RNN to learn to preserve information over many timesteps. In a vanilla RNN the hidden state is constantly being rewritten First off next time How about an RNN with separate memory which is added to LSTMs And then Creating more direct and linear pass through connections in model Attention residual connections etc. 60Long Short Term Memory RNNs LSTMs A type of RNN proposed by Hochreiter and Schmidhuber in 1997 as a solution to the problem of vanishing gradients Everyone cites that paper but really a crucial part of the modern LSTM is from Gers et al. 2000 Only started to be recognized as promising through the work of S s student Alex Graves c. 2006 Work in which he also invented CTC connectionist temporal classification for speech recognition But only really became well known after Hinton brought it to Google in 2013 Following Graves having been a postdoc with Hinton 61 Hochreiter and Schmidhuber 1997. Long short term memory. https www.bioinf.jku.at publications older 2604.pdf Gers Schmidhuber and Cummins 2000. Learning to Forget Continual Prediction with LSTM.', 'Problems with RNNs Vanishing and Exploding Gradients 48Vanishing gradient intuition 49Vanishing gradient intuition chain rule 50Vanishing gradient intuition chain rule 51Vanishing gradient intuition chain rule 52Vanishing gradient intuition What happens if these are small Vanishing gradient problem When these are small the gradient signal gets smaller and smaller as it backpropagates further 53Vanishing gradient proof sketch linear case Recall What if were the identity function 53 Source On the difficulty of training recurrent neural networks Pascanu et al 2013. http proceedings.mlr.press v28 pascanu13.pdf and supplemental materials at http proceedings.mlr.press v28 pascanu13 supp.pdf If Wh is small then this term gets exponentially problematic as becomes large chain rule Consider the gradient of the loss on step with respect to the hidden state on some previous step . Let chain rule value of ONLY READ IF INTERESTEDVanishing gradient proof sketch linear case What s wrong with Consider if the eigenvalues of are all less than 1 We can write as a basis What about nonlinear activations i.e. what we use Pretty much the same thing except the proof requires for some dependent on dimensionality and Source On the difficulty of training recurrent neural networks Pascanu et al 2013. http proceedings.mlr.press v28 pascanu13.pdf and supplemental materials at http proceedings.mlr.press v28 pascanu13 supp.pdf 54 eigenvectors using the eigenvectors of Approaches 0 as grows so gradient vanishes sufficient but not necessary ONLY READ IF INTERESTEDWhy is vanishing gradient a problem Gradient signal from far away is lost because it s much smaller than gradient signal from close by. So model weights are updated only with respect to near effects not long term effects. 56Effect of vanishing gradient on RNN LM 57 LM task When she tried to print her tickets she found that the printer was out of toner. She went to the stationery store to buy more toner. It was very overpriced. After installing the toner into the printer she finally printed her To learn from this training example the RNN LM needs to model the dependency between tickets on the 7th step and the target word tickets at the end.']",text-embedding-3-small,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.8623146416943716,1.0,0.91999408173033,0.0625
25,"What is the impact of bidirectional RNNs, and where are they applicable?","Bidirectional RNNs have the capability to consider both past and future words in a sequence, allowing them to capture more context and dependencies in the data. They are applicable in tasks where information from both directions is crucial for making accurate predictions, such as machine translation where understanding the context of the entire sentence is important for accurate translation. Additionally, bidirectional RNNs are useful in sentiment analysis, named entity recognition, and speech recognition tasks where context from both directions is valuable for making predictions.","['h h h Figure 8 A bi directional RNN model h t f W xt V h t 1 b 17 h t f W xt V h t 1 b 18 ÀÜ yt g Uht c g U h t h t c 19 RNNs can also be multi layered. Figure 9 shows a multi layer bi directional RNN where each lower layer feeds the next layer. As shown in this Ô¨Ågure in this network architecture at time step t each interme diate neuron receives one set of parameters from the previous time step in the same RNN layer and two sets of parameters from the previous RNN hidden layer one input comes from the left to right RNN and the other from the right to left RNN. To construct a Deep RNN with L layers the above relationships are modiÔ¨Åed to the relationships in Equations 20 and 21 where the input to each intermediate neuron at level i is the output of the RNN at layer i 1 at the same time step t. The output ÀÜ y at each time step is the result of propagating input parameters through all hidden layers Equation 22 . Going Deep h i t f W i ht i 1 h i t f W i ht i 1 yt g U h t L h y h 3 x Each memory layer passes an intermediate seq representation to the next. h 2 h 1 Figure 9 A deep bi directional RNN with three RNN layers. h i t f W i h i 1 t V i h i t 1 b i 20 h i t f W i h i 1 t V i h i t 1 b i 21 ÀÜ yt g Uht c g U h L t h L t c 22 2.6 Application RNN Translation Model Traditional translation models are quite complex they consist of nu merous machine learning algorithms applied to different stages of the language translation pipeline. In this section we discuss the potential for adopting RNNs as a replacement to traditional translation mod ules. Consider the RNN example model shown in Figure 10 here the German phrase Echt dicke Kiste is translated to Awesome sauce.', 'av steps. 3. Dealing wi vanishing g 3.1. Previous so Using an L1 or L2 help with explodin ters initialized wit Wrec is probably that the gradient tion found in secti ensure that durin exceeds 1. This a ple regime with a where any informa Figure 7 Gradient explosion clipping visualization To solve the problem of vanishing gradients we introduce two tech niques. The Ô¨Årst technique is that instead of initializing W hh ran domly start off from an identity matrix initialization. The second technique is to use the RectiÔ¨Åed Linear Units ReLU in stead of the sigmoid function. The derivative for the ReLU is either 0 or 1. This way gradients would Ô¨Çow through the neurons whose deriva tive is 1 without getting attenuated while propagating back through time steps. 2.5 Deep Bidirectional RNNs So far we have focused on RNNs that condition on past words to predict the next word in the sequence. It is possible to make predic tions based on future words by having the RNN model read through the corpus backwards. Irsoy et al. shows a bi directional deep neu ral network at each time step t this network maintains two hidden layers one for the left to right propagation and another for the right to left propagation. To maintain two hidden layers at any time this network consumes twice as much memory space for its weight and bias parameters. The Ô¨Ånal classiÔ¨Åcation result ÀÜ yt is generated through combining the score results produced by both RNN hidden layers. Fig ure 8 shows the bi directional network architecture and Equations 17 and 18 show the mathematical formulation behind setting up the bi directional RNN hidden layer. The only difference between these two relationships is in the direction of recursing through the corpus. Equa tion 19 shows the classiÔ¨Åcation relationship used for predicting the next word via summarizing past and future word representations. Bidirectionality h t f W xt V h t f W xt V yt g U h t h t y h x now represents summarizes the past and around a single token.']",text-embedding-3-small,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9320110807255396,0.8,0.9457046679883138,0.0357142857142857
26,Describe the concept of multi-layer or stacked RNNs and their benefit.,"Multi-layer or stacked RNNs involve applying multiple RNNs on top of each other, allowing the network to compute more complex representations. The lower RNN layers compute lower-level features, while the higher RNN layers compute higher-level features. Stacked RNNs work better than having just one layer of high-dimensional encodings. High-performing RNNs are usually multi-layered, with 2 to 4 layers being optimal for tasks like Neural Machine Translation. Skip connections or dense connections are often used to train deeper RNNs effectively. Transformer-based networks like BERT are typically deeper, with 12 or 24 layers. Ultimately, multi-layer RNNs enable the network to capture more intricate patterns and relationships in the data, leading to improved performance in various NLP tasks.

Source: Lecture03_RNNs_and_LLMs","['You will learn more about transformers including BERT in a couple of weeksMulti layer RNNs RNNs are already deep on one dimension they unroll over many timesteps We can also make them deep in another dimension by applying multiple RNNs this is a multi layer RNN. This allows the network to compute more complex representations The lower RNNs should compute lower level features and the higher RNNs should compute higher level features. Multi layer RNNs are also called stacked RNNs. 80Multi layer RNNs was terribly exciting the movie RNN layer 1 RNN layer 2 81 RNN layer 3 The hidden states from RNN layer i are the inputs to RNN layer i 1Multi layer RNNs in practice 39 Multi layer or stacked RNNs allow a network to compute more complex representations they work better than just have one layer of high dimensional encodings The lower RNNs should compute lower level features and the higher RNNs should compute higher level features. High performing RNNs are usually multi layer but aren t as deep as convolutional or feed forward networks For example In a 2017 paper Britz et al. find that for Neural Machine Translation 2 to 4 layers is best for the encoder RNN and 4 layers is best for the decoder RNN Often 2 layers is a lot better than 1 and 3 might be a little better than 2 Usually skip connections dense connections are needed to train deeper RNNs e.g. 8 layers Transformer based networks e.g. BERT are usually deeper like 12 or 24 layers. You will learn about Transformers later they have a lot of skipping like connections Massive Exploration of Neural Machine Translation Architecutres Britz et al 2017. https arxiv.org pdf 1703.03906.pdfTerminology By the end of the course You will understand phrases like stacked bidirectional LSTMs with residual connections and self attention The RNN described in this lecture simple vanilla Elman RNN You learned about other RNN flavors like LSTM and GRU and multi layer RNNs 83', 'h h h Figure 8 A bi directional RNN model h t f W xt V h t 1 b 17 h t f W xt V h t 1 b 18 ÀÜ yt g Uht c g U h t h t c 19 RNNs can also be multi layered. Figure 9 shows a multi layer bi directional RNN where each lower layer feeds the next layer. As shown in this Ô¨Ågure in this network architecture at time step t each interme diate neuron receives one set of parameters from the previous time step in the same RNN layer and two sets of parameters from the previous RNN hidden layer one input comes from the left to right RNN and the other from the right to left RNN. To construct a Deep RNN with L layers the above relationships are modiÔ¨Åed to the relationships in Equations 20 and 21 where the input to each intermediate neuron at level i is the output of the RNN at layer i 1 at the same time step t. The output ÀÜ y at each time step is the result of propagating input parameters through all hidden layers Equation 22 . Going Deep h i t f W i ht i 1 h i t f W i ht i 1 yt g U h t L h y h 3 x Each memory layer passes an intermediate seq representation to the next. h 2 h 1 Figure 9 A deep bi directional RNN with three RNN layers. h i t f W i h i 1 t V i h i t 1 b i 20 h i t f W i h i 1 t V i h i t 1 b i 21 ÀÜ yt g Uht c g U h L t h L t c 22 2.6 Application RNN Translation Model Traditional translation models are quite complex they consist of nu merous machine learning algorithms applied to different stages of the language translation pipeline. In this section we discuss the potential for adopting RNNs as a replacement to traditional translation mod ules. Consider the RNN example model shown in Figure 10 here the German phrase Echt dicke Kiste is translated to Awesome sauce.']",text-embedding-3-small,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9358869366896716,1.0,0.9533673581167378,0.0588235294117647
27,How does Dropout work during training and testing?,"During training, Dropout works by randomly dropping a subset of neurons with a certain probability (1-p) during each forward and backward pass. This means that some neurons are set to zero while others are kept alive with a probability p. During testing, all weights are multiplied by 1/p, and no dropout is applied. This technique helps prevent overfitting by effectively training multiple smaller networks simultaneously and averaging their predictions, leading to more meaningful learning from the data and higher performance overall on the task at hand.

Source: lecture02_notes_Backpropagation.txt","['Imposing such a con straint can also be interpreted as the prior Bayesian belief that the optimal weights are close to zero how close depends on the value of Œª. Choosing the right value of Œª is critical and must be chosen via hyperparameter tuning. Too high a value of Œª causes most of the weights to be set too close to 0 and the model does not learn anything meaningful from the training data often obtaining poor ac curacy on training validation and testing sets. Too low a value and we fall into the domain of overÔ¨Åtting once again. It must be noted that the bias terms are not regularized and do not contribute to the cost term above try thinking about why this is the case There are indeed other types of regularization that are sometimes used such as L1 regularization which sums over the absolute values rather than squares of parameter elements however this is less commonly applied in practice since it leads to sparsity of parameter weights. In the next section we discuss dropout which effectively acts as another form of regularization by randomly dropping i.e. setting to zero neurons in the forward pass. 2.3 Dropout Dropout is a powerful technique for regularization Ô¨Årst introduced by Srivastava et al. in Dropout A Simple Way to Prevent Neural Net works from OverÔ¨Åtting. The idea is simple yet effective during train ing we will randomly drop with some probability 1 p a subset of neurons during each forward backward pass or equivalently we will keep alive each neuron with a probability p . Then during testing we will use the full network to compute our predictions. The result is that the network typically learns more meaningful informa tion from the data is less likely to overÔ¨Åt and usually obtains higher performance overall on the task at hand. One intuitive reason why this technique should be so effective is that what dropout is doing is essentially doing is training exponentially many smaller networks at once and averaging over their predictions. Dropout applied to an artiÔ¨Åcial neural network. Image credits to Srivastava et al. In practice the way we introduce dropout is that we take the out put h of each layer of neurons and keep each neuron with prob ability p and else set it to 0. Then during back propagation we only pass gradients through neurons that were kept alive during the forward pass.', 'like an ensemble model Nowadays usually thought of as strong feature dependent regularizer Wager Wang Liang 2013DropoutDropout Srivastava Hinton Krizhevsky Sutskever Salakhutdinov 2012 JMLR 2014 During training For each data point each time Randomly set input to 0 with probability ùëù dropout ratio often p 0.5 except p 0.15 for input layer via dropout mask During testing Multiply all weights by 1 ùëù No other dropout ùë•2 ùë•3 0 ùë§1 ùë§2 ùë§3 ùë§4 ùëè 1 ùë¶ ùë§1ùë•1 ùë§2ùë•2 ùë§3ùë•3 ùë• ùë• 1 1 Train 1 Train 2 Test ùë•3 0 1 ùë¶ ùë§1ùë•1 ùë§3ùë•3 ùëè ùë§1 ùë§2 ùë§3 ùë§4 ùëè ùë•1 0 ùë•2 ùë•3 ùë•4 ùë§1 ùë§2 ùë§3 ùë§4 ùëè 1 ùë¶ 1 ùëù ùë§1ùë•1 ùë§2ùë•2 ùë§3ùë•3 ùë§4ùë•4 6Vectorization E.g. looping over word vectors versus concatenating them all into one large matrix and then multiplying the softmax weights with that matrix for loop 1000 loops best of 3 639 ¬µs per loop Using single a C x N matrix 10000 loops best of 3 53.8 ¬µs per loop Matrices are awesome Always try to use vectors and matrices rather than for loops The speed gain goes from 1 to 2 orders of magnitude with GPUs 7Parameter Initialization You normally must initialize weights to small random values i.e. not zero matrices To avoid symmetries that prevent learning specialization Initialize hidden layer biases to 0 and output or reconstruction biases to optimal value if weights were 0 e.g.']",text-embedding-3-small,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9691847791905802,1.0,0.9276308254435122,0.375
28,What is the role of parameter initialization in neural networks?,"Parameter initialization in neural networks is a key step towards achieving optimal performance. A common strategy is to initialize the weights to small random numbers distributed around 0. This helps in preventing symmetries that hinder learning. Additionally, biases are typically initialized to 0 for hidden layers and to optimal values for output or reconstruction biases. Proper parameter initialization can lead to faster convergence, lower error rates, and help maintain activation and gradient variances across layers, ultimately aiding in the training dynamics of the neural network.

Source: Lecture02_SEP775_NLP","['This is useful since input features are often measured in different units but we often want to initially consider all features as equally important. The way we accomplish this is by simply dividing the features by their respective standard deviation calculated across the training set. Whitening Not as commonly used as mean subtraction normalization whiten ing essentially converts the data to a have an identity covariance matrix that is features become uncorrelated and have a variance of 1. This is done by Ô¨Årst mean subtracting the data as usual to get X . We can then take the Singular Value Decomposition SVD of X to get matrices U S V. We then compute UX to project X into the basis deÔ¨Åned by the columns of U. We Ô¨Ånally divide each dimension of the result by the corresponding singular value in S to scale our data appropriately if a singular value is zero we can just divide by a small number instead . 2.6 Parameter Initialization A key step towards achieving superlative performance with a neu ral network is initializing the parameters in a reasonable way. A good starting strategy is to initialize the weights to small random numbers normally distributed around 0 and in practice this often words acceptably well. However in Understanding the dif ficulty of training deep feedforward neural networks 2010 Xavier et al study the effect of different weight and bias initialization schemes on training dynamics. The empirical Ô¨Åndings suggest that for sigmoid and tanh activation units faster convergence and lower error rates are achieved when the weights of a matrix W Rn l 1 n l are initialized randomly with a uniform distribution as follows W U s 6 n l n l 1 s 6 n l n l 1 Where n l is the number of input units to W fan in and n l 1 is the number of output units from W fan out . In this parameter initialization scheme bias units are initialized to 0. This approach attempts to maintain activation variances as well as backpropagated gradient variances across layers. Without such initialization the gradient variances which are a proxy for information generally decrease with backpropagation across layers. 2.7 Learning Strategies The rate magnitude of model parameter updates during training can be controlled using the learning rate.', 'like an ensemble model Nowadays usually thought of as strong feature dependent regularizer Wager Wang Liang 2013DropoutDropout Srivastava Hinton Krizhevsky Sutskever Salakhutdinov 2012 JMLR 2014 During training For each data point each time Randomly set input to 0 with probability ùëù dropout ratio often p 0.5 except p 0.15 for input layer via dropout mask During testing Multiply all weights by 1 ùëù No other dropout ùë•2 ùë•3 0 ùë§1 ùë§2 ùë§3 ùë§4 ùëè 1 ùë¶ ùë§1ùë•1 ùë§2ùë•2 ùë§3ùë•3 ùë• ùë• 1 1 Train 1 Train 2 Test ùë•3 0 1 ùë¶ ùë§1ùë•1 ùë§3ùë•3 ùëè ùë§1 ùë§2 ùë§3 ùë§4 ùëè ùë•1 0 ùë•2 ùë•3 ùë•4 ùë§1 ùë§2 ùë§3 ùë§4 ùëè 1 ùë¶ 1 ùëù ùë§1ùë•1 ùë§2ùë•2 ùë§3ùë•3 ùë§4ùë•4 6Vectorization E.g. looping over word vectors versus concatenating them all into one large matrix and then multiplying the softmax weights with that matrix for loop 1000 loops best of 3 639 ¬µs per loop Using single a C x N matrix 10000 loops best of 3 53.8 ¬µs per loop Matrices are awesome Always try to use vectors and matrices rather than for loops The speed gain goes from 1 to 2 orders of magnitude with GPUs 7Parameter Initialization You normally must initialize weights to small random values i.e. not zero matrices To avoid symmetries that prevent learning specialization Initialize hidden layer biases to 0 and output or reconstruction biases to optimal value if weights were 0 e.g.']",text-embedding-3-small,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9778954036888198,1.0,0.933717291322853,0.0714285714285714
29,"What are the differences between Vanilla RNNs, LSTMs, and GRUs?","Vanilla RNNs, LSTMs, and GRUs are all types of recurrent neural networks, but they differ in their architectures and capabilities. 

1. Vanilla RNNs: Vanilla RNNs suffer from the vanishing gradient problem, which makes it difficult for them to capture long-term dependencies in sequences. They have a simple architecture where the hidden state is updated based on the current input and the previous hidden state. However, they struggle with retaining information over long sequences due to the vanishing gradient problem.

2. LSTMs (Long Short-Term Memory): LSTMs were introduced to address the vanishing gradient problem in Vanilla RNNs. They have a more complex architecture with three gates: input gate, forget gate, and output gate. These gates control the flow of information into and out of the cell state, allowing LSTMs to capture long-term dependencies more effectively than Vanilla RNNs.

3. GRUs (Gated Recurrent Units): GRUs are a variation of LSTMs that aim to simplify the architecture while retaining the ability to capture long-term dependencies. GRUs combine the forget and input gates into a single update gate and merge the cell state and hidden state. This simplification makes GRUs computationally more efficient than LSTMs while still being effective in capturing long-term dependencies.

In summary, Vanilla RNNs have a simple architecture but struggle with capturing long-term dependencies, LSTMs have a more complex architecture with specialized gates to address the vanishing gradient problem, and GRUs are a simplified version of LSTMs that are computationally efficient while still being effective in capturing long-term dependencies.

Source: SEP 775 - Computational Natural Language Processing, lecture notes and provided context.","['We only forget when we re going to input something in its place. We only input new values to the state when we forget something older. A slightly more dramatic variation on the LSTM is the Gated Recurrent Unit or GRU introduced by Cho et al. 2014 http arxiv.org pdf 1406.1078v3.pdf . It combines the forget and input gates into a single update gate. It also merges the cell state and hidden state and makes some other changes. The resulting model is simpler than standard LSTM models and has been growing increasingly popular. These are only a few of the most notable LSTM variants. There are lots of others like Depth Gated RNNs by Yao et al. 2015 http arxiv.org pdf 1508.03790v2.pdf . There s also some completely different approach to tackling long term dependencies like Clockwork RNNs by Koutnik et al. 2014 http arxiv.org pdf 1402.3511v1.pdf . Which of these variants is best Do the differences matter Greff et al. 2015 http arxiv.org pdf 1503.04069.pdf do a nice comparison of popular variants finding that they re all about the same. Jozefowicz et al. 2015 http jmlr.org proceedings papers v37 jozefowicz15.pdf tested more than ten thousand RNN architectures finding some that worked better than LSTMs on certain tasks. Conclusion Earlier I mentioned the remarkable results people are achieving with RNNs. Essentially all of these are achieved using LSTMs. They really work a lot better for most tasks Written down as a set of equations LSTMs look pretty intimidating. Hopefully walking through them step by step in this essay has made them a bit more approachable.1 24 24 3 23 PM Understanding LSTM Networks colah s blog 7 7 Attention and Augmented Recurrent Neural Networks On Distill Conv Nets A Modular Perspective Neural Networks Manifolds and Topology Deep Learning NLP and Representations LSTMs were a big step in what we can accomplish with RNNs.', 'Extension V given a word sequence A B C in German whose transla tion is X Y in English instead of training the RNN using A B C X Y train it using C B A X Y. The intutition behind this technique is that A is more likely to be translated to X. Thus given the vanishing gradient problem discussed earlier reversing the order of the input words can help reduce the error rate in generating the output phrase. 3 Gated Recurrent Units Beyond the extensions discussed so far RNNs have been found to per form better with the use of more complex units for activation. So far we have discussed methods that transition from hidden state ht 1 to ht using an afÔ¨Åne transformation and a point wise nonlinearity. Here we discuss the use of a gated activation function thereby modifying the RNN architecture. What motivates this Well although RNNs can theoretically capture long term dependencies they are very hard to actually train to do this. Gated recurrent units are designed in a manner to have more persistent memory thereby making it easier for RNNs to capture long term dependencies. Let us see mathematically how a GRU uses ht 1 and xt to generate the next hidden state ht. We will then dive into the intuition of this architecture. zt œÉ W z xt U z ht 1 Update gate rt œÉ W r xt U r ht 1 Reset gate ht tanh rt Uht 1 Wxt New memory ht 1 zt ht zt ht 1 Hidden state The above equations can be thought of a GRU s four fundamental op erational stages and they have intuitive interpretations that make this model much more intellectually satisfying see Figure 12 1. New memory generation A new memory ht is the consolidation of a new input word xt with the past hidden state ht 1. Anthropomor phically this stage is the one who knows the recipe of combining a newly observed word with the past hidden state ht 1 to summarize this new word in light of the contextual past as the vector ht. 2. Reset Gate The reset signal rt is responsible for determining how important ht 1 is to the summarization ht.']",text-embedding-3-small,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9635808261014066,1.0,0.9572725395704148,0.037037037037037
30,What is the difference between traditional language models and conditioned language models?,"Traditional language models are generative models of text, while conditioned language models generate text according to some specification or input. Conditioned language models take into account additional information or context when generating text, allowing for more controlled and specific text generation based on the given input or condition.

Source: SEP 775 - Computational Natural Language Processing, lecture04 Conditioned Generation.","['file name: lecture04 Conditioned Generation. Hamidreza Mahyar mahyarh@mcmaster.ca Computational Natural Language Processing Conditioned GenerationLanguage Models Language models are generative models of text s P x The Malfoys said Hermione. Harry was watching him. He looked like Madame Maxime. When she strode up the wrong staircase to visit himself. I m afraid I ve definitely been suspended from power no chance indeed said Snape. He put his head back behind them and read groups as they crossed a corner and fluttered down onto their ink lamp and picked up his spoon. The doorbell rang. It was a lot cleaner down in London. Text Credit Max Deutsch https medium.com deep writingConditioned Language Models Not just generate text generate text according to some specification Input X Structured Data English Document Utterance Image Speech Output Y Text NL Description Japanese Short Description Response Text Transcript Task NL Generation Translation Summarization Response Generation Image Captioning Speech RecognitionFormulation and ModelingCalculating the Probability of a SentenceConditional Language ModelsOne Type of Language Model LSTM LSTM LSTM LSTM predict hate predict this predict movie predict s LSTM Mikolov et al. 2011 s I hate this movie predict ILSTM LSTM LSTM LSTM LSTM s LSTM LSTM LSTM LSTM argmax argmax argmax argmax argmax s One Type of Conditional Language Model I hate this movie kono eiga ga kirai hate this movie Sutskever et al. 2014 Encoder I DecoderHow to Pass Hidden State Initialize decoder w encoder Sutskever et al. 2014 encoder decoder Transform can be different dimensions encoder transform decoder encoder Input at every time step Kalchbrenner Blunsom 2013 decoder decoder decoderMethods of GenerationThe Generation Problem We have a model of P Y X how do we use it to generate a sentence Two methods Sampling Try to generate a random sentence according to the probability distribution. Argmax Try to generate the sentence with the highest probability.Ancestral Sampling Randomly generate words one by one.', 'For instance HLW 20 observe that larger models do not necessarily generalize better out of distribution. There is evidence that suggests that the generalization achieved under this paradigm can be poor because the model is overly speciÔ¨Åc to the training distribution and does not generalize well outside it YdC 19 MPL19 . Thus the performance of Ô¨Åne tuned models on speciÔ¨Åc benchmarks even when it is nominally at human level may exaggerate actual performance on the underlying task GSL 18 NK19 . Third humans do not require large supervised datasets to learn most language tasks a brief directive in natural language e.g. please tell me if this sentence describes something happy or something sad or at most a tiny number of demonstrations e.g. here are two examples of people acting brave please give a third example of bravery is often Figure 1.1 Language model meta learning. During unsupervised pre training a language model develops a broad set of skills and pattern recognition abilities. It then uses these abilities at inference time to rapidly adapt to or recognize the desired task. We use the term in context learning to describe the inner loop of this process which occurs within the forward pass upon each sequence. The sequences in this diagram are not intended to be representative of the data a model would see during pre training but are intended to show that there are sometimes repeated sub tasks embedded within a single sequence. 3Figure 1.2 Larger models make increasingly efÔ¨Åcient use of in context information. We show in context learning performance on a simple task requiring the model to remove random symbols from a word both with and without a natural language task description see Sec. 3.9.2 . The steeper in context learning curves for large models demonstrate improved ability to learn a task from contextual information. We see qualitatively similar behavior across a wide range of tasks. sufÔ¨Åcient to enable a human to perform a new task to at least a reasonable degree of competence. Aside from pointing to a conceptual limitation in our current NLP techniques this adaptability has practical advantages it allows humans to seamlessly mix together or switch between many tasks and skills for example performing addition during a lengthy dialogue.']",text-embedding-3-small,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9701408609702442,1.0,0.9629038673321232,0.0
31,How can LSTM networks be used in language modeling?,"LSTM networks can be used in language modeling to predict the next word in a sequence. They have been successful in tasks such as handwriting recognition, speech recognition, machine translation, parsing, image captioning, and generating language models. LSTMs have become the dominant approach for most NLP tasks due to their ability to handle sequential data effectively and capture long-term dependencies in the data. Additionally, LSTMs have been used to build language models that help in measuring progress on predicting language use and are a subcomponent of many NLP tasks involving generating text or estimating the probability of text.","['are hard to train Another solution lots of new deep feedforward convolutional architectures add more direct connections thus allowing the gradient to flow Deep Residual Learning for Image Recognition He et al 2015. https arxiv.org pdf 1512.03385.pdfLSTMs real world success 68 In 2013 2015 LSTMs started achieving state of the art results Successful tasks include handwriting recognition speech recognition machine translation parsing and image captioning as well as language models LSTMs became the dominant approach for most NLP tasks Now 2019 2023 Transformers have become dominant for all tasks For example in WMT a Machine Translation conference competition In WMT 2014 there were 0 neural machine translation systems In WMT 2016 the summary report contains RNN 44 times and these systems won In WMT 2019 RNN 7 times Transformer 105 times Source Findings of the 2016 Conference on Machine Translation WMT16 Bojar et al. 2016 http www.statmt.org wmt16 pdf W16 2301.pdf Source Findings of the 2018 Conference on Machine Translation WMT18 Bojar et al. 2018 http www.statmt.org wmt18 pdf WMT028.pdf Source Findings of the 2019 Conference on Machine Translation WMT19 Barrault et al. 2019 http www.statmt.org wmt18 pdf WMT028.pdf5. Recap 69 Language Model A system that predicts the next word Recurrent Neural Network A family of neural networks that Take sequential input of any length Apply the same weights on each step Can optionally produce output on each step Recurrent Neural Network Language Model We ve shown that RNNs are a great way to build a LM despite some problems RNNs are also useful for much moreWhy should we care about Language Modeling 70 Language Modeling is a benchmark task that helps us measure our progress on predicting language use Language Modeling is a subcomponent of many NLP tasks especially those involving generating text or estimating the probability of text Predictive typing Speech recognition Handwriting recognition Spelling grammar correction Authorship identification Machine translation Summarization Dialogue etc.', 'Everything else in NLP has now been rebuilt upon Language Modeling GPT 3 is an LMOther RNN ArchitecturesOther RNN uses RNNs can be used for sequence tagging e.g. part of speech tagging named entity recognition knocked over the vase the startled cat 72 VBN IN DT NN DT JJ NNthe movie a lot overall I enjoyed positive Sentence encoding 73 How to compute sentence encoding RNNs can be used for sentence classification e.g. sentiment classificationthe movie a lot overall I enjoyed positive Sentence encoding 74 equals How to compute sentence encoding Basic way Use final hidden state RNNs can be used for sentence classification e.g. sentiment classificationRNNs can be used for sentence classification the movie a lot overall I enjoyed positive Sentence encoding How to compute sentence encoding 75 Usually better Take element wise max or mean of all hidden states e.g. sentiment classificationRNNs can be used as an encoder module e.g. question answering machine translation many other tasks Context Ludwig van Beethoven was a German composer and pianist. A crucial figure Beethoven Question what nationality was Here the RNN acts as an encoder for the Question the hidden states represent the Question . The encoder is part of a larger neural system. Answer German 76 lots of neural architecture lots of neural architectureRNN LMs can be used to generate text e.g. speech recognition machine translation summarization RNN LM what s the the weather what s This is an example of a conditional language model. We ll see Machine Translation in much more detail next lectures. Input audio START conditioning 77Bidirectional RNNs was terribly exciting the movie Forward RNN Backward RNN Concatenated hidden states This contextual representation of terribly has both left and right context 78Bidirectional RNNs 79 Note bidirectional RNNs are only applicable if you have access to the entire input sequence They are not applicable to Language Modeling because in LM you only have left context available. If you do have entire input sequence e.g. any kind of encoding bidirectionality is powerful you should use it by default . For example BERT Bidirectional Encoder Representations from Transformers is a powerful pretrained contextual representation system built on bidirectionality.']",text-embedding-3-small,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.8908874928551955,0.6,0.949869518966776,0.0909090909090909
32,What are the methods of generation in conditioned language modeling?,"The methods of generation in conditioned language modeling are Sampling and Argmax. Sampling involves trying to generate a random sentence according to the probability distribution, while Argmax involves trying to generate the sentence with the highest probability.","['file name: lecture04 Conditioned Generation. Hamidreza Mahyar mahyarh@mcmaster.ca Computational Natural Language Processing Conditioned GenerationLanguage Models Language models are generative models of text s P x The Malfoys said Hermione. Harry was watching him. He looked like Madame Maxime. When she strode up the wrong staircase to visit himself. I m afraid I ve definitely been suspended from power no chance indeed said Snape. He put his head back behind them and read groups as they crossed a corner and fluttered down onto their ink lamp and picked up his spoon. The doorbell rang. It was a lot cleaner down in London. Text Credit Max Deutsch https medium.com deep writingConditioned Language Models Not just generate text generate text according to some specification Input X Structured Data English Document Utterance Image Speech Output Y Text NL Description Japanese Short Description Response Text Transcript Task NL Generation Translation Summarization Response Generation Image Captioning Speech RecognitionFormulation and ModelingCalculating the Probability of a SentenceConditional Language ModelsOne Type of Language Model LSTM LSTM LSTM LSTM predict hate predict this predict movie predict s LSTM Mikolov et al. 2011 s I hate this movie predict ILSTM LSTM LSTM LSTM LSTM s LSTM LSTM LSTM LSTM argmax argmax argmax argmax argmax s One Type of Conditional Language Model I hate this movie kono eiga ga kirai hate this movie Sutskever et al. 2014 Encoder I DecoderHow to Pass Hidden State Initialize decoder w encoder Sutskever et al. 2014 encoder decoder Transform can be different dimensions encoder transform decoder encoder Input at every time step Kalchbrenner Blunsom 2013 decoder decoder decoderMethods of GenerationThe Generation Problem We have a model of P Y X how do we use it to generate a sentence Two methods Sampling Try to generate a random sentence according to the probability distribution. Argmax Try to generate the sentence with the highest probability.Ancestral Sampling Randomly generate words one by one.', 'An exact method for sampling from P X no further work needed.Greedy Search One by one pick the single highest probability word Not exact real problems Will often generate the easy words first Will prefer multiple common words to one rare wordBeam Search Instead of picking one high probability word maintain several paths Some in reading materials more in a later classModel EnsemblingEnsembling Why Multiple models make somewhat uncorrelated errors Models tend to be more uncertain when they are about to make errors Smooths over idiosyncrasies of the model LSTM1 predict1 I LSTM2 predict2 Combine predictions from multiple models s sLinear Interpolation Take a weighted average of the M model probabilities Second term often set to uniform distribution 1 MLog linear Interpolation Weighted combination of log probabilities normalize Interpolation coefficient often set to uniform distribution 1 MLinear or Log Linear Think of it in logic Linear Logical OR the interpolated model likes any choice that a model gives a high probability use models with models that capture different traits necessary when any model can assign zero probability Log Linear Logical AND interpolated model only likes choices where all models agree use when you want to restrict possible answersParameter Averaging Problem Ensembling means we have to use M models at test time increasing our time memory complexity Parameter averaging is a cheap way to get some good effects of ensembling Basically write out models several times near the end of training and take the average of parametersEnsemble Distillation e.g. Kim et al. 2016 Problem parameter averaging only works for models within the same run Knowledge distillation trains a model to copy the ensemble Specifically it tries to match the description over predicted words Why We want the model to make the same mistakes as an ensemble Shown to increase accuracy notablyStacking What if we have two very different models where prediction of outputs is done in very different ways e.g. a phrase based translation model and a neural MT model Niehues et al. 2017 Stacking uses the output of one system in calculating features for another systemCase Studies in Conditional Language ModelingFrom Images e.g. Karpathy et al. 2015 Input is image features output is text Use standard image encoders e.g.']",text-embedding-3-small,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.0000000000000002,1.0,0.9533169432150654,0.0833333333333333
33,What is beam search and how is it used in language generation?,"Beam search is a decoding algorithm used in natural language processing for generating sequences like sentences. It is an alternative to greedy decoding, where instead of picking the single highest probability word at each step, beam search maintains several paths or hypotheses. On each step of the decoder, beam search keeps track of the k most probable partial translations (hypotheses), where k is the beam size (usually around 5 to 10 in practice). Each hypothesis has a score based on its log probability, and the algorithm searches for high-scoring hypotheses by tracking the top k on each step. While beam search is not guaranteed to find the optimal solution, it is much more efficient than exhaustive search, where all possible sequences are computed, making it a popular choice for language generation tasks.

Source: lecture05_Seq2Seq","['2014 Luong et al. 2015 The hidden states from RNN layer i are the inputs to RNN layer i 1Decoding Greedy decoding We saw how to generate or decode the target sentence by taking argmax on each step of the decoder he hit me with a pie END START he hit me with a pie This is greedy decoding take most probable word on each step argmax argmax argmax argmax 15 argmax argmax argmaxProblems with greedy decoding 16 Greedy decoding has no way to undo decisions Input il a m entart√© he he hit he hit a he hit me with a pie whoops no going back now How to fix thisExhaustive search decoding Ideally we want to find a length T translation y that maximizes We could try computing all possible sequences y This means that on each step t of the decoder we re tracking Vt possible partial translations where V is vocab size This O VT complexity is far too expensive 17Beam search decoding Core idea On each step of decoder keep track of the k most probable partial translations which we call hypotheses k is the beam size in practice around 5 to 10 in NMT A hypothesis has a score which is its log probability Scores are all negative and higher score is better We search for high scoring hypotheses tracking top k on each step Beam search is not guaranteed to find optimal solution But much more efficient than exhaustive search 18Beam search decoding example Beam size k 2. Blue numbers START Calculate prob dist of next word 19Beam search decoding example Beam size k 2. Blue numbers START he I 0.7 log PLM he START 0.9 log PLM I START Take top k words and compute scores 20Beam search decoding example Beam size k 2. Blue numbers hit struck was got START he I 10 0.7 0.9 2.9 log PLM struck START he 0.7 1.6 log PLM was START I 0.9 1.8 log PLM got START I 0.9 1.7 log PLM hit START he 0.7 For each of the k hypotheses find top k next words and calculate scoresBeam search decoding example Beam size k 2.', 'An exact method for sampling from P X no further work needed.Greedy Search One by one pick the single highest probability word Not exact real problems Will often generate the easy words first Will prefer multiple common words to one rare wordBeam Search Instead of picking one high probability word maintain several paths Some in reading materials more in a later classModel EnsemblingEnsembling Why Multiple models make somewhat uncorrelated errors Models tend to be more uncertain when they are about to make errors Smooths over idiosyncrasies of the model LSTM1 predict1 I LSTM2 predict2 Combine predictions from multiple models s sLinear Interpolation Take a weighted average of the M model probabilities Second term often set to uniform distribution 1 MLog linear Interpolation Weighted combination of log probabilities normalize Interpolation coefficient often set to uniform distribution 1 MLinear or Log Linear Think of it in logic Linear Logical OR the interpolated model likes any choice that a model gives a high probability use models with models that capture different traits necessary when any model can assign zero probability Log Linear Logical AND interpolated model only likes choices where all models agree use when you want to restrict possible answersParameter Averaging Problem Ensembling means we have to use M models at test time increasing our time memory complexity Parameter averaging is a cheap way to get some good effects of ensembling Basically write out models several times near the end of training and take the average of parametersEnsemble Distillation e.g. Kim et al. 2016 Problem parameter averaging only works for models within the same run Knowledge distillation trains a model to copy the ensemble Specifically it tries to match the description over predicted words Why We want the model to make the same mistakes as an ensemble Shown to increase accuracy notablyStacking What if we have two very different models where prediction of outputs is done in very different ways e.g. a phrase based translation model and a neural MT model Niehues et al. 2017 Stacking uses the output of one system in calculating features for another systemCase Studies in Conditional Language ModelingFrom Images e.g. Karpathy et al. 2015 Input is image features output is text Use standard image encoders e.g.']",text-embedding-3-small,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9218209045611296,0.8888888888888888,0.9149858996716024,1.0
34,Explain the concept of model ensembling in language models.,"Model ensembling in language models involves combining predictions from multiple models to improve overall performance. By using multiple models that make somewhat uncorrelated errors, the ensemble can reduce uncertainty and smooth over idiosyncrasies of individual models. This approach can be beneficial when different models capture different traits or when there is a need to handle zero probabilities effectively. Techniques such as parameter averaging and ensemble distillation are used to leverage the strengths of multiple models during testing or inference. Overall, model ensembling can enhance the accuracy and robustness of language models by leveraging diverse sources of information and expertise.

Source: SEP 775 - Computational Natural Language Processing, lecture notes.","['An exact method for sampling from P X no further work needed.Greedy Search One by one pick the single highest probability word Not exact real problems Will often generate the easy words first Will prefer multiple common words to one rare wordBeam Search Instead of picking one high probability word maintain several paths Some in reading materials more in a later classModel EnsemblingEnsembling Why Multiple models make somewhat uncorrelated errors Models tend to be more uncertain when they are about to make errors Smooths over idiosyncrasies of the model LSTM1 predict1 I LSTM2 predict2 Combine predictions from multiple models s sLinear Interpolation Take a weighted average of the M model probabilities Second term often set to uniform distribution 1 MLog linear Interpolation Weighted combination of log probabilities normalize Interpolation coefficient often set to uniform distribution 1 MLinear or Log Linear Think of it in logic Linear Logical OR the interpolated model likes any choice that a model gives a high probability use models with models that capture different traits necessary when any model can assign zero probability Log Linear Logical AND interpolated model only likes choices where all models agree use when you want to restrict possible answersParameter Averaging Problem Ensembling means we have to use M models at test time increasing our time memory complexity Parameter averaging is a cheap way to get some good effects of ensembling Basically write out models several times near the end of training and take the average of parametersEnsemble Distillation e.g. Kim et al. 2016 Problem parameter averaging only works for models within the same run Knowledge distillation trains a model to copy the ensemble Specifically it tries to match the description over predicted words Why We want the model to make the same mistakes as an ensemble Shown to increase accuracy notablyStacking What if we have two very different models where prediction of outputs is done in very different ways e.g. a phrase based translation model and a neural MT model Niehues et al. 2017 Stacking uses the output of one system in calculating features for another systemCase Studies in Conditional Language ModelingFrom Images e.g. Karpathy et al. 2015 Input is image features output is text Use standard image encoders e.g.', 'For instance HLW 20 observe that larger models do not necessarily generalize better out of distribution. There is evidence that suggests that the generalization achieved under this paradigm can be poor because the model is overly speciÔ¨Åc to the training distribution and does not generalize well outside it YdC 19 MPL19 . Thus the performance of Ô¨Åne tuned models on speciÔ¨Åc benchmarks even when it is nominally at human level may exaggerate actual performance on the underlying task GSL 18 NK19 . Third humans do not require large supervised datasets to learn most language tasks a brief directive in natural language e.g. please tell me if this sentence describes something happy or something sad or at most a tiny number of demonstrations e.g. here are two examples of people acting brave please give a third example of bravery is often Figure 1.1 Language model meta learning. During unsupervised pre training a language model develops a broad set of skills and pattern recognition abilities. It then uses these abilities at inference time to rapidly adapt to or recognize the desired task. We use the term in context learning to describe the inner loop of this process which occurs within the forward pass upon each sequence. The sequences in this diagram are not intended to be representative of the data a model would see during pre training but are intended to show that there are sometimes repeated sub tasks embedded within a single sequence. 3Figure 1.2 Larger models make increasingly efÔ¨Åcient use of in context information. We show in context learning performance on a simple task requiring the model to remove random symbols from a word both with and without a natural language task description see Sec. 3.9.2 . The steeper in context learning curves for large models demonstrate improved ability to learn a task from contextual information. We see qualitatively similar behavior across a wide range of tasks. sufÔ¨Åcient to enable a human to perform a new task to at least a reasonable degree of competence. Aside from pointing to a conceptual limitation in our current NLP techniques this adaptability has practical advantages it allows humans to seamlessly mix together or switch between many tasks and skills for example performing addition during a lengthy dialogue.']",text-embedding-3-small,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9687264226750372,1.0,0.933090753944826,0.0666666666666666
35,How does ensemble distillation differ from parameter averaging?,"Ensemble distillation differs from parameter averaging in that while parameter averaging involves taking the average of parameters from multiple models near the end of training to achieve some benefits of ensembling, ensemble distillation involves training a model to copy the ensemble by matching the distribution over predicted words. This is done to make the model make the same mistakes as an ensemble, ultimately increasing accuracy.","['An exact method for sampling from P X no further work needed.Greedy Search One by one pick the single highest probability word Not exact real problems Will often generate the easy words first Will prefer multiple common words to one rare wordBeam Search Instead of picking one high probability word maintain several paths Some in reading materials more in a later classModel EnsemblingEnsembling Why Multiple models make somewhat uncorrelated errors Models tend to be more uncertain when they are about to make errors Smooths over idiosyncrasies of the model LSTM1 predict1 I LSTM2 predict2 Combine predictions from multiple models s sLinear Interpolation Take a weighted average of the M model probabilities Second term often set to uniform distribution 1 MLog linear Interpolation Weighted combination of log probabilities normalize Interpolation coefficient often set to uniform distribution 1 MLinear or Log Linear Think of it in logic Linear Logical OR the interpolated model likes any choice that a model gives a high probability use models with models that capture different traits necessary when any model can assign zero probability Log Linear Logical AND interpolated model only likes choices where all models agree use when you want to restrict possible answersParameter Averaging Problem Ensembling means we have to use M models at test time increasing our time memory complexity Parameter averaging is a cheap way to get some good effects of ensembling Basically write out models several times near the end of training and take the average of parametersEnsemble Distillation e.g. Kim et al. 2016 Problem parameter averaging only works for models within the same run Knowledge distillation trains a model to copy the ensemble Specifically it tries to match the description over predicted words Why We want the model to make the same mistakes as an ensemble Shown to increase accuracy notablyStacking What if we have two very different models where prediction of outputs is done in very different ways e.g. a phrase based translation model and a neural MT model Niehues et al. 2017 Stacking uses the output of one system in calculating features for another systemCase Studies in Conditional Language ModelingFrom Images e.g. Karpathy et al. 2015 Input is image features output is text Use standard image encoders e.g.', 'All ensembles have at least 3 workers. TL DR 6.7B RL T 0.7 labeler labeler ensem ble length copying ROUGE 1.3B sup logprob 1.3B RM 6.7B sup logprob 6.7B RM labeler 60.4 5.9 66.0 7.6 55.8 2.2 52.7 2.1 49.9 2.1 48.0 2.2 57.4 2.0 47.3 2.2 62.3 2.1 labeler ensemble 66.0 7.6 80.0 8.9 65.0 10.6 35.0 10.5 45.0 11.1 75.0 9.8 40.0 10.5 75.0 9.8 length 55.8 2.2 80.0 8.9 48.1 2.2 50.3 2.2 30.0 2.1 62.0 2.1 30.4 2.0 59.8 2.2 copying 52.7 2.1 65.0 10.6 48.1 2.2 52.0 2.2 64.2 2.1 56.7 2.2 64.4 2.1 53.4 2.2 ROUGE 49.9 2.1 35.0 10.5 50.3 2.2 52.0 2.2 50.5 2.2 52.0 2.3 51.1 2.3 54.5 2.1 1.3B sup.']",text-embedding-3-small,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.987278043698859,1.0,0.9720746021728476,0.3333333333333333
36,What are some common evaluation metrics for conditioned language models?,"Some common evaluation metrics for conditioned language models include BLEU, embedding-based metrics such as BertScore, BLEURT, COMET, PRISM, and Perplexity. Additionally, meta-evaluation, which involves running human evaluation and automatic evaluation on the same outputs and calculating correlation, is another approach used in evaluating conditioned language models.","['2015 Input is image features output is text Use standard image encoders e.g. CNN Often pre trained on large databases such as ImageNetCheck out this review paperHow do we EvaluateBasic Evaluation Paradigm Use parallel test set Use system to generate translations Compare target translations w referenceHuman Evaluation Ask a human to do evaluation Final goal but slow expensive and sometimes inconsistentBLEU Works by comparing n gram overlap w reference Pros Easy to use good for measuring system improvement Cons Often doesn t match human eval bad for comparing very different systemsEmbedding based Metrics Recently many metrics based on neural models BertScore Find similarity between BERT embeddings unsupervised Zhang et al. 2020 BLEURT Train BERT to predict human evaluation scores Sellam et al. 2020 COMET Train model to predict human eval also using source sentence Rei et al. 2020 PRISM Model based on training paraphrasing model Thompson and Post 2020Perplexity Calculate the perplexity of the words in the held out set without doing generation Pros Naturally solves multiple reference problem Cons Doesn t consider decoding or actually generating output. May be reasonable for problems with lots of ambiguity.Which One to Use Meta evaluation runs human evaluation and automatic evaluation on the same outputs calculates correlation Examples WMT Metrics Task for MT Mathur et al. 2021 RealSumm for summarization Evaluation is hard especially with good systems Most metrics had no correlation w human eval over best systems at some WMT 2019 tasksQuestions', 'For this graph we also include 6 additional extra small models with as few as 100 000 parameters. As observed in KMH 20 language modeling performance follows a power law when making efÔ¨Åcient use of training compute. After extending this trend by two more orders of magnitude we observe only a slight if any departure from the power law. One might worry that these improvements in cross entropy loss come only from modeling spurious details of our training corpus. However we will see in the following sections that improvements in cross entropy loss lead to consistent performance gains across a broad spectrum of natural language tasks. Below we evaluate the 8 models described in Section 2 the 175 billion parameter parameter GPT 3 and 7 smaller models on a wide range of datasets. We group the datasets into 9 categories representing roughly similar tasks. In Section 3.1 we evaluate on traditional language modeling tasks and tasks that are similar to language modeling such as Cloze tasks and sentence paragraph completion tasks. In Section 3.2 we evaluate on closed book question answering tasks tasks which require using the information stored in the model s parameters to answer general knowledge questions. In Section 3.3 we evaluate the model s ability to translate between languages especially one shot and few shot . In Section 3.4 we evaluate the model s performance on Winograd Schema like tasks. In Section 3.5 we evaluate on datasets that involve commonsense reasoning or question answering. In Section 3.6 we evaluate on reading comprehension tasks in Section 3.7 we evaluate on the SuperGLUE benchmark suite and in 3.8 we brieÔ¨Çy explore NLI. Finally in Section 3.9 we invent some additional tasks designed especially to probe in context learning abilities these tasks focus on on the Ô¨Çy reasoning adaptation skills or open ended text synthesis. We evaluate all tasks in the few shot one shot and zero shot settings. 10Figure 3.1 Smooth scaling of performance with compute. Performance measured in terms of cross entropy validation loss follows a power law trend with the amount of compute used for training. The power law behavior observed in KMH 20 continues for an additional two orders of magnitude with only small deviations from the predicted curve.']",text-embedding-3-small,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.0000000000000002,0.7142857142857143,0.9231531346029664,0.05
37,What is the significance of stacking in model ensembling?,"Stacking is significant in model ensembling when we have two very different models that predict outputs in different ways. It involves using the output of one system to calculate features for another system. This approach can help improve the overall performance by leveraging the strengths of each individual model and combining their predictions effectively. By incorporating diverse models through stacking, we can potentially enhance the accuracy and robustness of the ensemble model.","['An exact method for sampling from P X no further work needed.Greedy Search One by one pick the single highest probability word Not exact real problems Will often generate the easy words first Will prefer multiple common words to one rare wordBeam Search Instead of picking one high probability word maintain several paths Some in reading materials more in a later classModel EnsemblingEnsembling Why Multiple models make somewhat uncorrelated errors Models tend to be more uncertain when they are about to make errors Smooths over idiosyncrasies of the model LSTM1 predict1 I LSTM2 predict2 Combine predictions from multiple models s sLinear Interpolation Take a weighted average of the M model probabilities Second term often set to uniform distribution 1 MLog linear Interpolation Weighted combination of log probabilities normalize Interpolation coefficient often set to uniform distribution 1 MLinear or Log Linear Think of it in logic Linear Logical OR the interpolated model likes any choice that a model gives a high probability use models with models that capture different traits necessary when any model can assign zero probability Log Linear Logical AND interpolated model only likes choices where all models agree use when you want to restrict possible answersParameter Averaging Problem Ensembling means we have to use M models at test time increasing our time memory complexity Parameter averaging is a cheap way to get some good effects of ensembling Basically write out models several times near the end of training and take the average of parametersEnsemble Distillation e.g. Kim et al. 2016 Problem parameter averaging only works for models within the same run Knowledge distillation trains a model to copy the ensemble Specifically it tries to match the description over predicted words Why We want the model to make the same mistakes as an ensemble Shown to increase accuracy notablyStacking What if we have two very different models where prediction of outputs is done in very different ways e.g. a phrase based translation model and a neural MT model Niehues et al. 2017 Stacking uses the output of one system in calculating features for another systemCase Studies in Conditional Language ModelingFrom Images e.g. Karpathy et al. 2015 Input is image features output is text Use standard image encoders e.g.', 'All ensembles have at least 3 workers. TL DR 6.7B RL T 0.7 labeler labeler ensem ble length copying ROUGE 1.3B sup logprob 1.3B RM 6.7B sup logprob 6.7B RM labeler 60.4 5.9 66.0 7.6 55.8 2.2 52.7 2.1 49.9 2.1 48.0 2.2 57.4 2.0 47.3 2.2 62.3 2.1 labeler ensemble 66.0 7.6 80.0 8.9 65.0 10.6 35.0 10.5 45.0 11.1 75.0 9.8 40.0 10.5 75.0 9.8 length 55.8 2.2 80.0 8.9 48.1 2.2 50.3 2.2 30.0 2.1 62.0 2.1 30.4 2.0 59.8 2.2 copying 52.7 2.1 65.0 10.6 48.1 2.2 52.0 2.2 64.2 2.1 56.7 2.2 64.4 2.1 53.4 2.2 ROUGE 49.9 2.1 35.0 10.5 50.3 2.2 52.0 2.2 50.5 2.2 52.0 2.3 51.1 2.3 54.5 2.1 1.3B sup.']",text-embedding-3-small,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9609509521143909,1.0,0.9346559262142746,0.3333333333333333
38,How are images used as input for conditioned language models?,Images can be used as input for conditioned language models by converting the image into a textual description or embedding that can be fed into the model alongside the text input. This allows the conditioned language model to generate text based on both the textual input and the visual information extracted from the image.,"['While the effective receptive Ô¨Åeld size in this Ô¨Ågure is the same for both schemes in 2D attention the memory block contains a more evenly balanced number of pixels next to and above the query block respectively. dence across channels Salimans et al. . For each pixel the number of parameters is 10 times the number of mixture components 10 for one unnormalized mixture probability three means three standard deviations and three coefÔ¨Åcients which capture the linear dependence. For 10 mixtures this translates to 100 parameters for each pixel for 32 32 im ages the network outputs 102 400 dimensions which is a 7x reduction enabling denser gradients and lower memory. 4. Inference Across all of the presented experiments we use categorical sampling during decoding with a tempered softmax Dahl et al. 2017 . We adjust the concentration of the distribution we sample from with a temperature œÑ 0 by which we divide the logits for the channel intensities. We tuned œÑ between 0.8 and 1.0 observing the highest perceptual quality in unconditioned and class conditional image generation with œÑ 1.0. For super resolution we present results for different temperatures in Table 5. 5. Experiments All code we used to develop train and evaluate our models is available in Tensor2Tensor Vaswani et al. 2018 . For all experiments we optimize with Adam Kingma Ba 2015 and vary the learning rate as speciÔ¨Åed in Vaswani et al. 2017 . We train our models on both p100 and k40 GPUs with batch sizes ranging from 1 to 8 per GPU.Image Transformer Table 3. Conditional image generations for all CIFAR 10 cate gories. Images on the left are from a model that achieves 3.03 bits dim on the test set. Images on the right are from our best non averaged model with 2.99 bits dim. Both models are able to generate convincing cars trucks and ships. Generated horses planes and birds also look reasonable. 5.1. Generative Image Modeling Our unconditioned and class conditioned image generation models both use 1D local attention with lq 256 and a total memory size of 512.', 'file name: lecture04 Conditioned Generation. Hamidreza Mahyar mahyarh@mcmaster.ca Computational Natural Language Processing Conditioned GenerationLanguage Models Language models are generative models of text s P x The Malfoys said Hermione. Harry was watching him. He looked like Madame Maxime. When she strode up the wrong staircase to visit himself. I m afraid I ve definitely been suspended from power no chance indeed said Snape. He put his head back behind them and read groups as they crossed a corner and fluttered down onto their ink lamp and picked up his spoon. The doorbell rang. It was a lot cleaner down in London. Text Credit Max Deutsch https medium.com deep writingConditioned Language Models Not just generate text generate text according to some specification Input X Structured Data English Document Utterance Image Speech Output Y Text NL Description Japanese Short Description Response Text Transcript Task NL Generation Translation Summarization Response Generation Image Captioning Speech RecognitionFormulation and ModelingCalculating the Probability of a SentenceConditional Language ModelsOne Type of Language Model LSTM LSTM LSTM LSTM predict hate predict this predict movie predict s LSTM Mikolov et al. 2011 s I hate this movie predict ILSTM LSTM LSTM LSTM LSTM s LSTM LSTM LSTM LSTM argmax argmax argmax argmax argmax s One Type of Conditional Language Model I hate this movie kono eiga ga kirai hate this movie Sutskever et al. 2014 Encoder I DecoderHow to Pass Hidden State Initialize decoder w encoder Sutskever et al. 2014 encoder decoder Transform can be different dimensions encoder transform decoder encoder Input at every time step Kalchbrenner Blunsom 2013 decoder decoder decoderMethods of GenerationThe Generation Problem We have a model of P Y X how do we use it to generate a sentence Two methods Sampling Try to generate a random sentence according to the probability distribution. Argmax Try to generate the sentence with the highest probability.Ancestral Sampling Randomly generate words one by one.']",text-embedding-3-small,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9918168868462132,1.0,0.9174466055279876,0.0
39,What challenges exist in evaluating conditioned language models?,"Some challenges in evaluating conditioned language models include the issue of generalization outside of the training distribution, the potential exaggeration of performance on specific benchmarks, the need for large supervised datasets for training, the lack of grounding in other domains of experience, the limitations of the pretraining objective, poor sample efficiency during pretraining, and ambiguity regarding few-shot learning capabilities at inference time.","['For instance HLW 20 observe that larger models do not necessarily generalize better out of distribution. There is evidence that suggests that the generalization achieved under this paradigm can be poor because the model is overly speciÔ¨Åc to the training distribution and does not generalize well outside it YdC 19 MPL19 . Thus the performance of Ô¨Åne tuned models on speciÔ¨Åc benchmarks even when it is nominally at human level may exaggerate actual performance on the underlying task GSL 18 NK19 . Third humans do not require large supervised datasets to learn most language tasks a brief directive in natural language e.g. please tell me if this sentence describes something happy or something sad or at most a tiny number of demonstrations e.g. here are two examples of people acting brave please give a third example of bravery is often Figure 1.1 Language model meta learning. During unsupervised pre training a language model develops a broad set of skills and pattern recognition abilities. It then uses these abilities at inference time to rapidly adapt to or recognize the desired task. We use the term in context learning to describe the inner loop of this process which occurs within the forward pass upon each sequence. The sequences in this diagram are not intended to be representative of the data a model would see during pre training but are intended to show that there are sometimes repeated sub tasks embedded within a single sequence. 3Figure 1.2 Larger models make increasingly efÔ¨Åcient use of in context information. We show in context learning performance on a simple task requiring the model to remove random symbols from a word both with and without a natural language task description see Sec. 3.9.2 . The steeper in context learning curves for large models demonstrate improved ability to learn a task from contextual information. We see qualitatively similar behavior across a wide range of tasks. sufÔ¨Åcient to enable a human to perform a new task to at least a reasonable degree of competence. Aside from pointing to a conceptual limitation in our current NLP techniques this adaptability has practical advantages it allows humans to seamlessly mix together or switch between many tasks and skills for example performing addition during a lengthy dialogue.', 'A more fundamental limitation of the general approach described in this paper scaling up any LM like model whether autoregressive or bidirectional is that it may eventually run into or could already be running into the limits of the 33pretraining objective. Our current objective weights every token equally and lacks a notion of what is most important to predict and what is less important. RRS20 demonstrate beneÔ¨Åts of customizing prediction to entities of interest. Also with self supervised objectives task speciÔ¨Åcation relies on forcing the desired task into a prediction problem whereas ultimately useful language systems for example virtual assistants might be better thought of as taking goal directed actions rather than just making predictions. Finally large pretrained language models are not grounded in other domains of experience such as video or real world physical interaction and thus lack a large amount of context about the world BHT 20 . For all these reasons scaling pure self supervised prediction is likely to hit limits and augmentation with a different approach is likely to be necessary. Promising future directions in this vein might include learning the objective function from humans ZSW 19a Ô¨Åne tuning with reinforcement learning or adding additional modalities such as images to provide grounding and a better model of the world CLY 19 . Another limitation broadly shared by language models is poor sample efÔ¨Åciency during pre training. While GPT 3 takes a step towards test time sample efÔ¨Åciency closer to that of humans one shot or zero shot it still sees much more text during pre training than a human sees in the their lifetime Lin20 . Improving pre training sample efÔ¨Åciency is an important direction for future work and might come from grounding in the physical world to provide additional information or from algorithmic improvements. A limitation or at least uncertainty associated with few shot learning in GPT 3 is ambiguity about whether few shot learning actually learns new tasks from scratch at inference time or if it simply recognizes and identiÔ¨Åes tasks that it has learned during training.']",text-embedding-3-small,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9932392273138442,0.8571428571428571,0.8500893131538232,0.04
40,What is the instructor's name for this course?,"The instructor for the course ""Introduction to Computational Natural Language Processing"" in Winter 2024 is Hamidreza Mahyar. 

Source: NLP Winter 2024 Course Outline","['file name: NLP Winter 2024 Course Outline. Course Outline 1. COURSE INFORMATION Session Offered Winter 2024 Course Name Introduction to Computational Natural Language Processing Course Code SEP 775 Date s and Time s of lectures Lectures In person Wednesday 3 30 pm 6 30 pm Program Name System and Technology Calendar Description This graduate course introduces fundamental concepts in computational natural language processing NLP and their applications to teaching students how to deal with textual data in Artificial Intelligence. This course demonstrates how machines can learn different tasks in natural language such as language modeling text generation machine translation and language understanding. In this regard we go over the most promising methods in this literature and the most recent state of the art techniques. Moreover this course explores different real world applications of NLP and helps students get hands on experience in this field. Instructor Hamidreza Mahyar E Mail mahyarh@mcmaster.ca Office Hours upon request 2. COURSE SPECIFICS Course Description This course covers some fundamental concepts in natural language processing NLP and how they apply in real world use cases. Instruction Type Code Type Hours per term C Classroom instruction 39 L Laboratory workshop or fieldwork T Tutorial DE Distance education Total Hours 39 Resources ISBN Textbook Title Edition Author Publisher Natural Language Processing Jacob Eisenstein Speech and Language Processing Dan Jurafsky and James H. Martin Deep Learning Ian Goodfellow Yoshua Bengio and Aaron Courville Prerequisite s Proficiency in Python programming language Some Machine Learning and or Deep Learning knowledge is a plus Corequisite s Antirequisite s Course Specific Policies All work must be shown to get full credit. Specific policy of Projects Assignments submissionsProjects Assignments will be conducted online by using related software. Completed Projects Assignments should be uploaded to the drop box before midnight of the due date. Projects Assignments missed due to legitimate reasons must be completed later mutually agreed with the instructor. Departmental Policies The use of cell phones iPods laptops and other personal electronic devices are prohibited from the classroom during the class time unless the instructor makes an explicit exception. Announcements made in class or placed on Avenue are considered to have been communicated to all students including those individuals that are not in class. Instructor has the right to submit work to software to identify plagiarism.', 'Instructor has the right to submit work to software to identify plagiarism. 3. SUB TOPIC S Week 1 Overview Human language Word meanings Word2Vec and Optimization basics Week 2 Intro to neural networks Gradients and backpropagation Week 3 Recurrent Neural Networks and Language Models Week 4 Conditioned Generation Week 5 Part 1 Self attention and transformers Part 2 Seq2Seq models Week 6 Midterm Test Week 7 Midterm Recess No class Week 8 Prompting Instruction Finetuning and RLHF Week 9 Question Answering systems Week 10 Multimodal models Week 11 Project presentation and Q A Week 12 Project presentation and Q A Week 13 Project presentation and Q A Classes end April 10th 4. ASSESSMENT OF LEARNING Weight 4 Assignments 20 4 Quizzes 10 Midterm Test 35 Group Project 35 TOTAL 100Note that the percentage distribution of each module may be adjusted. Percentage grades will be converted to letter grades and grade points per the University calendar. 5. LEARNING OUTCOMES By the end of this course the students will know about Introduction to Deep Learning Word representations Language models Pre trained language models Text generation Seq2Seq models Prompting Feedback systems Multimodal models NLP applications e.g. Q A systems 6. COURSE OUTLINE APPROVED ADVISORY STATEMENTS ANTI DISCRIMINATION The Faculty of Engineering is concerned with ensuring an environment that is free of all discrimination. If there is a problem individuals are reminded that they should contact the Associate Director Graduate Studies the Sexual Harassment Officer or the Human Rights Consultant as soon as possible. http www.mcmaster.ca policy General HR Discrimination_Harassment_Sexual_Harassment Prevention Response.pdf ACADEMIC INTEGRITY You are expected to exhibit honesty and use ethical behaviour in all aspects of the learning process. Academic credentials you earn are rooted in principles of honesty and academic integrity. It is your responsibility to understand what constitutes academic dishonesty. Academic dishonesty is to knowingly act or fail to act in a way that results or could result in unearned academic credit or advantage. This behaviour can result in serious consequences e.g.']",text-embedding-3-small,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.8698619914558052,1.0,0.8818268184179945,0.0869565217391304
41,What is the email address of the instructor for this course?,"The email address of the instructor for the ""Introduction to Computational Natural Language Processing"" course in Winter 2024 is mahyarh@mcmaster.ca. 

Source: NLP Winter 2024 Course Outline.","['file name: NLP Winter 2024 Course Outline. Course Outline 1. COURSE INFORMATION Session Offered Winter 2024 Course Name Introduction to Computational Natural Language Processing Course Code SEP 775 Date s and Time s of lectures Lectures In person Wednesday 3 30 pm 6 30 pm Program Name System and Technology Calendar Description This graduate course introduces fundamental concepts in computational natural language processing NLP and their applications to teaching students how to deal with textual data in Artificial Intelligence. This course demonstrates how machines can learn different tasks in natural language such as language modeling text generation machine translation and language understanding. In this regard we go over the most promising methods in this literature and the most recent state of the art techniques. Moreover this course explores different real world applications of NLP and helps students get hands on experience in this field. Instructor Hamidreza Mahyar E Mail mahyarh@mcmaster.ca Office Hours upon request 2. COURSE SPECIFICS Course Description This course covers some fundamental concepts in natural language processing NLP and how they apply in real world use cases. Instruction Type Code Type Hours per term C Classroom instruction 39 L Laboratory workshop or fieldwork T Tutorial DE Distance education Total Hours 39 Resources ISBN Textbook Title Edition Author Publisher Natural Language Processing Jacob Eisenstein Speech and Language Processing Dan Jurafsky and James H. Martin Deep Learning Ian Goodfellow Yoshua Bengio and Aaron Courville Prerequisite s Proficiency in Python programming language Some Machine Learning and or Deep Learning knowledge is a plus Corequisite s Antirequisite s Course Specific Policies All work must be shown to get full credit. Specific policy of Projects Assignments submissionsProjects Assignments will be conducted online by using related software. Completed Projects Assignments should be uploaded to the drop box before midnight of the due date. Projects Assignments missed due to legitimate reasons must be completed later mutually agreed with the instructor. Departmental Policies The use of cell phones iPods laptops and other personal electronic devices are prohibited from the classroom during the class time unless the instructor makes an explicit exception. Announcements made in class or placed on Avenue are considered to have been communicated to all students including those individuals that are not in class. Instructor has the right to submit work to software to identify plagiarism.', 'If you have any questions or concerns about such disclosure please discuss this with the course instructor. ONLINE PROCTORING Some courses may use online proctoring software for tests and exams. This software may require students to turn on their video camera present identification monitor and record their computer activities and or lock restrict their browser or other applications software during tests or exams. This software may be required to be installed before the test exam begins. COMMUNICATIONS It is the student s responsibility to Maintain current contact information with the University including address phone numbers and emergency contact information. Use the University provided e mail address or maintain a valid forwarding e mail address. Regularly check the official University communications channels. Official University communications are considered received if sent by postal mail by fax or by e mail to the student s designated primary e mail account via their @mcmaster.ca alias. Accept that forwarded e mails may be lost and that e mail is considered received if sent via the student s @mcmaster.ca alias. Check the McMaster Avenue email and course websites on a regular basis during the term. CONDUCT EXPECTATIONS As a McMaster student you have the right to experience and the responsibility to demonstrate respectful and dignified interactions within all of our living learning and working communities. These expectations are described in the Code of Student Rights Responsibilities the Code . All students share the responsibility of maintaining a positive environment for the academic and personal growth of all McMaster community members whether in person or online. It is essential that students be mindful of their interactions online as the Code remains in effect in virtual learning environments. The Code applies to any interactions that adversely affect disrupt or interfere with reasonable participation in University activities. Student disruptions or behaviours that interfere with university functions on online platforms e.g. use of Avenue 2 Learn WebEx or Zoom for delivery will be taken very seriously and will be investigated. Outcomes may include restriction or removal of the involved students access to these platforms. ACADEMIC ACCOMMODATION OF STUDENTS WITH DISABILITIES Students with disabilities who require academic accommodation must contact Student Accessibility Services SAS at 905 525 9140 ext. 28652 or sas@mcmaster.ca to make arrangements with a Program Coordinator.']",text-embedding-3-small,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.8988614592934341,1.0,0.8912044038318212,0.0
42,When is this course held every week?,This course is held in person every Wednesday from 3:30 pm to 6:30 pm.,"['file name: NLP Winter 2024 Course Outline. Course Outline 1. COURSE INFORMATION Session Offered Winter 2024 Course Name Introduction to Computational Natural Language Processing Course Code SEP 775 Date s and Time s of lectures Lectures In person Wednesday 3 30 pm 6 30 pm Program Name System and Technology Calendar Description This graduate course introduces fundamental concepts in computational natural language processing NLP and their applications to teaching students how to deal with textual data in Artificial Intelligence. This course demonstrates how machines can learn different tasks in natural language such as language modeling text generation machine translation and language understanding. In this regard we go over the most promising methods in this literature and the most recent state of the art techniques. Moreover this course explores different real world applications of NLP and helps students get hands on experience in this field. Instructor Hamidreza Mahyar E Mail mahyarh@mcmaster.ca Office Hours upon request 2. COURSE SPECIFICS Course Description This course covers some fundamental concepts in natural language processing NLP and how they apply in real world use cases. Instruction Type Code Type Hours per term C Classroom instruction 39 L Laboratory workshop or fieldwork T Tutorial DE Distance education Total Hours 39 Resources ISBN Textbook Title Edition Author Publisher Natural Language Processing Jacob Eisenstein Speech and Language Processing Dan Jurafsky and James H. Martin Deep Learning Ian Goodfellow Yoshua Bengio and Aaron Courville Prerequisite s Proficiency in Python programming language Some Machine Learning and or Deep Learning knowledge is a plus Corequisite s Antirequisite s Course Specific Policies All work must be shown to get full credit. Specific policy of Projects Assignments submissionsProjects Assignments will be conducted online by using related software. Completed Projects Assignments should be uploaded to the drop box before midnight of the due date. Projects Assignments missed due to legitimate reasons must be completed later mutually agreed with the instructor. Departmental Policies The use of cell phones iPods laptops and other personal electronic devices are prohibited from the classroom during the class time unless the instructor makes an explicit exception. Announcements made in class or placed on Avenue are considered to have been communicated to all students including those individuals that are not in class. Instructor has the right to submit work to software to identify plagiarism.', 'Instructor has the right to submit work to software to identify plagiarism. 3. SUB TOPIC S Week 1 Overview Human language Word meanings Word2Vec and Optimization basics Week 2 Intro to neural networks Gradients and backpropagation Week 3 Recurrent Neural Networks and Language Models Week 4 Conditioned Generation Week 5 Part 1 Self attention and transformers Part 2 Seq2Seq models Week 6 Midterm Test Week 7 Midterm Recess No class Week 8 Prompting Instruction Finetuning and RLHF Week 9 Question Answering systems Week 10 Multimodal models Week 11 Project presentation and Q A Week 12 Project presentation and Q A Week 13 Project presentation and Q A Classes end April 10th 4. ASSESSMENT OF LEARNING Weight 4 Assignments 20 4 Quizzes 10 Midterm Test 35 Group Project 35 TOTAL 100Note that the percentage distribution of each module may be adjusted. Percentage grades will be converted to letter grades and grade points per the University calendar. 5. LEARNING OUTCOMES By the end of this course the students will know about Introduction to Deep Learning Word representations Language models Pre trained language models Text generation Seq2Seq models Prompting Feedback systems Multimodal models NLP applications e.g. Q A systems 6. COURSE OUTLINE APPROVED ADVISORY STATEMENTS ANTI DISCRIMINATION The Faculty of Engineering is concerned with ensuring an environment that is free of all discrimination. If there is a problem individuals are reminded that they should contact the Associate Director Graduate Studies the Sexual Harassment Officer or the Human Rights Consultant as soon as possible. http www.mcmaster.ca policy General HR Discrimination_Harassment_Sexual_Harassment Prevention Response.pdf ACADEMIC INTEGRITY You are expected to exhibit honesty and use ethical behaviour in all aspects of the learning process. Academic credentials you earn are rooted in principles of honesty and academic integrity. It is your responsibility to understand what constitutes academic dishonesty. Academic dishonesty is to knowingly act or fail to act in a way that results or could result in unearned academic credit or advantage. This behaviour can result in serious consequences e.g.']",text-embedding-3-small,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9261015836751826,1.0,0.9727988425002738,0.0869565217391304
43,What is the name of the TA for this course?,"The TA for this course is not explicitly mentioned in the provided context information. If you need to know the TA's name, I recommend checking the official course website, contacting the instructor, or referring to any official communication from the course administration. Let me know if you need help with anything else.
Source: NLP Winter 2024 Course Outline","['file name: NLP Winter 2024 Course Outline. Course Outline 1. COURSE INFORMATION Session Offered Winter 2024 Course Name Introduction to Computational Natural Language Processing Course Code SEP 775 Date s and Time s of lectures Lectures In person Wednesday 3 30 pm 6 30 pm Program Name System and Technology Calendar Description This graduate course introduces fundamental concepts in computational natural language processing NLP and their applications to teaching students how to deal with textual data in Artificial Intelligence. This course demonstrates how machines can learn different tasks in natural language such as language modeling text generation machine translation and language understanding. In this regard we go over the most promising methods in this literature and the most recent state of the art techniques. Moreover this course explores different real world applications of NLP and helps students get hands on experience in this field. Instructor Hamidreza Mahyar E Mail mahyarh@mcmaster.ca Office Hours upon request 2. COURSE SPECIFICS Course Description This course covers some fundamental concepts in natural language processing NLP and how they apply in real world use cases. Instruction Type Code Type Hours per term C Classroom instruction 39 L Laboratory workshop or fieldwork T Tutorial DE Distance education Total Hours 39 Resources ISBN Textbook Title Edition Author Publisher Natural Language Processing Jacob Eisenstein Speech and Language Processing Dan Jurafsky and James H. Martin Deep Learning Ian Goodfellow Yoshua Bengio and Aaron Courville Prerequisite s Proficiency in Python programming language Some Machine Learning and or Deep Learning knowledge is a plus Corequisite s Antirequisite s Course Specific Policies All work must be shown to get full credit. Specific policy of Projects Assignments submissionsProjects Assignments will be conducted online by using related software. Completed Projects Assignments should be uploaded to the drop box before midnight of the due date. Projects Assignments missed due to legitimate reasons must be completed later mutually agreed with the instructor. Departmental Policies The use of cell phones iPods laptops and other personal electronic devices are prohibited from the classroom during the class time unless the instructor makes an explicit exception. Announcements made in class or placed on Avenue are considered to have been communicated to all students including those individuals that are not in class. Instructor has the right to submit work to software to identify plagiarism.', 'Instructor has the right to submit work to software to identify plagiarism. 3. SUB TOPIC S Week 1 Overview Human language Word meanings Word2Vec and Optimization basics Week 2 Intro to neural networks Gradients and backpropagation Week 3 Recurrent Neural Networks and Language Models Week 4 Conditioned Generation Week 5 Part 1 Self attention and transformers Part 2 Seq2Seq models Week 6 Midterm Test Week 7 Midterm Recess No class Week 8 Prompting Instruction Finetuning and RLHF Week 9 Question Answering systems Week 10 Multimodal models Week 11 Project presentation and Q A Week 12 Project presentation and Q A Week 13 Project presentation and Q A Classes end April 10th 4. ASSESSMENT OF LEARNING Weight 4 Assignments 20 4 Quizzes 10 Midterm Test 35 Group Project 35 TOTAL 100Note that the percentage distribution of each module may be adjusted. Percentage grades will be converted to letter grades and grade points per the University calendar. 5. LEARNING OUTCOMES By the end of this course the students will know about Introduction to Deep Learning Word representations Language models Pre trained language models Text generation Seq2Seq models Prompting Feedback systems Multimodal models NLP applications e.g. Q A systems 6. COURSE OUTLINE APPROVED ADVISORY STATEMENTS ANTI DISCRIMINATION The Faculty of Engineering is concerned with ensuring an environment that is free of all discrimination. If there is a problem individuals are reminded that they should contact the Associate Director Graduate Studies the Sexual Harassment Officer or the Human Rights Consultant as soon as possible. http www.mcmaster.ca policy General HR Discrimination_Harassment_Sexual_Harassment Prevention Response.pdf ACADEMIC INTEGRITY You are expected to exhibit honesty and use ethical behaviour in all aspects of the learning process. Academic credentials you earn are rooted in principles of honesty and academic integrity. It is your responsibility to understand what constitutes academic dishonesty. Academic dishonesty is to knowingly act or fail to act in a way that results or could result in unearned academic credit or advantage. This behaviour can result in serious consequences e.g.']",text-embedding-3-small,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.0,,0.8286185101900094,0.0
44,What is Word2vec?,"Word2vec is a framework for learning word vectors where each word in a fixed vocabulary is represented by a vector. It involves going through a large corpus of text, considering a center word and context outside words, and adjusting word vectors to maximize the probability of predicting context words given the center word or vice versa. The objective is to minimize an empirical loss function by estimating parameters using gradient-based methods.","['Word2vec Overview 21 Word2vec Mikolov et al. 2013 is a framework for learning word vectors Idea We have a large corpus body of text a long list of words Every word in a fixed vocabulary is represented by a vector Go through each position t in the text which has a center word c and context outside words o Use the similarity of the word vectors for c and o to calculate the probability of o given c or vice versa Keep adjusting the word vectors to maximize this probabilityWord2Vec Overview Example windows and process for computing ùëÉùë§ùë° ùëó ùë§ùë° crises banking into turning problems as center word at position t outside context words in window of size 2 outside context words in window of size 2 ùëÉùë§ùë° 1 ùë§ùë° 22 ùëÉùë§ùë° 2 ùë§ùë° ùëÉùë§ùë° 1 ùë§ùë° ùëÉùë§ùë° 2 ùë§ùë°Word2Vec Overview Example windows and process for computing ùëÉùë§ùë° ùëó ùë§ùë° crises banking into turning problems as center word at position t outside context words in window of size 2 outside context words in window of size 2 ùëÉùë§ùë° 2 ùë§ùë° ùëÉùë§ùë° 1 ùë§ùë° 23 ùëÉùë§ùë° 2 ùë§ùë° ùëÉùë§ùë° 1 ùë§ùë°Word2vec objective function For each position ùë° 1 ùëá predict context words within a window of fixed size m given center word ùë§ùë°. Data likelihood The objective function ùêΩùúÉis the average negative log likelihood Minimizing objective function Maximizing predictive accuracy sometimes called a cost or loss function 24Word2vec objective function We want to minimize the objective function Question How to calculate ùëÉùë§ùë° ùëó ùë§ùë° ùúÉ Answer We will use two vectors per word w', '3.3 Estimating a word2vec model from a corpus How do we train word2vec in practice Specifying the word2vec model is relatively transparent from the math we ve given above one constructs matrices U and V and can write out the math of the probability. However it may not yet be evident how to estimate the parameters 1 how to calculate the expectation in Eqn 5 for a given value of U and V and then 2 how to perform the minimization operation. Let s start with 1. Word2vec empirical loss. Let D be a set of documents d where each document is a sequence of words w d 1 . . . w d m with all w V. Let k N be a positive integer window size. Let s define how our center word random variable C and outside word r.v. O relate to this concrete dataset. O takes on the value of each word wi in each document and for each such wi the outside words are wi k . . . wi 1 wi 1 . . . wi k . So our Eqn 5 objective becomes L U V d D m i 1 k j 1 log pU V w d i j w d i 6 where you ll note we re taking the sum over 1 all documents of the sum over 2 all words in the document of the sum over 3 all words occuring in the window of the likelihood of the outside word given the center word. Now how do we do the minimization Gradient based estimation At a high level we try to find good U and V for the objective we ve specified by starting with a relatively uninformed guess and iteratively moving in the direction that locally best improves the guess. This is done by gradient based methods a full description of which is out of scope for this note.']",text-embedding-3-small,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999999999999994,1.0,0.9673166665545048,0.0833333333333333
45,What are the main learning goals of the course?,"The main learning goals of the course include Introduction to Deep Learning, Word representations, Language models, Pre-trained language models, Text generation, Seq2Seq models, Prompting Feedback systems, Multimodal models, and NLP applications such as Q&A systems. Students will also learn about various topics like Word2Vec, Optimization basics, Neural Networks, Gradients, Backpropagation, Recurrent Neural Networks, Self-attention and transformers, Conditioned Generation, Question Answering systems, and Project presentations. Additionally, the course aims to provide hands-on experience in dealing with textual data in Artificial Intelligence and understanding how machines can perform tasks related to natural language processing. 

Source: NLP Winter 2024 Course Outline.","['Instructor has the right to submit work to software to identify plagiarism. 3. SUB TOPIC S Week 1 Overview Human language Word meanings Word2Vec and Optimization basics Week 2 Intro to neural networks Gradients and backpropagation Week 3 Recurrent Neural Networks and Language Models Week 4 Conditioned Generation Week 5 Part 1 Self attention and transformers Part 2 Seq2Seq models Week 6 Midterm Test Week 7 Midterm Recess No class Week 8 Prompting Instruction Finetuning and RLHF Week 9 Question Answering systems Week 10 Multimodal models Week 11 Project presentation and Q A Week 12 Project presentation and Q A Week 13 Project presentation and Q A Classes end April 10th 4. ASSESSMENT OF LEARNING Weight 4 Assignments 20 4 Quizzes 10 Midterm Test 35 Group Project 35 TOTAL 100Note that the percentage distribution of each module may be adjusted. Percentage grades will be converted to letter grades and grade points per the University calendar. 5. LEARNING OUTCOMES By the end of this course the students will know about Introduction to Deep Learning Word representations Language models Pre trained language models Text generation Seq2Seq models Prompting Feedback systems Multimodal models NLP applications e.g. Q A systems 6. COURSE OUTLINE APPROVED ADVISORY STATEMENTS ANTI DISCRIMINATION The Faculty of Engineering is concerned with ensuring an environment that is free of all discrimination. If there is a problem individuals are reminded that they should contact the Associate Director Graduate Studies the Sexual Harassment Officer or the Human Rights Consultant as soon as possible. http www.mcmaster.ca policy General HR Discrimination_Harassment_Sexual_Harassment Prevention Response.pdf ACADEMIC INTEGRITY You are expected to exhibit honesty and use ethical behaviour in all aspects of the learning process. Academic credentials you earn are rooted in principles of honesty and academic integrity. It is your responsibility to understand what constitutes academic dishonesty. Academic dishonesty is to knowingly act or fail to act in a way that results or could result in unearned academic credit or advantage. This behaviour can result in serious consequences e.g.', 'file name: NLP Winter 2024 Course Outline. Course Outline 1. COURSE INFORMATION Session Offered Winter 2024 Course Name Introduction to Computational Natural Language Processing Course Code SEP 775 Date s and Time s of lectures Lectures In person Wednesday 3 30 pm 6 30 pm Program Name System and Technology Calendar Description This graduate course introduces fundamental concepts in computational natural language processing NLP and their applications to teaching students how to deal with textual data in Artificial Intelligence. This course demonstrates how machines can learn different tasks in natural language such as language modeling text generation machine translation and language understanding. In this regard we go over the most promising methods in this literature and the most recent state of the art techniques. Moreover this course explores different real world applications of NLP and helps students get hands on experience in this field. Instructor Hamidreza Mahyar E Mail mahyarh@mcmaster.ca Office Hours upon request 2. COURSE SPECIFICS Course Description This course covers some fundamental concepts in natural language processing NLP and how they apply in real world use cases. Instruction Type Code Type Hours per term C Classroom instruction 39 L Laboratory workshop or fieldwork T Tutorial DE Distance education Total Hours 39 Resources ISBN Textbook Title Edition Author Publisher Natural Language Processing Jacob Eisenstein Speech and Language Processing Dan Jurafsky and James H. Martin Deep Learning Ian Goodfellow Yoshua Bengio and Aaron Courville Prerequisite s Proficiency in Python programming language Some Machine Learning and or Deep Learning knowledge is a plus Corequisite s Antirequisite s Course Specific Policies All work must be shown to get full credit. Specific policy of Projects Assignments submissionsProjects Assignments will be conducted online by using related software. Completed Projects Assignments should be uploaded to the drop box before midnight of the due date. Projects Assignments missed due to legitimate reasons must be completed later mutually agreed with the instructor. Departmental Policies The use of cell phones iPods laptops and other personal electronic devices are prohibited from the classroom during the class time unless the instructor makes an explicit exception. Announcements made in class or placed on Avenue are considered to have been communicated to all students including those individuals that are not in class. Instructor has the right to submit work to software to identify plagiarism.']",text-embedding-3-small,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999999999999996,1.0,0.9231465154786204,0.1304347826086956
46,How is the course grade distributed?,"The course grade is distributed as follows: 4 Assignments worth 20%, 4 Quizzes worth 10%, Midterm Test worth 35%, and Group Project worth 35%, making a total of 100%. Please note that the percentage distribution of each module may be adjusted. Percentage grades will be converted to letter grades and grade points per the University calendar.","['Instructor has the right to submit work to software to identify plagiarism. 3. SUB TOPIC S Week 1 Overview Human language Word meanings Word2Vec and Optimization basics Week 2 Intro to neural networks Gradients and backpropagation Week 3 Recurrent Neural Networks and Language Models Week 4 Conditioned Generation Week 5 Part 1 Self attention and transformers Part 2 Seq2Seq models Week 6 Midterm Test Week 7 Midterm Recess No class Week 8 Prompting Instruction Finetuning and RLHF Week 9 Question Answering systems Week 10 Multimodal models Week 11 Project presentation and Q A Week 12 Project presentation and Q A Week 13 Project presentation and Q A Classes end April 10th 4. ASSESSMENT OF LEARNING Weight 4 Assignments 20 4 Quizzes 10 Midterm Test 35 Group Project 35 TOTAL 100Note that the percentage distribution of each module may be adjusted. Percentage grades will be converted to letter grades and grade points per the University calendar. 5. LEARNING OUTCOMES By the end of this course the students will know about Introduction to Deep Learning Word representations Language models Pre trained language models Text generation Seq2Seq models Prompting Feedback systems Multimodal models NLP applications e.g. Q A systems 6. COURSE OUTLINE APPROVED ADVISORY STATEMENTS ANTI DISCRIMINATION The Faculty of Engineering is concerned with ensuring an environment that is free of all discrimination. If there is a problem individuals are reminded that they should contact the Associate Director Graduate Studies the Sexual Harassment Officer or the Human Rights Consultant as soon as possible. http www.mcmaster.ca policy General HR Discrimination_Harassment_Sexual_Harassment Prevention Response.pdf ACADEMIC INTEGRITY You are expected to exhibit honesty and use ethical behaviour in all aspects of the learning process. Academic credentials you earn are rooted in principles of honesty and academic integrity. It is your responsibility to understand what constitutes academic dishonesty. Academic dishonesty is to knowingly act or fail to act in a way that results or could result in unearned academic credit or advantage. This behaviour can result in serious consequences e.g.', 'This behaviour can result in serious consequences e.g. the grade of zero on an assignment loss of credit with a notation on the transcript notation reads Grade of F assigned for academic dishonesty and or suspension or expulsion from the university. For information on the various types of academic dishonesty please refer to the Academic Integrity Policy located at https secretariat.mcmaster.ca university policies procedures guidelines The following illustrates only three forms of academic dishonesty plagiarism e.g. the submission of work that is not one s own or for which other credit has been obtained. improper collaboration in group work. copying or using unauthorized aids in tests and examinations. AUTHENTICITY PLAGIARISM DETECTION Some courses may use a web based service Turnitin.com to reveal authenticity and ownership of student submitted work. For courses using such software students will be expected to submit their work electronically either directly to Turnitin.com or via an online learning platform e.g. A2L etc. using plagiarism detection a service supported by Turnitin.com so it can be checked for academic dishonesty. Students who do not wish their work to be submitted through the plagiarism detection software must inform the Instructor before the assignment is due. No penalty will be assigned to a student who does not submit work to the plagiarism detection software. All submitted work is subject to normal verification that standards of academicintegrity have been upheld e.g. on line search other software etc. . For more details about McMaster s use of Turnitin.com please go to www.mcmaster.ca academicintegrity. COURSES WITH AN ON LINE ELEMENT Some courses may use on line elements e.g. e mail Avenue to Learn A2L LearnLink web pages capa Moodle ThinkingCap etc. . Students should be aware that when they access the electronic components of a course using these elements private information such as first and last names user names for the McMaster e mail accounts and program affiliation may become apparent to all other students in the same course. The available information is dependent on the technology used. Continuation in a course that uses on line elements will be deemed consent to this disclosure. If you have any questions or concerns about such disclosure please discuss this with the course instructor.']",text-embedding-3-small,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999999999999996,1.0,0.9593096804941648,0.0
47,What is the policy for late submissions in this course?,"The policy for late submissions in this course is not explicitly mentioned in the provided context information. However, it is advisable to refer to the course syllabus or contact the course instructor directly for specific details regarding late submissions. It is important to adhere to the guidelines provided by the instructor to avoid any penalties related to late submissions.","['This behaviour can result in serious consequences e.g. the grade of zero on an assignment loss of credit with a notation on the transcript notation reads Grade of F assigned for academic dishonesty and or suspension or expulsion from the university. For information on the various types of academic dishonesty please refer to the Academic Integrity Policy located at https secretariat.mcmaster.ca university policies procedures guidelines The following illustrates only three forms of academic dishonesty plagiarism e.g. the submission of work that is not one s own or for which other credit has been obtained. improper collaboration in group work. copying or using unauthorized aids in tests and examinations. AUTHENTICITY PLAGIARISM DETECTION Some courses may use a web based service Turnitin.com to reveal authenticity and ownership of student submitted work. For courses using such software students will be expected to submit their work electronically either directly to Turnitin.com or via an online learning platform e.g. A2L etc. using plagiarism detection a service supported by Turnitin.com so it can be checked for academic dishonesty. Students who do not wish their work to be submitted through the plagiarism detection software must inform the Instructor before the assignment is due. No penalty will be assigned to a student who does not submit work to the plagiarism detection software. All submitted work is subject to normal verification that standards of academicintegrity have been upheld e.g. on line search other software etc. . For more details about McMaster s use of Turnitin.com please go to www.mcmaster.ca academicintegrity. COURSES WITH AN ON LINE ELEMENT Some courses may use on line elements e.g. e mail Avenue to Learn A2L LearnLink web pages capa Moodle ThinkingCap etc. . Students should be aware that when they access the electronic components of a course using these elements private information such as first and last names user names for the McMaster e mail accounts and program affiliation may become apparent to all other students in the same course. The available information is dependent on the technology used. Continuation in a course that uses on line elements will be deemed consent to this disclosure. If you have any questions or concerns about such disclosure please discuss this with the course instructor.', '28652 or sas@mcmaster.ca to make arrangements with a Program Coordinator. For further information consult McMaster University s Academic Accommodation of Students with Disabilities policy. ACADEMIC ACCOMMODATION FOR RELIGIOUS INDIGENOUS OR SPIRITUAL OBSERVANCES RISOStudents requiring academic accommodation based on religious indigenous or spiritual observances should follow the procedures set out in the RISO policy. Students should submit their request to their Faculty Office normally within 10 working days of the beginning of term in which they anticipate a need for accommodation or to the Registrar s Office prior to their examinations. Students should also contact their instructors as soon as possible to make alternative arrangements for classes assignments and tests. http www.mcmaster.ca policy Students AcademicStudies Studentcode.pdf COPYRIGHT AND RECORDING Students are advised that lectures demonstrations performances and any other course material provided by an instructor include copyright protected works. The Copyright Act and copyright law protect every original literary dramatic musical and artistic work including lectures by University instructors The recording of lectures tutorials or other methods of instruction may occur during a course. Recording may be done by either the instructor for the purpose of authorized distribution or by a student for the purpose of personal study. Students should be aware that their voice and or image may be recorded by others during the class. Please speak with the instructor if this is a concern for you. EXTREME CIRCUMSTANCES The University reserves the right to change the dates and deadlines for any or all courses in extreme circumstances e.g. severe weather labour disruptions etc. . Changes will be communicated through regular McMaster communication channels such as McMaster Daily News A2L and or McMaster email.']",text-embedding-3-small,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.0,0.6666666666666666,0.872553856270941,0.0
48,What does the course hope to teach regarding word vectors?,"The course aims to teach about representing word semantics not as one hot vectors, but as a collection of features and relationships to linguistic categories and other words. It emphasizes the idea of building word vectors based on annotated information about words, such as grammatical, derivational, and semantic information, rather than solely relying on one hot vectors. Additionally, the course will explore the limitations of human annotated resources in comparison to methods that can draw vocabulary from naturally occurring text sources. It will also discuss the tradeoff between dimensionality and utility of word embeddings, as well as the importance of allowing data to determine text representations rather than relying solely on human ideas.","['2.3 Vectors from annotated discrete properties. Should we represent word semantics not as one hot vectors but instead as a collection of features and relationships to linguistic categories and other words For any word say runners there is a wealth of information we can annotate about that word. There is grammatical information like plurality there s derivational information like how the runners is something like the verb to run plus a notion of doer or agent think one who runs. There s also semantic information like how runners might be a hyponym of humans or animals or entities. A hyponym is a member of an is a relationship e.g. a runner is a human. There are substantial existing resources in English and a few other languages for various kinds of annotated information about words. WordNet Miller 1995 annotates for synonyms hyponyms and other semantic relations UniMorph Batsuren et al. 2022 annotates for morphology subword structure information across many lan guages. With such resources one could build word vectors that look something like vtea 0 0 1 . . . 1 plural noun 3rd singular verb hyponym of beverage . . . synonym of chai 3 In 2023 word vectors resulting from these methods are not the norm and they won t be the focus of this course. One main failure is that human annotated resources are always lacking in vocabulary compared to methods that can draw a vocabulary from a naturally occuring text source updating these resources is costly and they re always incomplete. Another failure is a tradeoff between dimension ality and utility of the embedding it takes a very high dimensional vector think much larger than the vocabulary size to represent all of these categories and modern neural methods that tend to operate on dense vectors do not behave well with such vectors. Finally a continual theme we ll see in this course is that human ideas of what the right representations should be for text tend to underperform methods that allow data to determine more aspects at least when one has a lot of data to learn from. 3 Distributional semantics and Word2vec A promise of deep learning is to learn rich representations of com plex objects from data. Increasingly relevant in NLP is the idea that we can unsupervisedly learn rich representations from data.', 'This has been done on various datasets that contain human judgement survey data. Model Size WS353 MC RG SCWS RW SVD 6B 35.3 35.1 42.5 38.3 25.6 SVD S 6B 56.5 71.5 71.0 53.6 34.7 SVD L 6B 65.7 72.7 75.1 56.5 37.0 CBOW 6B 57.2 65.6 68.2 57.0 32.5 SG 6B 62.8 65.2 69.7 58.1 37.2 GloVe 6B 65.8 72.7 77.8 53.9 38.1 SVD L 42B 74.0 76.4 74.1 58.3 39.9 GloVe 42B 75.9 83.6 82.9 59.6 47.8 CBOW 100B 68.4 79.6 75.4 59.4 45.5 Table 6 Here we see the correlations be tween of word vector similarities using different embedding techniques with different human judgment datasets 2.6 Further Reading Dealing With Ambiguity One might wonder how we handle the situation where we want to capture the same word with different vectors for its different uses in natural language. For instance run is both a noun and a verb and is used and interpreted differently based on the context. Improving Word Representations Via Global Context And Multi ple Word Prototypes Huang et al 2012 describes how such cases can also be handled in NLP. The essence of the method is the following 1. Gather Ô¨Åxed size context windows of all occurrences of the word for instance 5 before and 5 after 2. Each context is represented by a weighted average of the context words vectors using idf weighting 3. Apply spherical k means to cluster these context representations. 4. Finally each word occurrence is re labeled to its associated cluster and is used to train the word representation for that cluster.']",text-embedding-3-small,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9025042410701696,0.8,0.9086647537820284,0.0454545454545454
49,How does Word2vec represent word meaning?,"Word2vec represents word meaning by assigning a dense vector to each word in such a way that words with similar meanings have vectors that are close together in the vector space. This is achieved by capturing the context in which words appear in a large corpus of text and learning to encode similarity in the vectors themselves based on the words that frequently appear close by. The vectors are adjusted to maximize the probability of context words given the center word or vice versa, ultimately creating word embeddings that capture semantic relationships between words.","['500 000 Means one 1 the rest 0s 16Problem with words as discrete symbols 17 Example in web search if a user searches for Seattle motel we would like to match documents containing Seattle hotel But motel 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 hotel 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 These two vectors are orthogonal There is no natural notion of similarity for one hot vectors Solution Could try to rely on WordNet s list of synonyms to get similarity But it is well known to fail badly incompleteness etc. Instead learn to encode similarity in the vectors themselves Sec. 9.2.2Representing words by their context 21 Distributional semantics A word s meaning is given by the words that frequently appear close by You shall know a word by the company it keeps J. R. Firth 1957 11 One of the most successful ideas of modern statistical NLP When a word w appears in a text its context is the set of words that appear nearby within a fixed size window . We use the many contexts of w to build up a representation of w government debt problems turning into banking crises as happened in 2009 saying that Europe needs unified banking regulation to replace the hodgepodge India has just given its banking system a shot in the arm These context words will represent bankingWord vectors We will build a dense vector for each word chosen so that it is similar to vectors of words that appear in similar contexts measuring similarity as the vector dot scalar product Note word vectors are also called word embeddings or neural word representations They are a distributed representation banking 0.286 0.792 0.177 0.107 0.109 0.542 0.349 0.271 monetary 0.413 0.582 0.007 0.247 0.216 0.718 0.147 0.051 19Word meaning as a neural word vector visualization 0.286 0.792 0.177 0.107 0.109 0.542 0.349 0.271 0.487 expect 203.', 'Word2vec Overview 21 Word2vec Mikolov et al. 2013 is a framework for learning word vectors Idea We have a large corpus body of text a long list of words Every word in a fixed vocabulary is represented by a vector Go through each position t in the text which has a center word c and context outside words o Use the similarity of the word vectors for c and o to calculate the probability of o given c or vice versa Keep adjusting the word vectors to maximize this probabilityWord2Vec Overview Example windows and process for computing ùëÉùë§ùë° ùëó ùë§ùë° crises banking into turning problems as center word at position t outside context words in window of size 2 outside context words in window of size 2 ùëÉùë§ùë° 1 ùë§ùë° 22 ùëÉùë§ùë° 2 ùë§ùë° ùëÉùë§ùë° 1 ùë§ùë° ùëÉùë§ùë° 2 ùë§ùë°Word2Vec Overview Example windows and process for computing ùëÉùë§ùë° ùëó ùë§ùë° crises banking into turning problems as center word at position t outside context words in window of size 2 outside context words in window of size 2 ùëÉùë§ùë° 2 ùë§ùë° ùëÉùë§ùë° 1 ùë§ùë° 23 ùëÉùë§ùë° 2 ùë§ùë° ùëÉùë§ùë° 1 ùë§ùë°Word2vec objective function For each position ùë° 1 ùëá predict context words within a window of fixed size m given center word ùë§ùë°. Data likelihood The objective function ùêΩùúÉis the average negative log likelihood Minimizing objective function Maximizing predictive accuracy sometimes called a cost or loss function 24Word2vec objective function We want to minimize the objective function Question How to calculate ùëÉùë§ùë° ùëó ùë§ùë° ùúÉ Answer We will use two vectors per word w']",text-embedding-3-small,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9861516959628354,1.0,0.9559762956099018,0.2
50,What is the main focus of this lecture on neural networks?,"The main focus of this lecture on neural networks is to introduce single and multilayer neural networks, explain how they can be used for classification purposes, discuss how they can be trained using backpropagation, provide practical tips and tricks for training neural networks, and motivate the use of recurrent neural networks as a language model.","['file name: lecture02 notes Backpropagation.  Neural networks. Forward computation. Backward propagation. Neuron Units. Max margin Loss. Gradient checks. Xavier parameter initialization. Learning rates. Adagrad. This set of notes introduces single and multilayer neural networks and how they can be used for classiÔ¨Åcation purposes. We then dis cuss how they can be trained using a distributed gradient descent technique known as backpropagation. We will see how the chain rule can be used to make parameter updates sequentially. After a rigorous mathematical discussion of neural networks we will discuss some practical tips and tricks in training neural networks involving neuron units non linearities gradient checks Xavier parameter ini tialization learning rates Adagrad etc. Lastly we will motivate the use of recurrent neural networks as a language model. 1 Neural Networks Foundations Figure 1 We see here how a non linear decision boundary separates the data very well. This is the prowess of neural networks. Fun Fact Neural networks are biologically in spired classiÔ¨Åers which is why they are often called artiÔ¨Åcial neural networks to distinguish them from the organic kind. However in reality human neural networks are so much more capable and complex from artiÔ¨Åcial neural net works that it is usually better to not draw too many parallels between the two. We established in our previous discussions the need for non linear classiÔ¨Åers since most data are not linearly separable and thus our classiÔ¨Åcation performance on them is limited. Neural networks are a family of classiÔ¨Åers with non linear decision boundary as seen in Figure 1. Now that we know the sort of decision boundaries neural networks create let us see how they manage doing so. 1.1 A Neuron A neuron is a generic computational unit that takes n inputs and produces a single output. What differentiates the outputs of different neurons is their parameters also referred to as their weights . One of the most popular choices for neurons is the sigmoid or binary lo gistic regression unit.', 'file name: lecture03 RNNs and LLMs. Hamidreza Mahyar mahyarh@mcmaster.ca Computational Natural Language Processing Language Models and Recurrent Neural NetsLecture Plan 1. A bit more about neural networks Language modeling RNNs 2. A new NLP task Language Modeling 4. Problems with RNNs 5. Recap on RNNs LMs Reminders You should have started working on Assignment 1 2 motivates 3. A new family of neural networks Recurrent Neural Networks RNNs This is the most important concept in the class It leads to GPT 3 and ChatGPT Important and used in Ass1 but not the only way to build LMs0 model power Training error Test error Classic view Regularization works to prevent overfitting when we have a lot of features or later a very powerful deep model etc. Now Regularization produces models that generalize well when we have a big model We do not care that our models overfit on the training data even though they are hugely overfit error overfitting We have models with many parameters Regularization 3 A full loss function includes regularization over all parameters ùúÉ e.g. L2 regularizationDropout Srivastava Hinton Krizhevsky Sutskever Salakhutdinov 2012 JMLR 2014 4 Preventing Feature Co adaptation Good Regularization Method Use it everywhere Training time at each instance of evaluation in online SGD training randomly set 50 p of the inputs to each neuron to 0 Test time halve the model weights now twice as many Except usually only drop first layer inputs a little 15 or not at all This prevents feature co adaptation A feature cannot only be useful in the presence of particular other features In a single layer A kind of middle ground between Na√Øve Bayes where all feature weights are set independently and logistic regression models where weights are set in the context of all others Can be thought of as a form of model bagging i.e.']",text-embedding-3-small,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.0000000000000002,1.0,0.9088973675431102,0.0357142857142857
51,What are the non-linear functions mentioned for neural networks?,"The non-linear functions mentioned for neural networks are logistic, tanh, ReLU, Swish, Leaky ReLU, Parametric ReLU, and GELU. These functions are used for introducing non-linearities in neural networks to perform tasks like function approximation. 

Source: lecture02 Neural Nets. Hamidreza Mahyar","['file name: lecture02 Neural Nets. Hamidreza Mahyar mahyarh@mcmaster.ca Computational Natural Language Processing Neural Networks Gradients and BackpropagationBinary classification for center word being location We do supervised training and want high score if it s a location ùêΩùë°ùúÉ ùúéùë† 1 1 ùëí ùë† x xmuseums xin xParis xare xamazing predicted model probability of class f Some element wise non linear function e.g. logistic tanh ReLU 2 R5d Embedding of 1 hot wordsNeural computation Original McCulloch Pitts 1943 threshold unit ùüè ùëäùë• ùúÉ ùüè ùëäùë• ùúÉ 0 This function has no slope so no gradient based learning 3tanh is just a rescaled and shifted sigmoid 2 as steep 1 1 tanh z 2logistic 2z 1 Logistic and tanh are still used e.g. logistic to get a probability However now for deep networks the first thing to try is ReLU it trains quickly and performs well due to good gradient backflow. ReLU has a negative dead zone that recent proposals mitigate GELU is frequently used with Transformers BERT RoBERTa etc. Non linearities old and new logistic sigmoid tanh hard tanh 1 0 1 1 Swish arXiv 1710.05941 swish ùë• ùë•. logistic ùë• Rectified Linear Unit ReLU ReLU ùëß max ùëß 0 Leaky ReLU Parametric ReLU 0 0 0 GELU arXiv 1606.08415 GELU ùë• ùë•. ùëÉùëã ùë• ùëã ùëÅ 0 1 ùë•. logistic 1.702ùë•Non linearities i.e. f on previous slide Why they re needed Neural networks do function approximation e.g.', 'Then during back propagation we only pass gradients through neurons that were kept alive during the forward pass. Finally during testing we compute the forward pass using all of the neurons in the network. However a key sub tlety is that in order for dropout to work effectively the expected output of a neuron during testing should be approximately the same as it was during training else the magnitude of the outputs could be radically different and the behavior of the network is no longer well deÔ¨Åned. Thus we must typically divide the outputs of each neuron during testing by a certain value it is left as an exercise to the reader to determine what this value should be in order for the expected outputs during training and testing to be equivalent. 2.4 Neuron Units So far we have discussed neural networks that contain sigmoidal neurons to introduce nonlinearities however in many applications better networks can be designed using other activation functions. Some common choices are listed here with their function and gra dient deÔ¨Ånitions and these can be substituted with the sigmoidal functions discussed above. Sigmoid This is the default choice we have discussed the activation function œÉ is given by Figure 9 The response of a sigmoid nonlinearity œÉ z 1 1 exp z where œÉ z 0 1 The gradient of œÉ z is œÉ z exp z 1 exp z œÉ z 1 œÉ z Tanh The tanh function is an alternative to the sigmoid function that is often found to converge faster in practice. The primary differ ence between tanh and sigmoid is that tanh output ranges from 1 to 1 while the sigmoid ranges from 0 to 1. Figure 10 The response of a tanh nonlinearity tanh z exp z exp z exp z exp z 2œÉ 2z 1 where tanh z 1 1 The gradient of tanh z is tanh z 1 exp z exp z exp z exp z 2 1 tanh2 z Hard tanh The hard tanh function is sometimes preferred over the tanh function since it is computationally cheaper. It does however saturate for magnitudes of z greater than 1.']",text-embedding-3-small,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.990252641413291,1.0,0.9618858332808448,0.0588235294117647
52,What is the purpose of non-linearities in neural networks?,"The purpose of non-linearities in neural networks is to enable them to approximate complex functions that cannot be represented by linear transformations alone. Non-linearities allow neural networks to create non-linear decision boundaries, which are essential for improving classification performance on data that is not linearly separable.","['file name: lecture02 notes Backpropagation.  Neural networks. Forward computation. Backward propagation. Neuron Units. Max margin Loss. Gradient checks. Xavier parameter initialization. Learning rates. Adagrad. This set of notes introduces single and multilayer neural networks and how they can be used for classiÔ¨Åcation purposes. We then dis cuss how they can be trained using a distributed gradient descent technique known as backpropagation. We will see how the chain rule can be used to make parameter updates sequentially. After a rigorous mathematical discussion of neural networks we will discuss some practical tips and tricks in training neural networks involving neuron units non linearities gradient checks Xavier parameter ini tialization learning rates Adagrad etc. Lastly we will motivate the use of recurrent neural networks as a language model. 1 Neural Networks Foundations Figure 1 We see here how a non linear decision boundary separates the data very well. This is the prowess of neural networks. Fun Fact Neural networks are biologically in spired classiÔ¨Åers which is why they are often called artiÔ¨Åcial neural networks to distinguish them from the organic kind. However in reality human neural networks are so much more capable and complex from artiÔ¨Åcial neural net works that it is usually better to not draw too many parallels between the two. We established in our previous discussions the need for non linear classiÔ¨Åers since most data are not linearly separable and thus our classiÔ¨Åcation performance on them is limited. Neural networks are a family of classiÔ¨Åers with non linear decision boundary as seen in Figure 1. Now that we know the sort of decision boundaries neural networks create let us see how they manage doing so. 1.1 A Neuron A neuron is a generic computational unit that takes n inputs and produces a single output. What differentiates the outputs of different neurons is their parameters also referred to as their weights . One of the most popular choices for neurons is the sigmoid or binary lo gistic regression unit.', 'f on previous slide Why they re needed Neural networks do function approximation e.g. regression or classification Without non linearities deep neural networks can t do anything more than a linear transform Extra layers could just be compiled down into a single linear transform W1 W2 x Wx But with more layers that include non linearities they can approximate any complex function 5Training with cross entropy loss you use this in PyTorch Until now our objective was stated as to maximize the probability of the correct class y or equivalently we can minimize the negative log probability of that class Now restated in terms of cross entropy a concept from information theory Let the true probability distribution be p let our computed model probability be q The cross entropy is Assuming a ground truth or true or gold or target probability distribution that is 1 at the right class and 0 everywhere else p 0 0 1 0 0 then Because of one hot p the only term left is the negative log probability of the true class yi log ùëù ùë¶i ùë•i Cross entropy can be used in other ways with a more interesting p but for now just know that you ll want to use it as the loss in PyTorch 6Remember Stochastic Gradient Descent Update equation i.e. for each parameter In deep learning ùúÉincludes the data representation e.g. word vectors too How can we compute ùúÉùêΩ ùúÉ 1. By hand 2. Algorithmically the backpropagation algorithm ùõº step size or learning rate 7Lecture Plan 8 Gradients 1. Introduction 2. Matrix calculus 3.']",text-embedding-3-small,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.0000000000000004,1.0,0.929837958729138,0.04
53,What is cross entropy loss used for in PyTorch?,"Cross entropy loss is used in PyTorch to minimize the negative log probability of the correct class, which helps in training neural networks effectively by updating the model parameters to improve its performance in tasks like regression or classification. It is a concept from information theory that compares the true probability distribution with the computed model probability distribution. By using cross entropy loss, the model aims to improve its predictions by minimizing the difference between the two distributions. 

Source: Lecture02_Neural_Nets.","['f on previous slide Why they re needed Neural networks do function approximation e.g. regression or classification Without non linearities deep neural networks can t do anything more than a linear transform Extra layers could just be compiled down into a single linear transform W1 W2 x Wx But with more layers that include non linearities they can approximate any complex function 5Training with cross entropy loss you use this in PyTorch Until now our objective was stated as to maximize the probability of the correct class y or equivalently we can minimize the negative log probability of that class Now restated in terms of cross entropy a concept from information theory Let the true probability distribution be p let our computed model probability be q The cross entropy is Assuming a ground truth or true or gold or target probability distribution that is 1 at the right class and 0 everywhere else p 0 0 1 0 0 then Because of one hot p the only term left is the negative log probability of the true class yi log ùëù ùë¶i ùë•i Cross entropy can be used in other ways with a more interesting p but for now just know that you ll want to use it as the loss in PyTorch 6Remember Stochastic Gradient Descent Update equation i.e. for each parameter In deep learning ùúÉincludes the data representation e.g. word vectors too How can we compute ùúÉùêΩ ùúÉ 1. By hand 2. Algorithmically the backpropagation algorithm ùõº step size or learning rate 7Lecture Plan 8 Gradients 1. Introduction 2. Matrix calculus 3.', 'The cross entropy can never be lower than the true entropy so a model cannot err by underestimating the true entropy. We are Ô¨Ånally ready to see the relation between perplexity and cross entropy as we saw it in Eq. 3.51. Cross entropy is deÔ¨Åned in the limit as the length of the observed word sequence goes to inÔ¨Ånity. We will need an approximation to cross entropy relying on a sufÔ¨Åciently long sequence of Ô¨Åxed length. This approxima tion to the cross entropy of a model M P wi wi N 1 i 1 on a sequence of words W is H W 1 N logP w1w2 ...wN 3.53 The perplexity of a model P on a sequence of words W is now formally deÔ¨Åned as perplexity 2 raised to the power of this cross entropy3.9 SUMMARY 25 Perplexity W 2H W P w1w2 ...wN 1 N N s 1 P w1w2 ...wN N v u u t N Y i 1 1 P wi w1 ...wi 1 3.54 3.9 Summary This chapter introduced language modeling and the n gram one of the most widely used tools in language processing. Language models offer a way to assign a probability to a sentence or other sequence of words and to predict a word from preceding words. n grams are Markov models that estimate words from a Ô¨Åxed window of pre vious words. n gram probabilities can be estimated by counting in a corpus and normalizing the maximum likelihood estimate . n gram language models are evaluated extrinsically in some task or intrinsi cally using perplexity. The perplexity of a test set according to a language model is the geometric mean of the inverse test set probability computed by the model. Smoothing algorithms provide a more sophisticated way to estimate the prob ability of n grams. Commonly used smoothing algorithms for n grams rely on lower order n gram counts through backoff or interpolation. Both backoff and interpolation require discounting to create a probability dis tribution. Kneser Ney smoothing makes use of the probability of a word being a novel continuation.']",text-embedding-3-small,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999999999999988,1.0,0.9675997732045024,0.0476190476190476
54,What is the significance of the backpropagation algorithm?,"The significance of the backpropagation algorithm lies in its ability to efficiently calculate loss gradients for parameters in a neural network by utilizing the chain rule of differentiation. This allows for updating parameters through techniques like gradient descent, enabling the model to learn and improve its performance over time. Backpropagation is crucial for training neural networks as it provides a systematic way to adjust the model's parameters based on the error signal propagated backwards through the network, ultimately leading to better accuracy and convergence during the training process.","['No parameter updates are necessary if the cost is 0. Since we typically update parameters using gradient descent or a variant such as SGD we typically need the gradient information for any parameter as required in the update equation Œ∏ t 1 Œ∏ t Œ± Œ∏ t J Backpropagation is technique that allows us to use the chain rule of differentiation to calculate loss gradients for any parameter used in the feed forward computation on the model. To understand this further let us understand the toy network shown in Figure 5 for which we will perform backpropagation. Figure 5 This is a 4 2 1 neural network where neuron j on layer k receives input z k j and produces activation output a k j . Here we use a neural network with a single hidden layer and a single unit output. Let us establish some notation that will make it easier to generalize this model later xi is an input to the neural network. s is the output of the neural network. Each layer including the input and output layers has neurons which receive an input and produce an output. The j th neuron of layer k receives the scalar input z k j and produces the scalar activation output a k j . We will call the backpropagated error calculated at z k j as Œ¥ k j . Layer 1 refers to the input layer and not the Ô¨Årst hidden layer. For the input layer xj z 1 j a 1 j . W k is the transfer matrix that maps the output from the k th layer to the input to the k 1 th Thus W 1 W and W 2 U to put this new generalized notation in perspective of Section 1.3. Backpropagation Notation xi is an input to the neural network. s is the output of the neural net work. The j th neuron of layer k receives the scalar input z k j and produces the scalar activation output a k j . For the input layer xj z 1 j a 1 j . W k is the transfer matrix that maps the output from the k th layer to the input to the k 1 th. Thus W 1 W and W 2 UT using notation from Section 1.3.', 'Figure 6 This subnetwork shows the relevant parts of the network required to update W 1 ij Let us discuss the error sharing distribution interpretation of backpropagation better using Figure 6 as an example. Say we were to update W 1 14 1. We start with the an error signal of 1 propagating backwards from a 3 1 . 2. We then multiply this error by the local gradient of the neuron which maps z 3 1 to a 3 1 . This happens to be 1 in this case and thus the error is still 1. This is now known as Œ¥ 3 1 1. 3. At this point the error signal of 1 has reached z 3 1 . We now need to distribute the error signal so that the fair share of the error reaches to a 2 1 . 4. This amount is the error signal at z 3 1 Œ¥ 3 1 W 2 1 W 2 1 . Thus the error at a 2 1 W 2 1 . 5. As we did in step 2 we need to move the error across the neuron which maps z 2 1 to a 2 1 . We do this by multiplying the error signal at a 2 1 by the local gradient of the neuron which happens to be f z 2 1 . 6. Thus the error signal at z 2 1 is f z 2 1 W 2 1 . This is known as Œ¥ 2 1 . 7. Finally we need to distribute the fair share of the error to W 1 14 by simply multiplying it by the input it was responsible for for warding which happens to be a 1 4 . 8. Thus the gradient of the loss with respect to W 1 14 is calculated to be a 1 4 f z 2 1 W 2 1 . Notice that the result we arrive at using this approach is exactly the same as that we arrived at using explicit differentiation earlier.']",text-embedding-3-small,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.000000000000001,1.0,0.9525436040262028,0.03125
55,What does the Jacobian matrix represent in the context of neural networks?,"The Jacobian matrix in the context of neural networks represents an m x n matrix of partial derivatives, where m is the number of outputs and n is the number of inputs of a given function. It generalizes the concept of the gradient to functions with multiple outputs and inputs, providing a comprehensive view of how each input affects each output simultaneously. The Jacobian matrix is crucial for understanding the sensitivity of the outputs to changes in the inputs in complex neural network architectures. 

Source: lecture02_Neural_Nets.txt","['So if k i everything in the sum is zero and the gradient is zero. Otherwise the only nonzero element of the sum is when l j so we just get xj. Thus we Ô¨Ånd zk Wij xj if k i and 0 if otherwise. Another way of writing this is z Wij 0 . . . 0 xj 0 . . . 0 ith element Now let s compute J Wij J Wij J z z Wij Œ¥ z Wij m X k 1 Œ¥k zk Wij Œ¥ixj the only nonzero term in the sum is Œ¥i zi Wij . To get J W we want a ma trix where entry i j is Œ¥ixj. This matrix is equal to the outer product J W Œ¥T xT 6 Row vector time matrix with respect to the matrix z xW Œ¥ J z what is J W Œ¥ z W A similar computation to 5 shows that J W xT Œ¥ . 7 Cross entropy loss with respect to logits ÀÜ y softmax Œ∏ J CE y ÀÜ y what is J Œ∏ The gradient is J Œ∏ ÀÜ y y or ÀÜ y y T if y is a column vector . 4These identities will be enough to let you quickly compute the gradients for many neural networks. However it s important to know how to compute Jacobians for other functions as well in case they show up. Some examples if you want practice dot product of two vectors elementwise product of two vectors 2 norm of a vector. Feel free to use these identities in the assignments. One option is just to memorize them. Another option is to Ô¨Ågure them out by looking at the dimensions. For example only one ordering orientation of Œ¥ and x will produce the correct shape for J W assuming W is not square . 4 Gradient Layout Jacobean formulation is great for applying the chain rule you just have to mul tiply the Jacobians. However when doing SGD it s more convenient to follow the convention the shape of the gradient equals the shape of the parameter as we did when computing J W . That way subtracting the gradient times the learning rate from the parameters is easy. We expect answers to homework questions to follow this convention.', 'Introduction 2. Matrix calculus 3. BackpropagationComputing Gradients by Hand 9 Matrix calculus Fully vectorized gradients Multivariable calculus is just like single variable calculus if you use matrices Much faster and more useful than non vectorized gradients But doing a non vectorized gradient can be good for intuition recall the first lecture for an exampleGradients Given a function with 1 output and 1 input ùëìùë• ùë•3 It s gradient slope is its derivative ùëëf 3ùë•2 10 ùëëùë• How much will the output change if we change the input a bit At x 1 it changes about 3 times as much 1.013 1.03 At x 4 it changes about 48 times as much 4.013 64.48Gradients Given a function with 1 output and n inputs Its gradient is a vector of partial derivatives with respect to each input 11Jacobian Matrix Generalization of the Gradient Given a function with m outputs and n inputs It s Jacobian is an m x n matrix of partial derivatives 12Chain Rule For composition of one variable functions multiply derivatives For multiple variables functions multiply Jacobians 13Example Jacobian Elementwise activation Function 14Example Jacobian Elementwise activation Function Function has n outputs and n inputs n by n Jacobian 15Example Jacobian Elementwise activation Function 16Example Jacobian Elementwise activation Function 17Example Jacobian Elementwise activation Function 18Other Jacobians Compute these at home for practice Check your answers with the lecture notes 19Other Jacobians Compute these at home for practice Check your answers with the lecture notes 20Other Jacobians Compute these at home for practice Check your answers with the lecture notes Fine print This is the correct Jacobian. Later we discuss the shape convention using it the answer would be h. 21Other Jacobians Compute these at home for practice 22Back to our Neural Net x xmuseums xin xParis xare xamazing 23Back to our Neural Net Let s find Really we care about the gradient of the loss Jt but we will compute the gradient of the score for simplicity x xmuseums xin xParis xare xamazing 241.']",text-embedding-3-small,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999999999999988,1.0,0.9400541351341388,0.6956521739130435
56,How is the derivative with respect to a matrix described?,"The derivative with respect to a matrix is described as a Jacobian matrix. When computing the gradient of a loss function with respect to a matrix W, the Jacobian J W is arranged as an n x m matrix, where each entry represents the derivative of the loss function with respect to a specific entry in the matrix W. This arrangement allows for easier handling of gradients during optimization algorithms like gradient descent.","['So z x ij zi xj xj f xi f xi if i j 0 if otherwise So we see that the Jacobian z x is a diagonal matrix where the entry at i i is the derivative of f applied to xi. We can write this as z x diag f x . Since multiplication by a diagonal matrix is the same as doing elementwise multiplication by the diagonal we could also write f x when applying the chain rule. 5 Matrix times column vector with respect to the matrix z W x Œ¥ J z what is J W J z z W Œ¥ z W This is a bit more complicated than the other identities. The reason for in cluding J z in the above problem formulation will become clear in a moment. First suppose we have a loss function J a scalar and are computing its gradient with respect to a matrix W Rn m. Then we could think of J as a function of W taking nm inputs the entries of W to a single output J . This means the Jacobian J W would be a 1 nm vector. But in practice this is not a very useful way of arranging the gradient. It would be much nicer if the derivatives were in a n m matrix like this J W J W11 . . . J W1m . . . ... . . . J Wn1 . . . J Wnm Since this matrix has the same shape as W we could just subtract it times the learning rate from W when doing gradient descent. So in a slight abuse of notation let s Ô¨Ånd this matrix as J W instead. This way of arranging the gradients becomes complicated when computing z W . Unlike J z is a vector. So if we are trying to rearrange the gradients like with J W z W would be an n m n tensor Luckily we can avoid the issue by taking the gradient with respect to a single weight Wij instead. 3z Wij is just a vector which is much easier to deal with. We have zk m X l 1 Wklxl zk Wij m X l 1 xl Wij Wkl Note that Wij Wkl 1 if i k and j l and 0 if otherwise. So if k i everything in the sum is zero and the gradient is zero.', 'Using the regular chain rule we can compute the derivative of g as the Jacobian g x xg1 f1 x f2 x xg2 f1 x f2 x g1 f1 f1 x g1 f2 f2 x g2 f1 f1 x g2 f2 f2 x 1And we see this is the same as multiplying the two Jacobians g x g f f x g1 f1 g1 f2 g2 f1 g2 f2 f1 x f2 x 3 Useful Identities This section will now go over how to compute the Jacobian for several simple functions. It will provide some useful identities you can apply when taking neu ral network gradients. 1 Matrix times column vector with respect to the column vector z W x what is z x Suppose W Rn m. Then we can think of z as a function of x taking an m dimensional vector to an n dimensional vector. So its Jacobian will be n m. Note that zi m X k 1 Wikxk So an entry z x ij of the Jacobian will be z x ij zi xj xj m X k 1 Wikxk m X k 1 Wik xj xk Wij because xj xk 1 if k j and 0 if otherwise. So we see that z x W 2 Row vector times matrix with respect to the row vector z xW what is z x A computation similar to 1 shows that z x W T . 3 A vector with itself z x what is z x We have zi xi. So z x ij zi xj xj xi 1 if i j 0 if otherwise So we see that the Jacobian z x is a diagonal matrix where the entry at i i is 1. This is just the identity matrix z x I . When applying the chain 2rule this term will disappear because a matrix or vector multiplied by the identity matrix does not change. 4 An elementwise function applied a vector z f x what is z x Since f is being applied elementwise we have zi f xi .']",text-embedding-3-small,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9682711592357608,1.0,0.8858172847164687,0.0344827586206896
57,What role does automatic differentiation play in modern deep learning frameworks?,"Automatic differentiation plays a crucial role in modern deep learning frameworks as they compute gradients for you, which simplifies the process of training neural networks. Understanding how automatic differentiation works is essential for debugging and improving models, as backpropagation doesn't always work perfectly out of the box. It allows developers to focus on designing and implementing neural network architectures without having to manually calculate gradients, making the training process more efficient and less error-prone. (Source: SEP 775 - Computational Natural Language Processing, lecture02_Neural_Nets.txt)","['do backpropagation for you but mainly leave layer node writer to hand calculate the local derivative 74Backprop Implementations 75Implementation forward backward API 76Implementation forward backward API 77Manual Gradient checking Numeric Gradient For small h 1e 4 Easy to implement correctly But approximate and very slow You have to recompute f for every parameter of our model Useful for checking your implementation In the old days we hand wrote everything doing this everywhere was the key test Now much less needed you can use it to check layers are correctly implemented 78Summary We ve mastered the core technology of neural nets Backpropagation recursively and hence efficiently apply the chain rule along computation graph downstream gradient upstream gradient x local gradient Forward pass compute results of operations and save intermediate values Backward pass apply chain rule to compute gradients 79Why learn all these details about gradients 80 Modern deep learning frameworks compute gradients for you But why take a class on compilers or systems when they are implemented for you Understanding what is going on under the hood is useful Backpropagation doesn t always work perfectly out of the box Understanding why is crucial for debugging and improving models See Karpathy article https medium.com @karpathy yes you should understand backprop e2f06eab496b Example in future lecture exploding and vanishing gradients', 'Introduction 2. Matrix calculus 3. BackpropagationComputing Gradients by Hand 9 Matrix calculus Fully vectorized gradients Multivariable calculus is just like single variable calculus if you use matrices Much faster and more useful than non vectorized gradients But doing a non vectorized gradient can be good for intuition recall the first lecture for an exampleGradients Given a function with 1 output and 1 input ùëìùë• ùë•3 It s gradient slope is its derivative ùëëf 3ùë•2 10 ùëëùë• How much will the output change if we change the input a bit At x 1 it changes about 3 times as much 1.013 1.03 At x 4 it changes about 48 times as much 4.013 64.48Gradients Given a function with 1 output and n inputs Its gradient is a vector of partial derivatives with respect to each input 11Jacobian Matrix Generalization of the Gradient Given a function with m outputs and n inputs It s Jacobian is an m x n matrix of partial derivatives 12Chain Rule For composition of one variable functions multiply derivatives For multiple variables functions multiply Jacobians 13Example Jacobian Elementwise activation Function 14Example Jacobian Elementwise activation Function Function has n outputs and n inputs n by n Jacobian 15Example Jacobian Elementwise activation Function 16Example Jacobian Elementwise activation Function 17Example Jacobian Elementwise activation Function 18Other Jacobians Compute these at home for practice Check your answers with the lecture notes 19Other Jacobians Compute these at home for practice Check your answers with the lecture notes 20Other Jacobians Compute these at home for practice Check your answers with the lecture notes Fine print This is the correct Jacobian. Later we discuss the shape convention using it the answer would be h. 21Other Jacobians Compute these at home for practice 22Back to our Neural Net x xmuseums xin xParis xare xamazing 23Back to our Neural Net Let s find Really we care about the gradient of the loss Jt but we will compute the gradient of the score for simplicity x xmuseums xin xParis xare xamazing 241.']",text-embedding-3-small,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.0000000000000002,0.0,0.9206751587456968,0.1666666666666666
58,What is the purpose of manual gradient checking?,"The purpose of manual gradient checking is to compare analytical gradients with numerical gradients in order to verify the correctness of the analytical gradients. This process helps ensure that the gradients calculated analytically are accurate and can be used for training neural networks effectively. It provides a way to validate the implementation of backpropagation by comparing the gradients computed through backpropagation with numerical approximations, helping to catch errors in the gradient computation process. 

Source: SEP 775 - Computational Natural Language Processing, lecture02_notes_Backpropagation.txt","['We note that this deÔ¨Ånition of the numerical gradient follows very naturally from the deÔ¨Ånition of the derivative where in the scalar case f x f x œµ f x œµ Gradient checks are a great way to compare analytical and numerical gradients. Analytical gradients should be close and numerical gradients can be calculated using f Œ∏ J Œ∏ i J Œ∏ i 2œµ J Œ∏ i and J Œ∏ i can be evalu ated using two forward passes. An implementation of this can be seen in Snippet 2.1. Of course there is a slight difference the deÔ¨Ånition above only perturbs x in the positive direction to compute the gradient. While it would have been perfectly acceptable to deÔ¨Åne the numerical gradi ent in this way in practice it is often more precise and stable to use the centered difference formula where we perturb a parameter in both directions. The intuition is that to get a better approximation of the derivative slope around a point we need to examine the func tion f s behavior both to the left and right of that point. It can also be shown using Taylor s theorem that the centered difference formula has an error proportional to œµ2 which is quite small whereas the derivative deÔ¨Ånition is more error prone. Now a natural question you might ask is if this method is so pre cise why do we not use it to compute all of our network gradients instead of applying back propagation The simple answer as hinted earlier is inefÔ¨Åciency recall that every time we want to compute the gradient with respect to an element we need to make two forward passes through the network which will be computationally expen sive. Furthermore many large scale neural networks can contain millions of parameters and computing two passes per parameter is clearly not optimal. And since in optimization techniques such as SGD we must compute the gradients once per iteration for several thousands of iterations it is obvious that this method quickly grows intractable. This inefÔ¨Åciency is why we only use gradient check to verify the correctness of our analytic gradients which are much quicker to compute.', 'This means that when stochastic gradient descent SGD reaches the wall and does a gradient descent step it will be forced to jump across the valley moving perpen dicular to the steep walls possibly leaving the valley and disrupting the learning process. The dashed arrows in Fig. 6 correspond to ignoring the norm of this large step ensuring that the model stays close to the wall. The key insight is that all the steps taken when the gradient explodes are aligned with v and ignore other descent direction i.e. the model moves perpendicular to the wall . At the wall a small norm step in the direction of the gradient there fore merely pushes us back inside the smoother low curvature region besides the wall whereas a regular gradient step would bring us very far thus slowing or preventing further training. Instead with a bounded step we get back in that smooth region near the wall where SGD is free to explore other descent directions. The important addition in this scenario to the classical high curvature valley is that we assume that the val ley is wide as we have a large region around the wall where if we land we can rely on Ô¨Årst order methods to move towards the local minima. This is why just clipping the gradient might be suÔ¨Écient not requiring the use a second order method. Note that this algo rithm should work even when the rate of growth of the gradient is not the same as the one of the curvature a case for which a second order method would fail as the ratio between the gradient and curvature could still explode . Our hypothesis could also help to understand the re cent success of the Hessian Free approach compared to other second order methods. There are two key dif ferences between Hessian Free and most other second order algorithms. First it uses the full Hessian matrix and hence can deal with exploding directions that are not necessarily axis aligned. Second it computes a new estimate of the Hessian matrix before each up date step and can take into account abrupt changes in curvature such as the ones suggested by our hypothe sis while most other approaches use a smoothness as sumption i.e. averaging 2nd order signals over many steps. 3. Dealing with the exploding and vanishing gradient 3.1.']",text-embedding-3-small,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999999999999996,0.6666666666666666,0.9342756035921048,0.04
59,Why is understanding gradients important despite automatic differentiation?,"Understanding gradients is important despite automatic differentiation because it helps in debugging and improving models. Backpropagation doesn't always work perfectly out of the box, so understanding why is crucial for addressing issues like exploding and vanishing gradients. Source: SEP 775 - Computational Natural Language Processing, lecture02_Neural_Nets.txt","['We note that this deÔ¨Ånition of the numerical gradient follows very naturally from the deÔ¨Ånition of the derivative where in the scalar case f x f x œµ f x œµ Gradient checks are a great way to compare analytical and numerical gradients. Analytical gradients should be close and numerical gradients can be calculated using f Œ∏ J Œ∏ i J Œ∏ i 2œµ J Œ∏ i and J Œ∏ i can be evalu ated using two forward passes. An implementation of this can be seen in Snippet 2.1. Of course there is a slight difference the deÔ¨Ånition above only perturbs x in the positive direction to compute the gradient. While it would have been perfectly acceptable to deÔ¨Åne the numerical gradi ent in this way in practice it is often more precise and stable to use the centered difference formula where we perturb a parameter in both directions. The intuition is that to get a better approximation of the derivative slope around a point we need to examine the func tion f s behavior both to the left and right of that point. It can also be shown using Taylor s theorem that the centered difference formula has an error proportional to œµ2 which is quite small whereas the derivative deÔ¨Ånition is more error prone. Now a natural question you might ask is if this method is so pre cise why do we not use it to compute all of our network gradients instead of applying back propagation The simple answer as hinted earlier is inefÔ¨Åciency recall that every time we want to compute the gradient with respect to an element we need to make two forward passes through the network which will be computationally expen sive. Furthermore many large scale neural networks can contain millions of parameters and computing two passes per parameter is clearly not optimal. And since in optimization techniques such as SGD we must compute the gradients once per iteration for several thousands of iterations it is obvious that this method quickly grows intractable. This inefÔ¨Åciency is why we only use gradient check to verify the correctness of our analytic gradients which are much quicker to compute.', 'do backpropagation for you but mainly leave layer node writer to hand calculate the local derivative 74Backprop Implementations 75Implementation forward backward API 76Implementation forward backward API 77Manual Gradient checking Numeric Gradient For small h 1e 4 Easy to implement correctly But approximate and very slow You have to recompute f for every parameter of our model Useful for checking your implementation In the old days we hand wrote everything doing this everywhere was the key test Now much less needed you can use it to check layers are correctly implemented 78Summary We ve mastered the core technology of neural nets Backpropagation recursively and hence efficiently apply the chain rule along computation graph downstream gradient upstream gradient x local gradient Forward pass compute results of operations and save intermediate values Backward pass apply chain rule to compute gradients 79Why learn all these details about gradients 80 Modern deep learning frameworks compute gradients for you But why take a class on compilers or systems when they are implemented for you Understanding what is going on under the hood is useful Backpropagation doesn t always work perfectly out of the box Understanding why is crucial for debugging and improving models See Karpathy article https medium.com @karpathy yes you should understand backprop e2f06eab496b Example in future lecture exploding and vanishing gradients']",text-embedding-3-small,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999999999999994,0.75,0.936509961300784,0.0833333333333333
60,What is the core concept introduced in this class that leads to GPT-3 and ChatGPT?,"The core concept introduced in this class that leads to GPT-3 and ChatGPT is the idea of emergent zero-shot learning, which refers to the ability of models like GPT-2 and GPT-3 to perform tasks without specific examples or gradient updates by specifying the right sequence prediction problem. This allows these models to excel at various tasks without task-specific fine-tuning, showcasing their adaptability and versatility in natural language processing tasks.","['2019 Same architecture as GPT just bigger 117M 1.5B But trained on much more data 4GB 40GB of internet text data WebText Scrape links posted on Reddit w at least 3 upvotes rough proxy of human quality GPT 2018 GPT 2 2019 117M 1.5B 15One key emergent ability in GPT 2 is zero shot learning the ability to do many tasks with no examples and no gradient updates by simply Specifying the right sequence prediction problem e.g. question answering Passage Tom Brady... Q Where was Tom Brady born A ... Comparing probabilities of sequences e.g. Winograd Schema Challenge Levesque 2011 16 The cat couldn t fit into the hat because it was too big. Does it the cat or the hat Is P ...because the cat was too big P ...because the hat was too big Emergent zero shot learning Radford et al. 2019Emergent zero shot learning GPT 2 beats SoTA on language modeling benchmarks with no task specific fine tuning LAMBADA language modeling w long discourse dependencies Paperno et al. 2016 Radford et al. 2019 17Emergent zero shot learning 18 You can get interesting zero shot behavior if you re creative enough with how you specify your task Summarization on CNN DailyMail dataset See et al. 2017 SAN FRANCISCO California CNN A magnitude 4.2 earthquake shook the San Francisco ... overturn unstable Radford et al. 2019 2018 SoTA Supervised 287K Too Long Didn t Read Prompting objects. TL DR Select from article ROUGEEmergent abilities of large language models GPT 3 2020 19 GPT 3 175B parameters Brown et al.', 'These possibilities exist on a spectrum ranging from demonstrations in the training set that are drawn from exactly the same distribution as those at test time to recognizing the same task but in a different format to adapting to a speciÔ¨Åc style of a general task such as QA to learning a skill entirely de novo. Where GPT 3 is on this spectrum may also vary from task to task. Synthetic tasks such as wordscrambling or deÔ¨Åning nonsense words seem especially likely to be learned de novo whereas translation clearly must be learned during pretraining although possibly from data that is very different in organization and style than the test data. Ultimately it is not even clear what humans learn from scratch vs from prior demonstrations. Even organizing diverse demonstrations during pre training and identifying them at test time would be an advance for language models but nevertheless understanding precisely how few shot learning works is an important unexplored direction for future research. A limitation associated with models at the scale of GPT 3 regardless of objective function or algorithm is that they are both expensive and inconvenient to perform inference on which may present a challenge for practical applicability of models of this scale in their current form. One possible future direction to address this is distillation HVD15 of large models down to a manageable size for speciÔ¨Åc tasks. Large models such as GPT 3 contain a very wide range of skills most of which are not needed for a speciÔ¨Åc task suggesting that in principle aggressive distillation may be possible. Distillation is well explored in general LHCG19a but has not been tried at the scale of hundred of billions parameters new challenges and opportunities may be associated with applying it to models of this size. Finally GPT 3 shares some limitations common to most deep learning systems its decisions are not easily interpretable it is not necessarily well calibrated in its predictions on novel inputs as observed by the much higher variance in performance than humans on standard benchmarks and it retains the biases of the data it has been trained on. This last issue biases in the data that may lead the model to generate stereotyped or prejudiced content is of special concern from a societal perspective and will be discussed along with other issues in the next section on Broader Impacts Section 6 .']",text-embedding-3-small,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999346746160052,0.5,0.8785197484662766,0.0625
61,What is the purpose of Dropout as mentioned in the document?,"The purpose of Dropout, as mentioned in the document, is to act as a powerful technique for regularization in neural networks. During training, Dropout randomly drops a subset of neurons with a certain probability during each forward and backward pass, which helps prevent overfitting, allows the network to learn more meaningful information from the data, and typically leads to higher performance on the task at hand. By training exponentially many smaller networks at once and averaging over their predictions, Dropout helps improve the overall performance of the neural network.","['Imposing such a con straint can also be interpreted as the prior Bayesian belief that the optimal weights are close to zero how close depends on the value of Œª. Choosing the right value of Œª is critical and must be chosen via hyperparameter tuning. Too high a value of Œª causes most of the weights to be set too close to 0 and the model does not learn anything meaningful from the training data often obtaining poor ac curacy on training validation and testing sets. Too low a value and we fall into the domain of overÔ¨Åtting once again. It must be noted that the bias terms are not regularized and do not contribute to the cost term above try thinking about why this is the case There are indeed other types of regularization that are sometimes used such as L1 regularization which sums over the absolute values rather than squares of parameter elements however this is less commonly applied in practice since it leads to sparsity of parameter weights. In the next section we discuss dropout which effectively acts as another form of regularization by randomly dropping i.e. setting to zero neurons in the forward pass. 2.3 Dropout Dropout is a powerful technique for regularization Ô¨Årst introduced by Srivastava et al. in Dropout A Simple Way to Prevent Neural Net works from OverÔ¨Åtting. The idea is simple yet effective during train ing we will randomly drop with some probability 1 p a subset of neurons during each forward backward pass or equivalently we will keep alive each neuron with a probability p . Then during testing we will use the full network to compute our predictions. The result is that the network typically learns more meaningful informa tion from the data is less likely to overÔ¨Åt and usually obtains higher performance overall on the task at hand. One intuitive reason why this technique should be so effective is that what dropout is doing is essentially doing is training exponentially many smaller networks at once and averaging over their predictions. Dropout applied to an artiÔ¨Åcial neural network. Image credits to Srivastava et al. In practice the way we introduce dropout is that we take the out put h of each layer of neurons and keep each neuron with prob ability p and else set it to 0. Then during back propagation we only pass gradients through neurons that were kept alive during the forward pass.', 'like an ensemble model Nowadays usually thought of as strong feature dependent regularizer Wager Wang Liang 2013DropoutDropout Srivastava Hinton Krizhevsky Sutskever Salakhutdinov 2012 JMLR 2014 During training For each data point each time Randomly set input to 0 with probability ùëù dropout ratio often p 0.5 except p 0.15 for input layer via dropout mask During testing Multiply all weights by 1 ùëù No other dropout ùë•2 ùë•3 0 ùë§1 ùë§2 ùë§3 ùë§4 ùëè 1 ùë¶ ùë§1ùë•1 ùë§2ùë•2 ùë§3ùë•3 ùë• ùë• 1 1 Train 1 Train 2 Test ùë•3 0 1 ùë¶ ùë§1ùë•1 ùë§3ùë•3 ùëè ùë§1 ùë§2 ùë§3 ùë§4 ùëè ùë•1 0 ùë•2 ùë•3 ùë•4 ùë§1 ùë§2 ùë§3 ùë§4 ùëè 1 ùë¶ 1 ùëù ùë§1ùë•1 ùë§2ùë•2 ùë§3ùë•3 ùë§4ùë•4 6Vectorization E.g. looping over word vectors versus concatenating them all into one large matrix and then multiplying the softmax weights with that matrix for loop 1000 loops best of 3 639 ¬µs per loop Using single a C x N matrix 10000 loops best of 3 53.8 ¬µs per loop Matrices are awesome Always try to use vectors and matrices rather than for loops The speed gain goes from 1 to 2 orders of magnitude with GPUs 7Parameter Initialization You normally must initialize weights to small random values i.e. not zero matrices To avoid symmetries that prevent learning specialization Initialize hidden layer biases to 0 and output or reconstruction biases to optimal value if weights were 0 e.g.']",text-embedding-3-small,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9300597327790432,1.0,0.9248634002402332,0.0625
62,What are some approaches to address the vanishing gradient problem in RNNs?,"Some approaches to address the vanishing gradient problem in RNNs include using RNNs with separate memory (such as LSTMs), creating more direct and linear pass-through connections in the model (such as attention mechanisms and residual connections), and utilizing Long Short-Term Memory RNNs (LSTMs) proposed by Hochreiter and Schmidhuber in 1997. These approaches aim to tackle the challenge of preserving information over many timesteps and mitigating the issue of vanishing gradients in RNNs. 

Source: Lecture03_RNNs_and_LLMs","['Problems with RNNs Vanishing and Exploding Gradients 48Vanishing gradient intuition 49Vanishing gradient intuition chain rule 50Vanishing gradient intuition chain rule 51Vanishing gradient intuition chain rule 52Vanishing gradient intuition What happens if these are small Vanishing gradient problem When these are small the gradient signal gets smaller and smaller as it backpropagates further 53Vanishing gradient proof sketch linear case Recall What if were the identity function 53 Source On the difficulty of training recurrent neural networks Pascanu et al 2013. http proceedings.mlr.press v28 pascanu13.pdf and supplemental materials at http proceedings.mlr.press v28 pascanu13 supp.pdf If Wh is small then this term gets exponentially problematic as becomes large chain rule Consider the gradient of the loss on step with respect to the hidden state on some previous step . Let chain rule value of ONLY READ IF INTERESTEDVanishing gradient proof sketch linear case What s wrong with Consider if the eigenvalues of are all less than 1 We can write as a basis What about nonlinear activations i.e. what we use Pretty much the same thing except the proof requires for some dependent on dimensionality and Source On the difficulty of training recurrent neural networks Pascanu et al 2013. http proceedings.mlr.press v28 pascanu13.pdf and supplemental materials at http proceedings.mlr.press v28 pascanu13 supp.pdf 54 eigenvectors using the eigenvectors of Approaches 0 as grows so gradient vanishes sufficient but not necessary ONLY READ IF INTERESTEDWhy is vanishing gradient a problem Gradient signal from far away is lost because it s much smaller than gradient signal from close by. So model weights are updated only with respect to near effects not long term effects. 56Effect of vanishing gradient on RNN LM 57 LM task When she tried to print her tickets she found that the printer was out of toner. She went to the stationery store to buy more toner. It was very overpriced. After installing the toner into the printer she finally printed her To learn from this training example the RNN LM needs to model the dependency between tickets on the 7th step and the target word tickets at the end.', 'But if the gradient is small the model can t learn this dependency So the model is unable to predict similar long distance dependencies at test timeWhy is exploding gradient a problem If the gradient becomes too big then the SGD update step becomes too big learning rate gradient This can cause bad updates we take too large a step and reach a weird and bad parameter configuration with large loss You think you ve found a hill to climb but suddenly you re in Iowa In the worst case this will result in Inf or NaN in your network then you have to restart training from an earlier checkpoint 58Gradient clipping solution for exploding gradient 58 Gradient clipping if the norm of the gradient is greater than some threshold scale it down before applying SGD update Intuition take a step in the same direction but a smaller step In practice remembering to clip gradients is important but exploding gradients are an easy problem to solve Source On the difficulty of training recurrent neural networks Pascanu et al 2013. http proceedings.mlr.press v28 pascanu13.pdfHow to fix the vanishing gradient problem The main problem is that it s too difficult for the RNN to learn to preserve information over many timesteps. In a vanilla RNN the hidden state is constantly being rewritten First off next time How about an RNN with separate memory which is added to LSTMs And then Creating more direct and linear pass through connections in model Attention residual connections etc. 60Long Short Term Memory RNNs LSTMs A type of RNN proposed by Hochreiter and Schmidhuber in 1997 as a solution to the problem of vanishing gradients Everyone cites that paper but really a crucial part of the modern LSTM is from Gers et al. 2000 Only started to be recognized as promising through the work of S s student Alex Graves c. 2006 Work in which he also invented CTC connectionist temporal classification for speech recognition But only really became well known after Hinton brought it to Google in 2013 Following Graves having been a postdoc with Hinton 61 Hochreiter and Schmidhuber 1997. Long short term memory. https www.bioinf.jku.at publications older 2604.pdf Gers Schmidhuber and Cummins 2000. Learning to Forget Continual Prediction with LSTM.']",text-embedding-3-small,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.0000000000000009,1.0,0.8867885742759666,0.0625
63,"What is perplexity, and how is it used in evaluating language models?","Perplexity is a measure used in evaluating language models. It is the geometric mean of the probabilities assigned by the model to each word in a test set. A lower perplexity indicates that the model is more confident in predicting the test set. In evaluating language models, minimizing perplexity is equivalent to maximizing the test set probability according to the language model. Perplexity can also be interpreted as the weighted average branching factor of a language, representing the number of possible next words that can follow any word. It is important to note that the perplexity of two language models is only comparable if they use identical vocabularies, and intrinsic improvements in perplexity do not guarantee extrinsic improvements in language processing tasks like speech recognition or machine translation. Perplexity is commonly used as a quick check on an algorithm's performance, but any improvement in perplexity should be confirmed by a real task evaluation before concluding the model evaluation.

Source: Lecture notes ""N-gram Language Models""","['Here s the perplexity of W with a unigram language model just the geometric mean of the unigram probabilities perplexity W N v u u t N Y i 1 1 P wi 3.16 The perplexity of W computed with a bigram language model is still a geometric mean but now of the bigram probabilities perplexity W N v u u t N Y i 1 1 P wi wi 1 3.17 Note that because of the inverse in Eq. 3.15 the higher the conditional probabil ity of the word sequence the lower the perplexity. Thus minimizing perplexity is equivalent to maximizing the test set probability according to the language model. What we generally use for word sequence in Eq. 3.15 or Eq. 3.17 is the entire se quence of words in some test set. Since this sequence will cross many sentence boundaries we need to include the begin and end sentence markers s and s in the probability computation. We also need to include the end of sentence marker s but not the beginning of sentence marker s in the total count of word to kens N. There is another way to think about perplexity as the weighted average branch ing factor of a language. The branching factor of a language is the number of possi ble next words that can follow any word. Consider the task of recognizing the digits3.2 EVALUATING LANGUAGE MODELS 9 in English zero one two ... nine given that both in some training set and in some test set each of the 10 digits occurs with equal probability P 1 10. The perplexity of this mini language is in fact 10. To see that imagine a test string of digits of length N and assume that in the training set all the digits occurred with equal probability. By Eq. 3.15 the perplexity will be perplexity W P w1w2 ...wN 1 N 1 10 N 1 N 1 10 1 10 3.18 But suppose that the number zero is really frequent and occurs far more often than other numbers. Let s say that 0 occur 91 times in the training set and each of the other digits occurred 1 time each.', 'Any kind of knowledge of the test set can cause the perplexity to be artiÔ¨Åcially low. The perplexity of two language models is only comparable if they use identical vocabularies. An intrinsic improvement in perplexity does not guarantee an extrinsic im provement in the performance of a language processing task like speech recognition10 CHAPTER 3 N GRAM LANGUAGE MODELS or machine translation. Nonetheless because perplexity often correlates with such improvements it is commonly used as a quick check on an algorithm. But a model s improvement in perplexity should always be conÔ¨Årmed by an end to end evaluation of a real task before concluding the evaluation of the model. 3.3 Sampling sentences from a language model One important way to visualize what kind of knowledge a language model embodies is to sample from it. Sampling from a distribution means to choose random points sampling according to their likelihood. Thus sampling from a language model which rep resents a distribution over sentences means to generate some sentences choosing each sentence according to its likelihood as deÔ¨Åned by the model. Thus we are more likely to generate sentences that the model thinks have a high probability and less likely to generate sentences that the model thinks have a low probability. This technique of visualizing a language model by sampling was Ô¨Årst suggested very early on by Shannon 1951 and Miller and Selfridge 1950 . It s simplest to visualize how this works for the unigram case. Imagine all the words of the English language covering the probability space between 0 and 1 each word covering an interval proportional to its frequency. Fig. 3.3 shows a visualization using a unigram LM computed from the text of this book. We choose a random value between 0 and 1 Ô¨Ånd that point on the probability line and print the word whose interval includes this chosen value. We continue choosing random numbers and generating words until we randomly generate the sentence Ô¨Ånal token s .']",text-embedding-3-small,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9077103422875468,0.8,0.9467094731697058,0.0333333333333333
64,How does LSTM solve the vanishing gradient problem?,"LSTMs (Long Short Term Memory networks) were proposed as a solution to the vanishing gradient problem in RNNs. They address this issue by introducing a mechanism that allows the network to retain information over many time steps. This is achieved through the use of a separate memory component in LSTMs, which helps in preserving long-term dependencies by controlling the flow of information through a series of gates. Additionally, LSTMs incorporate more direct and linear pass-through connections in the model, such as attention mechanisms and residual connections, which further aid in mitigating the vanishing gradient problem. (Source: lecture03_RNNs_and_LLMs)","['But if the gradient is small the model can t learn this dependency So the model is unable to predict similar long distance dependencies at test timeWhy is exploding gradient a problem If the gradient becomes too big then the SGD update step becomes too big learning rate gradient This can cause bad updates we take too large a step and reach a weird and bad parameter configuration with large loss You think you ve found a hill to climb but suddenly you re in Iowa In the worst case this will result in Inf or NaN in your network then you have to restart training from an earlier checkpoint 58Gradient clipping solution for exploding gradient 58 Gradient clipping if the norm of the gradient is greater than some threshold scale it down before applying SGD update Intuition take a step in the same direction but a smaller step In practice remembering to clip gradients is important but exploding gradients are an easy problem to solve Source On the difficulty of training recurrent neural networks Pascanu et al 2013. http proceedings.mlr.press v28 pascanu13.pdfHow to fix the vanishing gradient problem The main problem is that it s too difficult for the RNN to learn to preserve information over many timesteps. In a vanilla RNN the hidden state is constantly being rewritten First off next time How about an RNN with separate memory which is added to LSTMs And then Creating more direct and linear pass through connections in model Attention residual connections etc. 60Long Short Term Memory RNNs LSTMs A type of RNN proposed by Hochreiter and Schmidhuber in 1997 as a solution to the problem of vanishing gradients Everyone cites that paper but really a crucial part of the modern LSTM is from Gers et al. 2000 Only started to be recognized as promising through the work of S s student Alex Graves c. 2006 Work in which he also invented CTC connectionist temporal classification for speech recognition But only really became well known after Hinton brought it to Google in 2013 Following Graves having been a postdoc with Hinton 61 Hochreiter and Schmidhuber 1997. Long short term memory. https www.bioinf.jku.at publications older 2604.pdf Gers Schmidhuber and Cummins 2000. Learning to Forget Continual Prediction with LSTM.', 'Problems with RNNs Vanishing and Exploding Gradients 48Vanishing gradient intuition 49Vanishing gradient intuition chain rule 50Vanishing gradient intuition chain rule 51Vanishing gradient intuition chain rule 52Vanishing gradient intuition What happens if these are small Vanishing gradient problem When these are small the gradient signal gets smaller and smaller as it backpropagates further 53Vanishing gradient proof sketch linear case Recall What if were the identity function 53 Source On the difficulty of training recurrent neural networks Pascanu et al 2013. http proceedings.mlr.press v28 pascanu13.pdf and supplemental materials at http proceedings.mlr.press v28 pascanu13 supp.pdf If Wh is small then this term gets exponentially problematic as becomes large chain rule Consider the gradient of the loss on step with respect to the hidden state on some previous step . Let chain rule value of ONLY READ IF INTERESTEDVanishing gradient proof sketch linear case What s wrong with Consider if the eigenvalues of are all less than 1 We can write as a basis What about nonlinear activations i.e. what we use Pretty much the same thing except the proof requires for some dependent on dimensionality and Source On the difficulty of training recurrent neural networks Pascanu et al 2013. http proceedings.mlr.press v28 pascanu13.pdf and supplemental materials at http proceedings.mlr.press v28 pascanu13 supp.pdf 54 eigenvectors using the eigenvectors of Approaches 0 as grows so gradient vanishes sufficient but not necessary ONLY READ IF INTERESTEDWhy is vanishing gradient a problem Gradient signal from far away is lost because it s much smaller than gradient signal from close by. So model weights are updated only with respect to near effects not long term effects. 56Effect of vanishing gradient on RNN LM 57 LM task When she tried to print her tickets she found that the printer was out of toner. She went to the stationery store to buy more toner. It was very overpriced. After installing the toner into the printer she finally printed her To learn from this training example the RNN LM needs to model the dependency between tickets on the 7th step and the target word tickets at the end.']",text-embedding-3-small,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.8623146416943716,0.75,0.9182580942094954,0.0625
65,"What is the impact of bidirectional RNNs, and where are they applicable?","Bidirectional RNNs have the capability to make predictions based on future words by reading through the corpus backwards. They maintain two hidden layers, one for left-to-right propagation and another for right-to-left propagation. This allows them to summarize both past and future word representations around a single token, impacting the model's ability to capture context from both directions. Bidirectional RNNs are applicable in tasks where understanding context from both past and future information is crucial, such as machine translation and sentiment analysis. 

Source: Lecture03 - Computational Natural Language Processing.","['h h h Figure 8 A bi directional RNN model h t f W xt V h t 1 b 17 h t f W xt V h t 1 b 18 ÀÜ yt g Uht c g U h t h t c 19 RNNs can also be multi layered. Figure 9 shows a multi layer bi directional RNN where each lower layer feeds the next layer. As shown in this Ô¨Ågure in this network architecture at time step t each interme diate neuron receives one set of parameters from the previous time step in the same RNN layer and two sets of parameters from the previous RNN hidden layer one input comes from the left to right RNN and the other from the right to left RNN. To construct a Deep RNN with L layers the above relationships are modiÔ¨Åed to the relationships in Equations 20 and 21 where the input to each intermediate neuron at level i is the output of the RNN at layer i 1 at the same time step t. The output ÀÜ y at each time step is the result of propagating input parameters through all hidden layers Equation 22 . Going Deep h i t f W i ht i 1 h i t f W i ht i 1 yt g U h t L h y h 3 x Each memory layer passes an intermediate seq representation to the next. h 2 h 1 Figure 9 A deep bi directional RNN with three RNN layers. h i t f W i h i 1 t V i h i t 1 b i 20 h i t f W i h i 1 t V i h i t 1 b i 21 ÀÜ yt g Uht c g U h L t h L t c 22 2.6 Application RNN Translation Model Traditional translation models are quite complex they consist of nu merous machine learning algorithms applied to different stages of the language translation pipeline. In this section we discuss the potential for adopting RNNs as a replacement to traditional translation mod ules. Consider the RNN example model shown in Figure 10 here the German phrase Echt dicke Kiste is translated to Awesome sauce.', 'av steps. 3. Dealing wi vanishing g 3.1. Previous so Using an L1 or L2 help with explodin ters initialized wit Wrec is probably that the gradient tion found in secti ensure that durin exceeds 1. This a ple regime with a where any informa Figure 7 Gradient explosion clipping visualization To solve the problem of vanishing gradients we introduce two tech niques. The Ô¨Årst technique is that instead of initializing W hh ran domly start off from an identity matrix initialization. The second technique is to use the RectiÔ¨Åed Linear Units ReLU in stead of the sigmoid function. The derivative for the ReLU is either 0 or 1. This way gradients would Ô¨Çow through the neurons whose deriva tive is 1 without getting attenuated while propagating back through time steps. 2.5 Deep Bidirectional RNNs So far we have focused on RNNs that condition on past words to predict the next word in the sequence. It is possible to make predic tions based on future words by having the RNN model read through the corpus backwards. Irsoy et al. shows a bi directional deep neu ral network at each time step t this network maintains two hidden layers one for the left to right propagation and another for the right to left propagation. To maintain two hidden layers at any time this network consumes twice as much memory space for its weight and bias parameters. The Ô¨Ånal classiÔ¨Åcation result ÀÜ yt is generated through combining the score results produced by both RNN hidden layers. Fig ure 8 shows the bi directional network architecture and Equations 17 and 18 show the mathematical formulation behind setting up the bi directional RNN hidden layer. The only difference between these two relationships is in the direction of recursing through the corpus. Equa tion 19 shows the classiÔ¨Åcation relationship used for predicting the next word via summarizing past and future word representations. Bidirectionality h t f W xt V h t f W xt V yt g U h t h t y h x now represents summarizes the past and around a single token.']",text-embedding-3-small,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9317676470495831,1.0,0.9370502991038152,0.0357142857142857
66,Describe the concept of multi-layer or stacked RNNs and their benefit.,"Multi-layer or stacked RNNs involve applying multiple RNNs on top of each other, allowing the network to compute more complex representations. The lower RNN layers are responsible for computing lower-level features, while the higher RNN layers compute higher-level features. Stacked RNNs work better than having just one layer of high-dimensional encodings. High-performing RNNs are usually multi-layered, but not as deep as convolutional or feedforward networks. For example, in Neural Machine Translation, 2 to 4 layers are found to be best for the encoder RNN and 4 layers for the decoder RNN. Skip connections or dense connections are often needed to train deeper RNNs effectively. Transformer-based networks, like BERT, are usually deeper, with 12 or 24 layers.

Source: Lecture03_RNNs_and_LLMs","['You will learn more about transformers including BERT in a couple of weeksMulti layer RNNs RNNs are already deep on one dimension they unroll over many timesteps We can also make them deep in another dimension by applying multiple RNNs this is a multi layer RNN. This allows the network to compute more complex representations The lower RNNs should compute lower level features and the higher RNNs should compute higher level features. Multi layer RNNs are also called stacked RNNs. 80Multi layer RNNs was terribly exciting the movie RNN layer 1 RNN layer 2 81 RNN layer 3 The hidden states from RNN layer i are the inputs to RNN layer i 1Multi layer RNNs in practice 39 Multi layer or stacked RNNs allow a network to compute more complex representations they work better than just have one layer of high dimensional encodings The lower RNNs should compute lower level features and the higher RNNs should compute higher level features. High performing RNNs are usually multi layer but aren t as deep as convolutional or feed forward networks For example In a 2017 paper Britz et al. find that for Neural Machine Translation 2 to 4 layers is best for the encoder RNN and 4 layers is best for the decoder RNN Often 2 layers is a lot better than 1 and 3 might be a little better than 2 Usually skip connections dense connections are needed to train deeper RNNs e.g. 8 layers Transformer based networks e.g. BERT are usually deeper like 12 or 24 layers. You will learn about Transformers later they have a lot of skipping like connections Massive Exploration of Neural Machine Translation Architecutres Britz et al 2017. https arxiv.org pdf 1703.03906.pdfTerminology By the end of the course You will understand phrases like stacked bidirectional LSTMs with residual connections and self attention The RNN described in this lecture simple vanilla Elman RNN You learned about other RNN flavors like LSTM and GRU and multi layer RNNs 83', 'h h h Figure 8 A bi directional RNN model h t f W xt V h t 1 b 17 h t f W xt V h t 1 b 18 ÀÜ yt g Uht c g U h t h t c 19 RNNs can also be multi layered. Figure 9 shows a multi layer bi directional RNN where each lower layer feeds the next layer. As shown in this Ô¨Ågure in this network architecture at time step t each interme diate neuron receives one set of parameters from the previous time step in the same RNN layer and two sets of parameters from the previous RNN hidden layer one input comes from the left to right RNN and the other from the right to left RNN. To construct a Deep RNN with L layers the above relationships are modiÔ¨Åed to the relationships in Equations 20 and 21 where the input to each intermediate neuron at level i is the output of the RNN at layer i 1 at the same time step t. The output ÀÜ y at each time step is the result of propagating input parameters through all hidden layers Equation 22 . Going Deep h i t f W i ht i 1 h i t f W i ht i 1 yt g U h t L h y h 3 x Each memory layer passes an intermediate seq representation to the next. h 2 h 1 Figure 9 A deep bi directional RNN with three RNN layers. h i t f W i h i 1 t V i h i t 1 b i 20 h i t f W i h i 1 t V i h i t 1 b i 21 ÀÜ yt g Uht c g U h L t h L t c 22 2.6 Application RNN Translation Model Traditional translation models are quite complex they consist of nu merous machine learning algorithms applied to different stages of the language translation pipeline. In this section we discuss the potential for adopting RNNs as a replacement to traditional translation mod ules. Consider the RNN example model shown in Figure 10 here the German phrase Echt dicke Kiste is translated to Awesome sauce.']",text-embedding-3-small,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.953830764252278,1.0,0.949996343901752,0.0588235294117647
67,How does Dropout work during training and testing?,"During training, Dropout works by randomly dropping a subset of neurons with some probability (1-p) during each forward-backward pass. This means that during training, a subset of neurons is set to zero while the rest are kept alive with a probability p. During testing, all weights are multiplied by 1/p, and no dropout is applied. This technique helps prevent overfitting, allows the network to learn more meaningful information from the data, and generally leads to higher performance on the task at hand. Essentially, Dropout trains multiple smaller networks at once and averages their predictions, which contributes to its effectiveness in regularization.

Source: SEP 775 - Computational Natural Language Processing, lecture03_RNNs_and_LLMs.","['Imposing such a con straint can also be interpreted as the prior Bayesian belief that the optimal weights are close to zero how close depends on the value of Œª. Choosing the right value of Œª is critical and must be chosen via hyperparameter tuning. Too high a value of Œª causes most of the weights to be set too close to 0 and the model does not learn anything meaningful from the training data often obtaining poor ac curacy on training validation and testing sets. Too low a value and we fall into the domain of overÔ¨Åtting once again. It must be noted that the bias terms are not regularized and do not contribute to the cost term above try thinking about why this is the case There are indeed other types of regularization that are sometimes used such as L1 regularization which sums over the absolute values rather than squares of parameter elements however this is less commonly applied in practice since it leads to sparsity of parameter weights. In the next section we discuss dropout which effectively acts as another form of regularization by randomly dropping i.e. setting to zero neurons in the forward pass. 2.3 Dropout Dropout is a powerful technique for regularization Ô¨Årst introduced by Srivastava et al. in Dropout A Simple Way to Prevent Neural Net works from OverÔ¨Åtting. The idea is simple yet effective during train ing we will randomly drop with some probability 1 p a subset of neurons during each forward backward pass or equivalently we will keep alive each neuron with a probability p . Then during testing we will use the full network to compute our predictions. The result is that the network typically learns more meaningful informa tion from the data is less likely to overÔ¨Åt and usually obtains higher performance overall on the task at hand. One intuitive reason why this technique should be so effective is that what dropout is doing is essentially doing is training exponentially many smaller networks at once and averaging over their predictions. Dropout applied to an artiÔ¨Åcial neural network. Image credits to Srivastava et al. In practice the way we introduce dropout is that we take the out put h of each layer of neurons and keep each neuron with prob ability p and else set it to 0. Then during back propagation we only pass gradients through neurons that were kept alive during the forward pass.', 'like an ensemble model Nowadays usually thought of as strong feature dependent regularizer Wager Wang Liang 2013DropoutDropout Srivastava Hinton Krizhevsky Sutskever Salakhutdinov 2012 JMLR 2014 During training For each data point each time Randomly set input to 0 with probability ùëù dropout ratio often p 0.5 except p 0.15 for input layer via dropout mask During testing Multiply all weights by 1 ùëù No other dropout ùë•2 ùë•3 0 ùë§1 ùë§2 ùë§3 ùë§4 ùëè 1 ùë¶ ùë§1ùë•1 ùë§2ùë•2 ùë§3ùë•3 ùë• ùë• 1 1 Train 1 Train 2 Test ùë•3 0 1 ùë¶ ùë§1ùë•1 ùë§3ùë•3 ùëè ùë§1 ùë§2 ùë§3 ùë§4 ùëè ùë•1 0 ùë•2 ùë•3 ùë•4 ùë§1 ùë§2 ùë§3 ùë§4 ùëè 1 ùë¶ 1 ùëù ùë§1ùë•1 ùë§2ùë•2 ùë§3ùë•3 ùë§4ùë•4 6Vectorization E.g. looping over word vectors versus concatenating them all into one large matrix and then multiplying the softmax weights with that matrix for loop 1000 loops best of 3 639 ¬µs per loop Using single a C x N matrix 10000 loops best of 3 53.8 ¬µs per loop Matrices are awesome Always try to use vectors and matrices rather than for loops The speed gain goes from 1 to 2 orders of magnitude with GPUs 7Parameter Initialization You normally must initialize weights to small random values i.e. not zero matrices To avoid symmetries that prevent learning specialization Initialize hidden layer biases to 0 and output or reconstruction biases to optimal value if weights were 0 e.g.']",text-embedding-3-small,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.8978898453845874,1.0,0.9247500244029166,0.0625
68,What is the role of parameter initialization in neural networks?,"Parameter initialization in neural networks is a key step towards achieving superlative performance. A good starting strategy is to initialize the weights to small random numbers normally distributed around 0. Different weight and bias initialization schemes can impact training dynamics, affecting convergence speed and error rates. For example, Xavier et al. (2010) found that for sigmoid and tanh activation units, initializing weights randomly with a uniform distribution can lead to faster convergence and lower error rates. This initialization strategy aims to maintain activation variances and backpropagated gradient variances across layers, which are crucial for effective training of neural networks. Without proper parameter initialization, gradient variances may decrease with backpropagation across layers, hindering the learning process.

Source: lecture02_notes_Backpropagation","['This is useful since input features are often measured in different units but we often want to initially consider all features as equally important. The way we accomplish this is by simply dividing the features by their respective standard deviation calculated across the training set. Whitening Not as commonly used as mean subtraction normalization whiten ing essentially converts the data to a have an identity covariance matrix that is features become uncorrelated and have a variance of 1. This is done by Ô¨Årst mean subtracting the data as usual to get X . We can then take the Singular Value Decomposition SVD of X to get matrices U S V. We then compute UX to project X into the basis deÔ¨Åned by the columns of U. We Ô¨Ånally divide each dimension of the result by the corresponding singular value in S to scale our data appropriately if a singular value is zero we can just divide by a small number instead . 2.6 Parameter Initialization A key step towards achieving superlative performance with a neu ral network is initializing the parameters in a reasonable way. A good starting strategy is to initialize the weights to small random numbers normally distributed around 0 and in practice this often words acceptably well. However in Understanding the dif ficulty of training deep feedforward neural networks 2010 Xavier et al study the effect of different weight and bias initialization schemes on training dynamics. The empirical Ô¨Åndings suggest that for sigmoid and tanh activation units faster convergence and lower error rates are achieved when the weights of a matrix W Rn l 1 n l are initialized randomly with a uniform distribution as follows W U s 6 n l n l 1 s 6 n l n l 1 Where n l is the number of input units to W fan in and n l 1 is the number of output units from W fan out . In this parameter initialization scheme bias units are initialized to 0. This approach attempts to maintain activation variances as well as backpropagated gradient variances across layers. Without such initialization the gradient variances which are a proxy for information generally decrease with backpropagation across layers. 2.7 Learning Strategies The rate magnitude of model parameter updates during training can be controlled using the learning rate.', 'like an ensemble model Nowadays usually thought of as strong feature dependent regularizer Wager Wang Liang 2013DropoutDropout Srivastava Hinton Krizhevsky Sutskever Salakhutdinov 2012 JMLR 2014 During training For each data point each time Randomly set input to 0 with probability ùëù dropout ratio often p 0.5 except p 0.15 for input layer via dropout mask During testing Multiply all weights by 1 ùëù No other dropout ùë•2 ùë•3 0 ùë§1 ùë§2 ùë§3 ùë§4 ùëè 1 ùë¶ ùë§1ùë•1 ùë§2ùë•2 ùë§3ùë•3 ùë• ùë• 1 1 Train 1 Train 2 Test ùë•3 0 1 ùë¶ ùë§1ùë•1 ùë§3ùë•3 ùëè ùë§1 ùë§2 ùë§3 ùë§4 ùëè ùë•1 0 ùë•2 ùë•3 ùë•4 ùë§1 ùë§2 ùë§3 ùë§4 ùëè 1 ùë¶ 1 ùëù ùë§1ùë•1 ùë§2ùë•2 ùë§3ùë•3 ùë§4ùë•4 6Vectorization E.g. looping over word vectors versus concatenating them all into one large matrix and then multiplying the softmax weights with that matrix for loop 1000 loops best of 3 639 ¬µs per loop Using single a C x N matrix 10000 loops best of 3 53.8 ¬µs per loop Matrices are awesome Always try to use vectors and matrices rather than for loops The speed gain goes from 1 to 2 orders of magnitude with GPUs 7Parameter Initialization You normally must initialize weights to small random values i.e. not zero matrices To avoid symmetries that prevent learning specialization Initialize hidden layer biases to 0 and output or reconstruction biases to optimal value if weights were 0 e.g.']",text-embedding-3-small,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.8356711454835025,1.0,0.913233322767758,0.0714285714285714
69,"What are the differences between Vanilla RNNs, LSTMs, and GRUs?","Vanilla RNNs suffer from the vanishing gradient problem, making it difficult for them to capture long-term dependencies effectively. LSTMs address this issue by introducing the concept of a cell state, forget gate, input gate, and output gate, allowing them to retain information over long sequences. GRUs, on the other hand, combine the forget and input gates into a single update gate and merge the cell state and hidden state, simplifying the architecture compared to LSTMs while still maintaining the ability to capture long-term dependencies effectively.

Source: file_path: C:\University Graduate\2024 Winter\SEP775_NLP\Final_Project\data\txt_from_pdf_with_file_name\Understanding_LSTM_Networks.txt","['We only forget when we re going to input something in its place. We only input new values to the state when we forget something older. A slightly more dramatic variation on the LSTM is the Gated Recurrent Unit or GRU introduced by Cho et al. 2014 http arxiv.org pdf 1406.1078v3.pdf . It combines the forget and input gates into a single update gate. It also merges the cell state and hidden state and makes some other changes. The resulting model is simpler than standard LSTM models and has been growing increasingly popular. These are only a few of the most notable LSTM variants. There are lots of others like Depth Gated RNNs by Yao et al. 2015 http arxiv.org pdf 1508.03790v2.pdf . There s also some completely different approach to tackling long term dependencies like Clockwork RNNs by Koutnik et al. 2014 http arxiv.org pdf 1402.3511v1.pdf . Which of these variants is best Do the differences matter Greff et al. 2015 http arxiv.org pdf 1503.04069.pdf do a nice comparison of popular variants finding that they re all about the same. Jozefowicz et al. 2015 http jmlr.org proceedings papers v37 jozefowicz15.pdf tested more than ten thousand RNN architectures finding some that worked better than LSTMs on certain tasks. Conclusion Earlier I mentioned the remarkable results people are achieving with RNNs. Essentially all of these are achieved using LSTMs. They really work a lot better for most tasks Written down as a set of equations LSTMs look pretty intimidating. Hopefully walking through them step by step in this essay has made them a bit more approachable.1 24 24 3 23 PM Understanding LSTM Networks colah s blog 7 7 Attention and Augmented Recurrent Neural Networks On Distill Conv Nets A Modular Perspective Neural Networks Manifolds and Topology Deep Learning NLP and Representations LSTMs were a big step in what we can accomplish with RNNs.', 'Extension V given a word sequence A B C in German whose transla tion is X Y in English instead of training the RNN using A B C X Y train it using C B A X Y. The intutition behind this technique is that A is more likely to be translated to X. Thus given the vanishing gradient problem discussed earlier reversing the order of the input words can help reduce the error rate in generating the output phrase. 3 Gated Recurrent Units Beyond the extensions discussed so far RNNs have been found to per form better with the use of more complex units for activation. So far we have discussed methods that transition from hidden state ht 1 to ht using an afÔ¨Åne transformation and a point wise nonlinearity. Here we discuss the use of a gated activation function thereby modifying the RNN architecture. What motivates this Well although RNNs can theoretically capture long term dependencies they are very hard to actually train to do this. Gated recurrent units are designed in a manner to have more persistent memory thereby making it easier for RNNs to capture long term dependencies. Let us see mathematically how a GRU uses ht 1 and xt to generate the next hidden state ht. We will then dive into the intuition of this architecture. zt œÉ W z xt U z ht 1 Update gate rt œÉ W r xt U r ht 1 Reset gate ht tanh rt Uht 1 Wxt New memory ht 1 zt ht zt ht 1 Hidden state The above equations can be thought of a GRU s four fundamental op erational stages and they have intuitive interpretations that make this model much more intellectually satisfying see Figure 12 1. New memory generation A new memory ht is the consolidation of a new input word xt with the past hidden state ht 1. Anthropomor phically this stage is the one who knows the recipe of combining a newly observed word with the past hidden state ht 1 to summarize this new word in light of the contextual past as the vector ht. 2. Reset Gate The reset signal rt is responsible for determining how important ht 1 is to the summarization ht.']",text-embedding-3-small,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.8745651825370278,1.0,0.9238183680588868,0.037037037037037
70,What is the difference between traditional language models and conditioned language models?,"Traditional language models are generative models of text, while conditioned language models not only generate text but do so according to some specification or input. Conditioned language models take structured data, such as English documents, utterances, images, or speech, as input and generate text as output based on that input. They are designed to generate text that is conditioned on specific input data, allowing for more targeted and specialized text generation compared to traditional language models. 

Source: Lecture04 Conditioned Generation","['file name: lecture04 Conditioned Generation. Hamidreza Mahyar mahyarh@mcmaster.ca Computational Natural Language Processing Conditioned GenerationLanguage Models Language models are generative models of text s P x The Malfoys said Hermione. Harry was watching him. He looked like Madame Maxime. When she strode up the wrong staircase to visit himself. I m afraid I ve definitely been suspended from power no chance indeed said Snape. He put his head back behind them and read groups as they crossed a corner and fluttered down onto their ink lamp and picked up his spoon. The doorbell rang. It was a lot cleaner down in London. Text Credit Max Deutsch https medium.com deep writingConditioned Language Models Not just generate text generate text according to some specification Input X Structured Data English Document Utterance Image Speech Output Y Text NL Description Japanese Short Description Response Text Transcript Task NL Generation Translation Summarization Response Generation Image Captioning Speech RecognitionFormulation and ModelingCalculating the Probability of a SentenceConditional Language ModelsOne Type of Language Model LSTM LSTM LSTM LSTM predict hate predict this predict movie predict s LSTM Mikolov et al. 2011 s I hate this movie predict ILSTM LSTM LSTM LSTM LSTM s LSTM LSTM LSTM LSTM argmax argmax argmax argmax argmax s One Type of Conditional Language Model I hate this movie kono eiga ga kirai hate this movie Sutskever et al. 2014 Encoder I DecoderHow to Pass Hidden State Initialize decoder w encoder Sutskever et al. 2014 encoder decoder Transform can be different dimensions encoder transform decoder encoder Input at every time step Kalchbrenner Blunsom 2013 decoder decoder decoderMethods of GenerationThe Generation Problem We have a model of P Y X how do we use it to generate a sentence Two methods Sampling Try to generate a random sentence according to the probability distribution. Argmax Try to generate the sentence with the highest probability.Ancestral Sampling Randomly generate words one by one.', 'For instance HLW 20 observe that larger models do not necessarily generalize better out of distribution. There is evidence that suggests that the generalization achieved under this paradigm can be poor because the model is overly speciÔ¨Åc to the training distribution and does not generalize well outside it YdC 19 MPL19 . Thus the performance of Ô¨Åne tuned models on speciÔ¨Åc benchmarks even when it is nominally at human level may exaggerate actual performance on the underlying task GSL 18 NK19 . Third humans do not require large supervised datasets to learn most language tasks a brief directive in natural language e.g. please tell me if this sentence describes something happy or something sad or at most a tiny number of demonstrations e.g. here are two examples of people acting brave please give a third example of bravery is often Figure 1.1 Language model meta learning. During unsupervised pre training a language model develops a broad set of skills and pattern recognition abilities. It then uses these abilities at inference time to rapidly adapt to or recognize the desired task. We use the term in context learning to describe the inner loop of this process which occurs within the forward pass upon each sequence. The sequences in this diagram are not intended to be representative of the data a model would see during pre training but are intended to show that there are sometimes repeated sub tasks embedded within a single sequence. 3Figure 1.2 Larger models make increasingly efÔ¨Åcient use of in context information. We show in context learning performance on a simple task requiring the model to remove random symbols from a word both with and without a natural language task description see Sec. 3.9.2 . The steeper in context learning curves for large models demonstrate improved ability to learn a task from contextual information. We see qualitatively similar behavior across a wide range of tasks. sufÔ¨Åcient to enable a human to perform a new task to at least a reasonable degree of competence. Aside from pointing to a conceptual limitation in our current NLP techniques this adaptability has practical advantages it allows humans to seamlessly mix together or switch between many tasks and skills for example performing addition during a lengthy dialogue.']",text-embedding-3-small,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999990071683664,0.75,0.9599304350746012,0.0
71,How can LSTM networks be used in language modeling?,"LSTM networks can be used in language modeling by predicting the next word in a sequence. They have been successful in tasks such as handwriting recognition, speech recognition, machine translation, parsing, image captioning, and generating language models. LSTMs have become the dominant approach for most NLP tasks, showcasing their effectiveness in capturing long-range dependencies in sequential data. Additionally, LSTMs have been instrumental in achieving state-of-the-art results in various NLP applications.","['are hard to train Another solution lots of new deep feedforward convolutional architectures add more direct connections thus allowing the gradient to flow Deep Residual Learning for Image Recognition He et al 2015. https arxiv.org pdf 1512.03385.pdfLSTMs real world success 68 In 2013 2015 LSTMs started achieving state of the art results Successful tasks include handwriting recognition speech recognition machine translation parsing and image captioning as well as language models LSTMs became the dominant approach for most NLP tasks Now 2019 2023 Transformers have become dominant for all tasks For example in WMT a Machine Translation conference competition In WMT 2014 there were 0 neural machine translation systems In WMT 2016 the summary report contains RNN 44 times and these systems won In WMT 2019 RNN 7 times Transformer 105 times Source Findings of the 2016 Conference on Machine Translation WMT16 Bojar et al. 2016 http www.statmt.org wmt16 pdf W16 2301.pdf Source Findings of the 2018 Conference on Machine Translation WMT18 Bojar et al. 2018 http www.statmt.org wmt18 pdf WMT028.pdf Source Findings of the 2019 Conference on Machine Translation WMT19 Barrault et al. 2019 http www.statmt.org wmt18 pdf WMT028.pdf5. Recap 69 Language Model A system that predicts the next word Recurrent Neural Network A family of neural networks that Take sequential input of any length Apply the same weights on each step Can optionally produce output on each step Recurrent Neural Network Language Model We ve shown that RNNs are a great way to build a LM despite some problems RNNs are also useful for much moreWhy should we care about Language Modeling 70 Language Modeling is a benchmark task that helps us measure our progress on predicting language use Language Modeling is a subcomponent of many NLP tasks especially those involving generating text or estimating the probability of text Predictive typing Speech recognition Handwriting recognition Spelling grammar correction Authorship identification Machine translation Summarization Dialogue etc.', 'Everything else in NLP has now been rebuilt upon Language Modeling GPT 3 is an LMOther RNN ArchitecturesOther RNN uses RNNs can be used for sequence tagging e.g. part of speech tagging named entity recognition knocked over the vase the startled cat 72 VBN IN DT NN DT JJ NNthe movie a lot overall I enjoyed positive Sentence encoding 73 How to compute sentence encoding RNNs can be used for sentence classification e.g. sentiment classificationthe movie a lot overall I enjoyed positive Sentence encoding 74 equals How to compute sentence encoding Basic way Use final hidden state RNNs can be used for sentence classification e.g. sentiment classificationRNNs can be used for sentence classification the movie a lot overall I enjoyed positive Sentence encoding How to compute sentence encoding 75 Usually better Take element wise max or mean of all hidden states e.g. sentiment classificationRNNs can be used as an encoder module e.g. question answering machine translation many other tasks Context Ludwig van Beethoven was a German composer and pianist. A crucial figure Beethoven Question what nationality was Here the RNN acts as an encoder for the Question the hidden states represent the Question . The encoder is part of a larger neural system. Answer German 76 lots of neural architecture lots of neural architectureRNN LMs can be used to generate text e.g. speech recognition machine translation summarization RNN LM what s the the weather what s This is an example of a conditional language model. We ll see Machine Translation in much more detail next lectures. Input audio START conditioning 77Bidirectional RNNs was terribly exciting the movie Forward RNN Backward RNN Concatenated hidden states This contextual representation of terribly has both left and right context 78Bidirectional RNNs 79 Note bidirectional RNNs are only applicable if you have access to the entire input sequence They are not applicable to Language Modeling because in LM you only have left context available. If you do have entire input sequence e.g. any kind of encoding bidirectionality is powerful you should use it by default . For example BERT Bidirectional Encoder Representations from Transformers is a powerful pretrained contextual representation system built on bidirectionality.']",text-embedding-3-small,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.8908757614319701,0.4,0.9492906535730437,0.0909090909090909
72,What are the methods of generation in conditioned language modeling?,"The methods of generation in conditioned language modeling are Sampling and Argmax. Sampling involves trying to generate a random sentence according to the probability distribution, while Argmax involves trying to generate the sentence with the highest probability. Another method mentioned is Ancestral Sampling, which involves randomly generating words one by one.","['file name: lecture04 Conditioned Generation. Hamidreza Mahyar mahyarh@mcmaster.ca Computational Natural Language Processing Conditioned GenerationLanguage Models Language models are generative models of text s P x The Malfoys said Hermione. Harry was watching him. He looked like Madame Maxime. When she strode up the wrong staircase to visit himself. I m afraid I ve definitely been suspended from power no chance indeed said Snape. He put his head back behind them and read groups as they crossed a corner and fluttered down onto their ink lamp and picked up his spoon. The doorbell rang. It was a lot cleaner down in London. Text Credit Max Deutsch https medium.com deep writingConditioned Language Models Not just generate text generate text according to some specification Input X Structured Data English Document Utterance Image Speech Output Y Text NL Description Japanese Short Description Response Text Transcript Task NL Generation Translation Summarization Response Generation Image Captioning Speech RecognitionFormulation and ModelingCalculating the Probability of a SentenceConditional Language ModelsOne Type of Language Model LSTM LSTM LSTM LSTM predict hate predict this predict movie predict s LSTM Mikolov et al. 2011 s I hate this movie predict ILSTM LSTM LSTM LSTM LSTM s LSTM LSTM LSTM LSTM argmax argmax argmax argmax argmax s One Type of Conditional Language Model I hate this movie kono eiga ga kirai hate this movie Sutskever et al. 2014 Encoder I DecoderHow to Pass Hidden State Initialize decoder w encoder Sutskever et al. 2014 encoder decoder Transform can be different dimensions encoder transform decoder encoder Input at every time step Kalchbrenner Blunsom 2013 decoder decoder decoderMethods of GenerationThe Generation Problem We have a model of P Y X how do we use it to generate a sentence Two methods Sampling Try to generate a random sentence according to the probability distribution. Argmax Try to generate the sentence with the highest probability.Ancestral Sampling Randomly generate words one by one.', 'An exact method for sampling from P X no further work needed.Greedy Search One by one pick the single highest probability word Not exact real problems Will often generate the easy words first Will prefer multiple common words to one rare wordBeam Search Instead of picking one high probability word maintain several paths Some in reading materials more in a later classModel EnsemblingEnsembling Why Multiple models make somewhat uncorrelated errors Models tend to be more uncertain when they are about to make errors Smooths over idiosyncrasies of the model LSTM1 predict1 I LSTM2 predict2 Combine predictions from multiple models s sLinear Interpolation Take a weighted average of the M model probabilities Second term often set to uniform distribution 1 MLog linear Interpolation Weighted combination of log probabilities normalize Interpolation coefficient often set to uniform distribution 1 MLinear or Log Linear Think of it in logic Linear Logical OR the interpolated model likes any choice that a model gives a high probability use models with models that capture different traits necessary when any model can assign zero probability Log Linear Logical AND interpolated model only likes choices where all models agree use when you want to restrict possible answersParameter Averaging Problem Ensembling means we have to use M models at test time increasing our time memory complexity Parameter averaging is a cheap way to get some good effects of ensembling Basically write out models several times near the end of training and take the average of parametersEnsemble Distillation e.g. Kim et al. 2016 Problem parameter averaging only works for models within the same run Knowledge distillation trains a model to copy the ensemble Specifically it tries to match the description over predicted words Why We want the model to make the same mistakes as an ensemble Shown to increase accuracy notablyStacking What if we have two very different models where prediction of outputs is done in very different ways e.g. a phrase based translation model and a neural MT model Niehues et al. 2017 Stacking uses the output of one system in calculating features for another systemCase Studies in Conditional Language ModelingFrom Images e.g. Karpathy et al. 2015 Input is image features output is text Use standard image encoders e.g.']",text-embedding-3-small,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.999997840343882,0.75,0.9486235296589066,0.0833333333333333
73,What is beam search and how is it used in language generation?,"Beam search is a decoding algorithm used in natural language processing for generating sequences like sentences. It is an alternative to greedy decoding, which selects the most probable word at each step. In beam search, instead of selecting just one word, the algorithm keeps track of the k most probable partial translations, called hypotheses, at each step of the decoding process. The beam size, k, determines how many hypotheses are considered. These hypotheses are scored based on their log probabilities, with higher scores indicating better translations. Beam search is more efficient than exhaustive search, where all possible sequences are computed, and while it is not guaranteed to find the optimal solution, it is a practical approach for generating sequences in tasks like neural machine translation.

Source: lecture05_Seq2Seq","['2014 Luong et al. 2015 The hidden states from RNN layer i are the inputs to RNN layer i 1Decoding Greedy decoding We saw how to generate or decode the target sentence by taking argmax on each step of the decoder he hit me with a pie END START he hit me with a pie This is greedy decoding take most probable word on each step argmax argmax argmax argmax 15 argmax argmax argmaxProblems with greedy decoding 16 Greedy decoding has no way to undo decisions Input il a m entart√© he he hit he hit a he hit me with a pie whoops no going back now How to fix thisExhaustive search decoding Ideally we want to find a length T translation y that maximizes We could try computing all possible sequences y This means that on each step t of the decoder we re tracking Vt possible partial translations where V is vocab size This O VT complexity is far too expensive 17Beam search decoding Core idea On each step of decoder keep track of the k most probable partial translations which we call hypotheses k is the beam size in practice around 5 to 10 in NMT A hypothesis has a score which is its log probability Scores are all negative and higher score is better We search for high scoring hypotheses tracking top k on each step Beam search is not guaranteed to find optimal solution But much more efficient than exhaustive search 18Beam search decoding example Beam size k 2. Blue numbers START Calculate prob dist of next word 19Beam search decoding example Beam size k 2. Blue numbers START he I 0.7 log PLM he START 0.9 log PLM I START Take top k words and compute scores 20Beam search decoding example Beam size k 2. Blue numbers hit struck was got START he I 10 0.7 0.9 2.9 log PLM struck START he 0.7 1.6 log PLM was START I 0.9 1.8 log PLM got START I 0.9 1.7 log PLM hit START he 0.7 For each of the k hypotheses find top k next words and calculate scoresBeam search decoding example Beam size k 2.', 'An exact method for sampling from P X no further work needed.Greedy Search One by one pick the single highest probability word Not exact real problems Will often generate the easy words first Will prefer multiple common words to one rare wordBeam Search Instead of picking one high probability word maintain several paths Some in reading materials more in a later classModel EnsemblingEnsembling Why Multiple models make somewhat uncorrelated errors Models tend to be more uncertain when they are about to make errors Smooths over idiosyncrasies of the model LSTM1 predict1 I LSTM2 predict2 Combine predictions from multiple models s sLinear Interpolation Take a weighted average of the M model probabilities Second term often set to uniform distribution 1 MLog linear Interpolation Weighted combination of log probabilities normalize Interpolation coefficient often set to uniform distribution 1 MLinear or Log Linear Think of it in logic Linear Logical OR the interpolated model likes any choice that a model gives a high probability use models with models that capture different traits necessary when any model can assign zero probability Log Linear Logical AND interpolated model only likes choices where all models agree use when you want to restrict possible answersParameter Averaging Problem Ensembling means we have to use M models at test time increasing our time memory complexity Parameter averaging is a cheap way to get some good effects of ensembling Basically write out models several times near the end of training and take the average of parametersEnsemble Distillation e.g. Kim et al. 2016 Problem parameter averaging only works for models within the same run Knowledge distillation trains a model to copy the ensemble Specifically it tries to match the description over predicted words Why We want the model to make the same mistakes as an ensemble Shown to increase accuracy notablyStacking What if we have two very different models where prediction of outputs is done in very different ways e.g. a phrase based translation model and a neural MT model Niehues et al. 2017 Stacking uses the output of one system in calculating features for another systemCase Studies in Conditional Language ModelingFrom Images e.g. Karpathy et al. 2015 Input is image features output is text Use standard image encoders e.g.']",text-embedding-3-small,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9218853282299084,1.0,0.914525955863322,1.0
74,Explain the concept of model ensembling in language models.,"Model ensembling in language models involves using multiple models that make somewhat uncorrelated errors to improve overall performance. By combining predictions from these models through techniques like linear interpolation or log linear interpolation, the ensemble model can benefit from the strengths of each individual model and mitigate their weaknesses. This approach helps to smooth over idiosyncrasies of the models and improve the overall accuracy of the predictions. Additionally, ensemble distillation is a technique where a model is trained to mimic the behavior of an ensemble of models, aiming to replicate the ensemble's decision-making process and improve accuracy. Overall, model ensembling is a powerful technique in language modeling to enhance performance and robustness. 

Source: SEP 775 - Computational Natural Language Processing, Final_Project\data\txt_from_pdf_with_file_name\lecture04_Conditioned_Generation.txt","['An exact method for sampling from P X no further work needed.Greedy Search One by one pick the single highest probability word Not exact real problems Will often generate the easy words first Will prefer multiple common words to one rare wordBeam Search Instead of picking one high probability word maintain several paths Some in reading materials more in a later classModel EnsemblingEnsembling Why Multiple models make somewhat uncorrelated errors Models tend to be more uncertain when they are about to make errors Smooths over idiosyncrasies of the model LSTM1 predict1 I LSTM2 predict2 Combine predictions from multiple models s sLinear Interpolation Take a weighted average of the M model probabilities Second term often set to uniform distribution 1 MLog linear Interpolation Weighted combination of log probabilities normalize Interpolation coefficient often set to uniform distribution 1 MLinear or Log Linear Think of it in logic Linear Logical OR the interpolated model likes any choice that a model gives a high probability use models with models that capture different traits necessary when any model can assign zero probability Log Linear Logical AND interpolated model only likes choices where all models agree use when you want to restrict possible answersParameter Averaging Problem Ensembling means we have to use M models at test time increasing our time memory complexity Parameter averaging is a cheap way to get some good effects of ensembling Basically write out models several times near the end of training and take the average of parametersEnsemble Distillation e.g. Kim et al. 2016 Problem parameter averaging only works for models within the same run Knowledge distillation trains a model to copy the ensemble Specifically it tries to match the description over predicted words Why We want the model to make the same mistakes as an ensemble Shown to increase accuracy notablyStacking What if we have two very different models where prediction of outputs is done in very different ways e.g. a phrase based translation model and a neural MT model Niehues et al. 2017 Stacking uses the output of one system in calculating features for another systemCase Studies in Conditional Language ModelingFrom Images e.g. Karpathy et al. 2015 Input is image features output is text Use standard image encoders e.g.', 'For instance HLW 20 observe that larger models do not necessarily generalize better out of distribution. There is evidence that suggests that the generalization achieved under this paradigm can be poor because the model is overly speciÔ¨Åc to the training distribution and does not generalize well outside it YdC 19 MPL19 . Thus the performance of Ô¨Åne tuned models on speciÔ¨Åc benchmarks even when it is nominally at human level may exaggerate actual performance on the underlying task GSL 18 NK19 . Third humans do not require large supervised datasets to learn most language tasks a brief directive in natural language e.g. please tell me if this sentence describes something happy or something sad or at most a tiny number of demonstrations e.g. here are two examples of people acting brave please give a third example of bravery is often Figure 1.1 Language model meta learning. During unsupervised pre training a language model develops a broad set of skills and pattern recognition abilities. It then uses these abilities at inference time to rapidly adapt to or recognize the desired task. We use the term in context learning to describe the inner loop of this process which occurs within the forward pass upon each sequence. The sequences in this diagram are not intended to be representative of the data a model would see during pre training but are intended to show that there are sometimes repeated sub tasks embedded within a single sequence. 3Figure 1.2 Larger models make increasingly efÔ¨Åcient use of in context information. We show in context learning performance on a simple task requiring the model to remove random symbols from a word both with and without a natural language task description see Sec. 3.9.2 . The steeper in context learning curves for large models demonstrate improved ability to learn a task from contextual information. We see qualitatively similar behavior across a wide range of tasks. sufÔ¨Åcient to enable a human to perform a new task to at least a reasonable degree of competence. Aside from pointing to a conceptual limitation in our current NLP techniques this adaptability has practical advantages it allows humans to seamlessly mix together or switch between many tasks and skills for example performing addition during a lengthy dialogue.']",text-embedding-3-small,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9686318869192608,1.0,0.9096205842331848,0.0666666666666666
75,How does ensemble distillation differ from parameter averaging?,"Ensemble distillation differs from parameter averaging in that while parameter averaging involves taking the average of parameters from multiple models near the end of training to achieve some effects of ensembling, ensemble distillation trains a model to copy the ensemble by matching the distribution over predicted words. This way, ensemble distillation aims to make the model make the same mistakes as an ensemble, ultimately increasing accuracy notably.","['An exact method for sampling from P X no further work needed.Greedy Search One by one pick the single highest probability word Not exact real problems Will often generate the easy words first Will prefer multiple common words to one rare wordBeam Search Instead of picking one high probability word maintain several paths Some in reading materials more in a later classModel EnsemblingEnsembling Why Multiple models make somewhat uncorrelated errors Models tend to be more uncertain when they are about to make errors Smooths over idiosyncrasies of the model LSTM1 predict1 I LSTM2 predict2 Combine predictions from multiple models s sLinear Interpolation Take a weighted average of the M model probabilities Second term often set to uniform distribution 1 MLog linear Interpolation Weighted combination of log probabilities normalize Interpolation coefficient often set to uniform distribution 1 MLinear or Log Linear Think of it in logic Linear Logical OR the interpolated model likes any choice that a model gives a high probability use models with models that capture different traits necessary when any model can assign zero probability Log Linear Logical AND interpolated model only likes choices where all models agree use when you want to restrict possible answersParameter Averaging Problem Ensembling means we have to use M models at test time increasing our time memory complexity Parameter averaging is a cheap way to get some good effects of ensembling Basically write out models several times near the end of training and take the average of parametersEnsemble Distillation e.g. Kim et al. 2016 Problem parameter averaging only works for models within the same run Knowledge distillation trains a model to copy the ensemble Specifically it tries to match the description over predicted words Why We want the model to make the same mistakes as an ensemble Shown to increase accuracy notablyStacking What if we have two very different models where prediction of outputs is done in very different ways e.g. a phrase based translation model and a neural MT model Niehues et al. 2017 Stacking uses the output of one system in calculating features for another systemCase Studies in Conditional Language ModelingFrom Images e.g. Karpathy et al. 2015 Input is image features output is text Use standard image encoders e.g.', 'All ensembles have at least 3 workers. TL DR 6.7B RL T 0.7 labeler labeler ensem ble length copying ROUGE 1.3B sup logprob 1.3B RM 6.7B sup logprob 6.7B RM labeler 60.4 5.9 66.0 7.6 55.8 2.2 52.7 2.1 49.9 2.1 48.0 2.2 57.4 2.0 47.3 2.2 62.3 2.1 labeler ensemble 66.0 7.6 80.0 8.9 65.0 10.6 35.0 10.5 45.0 11.1 75.0 9.8 40.0 10.5 75.0 9.8 length 55.8 2.2 80.0 8.9 48.1 2.2 50.3 2.2 30.0 2.1 62.0 2.1 30.4 2.0 59.8 2.2 copying 52.7 2.1 65.0 10.6 48.1 2.2 52.0 2.2 64.2 2.1 56.7 2.2 64.4 2.1 53.4 2.2 ROUGE 49.9 2.1 35.0 10.5 50.3 2.2 52.0 2.2 50.5 2.2 52.0 2.3 51.1 2.3 54.5 2.1 1.3B sup.']",text-embedding-3-small,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.987278043698859,,0.9705941945294329,0.3333333333333333
76,What are some common evaluation metrics for conditioned language models?,"Some common evaluation metrics for conditioned language models include BLEU, embedding-based metrics like BertScore, BLEURT, COMET, PRISM, and Perplexity. Additionally, meta-evaluation, which involves running human evaluation and automatic evaluation on the same outputs and calculating correlation, is also used to evaluate conditioned language models. 

Source: SEP 775 - Computational Natural Language Processing, lecture04_Conditioned_Generation","['2015 Input is image features output is text Use standard image encoders e.g. CNN Often pre trained on large databases such as ImageNetCheck out this review paperHow do we EvaluateBasic Evaluation Paradigm Use parallel test set Use system to generate translations Compare target translations w referenceHuman Evaluation Ask a human to do evaluation Final goal but slow expensive and sometimes inconsistentBLEU Works by comparing n gram overlap w reference Pros Easy to use good for measuring system improvement Cons Often doesn t match human eval bad for comparing very different systemsEmbedding based Metrics Recently many metrics based on neural models BertScore Find similarity between BERT embeddings unsupervised Zhang et al. 2020 BLEURT Train BERT to predict human evaluation scores Sellam et al. 2020 COMET Train model to predict human eval also using source sentence Rei et al. 2020 PRISM Model based on training paraphrasing model Thompson and Post 2020Perplexity Calculate the perplexity of the words in the held out set without doing generation Pros Naturally solves multiple reference problem Cons Doesn t consider decoding or actually generating output. May be reasonable for problems with lots of ambiguity.Which One to Use Meta evaluation runs human evaluation and automatic evaluation on the same outputs calculates correlation Examples WMT Metrics Task for MT Mathur et al. 2021 RealSumm for summarization Evaluation is hard especially with good systems Most metrics had no correlation w human eval over best systems at some WMT 2019 tasksQuestions', 'For this graph we also include 6 additional extra small models with as few as 100 000 parameters. As observed in KMH 20 language modeling performance follows a power law when making efÔ¨Åcient use of training compute. After extending this trend by two more orders of magnitude we observe only a slight if any departure from the power law. One might worry that these improvements in cross entropy loss come only from modeling spurious details of our training corpus. However we will see in the following sections that improvements in cross entropy loss lead to consistent performance gains across a broad spectrum of natural language tasks. Below we evaluate the 8 models described in Section 2 the 175 billion parameter parameter GPT 3 and 7 smaller models on a wide range of datasets. We group the datasets into 9 categories representing roughly similar tasks. In Section 3.1 we evaluate on traditional language modeling tasks and tasks that are similar to language modeling such as Cloze tasks and sentence paragraph completion tasks. In Section 3.2 we evaluate on closed book question answering tasks tasks which require using the information stored in the model s parameters to answer general knowledge questions. In Section 3.3 we evaluate the model s ability to translate between languages especially one shot and few shot . In Section 3.4 we evaluate the model s performance on Winograd Schema like tasks. In Section 3.5 we evaluate on datasets that involve commonsense reasoning or question answering. In Section 3.6 we evaluate on reading comprehension tasks in Section 3.7 we evaluate on the SuperGLUE benchmark suite and in 3.8 we brieÔ¨Çy explore NLI. Finally in Section 3.9 we invent some additional tasks designed especially to probe in context learning abilities these tasks focus on on the Ô¨Çy reasoning adaptation skills or open ended text synthesis. We evaluate all tasks in the few shot one shot and zero shot settings. 10Figure 3.1 Smooth scaling of performance with compute. Performance measured in terms of cross entropy validation loss follows a power law trend with the amount of compute used for training. The power law behavior observed in KMH 20 continues for an additional two orders of magnitude with only small deviations from the predicted curve.']",text-embedding-3-small,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.0000000000000002,0.8888888888888888,0.9160912851646664,0.0
77,What is the significance of stacking in model ensembling?,"Stacking is significant in model ensembling when we have two very different models that predict outputs in different ways. Stacking involves using the output of one system to calculate features for another system, allowing for the combination of diverse model predictions to improve overall performance. It is particularly useful when dealing with models that have complementary strengths and weaknesses, such as a phrase-based translation model and a neural machine translation model. By leveraging stacking, we can enhance the predictive power of ensembles by incorporating a variety of model perspectives and approaches. 

Source: SEP 775 - Computational Natural Language Processing, lecture04_Conditioned_Generation.","['An exact method for sampling from P X no further work needed.Greedy Search One by one pick the single highest probability word Not exact real problems Will often generate the easy words first Will prefer multiple common words to one rare wordBeam Search Instead of picking one high probability word maintain several paths Some in reading materials more in a later classModel EnsemblingEnsembling Why Multiple models make somewhat uncorrelated errors Models tend to be more uncertain when they are about to make errors Smooths over idiosyncrasies of the model LSTM1 predict1 I LSTM2 predict2 Combine predictions from multiple models s sLinear Interpolation Take a weighted average of the M model probabilities Second term often set to uniform distribution 1 MLog linear Interpolation Weighted combination of log probabilities normalize Interpolation coefficient often set to uniform distribution 1 MLinear or Log Linear Think of it in logic Linear Logical OR the interpolated model likes any choice that a model gives a high probability use models with models that capture different traits necessary when any model can assign zero probability Log Linear Logical AND interpolated model only likes choices where all models agree use when you want to restrict possible answersParameter Averaging Problem Ensembling means we have to use M models at test time increasing our time memory complexity Parameter averaging is a cheap way to get some good effects of ensembling Basically write out models several times near the end of training and take the average of parametersEnsemble Distillation e.g. Kim et al. 2016 Problem parameter averaging only works for models within the same run Knowledge distillation trains a model to copy the ensemble Specifically it tries to match the description over predicted words Why We want the model to make the same mistakes as an ensemble Shown to increase accuracy notablyStacking What if we have two very different models where prediction of outputs is done in very different ways e.g. a phrase based translation model and a neural MT model Niehues et al. 2017 Stacking uses the output of one system in calculating features for another systemCase Studies in Conditional Language ModelingFrom Images e.g. Karpathy et al. 2015 Input is image features output is text Use standard image encoders e.g.', 'All ensembles have at least 3 workers. TL DR 6.7B RL T 0.7 labeler labeler ensem ble length copying ROUGE 1.3B sup logprob 1.3B RM 6.7B sup logprob 6.7B RM labeler 60.4 5.9 66.0 7.6 55.8 2.2 52.7 2.1 49.9 2.1 48.0 2.2 57.4 2.0 47.3 2.2 62.3 2.1 labeler ensemble 66.0 7.6 80.0 8.9 65.0 10.6 35.0 10.5 45.0 11.1 75.0 9.8 40.0 10.5 75.0 9.8 length 55.8 2.2 80.0 8.9 48.1 2.2 50.3 2.2 30.0 2.1 62.0 2.1 30.4 2.0 59.8 2.2 copying 52.7 2.1 65.0 10.6 48.1 2.2 52.0 2.2 64.2 2.1 56.7 2.2 64.4 2.1 53.4 2.2 ROUGE 49.9 2.1 35.0 10.5 50.3 2.2 52.0 2.2 50.5 2.2 52.0 2.3 51.1 2.3 54.5 2.1 1.3B sup.']",text-embedding-3-small,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9609509521143909,0.6,0.929264964756929,0.3333333333333333
78,How are images used as input for conditioned language models?,"Images can be used as input for conditioned language models in various ways, such as in image captioning tasks where the model generates textual descriptions based on the content of the image, or in tasks where the model generates text conditioned on the features extracted from the image. This allows the model to generate text that is relevant to the visual content of the image provided as input.","['While the effective receptive Ô¨Åeld size in this Ô¨Ågure is the same for both schemes in 2D attention the memory block contains a more evenly balanced number of pixels next to and above the query block respectively. dence across channels Salimans et al. . For each pixel the number of parameters is 10 times the number of mixture components 10 for one unnormalized mixture probability three means three standard deviations and three coefÔ¨Åcients which capture the linear dependence. For 10 mixtures this translates to 100 parameters for each pixel for 32 32 im ages the network outputs 102 400 dimensions which is a 7x reduction enabling denser gradients and lower memory. 4. Inference Across all of the presented experiments we use categorical sampling during decoding with a tempered softmax Dahl et al. 2017 . We adjust the concentration of the distribution we sample from with a temperature œÑ 0 by which we divide the logits for the channel intensities. We tuned œÑ between 0.8 and 1.0 observing the highest perceptual quality in unconditioned and class conditional image generation with œÑ 1.0. For super resolution we present results for different temperatures in Table 5. 5. Experiments All code we used to develop train and evaluate our models is available in Tensor2Tensor Vaswani et al. 2018 . For all experiments we optimize with Adam Kingma Ba 2015 and vary the learning rate as speciÔ¨Åed in Vaswani et al. 2017 . We train our models on both p100 and k40 GPUs with batch sizes ranging from 1 to 8 per GPU.Image Transformer Table 3. Conditional image generations for all CIFAR 10 cate gories. Images on the left are from a model that achieves 3.03 bits dim on the test set. Images on the right are from our best non averaged model with 2.99 bits dim. Both models are able to generate convincing cars trucks and ships. Generated horses planes and birds also look reasonable. 5.1. Generative Image Modeling Our unconditioned and class conditioned image generation models both use 1D local attention with lq 256 and a total memory size of 512.', 'file name: lecture04 Conditioned Generation. Hamidreza Mahyar mahyarh@mcmaster.ca Computational Natural Language Processing Conditioned GenerationLanguage Models Language models are generative models of text s P x The Malfoys said Hermione. Harry was watching him. He looked like Madame Maxime. When she strode up the wrong staircase to visit himself. I m afraid I ve definitely been suspended from power no chance indeed said Snape. He put his head back behind them and read groups as they crossed a corner and fluttered down onto their ink lamp and picked up his spoon. The doorbell rang. It was a lot cleaner down in London. Text Credit Max Deutsch https medium.com deep writingConditioned Language Models Not just generate text generate text according to some specification Input X Structured Data English Document Utterance Image Speech Output Y Text NL Description Japanese Short Description Response Text Transcript Task NL Generation Translation Summarization Response Generation Image Captioning Speech RecognitionFormulation and ModelingCalculating the Probability of a SentenceConditional Language ModelsOne Type of Language Model LSTM LSTM LSTM LSTM predict hate predict this predict movie predict s LSTM Mikolov et al. 2011 s I hate this movie predict ILSTM LSTM LSTM LSTM LSTM s LSTM LSTM LSTM LSTM argmax argmax argmax argmax argmax s One Type of Conditional Language Model I hate this movie kono eiga ga kirai hate this movie Sutskever et al. 2014 Encoder I DecoderHow to Pass Hidden State Initialize decoder w encoder Sutskever et al. 2014 encoder decoder Transform can be different dimensions encoder transform decoder encoder Input at every time step Kalchbrenner Blunsom 2013 decoder decoder decoderMethods of GenerationThe Generation Problem We have a model of P Y X how do we use it to generate a sentence Two methods Sampling Try to generate a random sentence according to the probability distribution. Argmax Try to generate the sentence with the highest probability.Ancestral Sampling Randomly generate words one by one.']",text-embedding-3-small,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9918167469840448,1.0,0.9254619113606456,0.0
79,What challenges exist in evaluating conditioned language models?,"Some challenges in evaluating conditioned language models include the tendency for larger models to not necessarily generalize well out of distribution, the risk of models being overly specific to the training data and not generalizing effectively outside of it, the potential exaggeration of performance on specific benchmarks even when models perform at human-level, and the issue of poor sample efficiency during pre-training. Additionally, there is a limitation in the current approach of scaling up language models due to the lack of weighting tokens based on importance, the reliance on self-supervised objectives that may not capture goal-directed actions effectively, and the lack of grounding in other domains such as real-world interactions or video experiences. These challenges highlight the need for improvements in evaluation methods and model design to address these limitations.

Source: Language_Models_are_Few_Shot_Learners.txt","['For instance HLW 20 observe that larger models do not necessarily generalize better out of distribution. There is evidence that suggests that the generalization achieved under this paradigm can be poor because the model is overly speciÔ¨Åc to the training distribution and does not generalize well outside it YdC 19 MPL19 . Thus the performance of Ô¨Åne tuned models on speciÔ¨Åc benchmarks even when it is nominally at human level may exaggerate actual performance on the underlying task GSL 18 NK19 . Third humans do not require large supervised datasets to learn most language tasks a brief directive in natural language e.g. please tell me if this sentence describes something happy or something sad or at most a tiny number of demonstrations e.g. here are two examples of people acting brave please give a third example of bravery is often Figure 1.1 Language model meta learning. During unsupervised pre training a language model develops a broad set of skills and pattern recognition abilities. It then uses these abilities at inference time to rapidly adapt to or recognize the desired task. We use the term in context learning to describe the inner loop of this process which occurs within the forward pass upon each sequence. The sequences in this diagram are not intended to be representative of the data a model would see during pre training but are intended to show that there are sometimes repeated sub tasks embedded within a single sequence. 3Figure 1.2 Larger models make increasingly efÔ¨Åcient use of in context information. We show in context learning performance on a simple task requiring the model to remove random symbols from a word both with and without a natural language task description see Sec. 3.9.2 . The steeper in context learning curves for large models demonstrate improved ability to learn a task from contextual information. We see qualitatively similar behavior across a wide range of tasks. sufÔ¨Åcient to enable a human to perform a new task to at least a reasonable degree of competence. Aside from pointing to a conceptual limitation in our current NLP techniques this adaptability has practical advantages it allows humans to seamlessly mix together or switch between many tasks and skills for example performing addition during a lengthy dialogue.', 'A more fundamental limitation of the general approach described in this paper scaling up any LM like model whether autoregressive or bidirectional is that it may eventually run into or could already be running into the limits of the 33pretraining objective. Our current objective weights every token equally and lacks a notion of what is most important to predict and what is less important. RRS20 demonstrate beneÔ¨Åts of customizing prediction to entities of interest. Also with self supervised objectives task speciÔ¨Åcation relies on forcing the desired task into a prediction problem whereas ultimately useful language systems for example virtual assistants might be better thought of as taking goal directed actions rather than just making predictions. Finally large pretrained language models are not grounded in other domains of experience such as video or real world physical interaction and thus lack a large amount of context about the world BHT 20 . For all these reasons scaling pure self supervised prediction is likely to hit limits and augmentation with a different approach is likely to be necessary. Promising future directions in this vein might include learning the objective function from humans ZSW 19a Ô¨Åne tuning with reinforcement learning or adding additional modalities such as images to provide grounding and a better model of the world CLY 19 . Another limitation broadly shared by language models is poor sample efÔ¨Åciency during pre training. While GPT 3 takes a step towards test time sample efÔ¨Åciency closer to that of humans one shot or zero shot it still sees much more text during pre training than a human sees in the their lifetime Lin20 . Improving pre training sample efÔ¨Åciency is an important direction for future work and might come from grounding in the physical world to provide additional information or from algorithmic improvements. A limitation or at least uncertainty associated with few shot learning in GPT 3 is ambiguity about whether few shot learning actually learns new tasks from scratch at inference time or if it simply recognizes and identiÔ¨Åes tasks that it has learned during training.']",text-embedding-3-small,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9932392273138442,1.0,0.8651793262459562,0.04
80,What is the instructor's name for this course?,"The instructor's name for the course ""Introduction to Computational Natural Language Processing"" is Hamidreza Mahyar.","['file name: NLP Winter 2024 Course Outline. Course Outline 1. COURSE INFORMATION Session Offered Winter 2024 Course Name Introduction to Computational Natural Language Processing Course Code SEP 775 Date s and Time s of lectures Lectures In person Wednesday 3 30 pm 6 30 pm Program Name System and Technology Calendar Description This graduate course introduces fundamental concepts in computational natural language processing NLP and their applications to teaching students how to deal with textual data in Artificial Intelligence. This course demonstrates how machines can learn different tasks in natural language such as language modeling text generation machine translation and language understanding. In this regard we go over the most promising methods in this literature and the most recent state of the art techniques. Moreover this course explores different real world applications of NLP and helps students get hands on experience in this field. Instructor Hamidreza Mahyar E Mail mahyarh@mcmaster.ca Office Hours upon request 2.', '. Students should be aware that when they access the electronic components of a course using these elements private information such as first and last names user names for the McMaster e mail accounts and program affiliation may become apparent to all other students in the same course. The available information is dependent on the technology used. Continuation in a course that uses on line elements will be deemed consent to this disclosure. If you have any questions or concerns about such disclosure please discuss this with the course instructor. ONLINE PROCTORING Some courses may use online proctoring software for tests and exams. This software may require students to turn on their video camera present identification monitor and record their computer activities and or lock restrict their browser or other applications software during tests or exams. This software may be required to be installed before the test exam begins. COMMUNICATIONS It is the student s responsibility to Maintain current contact information with the University including address phone numbers and emergency contact information. Use the University provided e mail address or maintain a valid forwarding e mail address.', 'COURSE SPECIFICS Course Description This course covers some fundamental concepts in natural language processing NLP and how they apply in real world use cases. Instruction Type Code Type Hours per term C Classroom instruction 39 L Laboratory workshop or fieldwork T Tutorial DE Distance education Total Hours 39 Resources ISBN Textbook Title Edition Author Publisher Natural Language Processing Jacob Eisenstein Speech and Language Processing Dan Jurafsky and James H. Martin Deep Learning Ian Goodfellow Yoshua Bengio and Aaron Courville Prerequisite s Proficiency in Python programming language Some Machine Learning and or Deep Learning knowledge is a plus Corequisite s Antirequisite s Course Specific Policies All work must be shown to get full credit. Specific policy of Projects Assignments submissionsProjects Assignments will be conducted online by using related software. Completed Projects Assignments should be uploaded to the drop box before midnight of the due date. Projects Assignments missed due to legitimate reasons must be completed later mutually agreed with the instructor.', 'Students should submit their request to their Faculty Office normally within 10 working days of the beginning of term in which they anticipate a need for accommodation or to the Registrar s Office prior to their examinations. Students should also contact their instructors as soon as possible to make alternative arrangements for classes assignments and tests. http www.mcmaster.ca policy Students AcademicStudies Studentcode.pdf COPYRIGHT AND RECORDING Students are advised that lectures demonstrations performances and any other course material provided by an instructor include copyright protected works. The Copyright Act and copyright law protect every original literary dramatic musical and artistic work including lectures by University instructors The recording of lectures tutorials or other methods of instruction may occur during a course. Recording may be done by either the instructor for the purpose of authorized distribution or by a student for the purpose of personal study. Students should be aware that their voice and or image may be recorded by others during the class. Please speak with the instructor if this is a concern for you.']",text-embedding-3-small,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9157200533247656,1.0,0.9394654423691152,0.1
81,What is the email address of the instructor for this course?,"The email address of the instructor for the course ""Introduction to Computational Natural Language Processing"" (Course Code: SEP 775) is mahyarh@mcmaster.ca.

Source: NLP Winter 2024 Course Outline.","['. Students should be aware that when they access the electronic components of a course using these elements private information such as first and last names user names for the McMaster e mail accounts and program affiliation may become apparent to all other students in the same course. The available information is dependent on the technology used. Continuation in a course that uses on line elements will be deemed consent to this disclosure. If you have any questions or concerns about such disclosure please discuss this with the course instructor. ONLINE PROCTORING Some courses may use online proctoring software for tests and exams. This software may require students to turn on their video camera present identification monitor and record their computer activities and or lock restrict their browser or other applications software during tests or exams. This software may be required to be installed before the test exam begins. COMMUNICATIONS It is the student s responsibility to Maintain current contact information with the University including address phone numbers and emergency contact information. Use the University provided e mail address or maintain a valid forwarding e mail address.', 'Students should submit their request to their Faculty Office normally within 10 working days of the beginning of term in which they anticipate a need for accommodation or to the Registrar s Office prior to their examinations. Students should also contact their instructors as soon as possible to make alternative arrangements for classes assignments and tests. http www.mcmaster.ca policy Students AcademicStudies Studentcode.pdf COPYRIGHT AND RECORDING Students are advised that lectures demonstrations performances and any other course material provided by an instructor include copyright protected works. The Copyright Act and copyright law protect every original literary dramatic musical and artistic work including lectures by University instructors The recording of lectures tutorials or other methods of instruction may occur during a course. Recording may be done by either the instructor for the purpose of authorized distribution or by a student for the purpose of personal study. Students should be aware that their voice and or image may be recorded by others during the class. Please speak with the instructor if this is a concern for you.', 'file name: NLP Winter 2024 Course Outline. Course Outline 1. COURSE INFORMATION Session Offered Winter 2024 Course Name Introduction to Computational Natural Language Processing Course Code SEP 775 Date s and Time s of lectures Lectures In person Wednesday 3 30 pm 6 30 pm Program Name System and Technology Calendar Description This graduate course introduces fundamental concepts in computational natural language processing NLP and their applications to teaching students how to deal with textual data in Artificial Intelligence. This course demonstrates how machines can learn different tasks in natural language such as language modeling text generation machine translation and language understanding. In this regard we go over the most promising methods in this literature and the most recent state of the art techniques. Moreover this course explores different real world applications of NLP and helps students get hands on experience in this field. Instructor Hamidreza Mahyar E Mail mahyarh@mcmaster.ca Office Hours upon request 2.', 'Regularly check the official University communications channels. Official University communications are considered received if sent by postal mail by fax or by e mail to the student s designated primary e mail account via their @mcmaster.ca alias. Accept that forwarded e mails may be lost and that e mail is considered received if sent via the student s @mcmaster.ca alias. Check the McMaster Avenue email and course websites on a regular basis during the term. CONDUCT EXPECTATIONS As a McMaster student you have the right to experience and the responsibility to demonstrate respectful and dignified interactions within all of our living learning and working communities. These expectations are described in the Code of Student Rights Responsibilities the Code . All students share the responsibility of maintaining a positive environment for the academic and personal growth of all McMaster community members whether in person or online. It is essential that students be mindful of their interactions online as the Code remains in effect in virtual learning environments. The Code applies to any interactions that adversely affect disrupt or interfere with reasonable participation in University activities.']",text-embedding-3-small,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9164196233140022,1.0,0.8962192671649039,0.0
82,When is this course held every week?,"The course ""Introduction to Computational Natural Language Processing"" with course code SEP 775 is held in person on Wednesdays from 3:30 pm to 6:30 pm every week during the Winter 2024 session.","['3. SUB TOPIC S Week 1 Overview Human language Word meanings Word2Vec and Optimization basics Week 2 Intro to neural networks Gradients and backpropagation Week 3 Recurrent Neural Networks and Language Models Week 4 Conditioned Generation Week 5 Part 1 Self attention and transformers Part 2 Seq2Seq models Week 6 Midterm Test Week 7 Midterm Recess No class Week 8 Prompting Instruction Finetuning and RLHF Week 9 Question Answering systems Week 10 Multimodal models Week 11 Project presentation and Q A Week 12 Project presentation and Q A Week 13 Project presentation and Q A Classes end April 10th 4. ASSESSMENT OF LEARNING Weight 4 Assignments 20 4 Quizzes 10 Midterm Test 35 Group Project 35 TOTAL 100Note that the percentage distribution of each module may be adjusted. Percentage grades will be converted to letter grades and grade points per the University calendar. 5.', 'file name: NLP Winter 2024 Course Outline. Course Outline 1. COURSE INFORMATION Session Offered Winter 2024 Course Name Introduction to Computational Natural Language Processing Course Code SEP 775 Date s and Time s of lectures Lectures In person Wednesday 3 30 pm 6 30 pm Program Name System and Technology Calendar Description This graduate course introduces fundamental concepts in computational natural language processing NLP and their applications to teaching students how to deal with textual data in Artificial Intelligence. This course demonstrates how machines can learn different tasks in natural language such as language modeling text generation machine translation and language understanding. In this regard we go over the most promising methods in this literature and the most recent state of the art techniques. Moreover this course explores different real world applications of NLP and helps students get hands on experience in this field. Instructor Hamidreza Mahyar E Mail mahyarh@mcmaster.ca Office Hours upon request 2.', 'COURSE SPECIFICS Course Description This course covers some fundamental concepts in natural language processing NLP and how they apply in real world use cases. Instruction Type Code Type Hours per term C Classroom instruction 39 L Laboratory workshop or fieldwork T Tutorial DE Distance education Total Hours 39 Resources ISBN Textbook Title Edition Author Publisher Natural Language Processing Jacob Eisenstein Speech and Language Processing Dan Jurafsky and James H. Martin Deep Learning Ian Goodfellow Yoshua Bengio and Aaron Courville Prerequisite s Proficiency in Python programming language Some Machine Learning and or Deep Learning knowledge is a plus Corequisite s Antirequisite s Course Specific Policies All work must be shown to get full credit. Specific policy of Projects Assignments submissionsProjects Assignments will be conducted online by using related software. Completed Projects Assignments should be uploaded to the drop box before midnight of the due date. Projects Assignments missed due to legitimate reasons must be completed later mutually agreed with the instructor.', 'Students should submit their request to their Faculty Office normally within 10 working days of the beginning of term in which they anticipate a need for accommodation or to the Registrar s Office prior to their examinations. Students should also contact their instructors as soon as possible to make alternative arrangements for classes assignments and tests. http www.mcmaster.ca policy Students AcademicStudies Studentcode.pdf COPYRIGHT AND RECORDING Students are advised that lectures demonstrations performances and any other course material provided by an instructor include copyright protected works. The Copyright Act and copyright law protect every original literary dramatic musical and artistic work including lectures by University instructors The recording of lectures tutorials or other methods of instruction may occur during a course. Recording may be done by either the instructor for the purpose of authorized distribution or by a student for the purpose of personal study. Students should be aware that their voice and or image may be recorded by others during the class. Please speak with the instructor if this is a concern for you.']",text-embedding-3-small,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.8459265441514411,1.0,0.8547195606207922,0.1428571428571428
83,What is the name of the TA for this course?,"The TA for this course is not mentioned in the provided context information. If you need more specific details about the TA, you may need to refer to the official course documentation or contact the course instructor directly for that information.","['file name: NLP Winter 2024 Course Outline. Course Outline 1. COURSE INFORMATION Session Offered Winter 2024 Course Name Introduction to Computational Natural Language Processing Course Code SEP 775 Date s and Time s of lectures Lectures In person Wednesday 3 30 pm 6 30 pm Program Name System and Technology Calendar Description This graduate course introduces fundamental concepts in computational natural language processing NLP and their applications to teaching students how to deal with textual data in Artificial Intelligence. This course demonstrates how machines can learn different tasks in natural language such as language modeling text generation machine translation and language understanding. In this regard we go over the most promising methods in this literature and the most recent state of the art techniques. Moreover this course explores different real world applications of NLP and helps students get hands on experience in this field. Instructor Hamidreza Mahyar E Mail mahyarh@mcmaster.ca Office Hours upon request 2.', '. Students should be aware that when they access the electronic components of a course using these elements private information such as first and last names user names for the McMaster e mail accounts and program affiliation may become apparent to all other students in the same course. The available information is dependent on the technology used. Continuation in a course that uses on line elements will be deemed consent to this disclosure. If you have any questions or concerns about such disclosure please discuss this with the course instructor. ONLINE PROCTORING Some courses may use online proctoring software for tests and exams. This software may require students to turn on their video camera present identification monitor and record their computer activities and or lock restrict their browser or other applications software during tests or exams. This software may be required to be installed before the test exam begins. COMMUNICATIONS It is the student s responsibility to Maintain current contact information with the University including address phone numbers and emergency contact information. Use the University provided e mail address or maintain a valid forwarding e mail address.', 'COURSE SPECIFICS Course Description This course covers some fundamental concepts in natural language processing NLP and how they apply in real world use cases. Instruction Type Code Type Hours per term C Classroom instruction 39 L Laboratory workshop or fieldwork T Tutorial DE Distance education Total Hours 39 Resources ISBN Textbook Title Edition Author Publisher Natural Language Processing Jacob Eisenstein Speech and Language Processing Dan Jurafsky and James H. Martin Deep Learning Ian Goodfellow Yoshua Bengio and Aaron Courville Prerequisite s Proficiency in Python programming language Some Machine Learning and or Deep Learning knowledge is a plus Corequisite s Antirequisite s Course Specific Policies All work must be shown to get full credit. Specific policy of Projects Assignments submissionsProjects Assignments will be conducted online by using related software. Completed Projects Assignments should be uploaded to the drop box before midnight of the due date. Projects Assignments missed due to legitimate reasons must be completed later mutually agreed with the instructor.', '3. SUB TOPIC S Week 1 Overview Human language Word meanings Word2Vec and Optimization basics Week 2 Intro to neural networks Gradients and backpropagation Week 3 Recurrent Neural Networks and Language Models Week 4 Conditioned Generation Week 5 Part 1 Self attention and transformers Part 2 Seq2Seq models Week 6 Midterm Test Week 7 Midterm Recess No class Week 8 Prompting Instruction Finetuning and RLHF Week 9 Question Answering systems Week 10 Multimodal models Week 11 Project presentation and Q A Week 12 Project presentation and Q A Week 13 Project presentation and Q A Classes end April 10th 4. ASSESSMENT OF LEARNING Weight 4 Assignments 20 4 Quizzes 10 Midterm Test 35 Group Project 35 TOTAL 100Note that the percentage distribution of each module may be adjusted. Percentage grades will be converted to letter grades and grade points per the University calendar. 5.']",text-embedding-3-small,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.0,,0.833724864337395,0.0
84,What is Word2vec?,"Word2vec is a model that represents each word in a fixed vocabulary as a low-dimensional vector, much smaller than the vocabulary size. It learns the value of each word's vector to be predictive based on the distribution of words in a short context. The specific model described in the context is the skip-gram word2vec algorithm. It focuses on learning word embeddings by making predictions in local context windows. The word vectors are sometimes called distributed representations as the word's meaning is distributed across the vector dimensions. The parameters of these word vectors are optimized using iterative algorithms based on gradient descent. Source: Lecture01_notes_Introduction.txt, Efficient_Estimation_of_Word_Representations_in_Vector_Space.txt, lecture01_notes_wordvecs2.txt, Contextual_Word_Representations_A_Contextual_Introduction.txt.","['However many of the details of word2vec will hold true in methods that we ll proceed to further in the course so we ll focus our time on that. 3.2 Word2vec model and objective The word2vec model represents each word in a fixed vocabulary as a low dimensional much smaller than vocabulary size vector. It learns the value of each word s vector to be predictive via a simple function of the distribution of words in a usually short 2 4 words context. The model we ll describe here is called the skipgram word2vec algo rithm. Skipgram word2vec. As usual we have a finite vocabulary V. Let C O be random variables representing an unknown pair of C V a center word and O V an outside word appearing in the context of the center word . We ll use c o to refer to specific values of the random variables.', 'file name: Efficient Estimation of Word Representations in Vector Space. EfÔ¨Åcient Estimation of Word Representations in Vector Space Tomas Mikolov Google Inc. Mountain View CA tmikolov@google.com Kai Chen Google Inc. Mountain View CA kaichen@google.com Greg Corrado Google Inc. Mountain View CA gcorrado@google.com Jeffrey Dean Google Inc. Mountain View CA jeff@google.com Abstract We propose two novel model architectures for computing continuous vector repre sentations of words from very large data sets. The quality of these representations is measured in a word similarity task and the results are compared to the previ ously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set.', 'Jeffrey Pennington Richard Socher and Christopher D. Manning. 2014. GloVe Global Vectors for Word Repre sentation 1.1 Comparison with Previous Methods So far we have looked at two main classes of methods to Ô¨Ånd word embeddings. The Ô¨Årst set are count based and rely on matrix factor ization e.g. LSA HAL . While these methods effectively leverage global statistical information they are primarily used to capture word similarities and do poorly on tasks such as word analogy indi cating a sub optimal vector space structure. The other set of methods are shallow window based e.g. the skip gram and the CBOW mod els which learn word embeddings by making predictions in local context windows. These models demonstrate the capacity to capture complex linguistic patterns beyond word similarity but fail to make use of the global co occurrence statistics.', 'The word s meaning is distributed across the whole vector for this reason these vectors are sometimes called distributed representations.5 As corpora grew scalability became a challenge because the number of observable contexts grew as well. Underlying all word vector algorithms is the notion that the value placed in each dimension of each word type s vector is a parameter that will be optimized alongside all the other parameters to best Ô¨Åt the observed patterns of the words in the data. Since we view these parameters as continuous values and the notion of Ô¨Åtting the data can be operationalized as a smooth continuous objective function selecting the parameter values is done using iterative algorithms based on gradient descent. Using tools that had become popular in machine learning faster methods based on stochastic optimization were developed. One widely known collection of algorithms is available as the word2vec package Mikolov et al. 2013 . A common pattern arose in which industry researchers with large corpora and powerful computing infrastructure would construct word vectors using an established often expensive iterative method and then publish the vectors for anyone to use.']",text-embedding-3-small,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9869208964181452,1.0,0.9201541402454648,0.0344827586206896
85,What are the main learning goals of the course?,"The main learning goals of the course are:
1. Understanding the foundations of effective modern methods for deep learning applied to NLP, starting with basics and then covering key methods used in NLP in 2024 such as word vectors, feedforward networks, recurrent networks, attention, encoder-decoder models, transformers, and large pre-trained language models.
2. Developing a big picture understanding of human languages and the challenges in understanding and generating them using computers.
3. Gaining knowledge about various aspects of NLP including deep learning, word representations, language models, pre-trained language models, text generation, Seq2Seq models, prompting, feedback systems, multimodal models, and NLP applications like Q&A systems.

Source: SEP 775 - Computational Natural Language Processing, Course Context.","['learning goals 4 1. The foundations of the effective modern methods for deep learning applied to NLP Basics first then key methods used in NLP in 2024 Word vectors feed forward networks recurrent networks attention encoder decoder models transformers large pre trained language models etc. 2. A big picture understanding of human languages and the difficulties in understanding and producing them via computers 3.', '5. LEARNING OUTCOMES By the end of this course the students will know about Introduction to Deep Learning Word representations Language models Pre trained language models Text generation Seq2Seq models Prompting Feedback systems Multimodal models NLP applications e.g. Q A systems 6. COURSE OUTLINE APPROVED ADVISORY STATEMENTS ANTI DISCRIMINATION The Faculty of Engineering is concerned with ensuring an environment that is free of all discrimination. If there is a problem individuals are reminded that they should contact the Associate Director Graduate Studies the Sexual Harassment Officer or the Human Rights Consultant as soon as possible. http www.mcmaster.ca policy General HR Discrimination_Harassment_Sexual_Harassment Prevention Response.pdf ACADEMIC INTEGRITY You are expected to exhibit honesty and use ethical behaviour in all aspects of the learning process. Academic credentials you earn are rooted in principles of honesty and academic integrity. It is your responsibility to understand what constitutes academic dishonesty.', 'vision are also of interest. 1.2 Language and machines Human children interacting with a rich multi modality world and various forms of feedback acquire language with exceptional sample efficiency not observing that much language and compute efficiency brains are efficient computing machines With all the impressive advances in NLP in the last decades we are still nowhere close to developing learning machines that have a fraction of acquisition ability of children. One fundamental and still quite open problem in building language learning machines is the question of representa tion how should we represent language in a computer such that the computer can robustly process and or generate it This is where this course focuses on the tools provided by deep learning a highly effec tive toolkit for representing both the wild variety of natural language and some of the rules and structures it sometimes adheres to. Much of this course will be dedicated to this question of representation and the rest of this note will talk about a basic subquestion how do we represent words Before that though let s briefly discuss some of the applications you can hope to build after learning modern NLP techniques.', '3. SUB TOPIC S Week 1 Overview Human language Word meanings Word2Vec and Optimization basics Week 2 Intro to neural networks Gradients and backpropagation Week 3 Recurrent Neural Networks and Language Models Week 4 Conditioned Generation Week 5 Part 1 Self attention and transformers Part 2 Seq2Seq models Week 6 Midterm Test Week 7 Midterm Recess No class Week 8 Prompting Instruction Finetuning and RLHF Week 9 Question Answering systems Week 10 Multimodal models Week 11 Project presentation and Q A Week 12 Project presentation and Q A Week 13 Project presentation and Q A Classes end April 10th 4. ASSESSMENT OF LEARNING Weight 4 Assignments 20 4 Quizzes 10 Midterm Test 35 Group Project 35 TOTAL 100Note that the percentage distribution of each module may be adjusted. Percentage grades will be converted to letter grades and grade points per the University calendar. 5.']",text-embedding-3-small,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999999999999996,1.0,0.9538546846950732,0.0769230769230769
86,How is the course grade distributed?,"The course grade is distributed as follows:
- Assignments: 20%
- Quizzes: 10%
- Midterm Test: 35%
- Group Project: 35%
(Source: SEP 775 - Computational Natural Language Processing Course Outline)","['3. SUB TOPIC S Week 1 Overview Human language Word meanings Word2Vec and Optimization basics Week 2 Intro to neural networks Gradients and backpropagation Week 3 Recurrent Neural Networks and Language Models Week 4 Conditioned Generation Week 5 Part 1 Self attention and transformers Part 2 Seq2Seq models Week 6 Midterm Test Week 7 Midterm Recess No class Week 8 Prompting Instruction Finetuning and RLHF Week 9 Question Answering systems Week 10 Multimodal models Week 11 Project presentation and Q A Week 12 Project presentation and Q A Week 13 Project presentation and Q A Classes end April 10th 4. ASSESSMENT OF LEARNING Weight 4 Assignments 20 4 Quizzes 10 Midterm Test 35 Group Project 35 TOTAL 100Note that the percentage distribution of each module may be adjusted. Percentage grades will be converted to letter grades and grade points per the University calendar. 5.', 'For courses using such software students will be expected to submit their work electronically either directly to Turnitin.com or via an online learning platform e.g. A2L etc. using plagiarism detection a service supported by Turnitin.com so it can be checked for academic dishonesty. Students who do not wish their work to be submitted through the plagiarism detection software must inform the Instructor before the assignment is due. No penalty will be assigned to a student who does not submit work to the plagiarism detection software. All submitted work is subject to normal verification that standards of academicintegrity have been upheld e.g. on line search other software etc. . For more details about McMaster s use of Turnitin.com please go to www.mcmaster.ca academicintegrity. COURSES WITH AN ON LINE ELEMENT Some courses may use on line elements e.g. e mail Avenue to Learn A2L LearnLink web pages capa Moodle ThinkingCap etc. .', '. Students should be aware that when they access the electronic components of a course using these elements private information such as first and last names user names for the McMaster e mail accounts and program affiliation may become apparent to all other students in the same course. The available information is dependent on the technology used. Continuation in a course that uses on line elements will be deemed consent to this disclosure. If you have any questions or concerns about such disclosure please discuss this with the course instructor. ONLINE PROCTORING Some courses may use online proctoring software for tests and exams. This software may require students to turn on their video camera present identification monitor and record their computer activities and or lock restrict their browser or other applications software during tests or exams. This software may be required to be installed before the test exam begins. COMMUNICATIONS It is the student s responsibility to Maintain current contact information with the University including address phone numbers and emergency contact information. Use the University provided e mail address or maintain a valid forwarding e mail address.', 'Students should submit their request to their Faculty Office normally within 10 working days of the beginning of term in which they anticipate a need for accommodation or to the Registrar s Office prior to their examinations. Students should also contact their instructors as soon as possible to make alternative arrangements for classes assignments and tests. http www.mcmaster.ca policy Students AcademicStudies Studentcode.pdf COPYRIGHT AND RECORDING Students are advised that lectures demonstrations performances and any other course material provided by an instructor include copyright protected works. The Copyright Act and copyright law protect every original literary dramatic musical and artistic work including lectures by University instructors The recording of lectures tutorials or other methods of instruction may occur during a course. Recording may be done by either the instructor for the purpose of authorized distribution or by a student for the purpose of personal study. Students should be aware that their voice and or image may be recorded by others during the class. Please speak with the instructor if this is a concern for you.']",text-embedding-3-small,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999999999999996,0.8333333333333334,0.931533433604044,0.0344827586206896
87,What is the policy for late submissions in this course?,"The policy for late submissions in this course is that there are NO free late days, and there is a penalty of 1 off course grade per day late. Assignments are not accepted more than 3 days late unless given permission in advance.","['For courses using such software students will be expected to submit their work electronically either directly to Turnitin.com or via an online learning platform e.g. A2L etc. using plagiarism detection a service supported by Turnitin.com so it can be checked for academic dishonesty. Students who do not wish their work to be submitted through the plagiarism detection software must inform the Instructor before the assignment is due. No penalty will be assigned to a student who does not submit work to the plagiarism detection software. All submitted work is subject to normal verification that standards of academicintegrity have been upheld e.g. on line search other software etc. . For more details about McMaster s use of Turnitin.com please go to www.mcmaster.ca academicintegrity. COURSES WITH AN ON LINE ELEMENT Some courses may use on line elements e.g. e mail Avenue to Learn A2L LearnLink web pages capa Moodle ThinkingCap etc. .', 'Students should submit their request to their Faculty Office normally within 10 working days of the beginning of term in which they anticipate a need for accommodation or to the Registrar s Office prior to their examinations. Students should also contact their instructors as soon as possible to make alternative arrangements for classes assignments and tests. http www.mcmaster.ca policy Students AcademicStudies Studentcode.pdf COPYRIGHT AND RECORDING Students are advised that lectures demonstrations performances and any other course material provided by an instructor include copyright protected works. The Copyright Act and copyright law protect every original literary dramatic musical and artistic work including lectures by University instructors The recording of lectures tutorials or other methods of instruction may occur during a course. Recording may be done by either the instructor for the purpose of authorized distribution or by a student for the purpose of personal study. Students should be aware that their voice and or image may be recorded by others during the class. Please speak with the instructor if this is a concern for you.', 'An understanding of and ability to build systems in PyTorch for some of the major problems in NLP Word meaning dependency parsing machine translation question answeringCourse work and grading policy 5 4 Assignments 20 4 Quizzes 10 Midterm 35 Final Group Project 1 3 people 34 Late day policy NO free late days 1 off course grade per day late Assignments not accepted more than 3 days late unless given permission in advance Collaboration policy Understand allowed collaboration and how to document it Don t take code off the web acknowledge working with other students write your own assignment solutionsTrained on text data neural machine translation is quite good https kiswahili.tuko.co.keThe SEC said Musk your tweets are a blight. They really could cost you your job if you don t stop all this tweeting at night. Then Musk cried Why The tweets I wrote are not mean I don t use all caps and I m sure that my tweets are clean. But your tweets can move markets and that s why we re sore.', 'Departmental Policies The use of cell phones iPods laptops and other personal electronic devices are prohibited from the classroom during the class time unless the instructor makes an explicit exception. Announcements made in class or placed on Avenue are considered to have been communicated to all students including those individuals that are not in class. Instructor has the right to submit work to software to identify plagiarism. 3.']",text-embedding-3-small,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9832537859774184,1.0,0.9625397787656936,0.0434782608695652
88,What does the course hope to teach regarding word vectors?,The course hopes to teach that word meaning can be represented rather well by a high dimensional vector of real numbers.,"['file name: lecture01 wordvecs1. Hamidreza Mahyar mahyarh@mcmaster.ca Computational Natural Language Processing Overview of NLP and Word VectorsLecture Plan 2 Lecture 1 Introduction and Word Vectors 1. The course 2. Human language and word meaning 3. Word2vec introduction 4. Word2vec objective function gradients 5. Optimization basics 6. Looking at word vectors Key learning today The astounding result that word meaning can be represented rather well by a high dimensional vector of real numbersCourse logistics in brief 3 Instructor Hamidreza Mahyar Head TA Reza Namazi Time Wednesday 3 30pm 6 30pm We put a lot of other important information on Avenue to Learn. Please read itWhat do we hope to teach A.k.a. learning goals 4 1.', '8 CBOW 100B 68.4 79.6 75.4 59.4 45.5 Table 6 Here we see the correlations be tween of word vector similarities using different embedding techniques with different human judgment datasets 2.6 Further Reading Dealing With Ambiguity One might wonder how we handle the situation where we want to capture the same word with different vectors for its different uses in natural language. For instance run is both a noun and a verb and is used and interpreted differently based on the context. Improving Word Representations Via Global Context And Multi ple Word Prototypes Huang et al 2012 describes how such cases can also be handled in NLP. The essence of the method is the following 1. Gather Ô¨Åxed size context windows of all occurrences of the word for instance 5 before and 5 after 2. Each context is represented by a weighted average of the context words vectors using idf weighting 3. Apply spherical k means to cluster these context representations. 4.', 'We believe that word vectors trained on even larger data sets with larger dimensionality will perform signiÔ¨Åcantly better and will enable the development of new innovative applications. Another way to improve accuracy is to provide more than one example of the relationship. By using ten examples instead of one to form the relationship vector we average the individual vectors together we have observed improvement of accuracy of our best models by about 10 absolutely on the semantic syntactic test. It is also possible to apply the vector operations to solve different tasks. For example we have observed good accuracy for selecting out of the list words by computing average vector for a list of words and Ô¨Ånding the most distant word vector. This is a popular type of problems in certain human intelligence tests. Clearly there is still a lot of discoveries to be made using these techniques. 6 Conclusion In this paper we studied the quality of vector representations of words derived by various models on a collection of syntactic and semantic language tasks.', 'However many of the details of word2vec will hold true in methods that we ll proceed to further in the course so we ll focus our time on that. 3.2 Word2vec model and objective The word2vec model represents each word in a fixed vocabulary as a low dimensional much smaller than vocabulary size vector. It learns the value of each word s vector to be predictive via a simple function of the distribution of words in a usually short 2 4 words context. The model we ll describe here is called the skipgram word2vec algo rithm. Skipgram word2vec. As usual we have a finite vocabulary V. Let C O be random variables representing an unknown pair of C V a center word and O V an outside word appearing in the context of the center word . We ll use c o to refer to specific values of the random variables.']",text-embedding-3-small,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.8935645846756067,1.0,0.9644346981202137,0.0434782608695652
89,How does Word2vec represent word meaning?,"Word2vec represents each word in a fixed vocabulary as a low-dimensional vector, much smaller than the vocabulary size. It learns the value of each word's vector to be predictive based on the distribution of words in a usually short context. The model, specifically the skipgram word2vec algorithm, focuses on learning word vectors that are predictive based on the context in which the words appear. By training the word vectors to predict surrounding words in a sentence, Word2vec captures the distribution of the context in which a word appears, allowing words that frequently appear together to have similar vector representations. This approach helps Word2vec capture semantic relationships between words based on their usage in context.

Source: lecture01_notes_Introduction.txt","['As the word vectors are trained to predict the surrounding words in the sentence the vectors can be seen as representing the distribution of the context in which a word appears. These values are related logarithmically to the probabilities computed by the output layer so the sum of two word vectors is related to the product of the two context distributions. The product works here as the AND function words that are assigned high probabilities by both word vectors will have high probability and the other words will have low probability. Thus if Volga River appears frequently in the same sentence together with the words Russian and river the sum of these two word vectors will result in such a feature vector that is close to the vector of Volga River . 6 Comparison to Published Word Representations Many authors who previously worked on the neural network based representations of words have published their resulting models for further use and comparison amongst the most well known au thors are Collobert and Weston 2 Turian et al. 17 and Mnih and Hinton 10 .', 'However many of the details of word2vec will hold true in methods that we ll proceed to further in the course so we ll focus our time on that. 3.2 Word2vec model and objective The word2vec model represents each word in a fixed vocabulary as a low dimensional much smaller than vocabulary size vector. It learns the value of each word s vector to be predictive via a simple function of the distribution of words in a usually short 2 4 words context. The model we ll describe here is called the skipgram word2vec algo rithm. Skipgram word2vec. As usual we have a finite vocabulary V. Let C O be random variables representing an unknown pair of C V a center word and O V an outside word appearing in the context of the center word . We ll use c o to refer to specific values of the random variables.', 'More recently a square root type transformation in the form of Hellinger PCA HPCA Lebret and Collobert 2014 has been suggested as an effec tive way of learning word representations. Shallow Window Based Methods. Another approach is to learn word representations that aid in making predictions within local context win dows. For example Bengio et al. 2003 intro duced a model that learns word vector representa tions as part of a simple neural network architec ture for language modeling. Collobert and Weston 2008 decoupled the word vector training from the downstream training objectives which paved the way for Collobert et al. 2011 to use the full context of a word for learning the word represen tations rather than just the preceding context as is the case with language models. Recently the importance of the full neural net work structure for learning useful word repre sentations has been called into question.', 'Recently Mikolov et al. 2013c introduced a new evalua tion scheme based on word analogies that probes the Ô¨Åner structure of the word vector space by ex amining not the scalar distance between word vec tors but rather their various dimensions of dif ference. For example the analogy king is to queen as man is to woman should be encoded in the vector space by the vector equation king queen man woman. This evaluation scheme favors models that produce dimensions of mean ing thereby capturing the multi clustering idea of distributed representations Bengio 2009 . The two main model families for learning word vectors are 1 global matrix factorization meth ods such as latent semantic analysis LSA Deer wester et al. 1990 and 2 local context window methods such as the skip gram model of Mikolov et al. 2013c . Currently both families suffer sig niÔ¨Åcant drawbacks.']",text-embedding-3-small,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9584550878885092,0.8,0.9154487068136772,0.0434782608695652
90,What is the main focus of this lecture on neural networks?,"The main focus of this lecture on neural networks is on introducing single and multilayer neural networks for classification purposes, discussing how they can be trained using backpropagation, explaining the use of the chain rule for parameter updates, providing practical tips and tricks for training neural networks, and motivating the use of recurrent neural networks as a language model.","['This is the prowess of neural networks. Fun Fact Neural networks are biologically in spired classiÔ¨Åers which is why they are often called artiÔ¨Åcial neural networks to distinguish them from the organic kind. However in reality human neural networks are so much more capable and complex from artiÔ¨Åcial neural net works that it is usually better to not draw too many parallels between the two. We established in our previous discussions the need for non linear classiÔ¨Åers since most data are not linearly separable and thus our classiÔ¨Åcation performance on them is limited. Neural networks are a family of classiÔ¨Åers with non linear decision boundary as seen in Figure 1. Now that we know the sort of decision boundaries neural networks create let us see how they manage doing so. 1.1 A Neuron A neuron is a generic computational unit that takes n inputs and produces a single output. What differentiates the outputs of different neurons is their parameters also referred to as their weights .', 'everything doing this everywhere was the key test Now much less needed you can use it to check layers are correctly implemented 78Summary We ve mastered the core technology of neural nets Backpropagation recursively and hence efficiently apply the chain rule along computation graph downstream gradient upstream gradient x local gradient Forward pass compute results of operations and save intermediate values Backward pass apply chain rule to compute gradients 79Why learn all these details about gradients 80 Modern deep learning frameworks compute gradients for you But why take a class on compilers or systems when they are implemented for you Understanding what is going on under the hood is useful Backpropagation doesn t always work perfectly out of the box Understanding why is crucial for debugging and improving models See Karpathy article https medium.com @karpathy yes you should understand backprop e2f06eab496b Example in future lecture exploding and vanishing gradients', 'file name: lecture03 RNNs and LLMs. Hamidreza Mahyar mahyarh@mcmaster.ca Computational Natural Language Processing Language Models and Recurrent Neural NetsLecture Plan 1. A bit more about neural networks Language modeling RNNs 2. A new NLP task Language Modeling 4. Problems with RNNs 5. Recap on RNNs LMs Reminders You should have started working on Assignment 1 2 motivates 3. A new family of neural networks Recurrent Neural Networks RNNs This is the most important concept in the class It leads to GPT 3 and ChatGPT Important and used in Ass1 but not the only way to build LMs0 model power Training error Test error Classic view Regularization works to prevent overfitting when we have a lot of features or later a very powerful deep model etc.', 'file name: lecture02 notes Backpropagation.  Neural networks. Forward computation. Backward propagation. Neuron Units. Max margin Loss. Gradient checks. Xavier parameter initialization. Learning rates. Adagrad. This set of notes introduces single and multilayer neural networks and how they can be used for classiÔ¨Åcation purposes. We then dis cuss how they can be trained using a distributed gradient descent technique known as backpropagation. We will see how the chain rule can be used to make parameter updates sequentially. After a rigorous mathematical discussion of neural networks we will discuss some practical tips and tricks in training neural networks involving neuron units non linearities gradient checks Xavier parameter ini tialization learning rates Adagrad etc. Lastly we will motivate the use of recurrent neural networks as a language model. 1 Neural Networks Foundations Figure 1 We see here how a non linear decision boundary separates the data very well. This is the prowess of neural networks.']",text-embedding-3-small,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.0000000000000002,1.0,0.9175241449286085,0.0357142857142857
91,What are the non-linear functions mentioned for neural networks?,"The non-linear functions mentioned for neural networks are logistic sigmoid, tanh, hard tanh, Swish, Rectified Linear Unit (ReLU), Leaky ReLU, Parametric ReLU, and GELU. These functions are used to introduce non-linearities in neural networks, allowing them to approximate complex functions effectively.

Source: lecture02_Neural_Nets.txt","['Non linearities old and new logistic sigmoid tanh hard tanh 1 0 1 1 Swish arXiv 1710.05941 swish ùë• ùë•. logistic ùë• Rectified Linear Unit ReLU ReLU ùëß max ùëß 0 Leaky ReLU Parametric ReLU 0 0 0 GELU arXiv 1606.08415 GELU ùë• ùë•. ùëÉùëã ùë• ùëã ùëÅ 0 1 ùë•. logistic 1.702ùë•Non linearities i.e. f on previous slide Why they re needed Neural networks do function approximation e.g. regression or classification Without non linearities deep neural networks can t do anything more than a linear transform Extra layers could just be compiled down into a single linear transform W1 W2 x Wx But with more layers that include non linearities they can approximate any complex function 5Training with cross entropy loss you use this in', 'This is the prowess of neural networks. Fun Fact Neural networks are biologically in spired classiÔ¨Åers which is why they are often called artiÔ¨Åcial neural networks to distinguish them from the organic kind. However in reality human neural networks are so much more capable and complex from artiÔ¨Åcial neural net works that it is usually better to not draw too many parallels between the two. We established in our previous discussions the need for non linear classiÔ¨Åers since most data are not linearly separable and thus our classiÔ¨Åcation performance on them is limited. Neural networks are a family of classiÔ¨Åers with non linear decision boundary as seen in Figure 1. Now that we know the sort of decision boundaries neural networks create let us see how they manage doing so. 1.1 A Neuron A neuron is a generic computational unit that takes n inputs and produces a single output. What differentiates the outputs of different neurons is their parameters also referred to as their weights .', '3.5 Non linear ClassiÔ¨Åers Figure 9 Here we see that many exam ples are wrongly classiÔ¨Åed even though the best linear decision boundary is chosen. This is due linear decision boundaries have limited model capacity for this dataset. We now introduce the need for non linear classiÔ¨Åcation models such as neural networks. We see in Figure 9 that a linear classiÔ¨Åer mis classiÔ¨Åes many datapoints. Using a non linear decision boundary as shown in Figure 10 we manage to classify all training points accu rately. Although oversimpliÔ¨Åed this is a classic case demonstrating the need for non linear decision boundaries. In the next set of notes we study neural networks as a class of non linear models that have performed particularly well in deep learning applications. Figure 10 Here we see that the non linear decision boundary allows for much better classiÔ¨Åcation of datapoints.', 'One of the most popular choices for neurons is the sigmoid or binary lo gistic regression unit. This unit takes an n dimensional input vector x and produces the scalar activation output a. This neuron is also associated with an n dimensional weight vector w and a bias scalar b. The output of this neuron is then a 1 1 exp wTx b We can also combine the weights and bias term above to equiva Neuron A neuron is the fundamental building block of neural networks. We will see that a neuron can be one of many functions that allows for non linearities to accrue in the network. lently formulate a 1 1 exp wT b x 1 Figure 2 This image captures how in a sigmoid neuron the input vector x is Ô¨Årst scaled summed added to a bias unit and then passed to the squashing sigmoid function. This formulation can be visualized in the manner shown in Fig ure 2.']",text-embedding-3-small,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.990252641413291,0.6,0.959879757978364,0.037037037037037
92,What is the purpose of non-linearities in neural networks?,"The purpose of non-linearities in neural networks is to enable them to approximate any complex function. Without non-linearities, deep neural networks can only perform a linear transform, but with the inclusion of non-linearities in more layers, neural networks can approximate complex functions effectively.","['Non linearities old and new logistic sigmoid tanh hard tanh 1 0 1 1 Swish arXiv 1710.05941 swish ùë• ùë•. logistic ùë• Rectified Linear Unit ReLU ReLU ùëß max ùëß 0 Leaky ReLU Parametric ReLU 0 0 0 GELU arXiv 1606.08415 GELU ùë• ùë•. ùëÉùëã ùë• ùëã ùëÅ 0 1 ùë•. logistic 1.702ùë•Non linearities i.e. f on previous slide Why they re needed Neural networks do function approximation e.g. regression or classification Without non linearities deep neural networks can t do anything more than a linear transform Extra layers could just be compiled down into a single linear transform W1 W2 x Wx But with more layers that include non linearities they can approximate any complex function 5Training with cross entropy loss you use this in', 'This is the prowess of neural networks. Fun Fact Neural networks are biologically in spired classiÔ¨Åers which is why they are often called artiÔ¨Åcial neural networks to distinguish them from the organic kind. However in reality human neural networks are so much more capable and complex from artiÔ¨Åcial neural net works that it is usually better to not draw too many parallels between the two. We established in our previous discussions the need for non linear classiÔ¨Åers since most data are not linearly separable and thus our classiÔ¨Åcation performance on them is limited. Neural networks are a family of classiÔ¨Åers with non linear decision boundary as seen in Figure 1. Now that we know the sort of decision boundaries neural networks create let us see how they manage doing so. 1.1 A Neuron A neuron is a generic computational unit that takes n inputs and produces a single output. What differentiates the outputs of different neurons is their parameters also referred to as their weights .', 'SpeciÔ¨Åcally it introduces a special set of units called LSTM units which are linear and have a recurrent connection to itself which is Ô¨Åxed to 1. The Ô¨Çow of information into the unit and from the unit is guarded by an input and output gates their behaviour is learned . There are several variations of this basic structure. This solution does not address explicitly the exploding gradients problem. Sutskever et al. 2011 use the Hessian Free opti mizer in conjunction with structural damping a spe ciÔ¨Åc damping strategy of the Hessian. This approach seems to deal very well with the vanishing gradient though more detailed analysis is still missing. Pre sumably this method works because in high dimen sional spaces there is a high probability for long term components to be orthogonal to short term ones. This would allow the Hessian to rescale these components independently. In practice one can not guarantee that this property holds.', '2 Background A feed forward neural network is a non linear mapping from a input pattern x to an output vector y. Consider the lth hidden layer in a deep feed forward neural network and let al be the vector representation of the summed inputs to the neurons in that layer. The summed inputs are computed through a linear projection with the weight matrix W l and the bottom up inputs hl given as follows al i wl i hl hl 1 i f al i bl i 1 where f is an element wise non linear function and wl i is the incoming weights to the ith hidden units and bl i is the scalar bias parameter. The parameters in the neural network are learnt using gradient based optimization algorithms with the gradients being computed by back propagation. One of the challenges of deep learning is that the gradients with respect to the weights in one layer are highly dependent on the outputs of the neurons in the previous layer especially if these outputs change in a highly correlated way. Batch normalization Ioffe and Szegedy 2015 was proposed to reduce such undesirable covariate shift .']",text-embedding-3-small,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.999997702681424,1.0,0.9391147376468416,0.037037037037037
93,What is cross entropy loss used for in PyTorch?,"Cross entropy loss is used in PyTorch to minimize the negative log probability of the correct class, which is equivalent to maximizing the probability of the correct class. It involves comparing the true probability distribution (p) with the computed model probability distribution (q) to calculate the cross entropy. This loss function is commonly used in PyTorch for tasks such as classification.","['with cross entropy loss you use this in PyTorch Until now our objective was stated as to maximize the probability of the correct class y or equivalently we can minimize the negative log probability of that class Now restated in terms of cross entropy a concept from information theory Let the true probability distribution be p let our computed model probability be q The cross entropy is Assuming a ground truth or true or gold or target probability distribution that is 1 at the right class and 0 everywhere else p 0 0 1 0 0 then Because of one hot p the only term left is the negative log probability of the true class yi log ùëù ùë¶i ùë•i Cross entropy can be used in other ways with a more interesting p but for now just know that you ll want to use it as the loss in PyTorch 6Remember Stochastic Gradient Descent Update equation i.e. for each parameter In deep learning ùúÉincludes the data representation e.g.', 'Now we are ready to introduce cross entropy. The cross entropy is useful when cross entropy we don t know the actual probability distribution p that generated some data. It allows us to use some m which is a model of p i.e. an approximation to p . The cross entropy of m on p is deÔ¨Åned by H p m lim n 1 n X W L p w1 ... wn logm w1 ... wn 3.50 That is we draw sequences according to the probability distribution p but sum the log of their probabilities according to m. Again following the Shannon McMillan Breiman theorem for a stationary er godic process H p m lim n 1 n logm w1w2 ...wn 3.51 This means that as for entropy we can estimate the cross entropy of a model m on some distribution p by taking a single sequence that is long enough instead of summing over all possible sequences. What makes the cross entropy useful is that the cross entropy H p m is an up per bound on the entropy H p .', 'For any model m H p H p m 3.52 This means that we can use some simpliÔ¨Åed model m to help estimate the true en tropy of a sequence of symbols drawn according to probability p. The more accurate m is the closer the cross entropy H p m will be to the true entropy H p . Thus the difference between H p m and H p is a measure of how accurate a model is. Between two models m1 and m2 the more accurate model will be the one with the lower cross entropy. The cross entropy can never be lower than the true entropy so a model cannot err by underestimating the true entropy. We are Ô¨Ånally ready to see the relation between perplexity and cross entropy as we saw it in Eq. 3.51. Cross entropy is deÔ¨Åned in the limit as the length of the observed word sequence goes to inÔ¨Ånity. We will need an approximation to cross entropy relying on a sufÔ¨Åciently long sequence of Ô¨Åxed length.', 'Non linearities old and new logistic sigmoid tanh hard tanh 1 0 1 1 Swish arXiv 1710.05941 swish ùë• ùë•. logistic ùë• Rectified Linear Unit ReLU ReLU ùëß max ùëß 0 Leaky ReLU Parametric ReLU 0 0 0 GELU arXiv 1606.08415 GELU ùë• ùë•. ùëÉùëã ùë• ùëã ùëÅ 0 1 ùë•. logistic 1.702ùë•Non linearities i.e. f on previous slide Why they re needed Neural networks do function approximation e.g. regression or classification Without non linearities deep neural networks can t do anything more than a linear transform Extra layers could just be compiled down into a single linear transform W1 W2 x Wx But with more layers that include non linearities they can approximate any complex function 5Training with cross entropy loss you use this in']",text-embedding-3-small,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999999999999988,0.75,0.967789830267552,0.5
94,What is the significance of the backpropagation algorithm?,"The significance of the backpropagation algorithm lies in its ability to efficiently calculate loss gradients for parameters in a neural network by utilizing the chain rule of differentiation. This allows for the optimization of the network's parameters through techniques like gradient descent, ultimately leading to improved performance by enabling the network to learn more meaningful information from the data, reduce overfitting, and achieve higher overall task performance. Additionally, backpropagation facilitates error propagation through the network, enabling adjustments to be made based on the calculated gradients, which is crucial for training deep learning models effectively.

Source: Lecture02_Notes_Backpropagation","['The result is that the network typically learns more meaningful informa tion from the data is less likely to overÔ¨Åt and usually obtains higher performance overall on the task at hand. One intuitive reason why this technique should be so effective is that what dropout is doing is essentially doing is training exponentially many smaller networks at once and averaging over their predictions. Dropout applied to an artiÔ¨Åcial neural network. Image credits to Srivastava et al. In practice the way we introduce dropout is that we take the out put h of each layer of neurons and keep each neuron with prob ability p and else set it to 0. Then during back propagation we only pass gradients through neurons that were kept alive during the forward pass. Finally during testing we compute the forward pass using all of the neurons in the network.', 'No parameter updates are necessary if the cost is 0. Since we typically update parameters using gradient descent or a variant such as SGD we typically need the gradient information for any parameter as required in the update equation Œ∏ t 1 Œ∏ t Œ± Œ∏ t J Backpropagation is technique that allows us to use the chain rule of differentiation to calculate loss gradients for any parameter used in the feed forward computation on the model. To understand this further let us understand the toy network shown in Figure 5 for which we will perform backpropagation. Figure 5 This is a 4 2 1 neural network where neuron j on layer k receives input z k j and produces activation output a k j . Here we use a neural network with a single hidden layer and a single unit output. Let us establish some notation that will make it easier to generalize this model later xi is an input to the neural network. s is the output of the neural network. Each layer including the input and output layers has neurons which receive an input and produce an output.', 'The intuition is that to get a better approximation of the derivative slope around a point we need to examine the func tion f s behavior both to the left and right of that point. It can also be shown using Taylor s theorem that the centered difference formula has an error proportional to œµ2 which is quite small whereas the derivative deÔ¨Ånition is more error prone. Now a natural question you might ask is if this method is so pre cise why do we not use it to compute all of our network gradients instead of applying back propagation The simple answer as hinted earlier is inefÔ¨Åciency recall that every time we want to compute the gradient with respect to an element we need to make two forward passes through the network which will be computationally expen sive. Furthermore many large scale neural networks can contain millions of parameters and computing two passes per parameter is clearly not optimal. And since in optimization techniques such as SGD we must compute the gradients once per iteration for several thousands of iterations it is obvious that this method quickly grows intractable.', 'essentially the error propagating backwards from the i th neu ron in layer 2. a 1 j is an input fed to i th neuron in layer 2 when scaled by Wij. Figure 6 This subnetwork shows the relevant parts of the network required to update W 1 ij Let us discuss the error sharing distribution interpretation of backpropagation better using Figure 6 as an example. Say we were to update W 1 14 1. We start with the an error signal of 1 propagating backwards from a 3 1 . 2. We then multiply this error by the local gradient of the neuron which maps z 3 1 to a 3 1 . This happens to be 1 in this case and thus the error is still 1. This is now known as Œ¥ 3 1 1. 3. At this point the error signal of 1 has reached z 3 1 .']",text-embedding-3-small,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.000000000000001,1.0,0.9409748510212887,0.0344827586206896
95,What does the Jacobian matrix represent in the context of neural networks?,"The Jacobian matrix in the context of neural networks represents a matrix of partial derivatives, where it is an m x n matrix of partial derivatives for a function with m outputs and n inputs. It generalizes the concept of the gradient for functions with multiple inputs and outputs, providing information about how each output changes with respect to each input. It is crucial for understanding the sensitivity of the network's output to changes in its parameters or inputs. 

Source: Lecture02_Neural_Nets","['So its Jacobian will be n m. Note that zi m X k 1 Wikxk So an entry z x ij of the Jacobian will be z x ij zi xj xj m X k 1 Wikxk m X k 1 Wik xj xk Wij because xj xk 1 if k j and 0 if otherwise. So we see that z x W 2 Row vector times matrix with respect to the row vector z xW what is z x A computation similar to 1 shows that z x W T . 3 A vector with itself z x what is z x We have zi xi. So z x ij zi xj xj xi 1 if i j 0 if otherwise So we see that the Jacobian z x is a diagonal matrix where the entry at i i is 1. This is just the identity matrix z x I . When applying the chain 2rule this term will disappear because a matrix or vector multiplied by the identity matrix does not change.', '21Other Jacobians Compute these at home for practice 22Back to our Neural Net x xmuseums xin xParis xare xamazing 23Back to our Neural Net Let s find Really we care about the gradient of the loss Jt but we will compute the gradient of the score for simplicity x xmuseums xin xParis xare xamazing 241. Break up equations into simple pieces Carefully define your variables and keep track of their dimensionality 252. Apply the chain rule 262. Apply the chain rule 272. Apply the chain rule 282. Apply the chain rule 293. Write out the Jacobians Useful Jacobians from previous slide 303. Write out the Jacobians ùíñùëá Useful Jacobians from previous slide 313. Write out the Jacobians ùíñùëá Useful Jacobians from previous slide 323. Write out the Jacobians ùíñùëá Useful Jacobians from previous slide 333.', '013 64.48Gradients Given a function with 1 output and n inputs Its gradient is a vector of partial derivatives with respect to each input 11Jacobian Matrix Generalization of the Gradient Given a function with m outputs and n inputs It s Jacobian is an m x n matrix of partial derivatives 12Chain Rule For composition of one variable functions multiply derivatives For multiple variables functions multiply Jacobians 13Example Jacobian Elementwise activation Function 14Example Jacobian Elementwise activation Function Function has n outputs and n inputs n by n Jacobian 15Example Jacobian Elementwise activation Function 16Example Jacobian Elementwise activation Function 17Example Jacobian Elementwise activation Function 18Other Jacobians Compute these at home for practice Check your answers with the lecture notes 19Other Jacobians Compute these at home for practice Check your answers with the lecture notes 20Other Jacobians Compute these at home for practice Check your answers with the lecture notes Fine print This is the correct Jacobian. Later we discuss the shape convention using it the answer would be h.', 'Write out the Jacobians ùíñùëá ùíñùëá Useful Jacobians from previous slide . 34 Hadamard product element wise multiplication of 2 vectors to give vectorRe using Computation Suppose we now want to compute Using the chain rule again 35Re using Computation Suppose we now want to compute Using the chain rule again The same Let s avoid duplicated computation 36Re using Computation Suppose we now want to compute Using the chain rule again ùõøis the upstream gradient error signal 37 ùíñùëáDerivative with respect to Matrix Output shape What does look like 1 output nm inputs 1 by nm Jacobian Inconvenient to then do 38Derivative with respect to Matrix Output shape What does look like 1 output nm inputs 1 by nm Jacobian Inconvenient to then do Instead we leave pure math and use the shape convention the shape of the gradient is the shape of the parameters So is n by m 39Derivative with respect to Matrix What is is going to be in our answer The other term should']",text-embedding-3-small,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999999999999988,1.0,0.937541251674308,0.0476190476190476
96,How is the derivative with respect to a matrix described?,The derivative with respect to a matrix is described using the Jacobian matrix. The Jacobian matrix is useful because we can apply the chain rule to a vector-valued function by multiplying Jacobians. It helps in computing the gradients efficiently for matrix operations.,"['. f1 xn . . . ... . . . fm x1 . . . fm xn That is f x ij fi xj which is just a standard non vector derivative . The Jacobian matrix will be useful for us because we can apply the chain rule to a vector valued function just by multiplying Jacobians. As a little illustration of this suppose we have a function f x f1 x f2 x taking a scalar to a vector of size 2 and a function g y g1 y1 y2 g2 y1 y2 taking a vector of size two to a vector of size two. Now let s compose them to get g x g1 f1 x f2 x g2 f1 x f2 x .', '4 An elementwise function applied a vector z f x what is z x Since f is being applied elementwise we have zi f xi . So z x ij zi xj xj f xi f xi if i j 0 if otherwise So we see that the Jacobian z x is a diagonal matrix where the entry at i i is the derivative of f applied to xi. We can write this as z x diag f x . Since multiplication by a diagonal matrix is the same as doing elementwise multiplication by the diagonal we could also write f x when applying the chain rule. 5 Matrix times column vector with respect to the matrix z W x Œ¥ J z what is J W J z z W Œ¥ z W This is a bit more complicated than the other identities. The reason for in cluding J z in the above problem formulation will become clear in a moment.', 'First suppose we have a loss function J a scalar and are computing its gradient with respect to a matrix W Rn m. Then we could think of J as a function of W taking nm inputs the entries of W to a single output J . This means the Jacobian J W would be a 1 nm vector. But in practice this is not a very useful way of arranging the gradient. It would be much nicer if the derivatives were in a n m matrix like this J W J W11 . . . J W1m . . . ... . . . J Wn1 . . . J Wnm Since this matrix has the same shape as W we could just subtract it times the learning rate from W when doing gradient descent. So in a slight abuse of notation let s Ô¨Ånd this matrix as J W instead. This way of arranging the gradients becomes complicated when computing z W . Unlike J z is a vector.', 'Using the regular chain rule we can compute the derivative of g as the Jacobian g x xg1 f1 x f2 x xg2 f1 x f2 x g1 f1 f1 x g1 f2 f2 x g2 f1 f1 x g2 f2 f2 x 1And we see this is the same as multiplying the two Jacobians g x g f f x g1 f1 g1 f2 g2 f1 g2 f2 f1 x f2 x 3 Useful Identities This section will now go over how to compute the Jacobian for several simple functions. It will provide some useful identities you can apply when taking neu ral network gradients. 1 Matrix times column vector with respect to the column vector z W x what is z x Suppose W Rn m. Then we can think of z as a function of x taking an m dimensional vector to an n dimensional vector.']",text-embedding-3-small,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.8877782074100343,1.0,0.8892445262902104,0.0434782608695652
97,What role does automatic differentiation play in modern deep learning frameworks?,"Automatic differentiation plays a crucial role in modern deep learning frameworks as it allows for efficient computation of gradients for training neural networks. By automatically computing the derivatives of the loss function with respect to the model parameters, automatic differentiation simplifies the process of optimizing neural networks through techniques like backpropagation. This capability is essential for training complex deep learning models with numerous layers and parameters effectively.

Source: lecture02_Neural_Nets.txt","['Non differentiable points arise because we use a hard transfer function 5 and because we use a max layer 7 in the sentence approach network. Fortunately stochastic gradient still converges to a meaningful local minimum despite such minor differentiability problems Bottou 1991 1998 . Stochastic gradient iterations that hit a non differentiability are simply skipped. Remark 6 Modular Approach The well known back propagation algorithm LeCun 1985 Rumelhart et al. 1986 computes gradients using the chain rule. The chain rule can also be used in a modular implementation.11 Our modules correspond to the boxes in Figure 1 and Figure 2. Given derivatives with respect to its outputs each module can independently compute derivatives with respect to its inputs and with respect to its trainable parameters as proposed by Bottou and Gallinari 1991 . This allows us to easily build variants of our networks. For details about gradient computations see Appendix A. Remark 7 Tricks Many tricks have been reported for training neural networks LeCun et al.', 'everything doing this everywhere was the key test Now much less needed you can use it to check layers are correctly implemented 78Summary We ve mastered the core technology of neural nets Backpropagation recursively and hence efficiently apply the chain rule along computation graph downstream gradient upstream gradient x local gradient Forward pass compute results of operations and save intermediate values Backward pass apply chain rule to compute gradients 79Why learn all these details about gradients 80 Modern deep learning frameworks compute gradients for you But why take a class on compilers or systems when they are implemented for you Understanding what is going on under the hood is useful Backpropagation doesn t always work perfectly out of the box Understanding why is crucial for debugging and improving models See Karpathy article https medium.com @karpathy yes you should understand backprop e2f06eab496b Example in future lecture exploding and vanishing gradients', '2507COLLOBERT WESTON BOTTOU KARLEN KAVUKCUOGLU AND KUKSA 3.4.3 STOCHASTIC GRADIENT Maximizing 8 with stochastic gradient Bottou 1991 is achieved by iteratively selecting a random example x y and making a gradient step Œ∏ Œ∏ Œª log p y x Œ∏ Œ∏ 16 where Œª is a chosen learning rate. Our neural networks described in Figure 1 and Figure 2 are a succession of layers that correspond to successive composition of functions. The neural network is Ô¨Ånally composed with the word level log likelihood 11 or successively composed in the re cursion 14 if using the sentence level log likelihood 13 . Thus an analytical formulation of the derivative 16 can be computed by applying the differentiation chain rule through the network and through the word level log likelihood 11 or through the recurrence 14 . Remark 5 Differentiability Our cost functions are differentiable almost everywhere.', 'Another approach is to allow the learning rate to decrease over time such that Œ± t Œ±0œÑ max t œÑ In the above scheme Œ±0 is a tunable parameter and represents the starting learning rate. œÑ is also a tunable parameter and represents the time at which the learning rate should start reducing. In practice this method has been found to work quite well. In the next section we discuss another method for adaptive gradient descent which does not require hand set learning rates. 2.8 Momentum Updates Momentum methods a variant of gradient descent inspired by the study of dynamics and motion in physics attempt to use the veloc ity of updates as a more effective update scheme. Pseudocode for momentum updates is shown below Snippet 2.2 Computes a standard momentum update on parameters x v mu v alpha grad_x x v 2.9 Adaptive Optimization Methods AdaGrad is an implementation of standard stochastic gradient de scent SGD with one key difference the learning rate can vary for each parameter.']",text-embedding-3-small,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.0000000000000002,0.5,0.9114183586390944,0.0
98,What is the purpose of manual gradient checking?,"The purpose of manual gradient checking is to verify the correctness of analytic gradients by comparing them with numerical gradients, which are much quicker to compute. This helps ensure that the gradients calculated by the model are accurate and can help in debugging the implementation of gradient calculations.","['This inefÔ¨Åciency is why we only use gradient check to verify the correctness of our analytic gradients which are much quicker to compute. A standard implementation of gradient check is shown below Snippet 2.1 def eval_numerical_gradient f x a naive implementation of numerical gradient of f at x f should be a function that takes a single argument x is the point numpy array to evaluate the gradient at fx f x evaluate function value at original point grad np.zeros x.shape h 0.00001 iterate over all indexes in x it np.nditer x flags multi_index op_flags readwrite while not it.finished evaluate function at x h ix it.multi_index old_value x ix x ix old_value h increment by h fxh_left f x evaluate f x h x ix old_value h decrement by h fxh_right f x evaluate f x h x ix old_value restore to previous value very important compute the partial derivative grad ix fxh_left fxh_right 2 h the slope it.iternext step to next dimension return grad 2.', 'We can compute them as follows Œ¥1 J Œ∏ ÀÜ y y T this is just identity 7 Œ¥2 J z J Œ∏ Œ∏ h h z using the chain rule Œ¥1 Œ∏ h h z substituting in Œ¥1 Œ¥1 U h z using identity 1 Œ¥1 U ReLU z using identity 4 Œ¥1 U sgn h we computed this earlier A good way of checking our work is by looking at the dimensions of the Jaco bians J z Œ¥1 U sgn h 1 Dh 1 Nc Nc Dh Dh 6We see that the dimensions of all the terms in the gradient match up i.e. the number of columns in a term equals the number of rows in the next term . This will always be the case if we computed our gradients correctly. Now we can use the error terms to compute our gradients. Note that we trans pose out answers when computing the gradients for column vectors terms to follow the shape convention.', 'Briefly the gradient think derivative U f of a scalar function f with respect to a parameter matrix U represents the direction to locally move U in in order to maximally increase the value of f. So in practice we do something like drawing the initial U 0 and V 0 randomly as U V N 0 0.001 V d matrices of independent draws from a zero centered normal distribution with small variance and then perform some number of iterations of the following process U i 1 U i Œ± UL U i V i . 7 This should be read as setting the value of U at iteration i 1 as the value of U at the previous iteration plus a small Œ± small step in the direction that locally best improves U with respect to the objective L U V we specified in Eqn 6. Stochastic gradients. There s a crucial detail remaining here beyond how to compute the gradient function U which we ll discuss later computing L U V is exceptionally expensive as it walks over the entire dataset.', 'Some examples if you want practice dot product of two vectors elementwise product of two vectors 2 norm of a vector. Feel free to use these identities in the assignments. One option is just to memorize them. Another option is to Ô¨Ågure them out by looking at the dimensions. For example only one ordering orientation of Œ¥ and x will produce the correct shape for J W assuming W is not square . 4 Gradient Layout Jacobean formulation is great for applying the chain rule you just have to mul tiply the Jacobians. However when doing SGD it s more convenient to follow the convention the shape of the gradient equals the shape of the parameter as we did when computing J W . That way subtracting the gradient times the learning rate from the parameters is easy. We expect answers to homework questions to follow this convention. Therefore if you compute the gradient of a column vector using Jacobian formulation you should take the transpose when reporting your Ô¨Ånal answer so the gradient is a column vector. Another option is to always follow the convention.']",text-embedding-3-small,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999401816589696,0.8,0.9369455067587223,0.0454545454545454
99,Why is understanding gradients important despite automatic differentiation?,"Understanding gradients is important despite automatic differentiation because it allows for a deeper insight into how neural networks are trained and optimized. It helps in debugging and improving models when automatic methods may not work perfectly out of the box. Additionally, understanding gradients is crucial for knowing why certain optimization issues like exploding and vanishing gradients may occur. Overall, having a solid understanding of gradients provides a strong foundation for effectively working with neural networks.

Source: Lecture02_Neural_Nets","['everything doing this everywhere was the key test Now much less needed you can use it to check layers are correctly implemented 78Summary We ve mastered the core technology of neural nets Backpropagation recursively and hence efficiently apply the chain rule along computation graph downstream gradient upstream gradient x local gradient Forward pass compute results of operations and save intermediate values Backward pass apply chain rule to compute gradients 79Why learn all these details about gradients 80 Modern deep learning frameworks compute gradients for you But why take a class on compilers or systems when they are implemented for you Understanding what is going on under the hood is useful Backpropagation doesn t always work perfectly out of the box Understanding why is crucial for debugging and improving models See Karpathy article https medium.com @karpathy yes you should understand backprop e2f06eab496b Example in future lecture exploding and vanishing gradients', 'Non differentiable points arise because we use a hard transfer function 5 and because we use a max layer 7 in the sentence approach network. Fortunately stochastic gradient still converges to a meaningful local minimum despite such minor differentiability problems Bottou 1991 1998 . Stochastic gradient iterations that hit a non differentiability are simply skipped. Remark 6 Modular Approach The well known back propagation algorithm LeCun 1985 Rumelhart et al. 1986 computes gradients using the chain rule. The chain rule can also be used in a modular implementation.11 Our modules correspond to the boxes in Figure 1 and Figure 2. Given derivatives with respect to its outputs each module can independently compute derivatives with respect to its inputs and with respect to its trainable parameters as proposed by Bottou and Gallinari 1991 . This allows us to easily build variants of our networks. For details about gradient computations see Appendix A. Remark 7 Tricks Many tricks have been reported for training neural networks LeCun et al.', 'Briefly the gradient think derivative U f of a scalar function f with respect to a parameter matrix U represents the direction to locally move U in in order to maximally increase the value of f. So in practice we do something like drawing the initial U 0 and V 0 randomly as U V N 0 0.001 V d matrices of independent draws from a zero centered normal distribution with small variance and then perform some number of iterations of the following process U i 1 U i Œ± UL U i V i . 7 This should be read as setting the value of U at iteration i 1 as the value of U at the previous iteration plus a small Œ± small step in the direction that locally best improves U with respect to the objective L U V we specified in Eqn 6. Stochastic gradients. There s a crucial detail remaining here beyond how to compute the gradient function U which we ll discuss later computing L U V is exceptionally expensive as it walks over the entire dataset.', 'We note that this deÔ¨Ånition of the numerical gradient follows very naturally from the deÔ¨Ånition of the derivative where in the scalar case f x f x œµ f x œµ Gradient checks are a great way to compare analytical and numerical gradients. Analytical gradients should be close and numerical gradients can be calculated using f Œ∏ J Œ∏ i J Œ∏ i 2œµ J Œ∏ i and J Œ∏ i can be evalu ated using two forward passes. An implementation of this can be seen in Snippet 2.1. Of course there is a slight difference the deÔ¨Ånition above only perturbs x in the positive direction to compute the gradient. While it would have been perfectly acceptable to deÔ¨Åne the numerical gradi ent in this way in practice it is often more precise and stable to use the centered difference formula where we perturb a parameter in both directions.']",text-embedding-3-small,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9536899962692664,0.8,0.9299096747090584,0.05
100,What is the core concept introduced in this class that leads to GPT-3 and ChatGPT?,"The core concept introduced in this class that leads to GPT-3 and ChatGPT is the idea of large language models with emergent abilities, such as zero-shot learning and in-context learning, which enable these models to perform a wide range of tasks without the need for explicit examples or gradient updates. These models, like GPT-3 and ChatGPT, have been able to achieve impressive results in natural language processing tasks due to their scale, data training, and architecture improvements.
Source: Lecture07_Prompting_Instruction_Finetuning_and_RLHF","['entailment START The man is in the doorway DELIM The person is near the door EXTRACT 14 DecoderEmergent abilities of large language models GPT 2 2019 Let s revisit the Generative Pretrained Transformer GPT models from OpenAI as an example GPT 2 1.5B parameters Radford et al. 2019 Same architecture as GPT just bigger 117M 1.5B But trained on much more data 4GB 40GB of internet text data WebText Scrape links posted on Reddit w at least 3 upvotes rough proxy of human quality GPT 2018 GPT 2 2019 117M 1.5B 15One key emergent ability in GPT 2 is zero shot learning the ability to do many tasks with no examples and no gradient updates by simply Specifying the right sequence prediction problem e.g. question answering Passage Tom Brady... Q Where was Tom Brady born A ... Comparing probabilities of sequences e.g.', 'Section 5 discusses limitations of GPT 3. Section 6 discusses broader impacts. Section 7 reviews related work and Section 8 concludes. 2 Approach Our basic pre training approach including model data and training is similar to the process described in RWC 19 with relatively straightforward scaling up of the model size dataset size and diversity and length of training. Our use of in context learning is also similar to RWC 19 but in this work we systematically explore different settings for learning within the context. Therefore we start this section by explicitly deÔ¨Åning and contrasting the different settings that we will be evaluating GPT 3 on or could in principle evaluate GPT 3 on. These settings can be seen as lying on a spectrum of how much task speciÔ¨Åc data they tend to rely on.', 'First despite the strong quantitative and qualitative improvements of GPT 3 particularly compared to its direct predecessor GPT 2 it still has notable weaknesses in text synthesis and several NLP tasks. On text synthesis although the overall quality is high GPT 3 samples still sometimes repeat themselves semantically at the document level start to lose coherence over sufÔ¨Åciently long passages contradict themselves and occasionally contain non sequitur sentences or paragraphs. We will release a collection of 500 uncurated unconditional samples to help provide a better sense of GPT 3 s limitations and strengths at text synthesis. Within the domain of discrete language tasks we have noticed informally that GPT 3 seems to have special difÔ¨Åculty with common sense physics despite doing well on some datasets such as PIQA BZB 19 that test this domain. SpeciÔ¨Åcally GPT 3 has difÔ¨Åculty with questions of the type If I put cheese into the fridge will it melt .', 'Overall in context learning with GPT 3 shows mixed results on commonsense reasoning tasks with only small and inconsistent gains observed in the one and few shot learning settings for both PIQA and ARC but a signiÔ¨Åcant improvement is observed on OpenBookQA. GPT 3 sets SOTA on the new PIQA dataset in all evaluation settings. 3.6 Reading Comprehension Next we evaluate GPT 3 on the task of reading comprehension. We use a suite of 5 datasets including abstractive multiple choice and span based answer formats in both dialog and single question settings. We observe a wide spread in GPT 3 s performance across these datasets suggestive of varying capability with different answer formats. In general we observe GPT 3 is on par with initial baselines and early results trained using contextual representations on each respective dataset.']",text-embedding-3-small,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999971491124454,0.4,0.8960184228067181,0.0
101,What is the purpose of Dropout as mentioned in the document?,"The purpose of Dropout, as mentioned in the document, is to act as a powerful technique for regularization in neural networks. During training, Dropout randomly drops a subset of neurons with a certain probability during each forward-backward pass, or keeps each neuron alive with a probability \(p\). This helps prevent overfitting by training exponentially many smaller networks at once and averaging over their predictions. During testing, the full network is used to compute predictions. Additionally, Dropout helps ensure that the expected output of a neuron during testing is approximately the same as during training to maintain well-defined network behavior. 

Source: Lecture02_Notes_Backpropagation","['The result is that the network typically learns more meaningful informa tion from the data is less likely to overÔ¨Åt and usually obtains higher performance overall on the task at hand. One intuitive reason why this technique should be so effective is that what dropout is doing is essentially doing is training exponentially many smaller networks at once and averaging over their predictions. Dropout applied to an artiÔ¨Åcial neural network. Image credits to Srivastava et al. In practice the way we introduce dropout is that we take the out put h of each layer of neurons and keep each neuron with prob ability p and else set it to 0. Then during back propagation we only pass gradients through neurons that were kept alive during the forward pass. Finally during testing we compute the forward pass using all of the neurons in the network.', 'It must be noted that the bias terms are not regularized and do not contribute to the cost term above try thinking about why this is the case There are indeed other types of regularization that are sometimes used such as L1 regularization which sums over the absolute values rather than squares of parameter elements however this is less commonly applied in practice since it leads to sparsity of parameter weights. In the next section we discuss dropout which effectively acts as another form of regularization by randomly dropping i.e. setting to zero neurons in the forward pass. 2.3 Dropout Dropout is a powerful technique for regularization Ô¨Årst introduced by Srivastava et al. in Dropout A Simple Way to Prevent Neural Net works from OverÔ¨Åtting. The idea is simple yet effective during train ing we will randomly drop with some probability 1 p a subset of neurons during each forward backward pass or equivalently we will keep alive each neuron with a probability p . Then during testing we will use the full network to compute our predictions.', 'However a key sub tlety is that in order for dropout to work effectively the expected output of a neuron during testing should be approximately the same as it was during training else the magnitude of the outputs could be radically different and the behavior of the network is no longer well deÔ¨Åned. Thus we must typically divide the outputs of each neuron during testing by a certain value it is left as an exercise to the reader to determine what this value should be in order for the expected outputs during training and testing to be equivalent. 2.4 Neuron Units So far we have discussed neural networks that contain sigmoidal neurons to introduce nonlinearities however in many applications better networks can be designed using other activation functions. Some common choices are listed here with their function and gra dient deÔ¨Ånitions and these can be substituted with the sigmoidal functions discussed above.', 'like an ensemble model Nowadays usually thought of as strong feature dependent regularizer Wager Wang Liang 2013DropoutDropout Srivastava Hinton Krizhevsky Sutskever Salakhutdinov 2012 JMLR 2014 During training For each data point each time Randomly set input to 0 with probability ùëù dropout ratio often p 0.5 except p 0.15 for input layer via dropout mask During testing Multiply all weights by 1 ùëù No other dropout ùë•2 ùë•3 0 ùë§1 ùë§2 ùë§3 ùë§4 ùëè 1 ùë¶ ùë§1ùë•1 ùë§2ùë•2 ùë§3ùë•3 ùë• ùë• 1 1 Train 1 Train 2 Test ùë•3 0 1 ùë¶ ùë§1ùë•1 ùë§3ùë•3 ùëè ùë§1']",text-embedding-3-small,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9300597327790432,1.0,0.9103728548349284,0.3529411764705882
102,What are some approaches to address the vanishing gradient problem in RNNs?,"Some approaches to address the vanishing gradient problem in RNNs include:
1. Using Long Short Term Memory RNNs (LSTMs) proposed by Hochreiter and Schmidhuber in 1997 as a solution to the vanishing gradients issue.
2. Implementing separate memory in RNNs, which is added to LSTMs.
3. Creating more direct and linear pass-through connections in the model, such as attention mechanisms and residual connections.

Source: Lecture03_RNNs_and_LLMs.txt","['Problems with RNNs Vanishing and Exploding Gradients 48Vanishing gradient intuition 49Vanishing gradient intuition chain rule 50Vanishing gradient intuition chain rule 51Vanishing gradient intuition chain rule 52Vanishing gradient intuition What happens if these are small Vanishing gradient problem When these are small the gradient signal gets smaller and smaller as it backpropagates further 53Vanishing gradient proof sketch linear case Recall What if were the identity function 53 Source On the difficulty of training recurrent neural networks Pascanu et al 2013. http proceedings.mlr.press v28 pascanu13.pdf and supplemental materials at http proceedings.mlr.press v28 pascanu13 supp.pdf If Wh is small then this term gets exponentially problematic as becomes large chain rule Consider the gradient of the loss on step with respect to the hidden state on some previous step .', 'NaN which is easily detectable at runtime this issue is called the Gradient Explosion Problem. When the gradient value goes to zero however it can go undetected while dras tically reducing the learning quality of the model for far away words in the corpus this issue is called the Vanishing Gradient Problem. Due to vanishing gradients we don t know whether there is no dependency between steps t and t n in the data or we just cannot capture the true dependency due to this issue. To gain practical intuition about the vanishing gradient problem you may visit the following example website. 2.4 Solution to the Exploding Vanishing Gradients Now that we gained intuition about the nature of the vanishing gradi ents problem and how it manifests itself in deep neural networks let us focus on a simple and practical heuristic to solve these problems. To solve the problem of exploding gradients Thomas Mikolov Ô¨Årst introduced a simple heuristic solution that clips gradients to a small number whenever they explode.', 'press v28 pascanu13.pdfHow to fix the vanishing gradient problem The main problem is that it s too difficult for the RNN to learn to preserve information over many timesteps. In a vanilla RNN the hidden state is constantly being rewritten First off next time How about an RNN with separate memory which is added to LSTMs And then Creating more direct and linear pass through connections in model Attention residual connections etc. 60Long Short Term Memory RNNs LSTMs A type of RNN proposed by Hochreiter and Schmidhuber in 1997 as a solution to the problem of vanishing gradients Everyone cites that paper but really a crucial part of the modern LSTM is from Gers et al. 2000 Only started to be recognized as promising through the work of S s student Alex Graves c.', 'Let chain rule value of ONLY READ IF INTERESTEDVanishing gradient proof sketch linear case What s wrong with Consider if the eigenvalues of are all less than 1 We can write as a basis What about nonlinear activations i.e. what we use Pretty much the same thing except the proof requires for some dependent on dimensionality and Source On the difficulty of training recurrent neural networks Pascanu et al 2013. http proceedings.mlr.press v28 pascanu13.pdf and supplemental materials at http proceedings.mlr.press v28 pascanu13 supp.pdf 54 eigenvectors using the eigenvectors of Approaches 0 as grows so gradient vanishes sufficient but not necessary ONLY READ IF INTERESTEDWhy is vanishing gradient a problem Gradient signal from far away is lost because it s much smaller than gradient signal from close by. So model weights are updated only with respect to near effects not long term effects.']",text-embedding-3-small,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.0000000000000009,0.6666666666666666,0.8710039178875624,0.0769230769230769
103,"What is perplexity, and how is it used in evaluating language models?","Perplexity is a measurement used to evaluate language models by calculating how well a probability distribution or probability model predicts a sample. It is a function of both the text and the language model, and different language models will have different perplexities. Perplexity can be used to compare different n-gram models, where a lower perplexity indicates a better model. It is commonly used as a quick check on an algorithm's performance, but improvements in perplexity should always be confirmed by an end-to-end evaluation of a real task before concluding the evaluation of the model. In essence, perplexity measures how well a language model predicts a given sample of text, with lower perplexity values indicating better predictive performance.

Source: Lecture notes on N-gram Language Models in SEP 775 - Computational Natural Language Processing.","['We mentioned above that perplexity is a function of both the text and the lan guage model given a text W different language models will have different perplex ities. Because of this perplexity can be used to compare different n gram models. Let s look at an example in which we trained unigram bigram and trigram gram mars on 38 million words including start of sentence tokens from the Wall Street Journal using a 19 979 word vocabulary. We then computed the perplexity of each of these models on a test set of 1.5 million words using Eq. 3.16 for unigrams Eq. 3.17 for bigrams and the corresponding equation for trigrams. The table below shows the perplexity of a 1.5 million word WSJ test set according to each of these grammars. Unigram Bigram Trigram Perplexity 962 170 109 As we see above the more information the n gram gives us about the word sequence the higher the probability the n gram will assign to the string.', 'Nonetheless because perplexity often correlates with such improvements it is commonly used as a quick check on an algorithm. But a model s improvement in perplexity should always be conÔ¨Årmed by an end to end evaluation of a real task before concluding the evaluation of the model. 3.3 Sampling sentences from a language model One important way to visualize what kind of knowledge a language model embodies is to sample from it. Sampling from a distribution means to choose random points sampling according to their likelihood. Thus sampling from a language model which rep resents a distribution over sentences means to generate some sentences choosing each sentence according to its likelihood as deÔ¨Åned by the model. Thus we are more likely to generate sentences that the model thinks have a high probability and less likely to generate sentences that the model thinks have a low probability. This technique of visualizing a language model by sampling was Ô¨Årst suggested very early on by Shannon 1951 and Miller and Selfridge 1950 . It s simplest to visualize how this works for the unigram case.', 'We also need to include the end of sentence marker s but not the beginning of sentence marker s in the total count of word to kens N. There is another way to think about perplexity as the weighted average branch ing factor of a language. The branching factor of a language is the number of possi ble next words that can follow any word. Consider the task of recognizing the digits3.2 EVALUATING LANGUAGE MODELS 9 in English zero one two ... nine given that both in some training set and in some test set each of the 10 digits occurs with equal probability P 1 10. The perplexity of this mini language is in fact 10. To see that imagine a test string of digits of length N and assume that in the training set all the digits occurred with equal probability. By Eq.', '2013 for the details.22 CHAPTER 3 N GRAM LANGUAGE MODELS 3.8 Advanced Perplexity s Relation to Entropy We introduced perplexity in Section 3.2.1 as a way to evaluate n gram models on a test set. A better n gram model is one that assigns a higher probability to the test data and perplexity is a normalized version of the probability of the test set. The perplexity measure actually arises from the information theoretic concept of cross entropy which explains otherwise mysterious properties of perplexity why the inverse probability for example and its relationship to entropy. Entropy is a Entropy measure of information. Given a random variable X ranging over whatever we are predicting words letters parts of speech the set of which we ll call œá and with a particular probability function call it p x the entropy of the random variable X is H X X x œá p x log2 p x 3.43 The log can in principle be computed in any base.']",text-embedding-3-small,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9078634494639308,0.8888888888888888,0.9418502370566616,0.0357142857142857
104,How does LSTM solve the vanishing gradient problem?,"LSTM (Long Short-Term Memory) units address the vanishing gradient problem by introducing a special set of units with a linear structure and a recurrent connection fixed to 1. These units have input and output gates that control the flow of information, allowing the model to learn how to retain and propagate information over long sequences. Additionally, LSTM units use a specific damping strategy called structural damping, which helps in dealing with the vanishing gradient issue by rescaling long-term and short-term components independently in high-dimensional spaces. This approach has shown effectiveness in mitigating the vanishing gradient problem in practice.","['NaN which is easily detectable at runtime this issue is called the Gradient Explosion Problem. When the gradient value goes to zero however it can go undetected while dras tically reducing the learning quality of the model for far away words in the corpus this issue is called the Vanishing Gradient Problem. Due to vanishing gradients we don t know whether there is no dependency between steps t and t n in the data or we just cannot capture the true dependency due to this issue. To gain practical intuition about the vanishing gradient problem you may visit the following example website. 2.4 Solution to the Exploding Vanishing Gradients Now that we gained intuition about the nature of the vanishing gradi ents problem and how it manifests itself in deep neural networks let us focus on a simple and practical heuristic to solve these problems. To solve the problem of exploding gradients Thomas Mikolov Ô¨Årst introduced a simple heuristic solution that clips gradients to a small number whenever they explode.', 'Problems with RNNs Vanishing and Exploding Gradients 48Vanishing gradient intuition 49Vanishing gradient intuition chain rule 50Vanishing gradient intuition chain rule 51Vanishing gradient intuition chain rule 52Vanishing gradient intuition What happens if these are small Vanishing gradient problem When these are small the gradient signal gets smaller and smaller as it backpropagates further 53Vanishing gradient proof sketch linear case Recall What if were the identity function 53 Source On the difficulty of training recurrent neural networks Pascanu et al 2013. http proceedings.mlr.press v28 pascanu13.pdf and supplemental materials at http proceedings.mlr.press v28 pascanu13 supp.pdf If Wh is small then this term gets exponentially problematic as becomes large chain rule Consider the gradient of the loss on step with respect to the hidden state on some previous step .', 'SpeciÔ¨Åcally it introduces a special set of units called LSTM units which are linear and have a recurrent connection to itself which is Ô¨Åxed to 1. The Ô¨Çow of information into the unit and from the unit is guarded by an input and output gates their behaviour is learned . There are several variations of this basic structure. This solution does not address explicitly the exploding gradients problem. Sutskever et al. 2011 use the Hessian Free opti mizer in conjunction with structural damping a spe ciÔ¨Åc damping strategy of the Hessian. This approach seems to deal very well with the vanishing gradient though more detailed analysis is still missing. Pre sumably this method works because in high dimen sional spaces there is a high probability for long term components to be orthogonal to short term ones. This would allow the Hessian to rescale these components independently. In practice one can not guarantee that this property holds.', 'press v28 pascanu13.pdfHow to fix the vanishing gradient problem The main problem is that it s too difficult for the RNN to learn to preserve information over many timesteps. In a vanilla RNN the hidden state is constantly being rewritten First off next time How about an RNN with separate memory which is added to LSTMs And then Creating more direct and linear pass through connections in model Attention residual connections etc. 60Long Short Term Memory RNNs LSTMs A type of RNN proposed by Hochreiter and Schmidhuber in 1997 as a solution to the problem of vanishing gradients Everyone cites that paper but really a crucial part of the modern LSTM is from Gers et al. 2000 Only started to be recognized as promising through the work of S s student Alex Graves c.']",text-embedding-3-small,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9612453513455184,0.6666666666666666,0.9227873175068192,0.2631578947368421
105,"What is the impact of bidirectional RNNs, and where are they applicable?","The impact of bidirectional RNNs is that they allow predictions based on future words by reading through the corpus backwards, maintaining two hidden layers for left to right and right to left propagation at each time step. However, bidirectional RNNs are only applicable if you have access to the entire input sequence. They are not applicable to Language Modeling because in LM, you only have left context available. If you do have the entire input sequence, bidirectionality is powerful and should be used by default. An example of a powerful pretrained contextual representation system built on bidirectionality is BERT (Bidirectional Encoder Representations from Transformers).","['The Ô¨Årst technique is that instead of initializing W hh ran domly start off from an identity matrix initialization. The second technique is to use the RectiÔ¨Åed Linear Units ReLU in stead of the sigmoid function. The derivative for the ReLU is either 0 or 1. This way gradients would Ô¨Çow through the neurons whose deriva tive is 1 without getting attenuated while propagating back through time steps. 2.5 Deep Bidirectional RNNs So far we have focused on RNNs that condition on past words to predict the next word in the sequence. It is possible to make predic tions based on future words by having the RNN model read through the corpus backwards. Irsoy et al. shows a bi directional deep neu ral network at each time step t this network maintains two hidden layers one for the left to right propagation and another for the right to left propagation.', 'h h h Figure 8 A bi directional RNN model h t f W xt V h t 1 b 17 h t f W xt V h t 1 b 18 ÀÜ yt g Uht c g U h t h t c 19 RNNs can also be multi layered. Figure 9 shows a multi layer bi directional RNN where each lower layer feeds the next layer. As shown in this Ô¨Ågure in this network architecture at time step t each interme diate neuron receives one set of parameters from the previous time step in the same RNN layer and two sets of parameters from the previous RNN hidden layer one input comes from the left to right RNN and the other from the right to left RNN.', 'Answer German 76 lots of neural architecture lots of neural architectureRNN LMs can be used to generate text e.g. speech recognition machine translation summarization RNN LM what s the the weather what s This is an example of a conditional language model. We ll see Machine Translation in much more detail next lectures. Input audio START conditioning 77Bidirectional RNNs was terribly exciting the movie Forward RNN Backward RNN Concatenated hidden states This contextual representation of terribly has both left and right context 78Bidirectional RNNs 79 Note bidirectional RNNs are only applicable if you have access to the entire input sequence They are not applicable to Language Modeling because in LM you only have left context available. If you do have entire input sequence e.g. any kind of encoding bidirectionality is powerful you should use it by default . For example BERT Bidirectional Encoder Representations from Transformers is a powerful pretrained contextual representation system built on bidirectionality.', 'In contrast to these models BIDAF does not summarize the two modalities in the attention layer and instead lets the attention vectors Ô¨Çow into the modeling RNN layer. The third group considered as variants of Memory Network Weston et al. 2015 repeats comput ing an attention vector between the query and the context through multiple layers typically referred to as multi hop Sordoni et al. 2016 Dhingra et al. 2016 . Shen et al. 2016 combine Memory Networks with Reinforcement Learning in order to dynamically control the number of hops. One can also extend our BIDAF model to incorporate multiple hops. Visual question answering. The task of question answering has also gained a lot of interest in the computer vision community. Early works on visual question answering VQA involved encoding the question using an RNN encoding the image using a CNN and combining them to answer the question Antol et al. 2015 Malinowski et al. 2015 .']",text-embedding-3-small,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9668585653326168,1.0,0.9374486112365154,0.0454545454545454
106,Describe the concept of multi-layer or stacked RNNs and their benefit.,"Multi-layer or stacked RNNs involve applying multiple RNNs on top of each other, allowing the network to compute more complex representations. The lower RNN layers compute lower-level features, while the higher RNN layers compute higher-level features. This architecture helps in capturing intricate patterns and dependencies in the data, leading to improved performance compared to using just a single layer of RNN. Additionally, in practice, high-performing RNNs are often multi-layered, with 2 to 4 layers being optimal for tasks like Neural Machine Translation. Skip connections or dense connections are typically used to facilitate training deeper RNNs effectively. Transformer-based networks like BERT, on the other hand, are usually deeper, with around 12 to 24 layers for enhanced performance.

Source: Lecture03_RNNs_and_LLMs, Understanding_LSTM_Networks","['You will learn more about transformers including BERT in a couple of weeksMulti layer RNNs RNNs are already deep on one dimension they unroll over many timesteps We can also make them deep in another dimension by applying multiple RNNs this is a multi layer RNN. This allows the network to compute more complex representations The lower RNNs should compute lower level features and the higher RNNs should compute higher level features. Multi layer RNNs are also called stacked RNNs. 80Multi layer RNNs was terribly exciting the movie RNN layer 1 RNN layer 2 81 RNN layer 3 The hidden states from RNN layer i are the inputs to RNN layer i 1Multi layer RNNs in practice 39 Multi layer or stacked RNNs allow a network to compute more complex representations they work better than just have one layer of high dimensional encodings The lower RNNs should compute lower level features and the higher RNNs should compute higher level features.', 'High performing RNNs are usually multi layer but aren t as deep as convolutional or feed forward networks For example In a 2017 paper Britz et al. find that for Neural Machine Translation 2 to 4 layers is best for the encoder RNN and 4 layers is best for the decoder RNN Often 2 layers is a lot better than 1 and 3 might be a little better than 2 Usually skip connections dense connections are needed to train deeper RNNs e.g. 8 layers Transformer based networks e.g. BERT are usually deeper like 12 or 24 layers.', 'h h h Figure 8 A bi directional RNN model h t f W xt V h t 1 b 17 h t f W xt V h t 1 b 18 ÀÜ yt g Uht c g U h t h t c 19 RNNs can also be multi layered. Figure 9 shows a multi layer bi directional RNN where each lower layer feeds the next layer. As shown in this Ô¨Ågure in this network architecture at time step t each interme diate neuron receives one set of parameters from the previous time step in the same RNN layer and two sets of parameters from the previous RNN hidden layer one input comes from the left to right RNN and the other from the right to left RNN.', 'I ll leave discussion of the amazing feats one can achieve with RNNs to Andrej Karpathy s excellent blog post The Unreasonable Effectiveness of A xt ht1 24 24 3 23 PM Understanding LSTM Networks colah s blog 2 7 Recurrent Neural Networks http karpathy.github.io 2015 05 21 rnn effectiveness . But they really are pretty amazing. Essential to these successes is the use of LSTMs a very special kind of recurrent neural network which works for many tasks much much better than the standard version. Almost all exciting results based on recurrent neural networks are achieved with them. It s these LSTMs that this essay will explore. The Problem of Long Term Dependencies One of the appeals of RNNs is the idea that they might be able to connect previous information to the present task such as using previous video frames might inform the understanding of the present frame. If RNNs could do this they d be extremely useful. But can they It depends.']",text-embedding-3-small,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9407347153301412,1.0,0.9461702133188185,0.0625
107,How does Dropout work during training and testing?,"During training, Dropout works by randomly dropping a subset of neurons with a certain probability (1-p) during each forward and backward pass. This means that each neuron has a probability p of being kept alive. During testing, all neurons are used in the network to compute predictions without any dropout applied.

Source: SEP775_NLP lecture02_notes_Backpropagation","['The result is that the network typically learns more meaningful informa tion from the data is less likely to overÔ¨Åt and usually obtains higher performance overall on the task at hand. One intuitive reason why this technique should be so effective is that what dropout is doing is essentially doing is training exponentially many smaller networks at once and averaging over their predictions. Dropout applied to an artiÔ¨Åcial neural network. Image credits to Srivastava et al. In practice the way we introduce dropout is that we take the out put h of each layer of neurons and keep each neuron with prob ability p and else set it to 0. Then during back propagation we only pass gradients through neurons that were kept alive during the forward pass. Finally during testing we compute the forward pass using all of the neurons in the network.', 'However a key sub tlety is that in order for dropout to work effectively the expected output of a neuron during testing should be approximately the same as it was during training else the magnitude of the outputs could be radically different and the behavior of the network is no longer well deÔ¨Åned. Thus we must typically divide the outputs of each neuron during testing by a certain value it is left as an exercise to the reader to determine what this value should be in order for the expected outputs during training and testing to be equivalent. 2.4 Neuron Units So far we have discussed neural networks that contain sigmoidal neurons to introduce nonlinearities however in many applications better networks can be designed using other activation functions. Some common choices are listed here with their function and gra dient deÔ¨Ånitions and these can be substituted with the sigmoidal functions discussed above.', 'like an ensemble model Nowadays usually thought of as strong feature dependent regularizer Wager Wang Liang 2013DropoutDropout Srivastava Hinton Krizhevsky Sutskever Salakhutdinov 2012 JMLR 2014 During training For each data point each time Randomly set input to 0 with probability ùëù dropout ratio often p 0.5 except p 0.15 for input layer via dropout mask During testing Multiply all weights by 1 ùëù No other dropout ùë•2 ùë•3 0 ùë§1 ùë§2 ùë§3 ùë§4 ùëè 1 ùë¶ ùë§1ùë•1 ùë§2ùë•2 ùë§3ùë•3 ùë• ùë• 1 1 Train 1 Train 2 Test ùë•3 0 1 ùë¶ ùë§1ùë•1 ùë§3ùë•3 ùëè ùë§1', 'It must be noted that the bias terms are not regularized and do not contribute to the cost term above try thinking about why this is the case There are indeed other types of regularization that are sometimes used such as L1 regularization which sums over the absolute values rather than squares of parameter elements however this is less commonly applied in practice since it leads to sparsity of parameter weights. In the next section we discuss dropout which effectively acts as another form of regularization by randomly dropping i.e. setting to zero neurons in the forward pass. 2.3 Dropout Dropout is a powerful technique for regularization Ô¨Årst introduced by Srivastava et al. in Dropout A Simple Way to Prevent Neural Net works from OverÔ¨Åtting. The idea is simple yet effective during train ing we will randomly drop with some probability 1 p a subset of neurons during each forward backward pass or equivalently we will keep alive each neuron with a probability p . Then during testing we will use the full network to compute our predictions.']",text-embedding-3-small,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9935167630262984,1.0,0.9135938078978126,0.0588235294117647
108,What is the role of parameter initialization in neural networks?,"The role of parameter initialization in neural networks is crucial for achieving optimal performance during training. Proper initialization of weights and biases helps to prevent issues such as vanishing or exploding gradients, which can hinder the learning process. By initializing parameters in a reasonable way, such as with small random numbers around 0 or using specific initialization schemes, neural networks can converge faster, achieve lower error rates, and maintain activation and gradient variances across layers. This initialization step sets the foundation for effective training and can significantly impact the overall performance of the neural network.

Source: lecture02_notes_Backpropagation.txt","['The empirical Ô¨Åndings suggest that for sigmoid and tanh activation units faster convergence and lower error rates are achieved when the weights of a matrix W Rn l 1 n l are initialized randomly with a uniform distribution as follows W U s 6 n l n l 1 s 6 n l n l 1 Where n l is the number of input units to W fan in and n l 1 is the number of output units from W fan out . In this parameter initialization scheme bias units are initialized to 0. This approach attempts to maintain activation variances as well as backpropagated gradient variances across layers. Without such initialization the gradient variances which are a proxy for information generally decrease with backpropagation across layers. 2.7 Learning Strategies The rate magnitude of model parameter updates during training can be controlled using the learning rate.', 'We should also note that such initialization does not prevent cross ing the boundary between basins of attraction which as shown could happen even though no bifurcation boundary is crossed. Teacher forcing is a more interesting yet a not very well understood solution. It can be seen as a way of initializing the model in the right regime and the rightOn the diÔ¨Éculty of training Recurrent Neural Networks region of space. It has been shown that in practice it can reduce the chance that gradients explode and even allow training generator models or models that work with unbounded amounts of memory Pascanu and Jaeger 2011 Doya and Yoshizawa 1991 . One important downside is that it requires a target to be deÔ¨Åned at every time step. In Hochreiter and Schmidhuber 1997 Graves et al. 2009 a solution is proposed for the vanishing gra dients problem where the structure of the model is changed.', 'We can then take the Singular Value Decomposition SVD of X to get matrices U S V. We then compute UX to project X into the basis deÔ¨Åned by the columns of U. We Ô¨Ånally divide each dimension of the result by the corresponding singular value in S to scale our data appropriately if a singular value is zero we can just divide by a small number instead . 2.6 Parameter Initialization A key step towards achieving superlative performance with a neu ral network is initializing the parameters in a reasonable way. A good starting strategy is to initialize the weights to small random numbers normally distributed around 0 and in practice this often words acceptably well. However in Understanding the dif ficulty of training deep feedforward neural networks 2010 Xavier et al study the effect of different weight and bias initialization schemes on training dynamics.', 'A very simple ex ample is a function from two dimensional inputs to two dimensional outputs such as output 1 p1 input 1 p2 input 2 p3 input 1 input 2 p4 output 2 p5 tanh output 1 p6 input 1 p7 input 2 p8 input 1 input 2 p9 Neural networks are almost always deÔ¨Åned in terms of parameters here denoted by p1 . . . p9 which are automatically chosen using standard machine learning algorithms. Typically they include at least one transformation that is not linear e.g. the hyperbolic tangent above . The illustration displays the toy neural network as a computation graph inside the black box with inputs on the left outputs on the right and each parameter corresponds to a gray box placed along the top and bottom. Round nodes inside the box correspond to intermediate operations addition multiplication and tanh .']",text-embedding-3-small,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9747814955982458,1.0,0.9192326364065982,0.0526315789473684
109,"What are the differences between Vanilla RNNs, LSTMs, and GRUs?","Vanilla RNNs suffer from the vanishing gradient problem, which makes it difficult for them to capture long-term dependencies. LSTMs (Long Short-Term Memory) were introduced to address this issue by incorporating mechanisms like forget gates, input gates, and output gates to regulate the flow of information. GRUs (Gated Recurrent Units) are similar to LSTMs but are simpler in structure as they combine the forget and input gates into a single update gate and merge the cell state and hidden state. GRUs are designed to have more persistent memory, making it easier for RNNs to capture long-term dependencies compared to Vanilla RNNs. 

Source: lecture03_notes_LM_RNN.txt","['Which of these variants is best Do the differences matter Greff et al. 2015 http arxiv.org pdf 1503.04069.pdf do a nice comparison of popular variants finding that they re all about the same. Jozefowicz et al. 2015 http jmlr.org proceedings papers v37 jozefowicz15.pdf tested more than ten thousand RNN architectures finding some that worked better than LSTMs on certain tasks. Conclusion Earlier I mentioned the remarkable results people are achieving with RNNs. Essentially all of these are achieved using LSTMs. They really work a lot better for most tasks Written down as a set of equations LSTMs look pretty intimidating.', 'Figure 12 The detailed internals of a GRU It is important to note that to train a GRU we need to learn all the different parameters W U W r U r W z U z . These follow the same backpropagation procedure we have seen in the past. 4 Long Short Term Memories Long Short Term Memories are another type of complex activation unit that differ a little from GRUs. The motivation for using these is similar to those for GRUs however the architecture of such units does differ.', 'Gated recurrent units are designed in a manner to have more persistent memory thereby making it easier for RNNs to capture long term dependencies. Let us see mathematically how a GRU uses ht 1 and xt to generate the next hidden state ht. We will then dive into the intuition of this architecture. zt œÉ W z xt U z ht 1 Update gate rt œÉ W r xt U r ht 1 Reset gate ht tanh rt Uht 1 Wxt New memory ht 1 zt ht zt ht 1 Hidden state The above equations can be thought of a GRU s four fundamental op erational stages and they have intuitive interpretations that make this model much more intellectually satisfying see Figure 12 1. New memory generation A new memory ht is the consolidation of a new input word xt with the past hidden state ht 1.', 'We only forget when we re going to input something in its place. We only input new values to the state when we forget something older. A slightly more dramatic variation on the LSTM is the Gated Recurrent Unit or GRU introduced by Cho et al. 2014 http arxiv.org pdf 1406.1078v3.pdf . It combines the forget and input gates into a single update gate. It also merges the cell state and hidden state and makes some other changes. The resulting model is simpler than standard LSTM models and has been growing increasingly popular. These are only a few of the most notable LSTM variants. There are lots of others like Depth Gated RNNs by Yao et al. 2015 http arxiv.org pdf 1508.03790v2.pdf . There s also some completely different approach to tackling long term dependencies like Clockwork RNNs by Koutnik et al. 2014 http arxiv.org pdf 1402.3511v1.pdf .']",text-embedding-3-small,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9948495283046253,1.0,0.9499671870580652,0.0434782608695652
110,What is the difference between traditional language models and conditioned language models?,"Traditional language models generate text based on the probability of a word given the previous words in a sequence, without any additional conditioning. Conditioned language models, on the other hand, take into account additional information or conditions when generating text, such as specific prompts or contexts provided to the model. This allows conditioned language models to generate more contextually relevant and targeted text compared to traditional language models.","['Recent efforts include RSR 19 RRS20 which Ô¨Åne tuned an 11 billion parameter language model and GLT 20 which focused on attending over a large corpus of data at test time. Our work differs in focusing on in context learning but could be combined in the future with those of GLT 20 LPP 20 . Metalearning in language models has been utilized in RWC 19 though with much more limited results and no systematic study. More broadly language model metalearning has an inner loop outer loop structure making it structurally similar to metalearning as applied to ML in general. Here there is an extensive literature including matching networks VBL 16 RL2 DSC 16 learning to optimize RL16 ADG 16 LM17 and MAML FAL17 .', 'file name: Fine Tuning and Masked Language Models. Speech and Language Processing. Daniel Jurafsky James H. Martin. Copyright 2023. All rights reserved. Draft of February 3 2024. CHAPTER 11 Fine Tuning and Masked Lan guage Models Larvatus prodeo Masked I go forward Descartes In the previous chapter we saw how to pretrain transformer language models and how these pretrained models can be used as a tool for many kinds of NLP tasks by casting the tasks as word prediction. The models we introduced in Chapter 10 to do this task are causal or left to right transformer models. In this chapter we ll introduce a second paradigm for pretrained language mod els called the bidirectional transformer encoder trained via masked language modeling a method that allows the model to see entire texts at a time including masked language modeling both the right and left context. We ll introduce the most widely used version of the masked language modeling architecture the BERT model Devlin et al. 2019 .', 'This approach includes ALBERT LCG 19 as well as general HVD15 and 39task speciÔ¨Åc SDCW19 JYS 19 KR16 approaches to distillation of language models. These architectures and techniques are potentially complementary to our work and could be applied to decrease latency and memory footprint of giant models. As Ô¨Åne tuned language models have neared human performance on many standard benchmark tasks considerable effort has been devoted to constructing more difÔ¨Åcult or open ended tasks including question answering KPR 19 IBGC 14 CCE 18 MCKS18 reading comprehension CHI 18 RCM19 and adversarially constructed datasets designed to be difÔ¨Åcult for existing language models SBBC19 NWD 19 . In this work we test our models on many of these datasets. Many previous efforts have focused speciÔ¨Åcally on question answering which constitutes a signiÔ¨Åcant fraction of the tasks we tested on.', 'These possibilities exist on a spectrum ranging from demonstrations in the training set that are drawn from exactly the same distribution as those at test time to recognizing the same task but in a different format to adapting to a speciÔ¨Åc style of a general task such as QA to learning a skill entirely de novo. Where GPT 3 is on this spectrum may also vary from task to task. Synthetic tasks such as wordscrambling or deÔ¨Åning nonsense words seem especially likely to be learned de novo whereas translation clearly must be learned during pretraining although possibly from data that is very different in organization and style than the test data. Ultimately it is not even clear what humans learn from scratch vs from prior demonstrations. Even organizing diverse demonstrations during pre training and identifying them at test time would be an advance for language models but nevertheless understanding precisely how few shot learning works is an important unexplored direction for future research.']",text-embedding-3-small,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.0000000000000002,0.6666666666666666,0.9594075374819996,0.04
111,How can LSTM networks be used in language modeling?,"LSTM networks can be used in language modeling by providing a way to model long-range dependencies in sequential data, such as text. LSTM networks are a type of recurrent neural network that can effectively capture and remember important information from earlier parts of the input sequence, making them well-suited for tasks like language modeling where understanding context is crucial for predicting the next word in a sequence. By utilizing LSTM networks, language models can better handle and generate coherent and contextually relevant text.","['Recap 69 Language Model A system that predicts the next word Recurrent Neural Network A family of neural networks that Take sequential input of any length Apply the same weights on each step Can optionally produce output on each step Recurrent Neural Network Language Model We ve shown that RNNs are a great way to build a LM despite some problems RNNs are also useful for much moreWhy should we care about Language Modeling 70 Language Modeling is a benchmark task that helps us measure our progress on predicting language use Language Modeling is a subcomponent of many NLP tasks especially those involving generating text or estimating the probability of text Predictive typing Speech recognition Handwriting recognition Spelling grammar correction Authorship identification Machine translation Summarization Dialogue etc. Everything else in NLP has now been rebuilt upon Language Modeling GPT 3 is an LMOther RNN ArchitecturesOther RNN uses RNNs can be used for sequence tagging e.g.', 'Language Modeling exams minds laptops Language Modeling is the task of predicting what word comes next books the students opened their 10Language Modeling You can also think of a Language Model as a system that assigns a probability to a piece of text For example if we have some text then the probability of this text according to the Language Model is This is what our LM provides 11You use Language Models every day 12You use Language Models every day 13n gram Language Models 14 the students opened their Question How to learn a Language Model Answer pre Deep Learning learn an n gram Language Model Definition An n gram is a chunk of n consecutive words.', 'Recent efforts include RSR 19 RRS20 which Ô¨Åne tuned an 11 billion parameter language model and GLT 20 which focused on attending over a large corpus of data at test time. Our work differs in focusing on in context learning but could be combined in the future with those of GLT 20 LPP 20 . Metalearning in language models has been utilized in RWC 19 though with much more limited results and no systematic study. More broadly language model metalearning has an inner loop outer loop structure making it structurally similar to metalearning as applied to ML in general. Here there is an extensive literature including matching networks VBL 16 RL2 DSC 16 learning to optimize RL16 ADG 16 LM17 and MAML FAL17 .', 'Language models are unsupervised multitask learners 2019. Colin Raffel Noam Shazeer Adam Roberts Katherine Lee Sharan Narang Michael Matena Yanqi Zhou Wei Li and Peter J. Liu. Exploring the limits of transfer learning with a uniÔ¨Åed text to text transformer 2019. arXiv 1910.10683. Martin Sundermeyer Ralf Schl uter and Hermann Ney. LSTM neural networks for language modeling. In Proc. of Interspeech 2012. 14Peter D. Turney and Patrick Pantel. From frequency to meaning Vector space models of semantics. Journal of ArtiÔ¨Åcial Intelligence Research 37 1 141 188 2010. Zhilin Yang Zihang Dai Yiming Yang Jaime Carbonell Ruslan Salakhutdinov and Quoc V. Le. XLNet Generalized autoregressive pretraining for language understanding. In Proc. of NeurIPS 2019.']",text-embedding-3-small,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999983525936428,1.0,0.9545920724730476,0.0434782608695652
112,What are the methods of generation in conditioned language modeling?,"The methods of generation in conditioned language modeling are Sampling and Argmax. Ancestral Sampling involves randomly generating words one by one, which is an exact method for sampling from P X. Greedy Search involves picking the single highest probability word one by one, but it may not be exact and tends to generate easy words first. Beam Search maintains several paths instead of picking one high probability word. 

Source: SEP 775 - Computational Natural Language Processing, lecture04_Conditioned_Generation","['It was a lot cleaner down in London. Text Credit Max Deutsch https medium.com deep writingConditioned Language Models Not just generate text generate text according to some specification Input X Structured Data English Document Utterance Image Speech Output Y Text NL Description Japanese Short Description Response Text Transcript Task NL Generation Translation Summarization Response Generation Image Captioning Speech RecognitionFormulation and ModelingCalculating the Probability of a SentenceConditional Language ModelsOne Type of Language Model LSTM LSTM LSTM LSTM predict hate predict this predict movie predict s LSTM Mikolov et al. 2011 s I hate this movie predict ILSTM LSTM LSTM LSTM LSTM s LSTM LSTM LSTM LSTM argmax argmax argmax argmax argmax s One Type of Conditional Language Model I hate this movie kono eiga ga kirai hate this movie Sutskever et al. 2014 Encoder I DecoderHow to Pass Hidden State Initialize decoder w encoder Sutskever et al.', 'e.g. Kim et al. 2016 Problem parameter averaging only works for models within the same run Knowledge distillation trains a model to copy the ensemble Specifically it tries to match the description over predicted words Why We want the model to make the same mistakes as an ensemble Shown to increase accuracy notablyStacking What if we have two very different models where prediction of outputs is done in very different ways e.g. a phrase based translation model and a neural MT model Niehues et al. 2017 Stacking uses the output of one system in calculating features for another systemCase Studies in Conditional Language ModelingFrom Images e.g. Karpathy et al. 2015 Input is image features output is text Use standard image encoders e.g.', '2014 encoder decoder Transform can be different dimensions encoder transform decoder encoder Input at every time step Kalchbrenner Blunsom 2013 decoder decoder decoderMethods of GenerationThe Generation Problem We have a model of P Y X how do we use it to generate a sentence Two methods Sampling Try to generate a random sentence according to the probability distribution. Argmax Try to generate the sentence with the highest probability.Ancestral Sampling Randomly generate words one by one. An exact method for sampling from P X no further work needed.Greedy Search One by one pick the single highest probability word Not exact real problems Will often generate the easy words first Will prefer multiple common words to one rare wordBeam Search Instead of picking one high probability word maintain several paths Some in reading materials more in a later classModel EnsemblingEnsembling Why Multiple models make somewhat uncorrelated errors Models tend to be more uncertain when they are about to make errors Smooths over idiosyncrasies of the model LSTM1 predict1 I LSTM2 predict2 Combine predictions from multiple models s sLinear Interpolation Take', '1994. On structuring prob abilistic dependencies in stochastic language modelling. Computer Speech and Language 8 1 38. Schwenk H. 2007. Continuous space language models. Computer Speech Language 21 3 492 518. Shannon C. E. 1948. A mathematical theory of commu nication. Bell System Technical Journal 27 3 379 423. Continued in the following volume. Shannon C. E. 1951. Prediction and entropy of printed En glish. Bell System Technical Journal 30 50 64. Stolcke A. 1998. Entropy based pruning of backoff lan guage models. Proc. DARPA Broadcast News Transcrip tion and Understanding Workshop. Stolcke A. 2002. SRILM an extensible language modeling toolkit. ICSLP. Talbot D. and M. Osborne. 2007.']",text-embedding-3-small,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.0000000000000002,1.0,0.9099788295614,0.0357142857142857
113,What is beam search and how is it used in language generation?,"Beam search is a decoding algorithm commonly used in language generation tasks, such as machine translation and text generation. The core idea of beam search is to keep track of the k most probable partial translations, which are referred to as hypotheses, at each step of the decoding process. The parameter k, known as the beam size, determines how many hypotheses are retained at each step. These hypotheses are scored based on their log probabilities, with higher scores indicating better translations. By maintaining and expanding the top k hypotheses at each step, beam search explores multiple potential translation paths simultaneously, allowing for a more efficient search for the best overall translation compared to exhaustive search methods. While beam search is not guaranteed to find the optimal solution, it strikes a balance between accuracy and efficiency in generating sequences of words in natural language processing tasks.

Source: lecture05_Seq2Seq.txt","['top k on each step Beam search is not guaranteed to find optimal solution But much more efficient than exhaustive search 18Beam search decoding example Beam size k 2. Blue numbers START Calculate prob dist of next word 19Beam search decoding example Beam size k 2. Blue numbers START he I 0.7 log PLM he START 0.9 log PLM I START Take top k words and compute scores 20Beam search decoding example Beam size k 2. Blue numbers hit struck was got START he I 10 0.7 0.9 2.9 log PLM struck START he 0.7 1.6 log PLM was START I 0.9 1.8 log PLM got START I 0.9 1.7 log PLM hit START he 0.7 For each of the k hypotheses find top k next words and calculate scoresBeam search decoding example Beam size k 2.', 'decoder he hit me with a pie END START he hit me with a pie This is greedy decoding take most probable word on each step argmax argmax argmax argmax 15 argmax argmax argmaxProblems with greedy decoding 16 Greedy decoding has no way to undo decisions Input il a m entart√© he he hit he hit a he hit me with a pie whoops no going back now How to fix thisExhaustive search decoding Ideally we want to find a length T translation y that maximizes We could try computing all possible sequences y This means that on each step t of the decoder we re tracking Vt possible partial translations where V is vocab size This O VT complexity is far too expensive 17Beam search decoding Core idea On each step of decoder keep track of the k most probable partial translations which we call hypotheses k is the beam size in practice around 5 to 10 in NMT A hypothesis has a score which is its log probability Scores are all negative and higher score is better We search for high scoring hypotheses tracking top k on each step Beam search is not guaranteed', 'For simplicity we use greedy search for all generative tasks compared with beam search used in Brown et al. 2020 . Unlike GPT 3 which chooses the number of few shot exemplars k via best dev set performance for few shot LaMDA PT we choose the highest k that Ô¨Åts in the context length of 1024 tokens from k 1 3 5 10 . For DROP Dua et al. 2019 and SQuADv2 Rajpurkar et al. 2018 based on email correspondence with Brown et al. 2020 their deÔ¨Ånition of zero shot differs from ours in that they actually use exemplars but only from the same passage as the inference question each passage has more than one question . Hence GPT 3 zero shot results are not directly comparable with ours for DROP and SQuADv2. We mark these results using the symbol.', '2014 encoder decoder Transform can be different dimensions encoder transform decoder encoder Input at every time step Kalchbrenner Blunsom 2013 decoder decoder decoderMethods of GenerationThe Generation Problem We have a model of P Y X how do we use it to generate a sentence Two methods Sampling Try to generate a random sentence according to the probability distribution. Argmax Try to generate the sentence with the highest probability.Ancestral Sampling Randomly generate words one by one. An exact method for sampling from P X no further work needed.Greedy Search One by one pick the single highest probability word Not exact real problems Will often generate the easy words first Will prefer multiple common words to one rare wordBeam Search Instead of picking one high probability word maintain several paths Some in reading materials more in a later classModel EnsemblingEnsembling Why Multiple models make somewhat uncorrelated errors Models tend to be more uncertain when they are about to make errors Smooths over idiosyncrasies of the model LSTM1 predict1 I LSTM2 predict2 Combine predictions from multiple models s sLinear Interpolation Take']",text-embedding-3-small,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.8844441956736654,1.0,0.9009611846527942,0.2307692307692307
114,Explain the concept of model ensembling in language models.,"Model ensembling in language models involves combining the predictions of multiple individual models to improve overall performance. This can be done by training different models with varying architectures or training data, and then aggregating their predictions to make a final decision. Ensembling can help improve accuracy, generalization, and robustness of language models by leveraging the strengths of each individual model. It is a common technique used in machine learning to boost performance by reducing errors and overfitting.","['The methods of Chapter 6 like word2vec or GloVe learned a single vector embedding for each unique word w in the vocabulary. By contrast with contextual embeddings such as those learned by masked language models like BERT each word w will be represented by a different vector each time it appears in a different context. While the causal language models of Chapter 10 also use contextual embeddings the embeddings created by masked language models seem to function particularly well as representations. 11.1 Bidirectional Transformer Encoders Let s begin by introducing the bidirectional transformer encoder that underlies mod els like BERT and its descendants like RoBERTa Liu et al. 2019 or SpanBERT Joshi et al. 2020 . In Chapter 10 we explored causal left to right transformers that can serve as the basis for powerful language models models that can eas ily be applied to autoregressive generation problems such as contextual generation summarization and machine translation.', 'Recent efforts include RSR 19 RRS20 which Ô¨Åne tuned an 11 billion parameter language model and GLT 20 which focused on attending over a large corpus of data at test time. Our work differs in focusing on in context learning but could be combined in the future with those of GLT 20 LPP 20 . Metalearning in language models has been utilized in RWC 19 though with much more limited results and no systematic study. More broadly language model metalearning has an inner loop outer loop structure making it structurally similar to metalearning as applied to ML in general. Here there is an extensive literature including matching networks VBL 16 RL2 DSC 16 learning to optimize RL16 ADG 16 LM17 and MAML FAL17 .', 'e.g. Kim et al. 2016 Problem parameter averaging only works for models within the same run Knowledge distillation trains a model to copy the ensemble Specifically it tries to match the description over predicted words Why We want the model to make the same mistakes as an ensemble Shown to increase accuracy notablyStacking What if we have two very different models where prediction of outputs is done in very different ways e.g. a phrase based translation model and a neural MT model Niehues et al. 2017 Stacking uses the output of one system in calculating features for another systemCase Studies in Conditional Language ModelingFrom Images e.g. Karpathy et al. 2015 Input is image features output is text Use standard image encoders e.g.', 'Here s a summary of the main points that we covered Bidirectional encoders can be used to generate contextualized representations of input embeddings using the entire input context. Pretrained language models based on bidirectional encoders can be learned using a masked language model objective where a model is trained to guess the missing information from an input. Pretrained language models can be Ô¨Åne tuned for speciÔ¨Åc applications by adding lightweight classiÔ¨Åer layers on top of the outputs of the pretrained model. Bibliographical and Historical Notes22 Chapter 11 Fine Tuning and Masked Language Models Chaplot D. S. and R. Salakhutdinov. 2018. Knowledge based word sense disambiguation using topic models. AAAI. Clark K. M. T. Luong Q. V. Le and C. D. Manning. 2020. Electra Pre training text encoders as discrimina tors rather than generators. ICLR.']",text-embedding-3-small,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9687264226750372,0.0,0.928315341354251,0.0454545454545454
115,How does ensemble distillation differ from parameter averaging?,"Ensemble distillation differs from parameter averaging in that ensemble distillation trains a model to copy the ensemble, specifically trying to match the description over predicted words to make the same mistakes as an ensemble and increase accuracy notably. On the other hand, parameter averaging is a cheap way to get some good effects of ensembling by averaging out parameters of models several times near the end of training. 

Source: SEP 775 - Computational Natural Language Processing, lecture04_Conditioned_Generation.","['predictions from multiple models s sLinear Interpolation Take a weighted average of the M model probabilities Second term often set to uniform distribution 1 MLog linear Interpolation Weighted combination of log probabilities normalize Interpolation coefficient often set to uniform distribution 1 MLinear or Log Linear Think of it in logic Linear Logical OR the interpolated model likes any choice that a model gives a high probability use models with models that capture different traits necessary when any model can assign zero probability Log Linear Logical AND interpolated model only likes choices where all models agree use when you want to restrict possible answersParameter Averaging Problem Ensembling means we have to use M models at test time increasing our time memory complexity Parameter averaging is a cheap way to get some good effects of ensembling Basically write out models several times near the end of training and take the average of parametersEnsemble Distillation e.g. Kim et al.', 'e.g. Kim et al. 2016 Problem parameter averaging only works for models within the same run Knowledge distillation trains a model to copy the ensemble Specifically it tries to match the description over predicted words Why We want the model to make the same mistakes as an ensemble Shown to increase accuracy notablyStacking What if we have two very different models where prediction of outputs is done in very different ways e.g. a phrase based translation model and a neural MT model Niehues et al. 2017 Stacking uses the output of one system in calculating features for another systemCase Studies in Conditional Language ModelingFrom Images e.g. Karpathy et al. 2015 Input is image features output is text Use standard image encoders e.g.', 'Note in the entry for labeler vs. labeler ensemble the ensembles are slightly smaller than for other comparisons because we need to exclude the labeler being predicted. All ensembles have at least 3 workers. 36TL DR 6.7B sup. T 0.7 labeler labeler ensem ble length copying ROUGE 1.3B sup logprob 1.3B RM 6.7B sup logprob 6.7B RM labeler 70.8 2.6 73.1 2.9 56.9 0.6 56.4 0.6 56.9 0.6 54.5 1.2 67.5 1.1 54.3 1.2 69.7 1.1 labeler ensemble 73.1 2.9 55.0 5.1 54.5 4.8 66.7 4.7 61.1 11.', 'Yet it is often difÔ¨Åcult to Ô¨Ågure out which ideas are most responsible for the state of the art performance of a large ensemble. Because neural networks are nonconvex training runs with different initial parameters usually give different solutions. Table 11 reports results obtained for the CHUNK and NER task after ten training runs with random initial parameters. Voting the ten network outputs on a per tag basis voting ensemble leads to a small improvement over the average network performance. We have also tried a more sophisticated ensemble approach the ten network output scores before sentence level likelihood were combined with an additional linear layer 4 and then fed to a new sentence level likelihood 13 . The parameters of the combining layers were then trained on the existing training set while keeping the ten networks Ô¨Åxed joined ensemble . This approach did not improve on simple voting. These ensembles come of course at the expense of a ten fold increase of the running time. On the other hand multiple training times could be improved using smart sampling strategies Neal 1996 .']",text-embedding-3-small,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9915186957992392,1.0,0.9684603009904528,0.0666666666666666
116,What are some common evaluation metrics for conditioned language models?,"Some common evaluation metrics for conditioned language models include BLEU, Embedding-based Metrics like BertScore, BLEURT, COMET, PRISM, and Perplexity. These metrics are used to evaluate the performance of language models in tasks such as translation, paraphrasing, and generation.","['CNN Often pre trained on large databases such as ImageNetCheck out this review paperHow do we EvaluateBasic Evaluation Paradigm Use parallel test set Use system to generate translations Compare target translations w referenceHuman Evaluation Ask a human to do evaluation Final goal but slow expensive and sometimes inconsistentBLEU Works by comparing n gram overlap w reference Pros Easy to use good for measuring system improvement Cons Often doesn t match human eval bad for comparing very different systemsEmbedding based Metrics Recently many metrics based on neural models BertScore Find similarity between BERT embeddings unsupervised Zhang et al. 2020 BLEURT Train BERT to predict human evaluation scores Sellam et al. 2020 COMET Train model to predict human eval also using source sentence Rei et al. 2020 PRISM Model based on training paraphrasing model Thompson and Post 2020Perplexity Calculate the perplexity of the words in the held out set without doing generation Pros Naturally solves multiple reference problem Cons Doesn t consider decoding or actually generating output.', 'May be reasonable for problems with lots of ambiguity.Which One to Use Meta evaluation runs human evaluation and automatic evaluation on the same outputs calculates correlation Examples WMT Metrics Task for MT Mathur et al. 2021 RealSumm for summarization Evaluation is hard especially with good systems Most metrics had no correlation w human eval over best systems at some WMT 2019 tasksQuestions', 'Below we evaluate the 8 models described in Section 2 the 175 billion parameter parameter GPT 3 and 7 smaller models on a wide range of datasets. We group the datasets into 9 categories representing roughly similar tasks. In Section 3.1 we evaluate on traditional language modeling tasks and tasks that are similar to language modeling such as Cloze tasks and sentence paragraph completion tasks. In Section 3.2 we evaluate on closed book question answering tasks tasks which require using the information stored in the model s parameters to answer general knowledge questions. In Section 3.3 we evaluate the model s ability to translate between languages especially one shot and few shot . In Section 3.4 we evaluate the model s performance on Winograd Schema like tasks. In Section 3.5 we evaluate on datasets that involve commonsense reasoning or question answering.', 'This approach includes ALBERT LCG 19 as well as general HVD15 and 39task speciÔ¨Åc SDCW19 JYS 19 KR16 approaches to distillation of language models. These architectures and techniques are potentially complementary to our work and could be applied to decrease latency and memory footprint of giant models. As Ô¨Åne tuned language models have neared human performance on many standard benchmark tasks considerable effort has been devoted to constructing more difÔ¨Åcult or open ended tasks including question answering KPR 19 IBGC 14 CCE 18 MCKS18 reading comprehension CHI 18 RCM19 and adversarially constructed datasets designed to be difÔ¨Åcult for existing language models SBBC19 NWD 19 . In this work we test our models on many of these datasets. Many previous efforts have focused speciÔ¨Åcally on question answering which constitutes a signiÔ¨Åcant fraction of the tasks we tested on.']",text-embedding-3-small,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.0000000000000002,1.0,0.9277852902047032,0.0714285714285714
117,What is the significance of stacking in model ensembling?,"Stacking is significant in model ensembling because it allows for combining predictions from different models, even if they use different approaches or techniques, to improve overall performance. By using the output of one system to calculate features for another system, stacking can help capture different aspects of the data and improve the accuracy of the final prediction. It is particularly useful when dealing with very different models that have diverse ways of predicting outputs.","['e.g. Kim et al. 2016 Problem parameter averaging only works for models within the same run Knowledge distillation trains a model to copy the ensemble Specifically it tries to match the description over predicted words Why We want the model to make the same mistakes as an ensemble Shown to increase accuracy notablyStacking What if we have two very different models where prediction of outputs is done in very different ways e.g. a phrase based translation model and a neural MT model Niehues et al. 2017 Stacking uses the output of one system in calculating features for another systemCase Studies in Conditional Language ModelingFrom Images e.g. Karpathy et al. 2015 Input is image features output is text Use standard image encoders e.g.', 'Yet it is often difÔ¨Åcult to Ô¨Ågure out which ideas are most responsible for the state of the art performance of a large ensemble. Because neural networks are nonconvex training runs with different initial parameters usually give different solutions. Table 11 reports results obtained for the CHUNK and NER task after ten training runs with random initial parameters. Voting the ten network outputs on a per tag basis voting ensemble leads to a small improvement over the average network performance. We have also tried a more sophisticated ensemble approach the ten network output scores before sentence level likelihood were combined with an additional linear layer 4 and then fed to a new sentence level likelihood 13 . The parameters of the combining layers were then trained on the existing training set while keeping the ten networks Ô¨Åxed joined ensemble . This approach did not improve on simple voting. These ensembles come of course at the expense of a ten fold increase of the running time. On the other hand multiple training times could be improved using smart sampling strategies Neal 1996 .', 'Note in the entry for labeler vs. labeler ensemble the ensembles are slightly smaller than for other comparisons because we need to exclude the labeler being predicted. All ensembles have at least 3 workers. 36TL DR 6.7B sup. T 0.7 labeler labeler ensem ble length copying ROUGE 1.3B sup logprob 1.3B RM 6.7B sup logprob 6.7B RM labeler 70.8 2.6 73.1 2.9 56.9 0.6 56.4 0.6 56.9 0.6 54.5 1.2 67.5 1.1 54.3 1.2 69.7 1.1 labeler ensemble 73.1 2.9 55.0 5.1 54.5 4.8 66.7 4.7 61.1 11.', 'predictions from multiple models s sLinear Interpolation Take a weighted average of the M model probabilities Second term often set to uniform distribution 1 MLog linear Interpolation Weighted combination of log probabilities normalize Interpolation coefficient often set to uniform distribution 1 MLinear or Log Linear Think of it in logic Linear Logical OR the interpolated model likes any choice that a model gives a high probability use models with models that capture different traits necessary when any model can assign zero probability Log Linear Logical AND interpolated model only likes choices where all models agree use when you want to restrict possible answersParameter Averaging Problem Ensembling means we have to use M models at test time increasing our time memory complexity Parameter averaging is a cheap way to get some good effects of ensembling Basically write out models several times near the end of training and take the average of parametersEnsemble Distillation e.g. Kim et al.']",text-embedding-3-small,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.982933996761466,1.0,0.9315848132107948,0.0666666666666666
118,How are images used as input for conditioned language models?,"Images are used as input for conditioned language models by representing the image classes as learned d-dimensional embeddings per class and adding the respective embedding to the input representation of every input position together with the positional encodings. This allows for conditioning on image classes in the model. Additionally, in some cases, image features are used as input for generating text output in conditional language modeling.","['Generated horses planes and birds also look reasonable. 5.1. Generative Image Modeling Our unconditioned and class conditioned image generation models both use 1D local attention with lq 256 and a total memory size of 512. On CIFAR 10 our best unconditional models achieve a perplexity of 2.90 bits dim on the test set using either DMOL or categorical. For categorical we use 12 layers with d 512 heads 4 feed forward dimension 2048 with a dropout of 0.3. In DMOL our best conÔ¨Åg uses 14 layers d 256 heads 8 feed forward dimension 512 and a dropout of 0.2. This is a considerable improvement over two baselines the PixelRNN van den Oord et al. 2016a and PixelCNN Salimans et al. . Introduced after the Image Transformer the also self attention based Pixel SNAIL model reaches a signiÔ¨Åcantly lower perplexity of 2.85 bits dim on CIFAR 10 Chen et al.', 'e.g. Kim et al. 2016 Problem parameter averaging only works for models within the same run Knowledge distillation trains a model to copy the ensemble Specifically it tries to match the description over predicted words Why We want the model to make the same mistakes as an ensemble Shown to increase accuracy notablyStacking What if we have two very different models where prediction of outputs is done in very different ways e.g. a phrase based translation model and a neural MT model Niehues et al. 2017 Stacking uses the output of one system in calculating features for another systemCase Studies in Conditional Language ModelingFrom Images e.g. Karpathy et al. 2015 Input is image features output is text Use standard image encoders e.g.', '5.2. Conditioning on Image Class We represent the image classes as learned d dimensional embeddings per class and simply add the respective em bedding to the input representation of every input position together with the positional encodings. We trained the class conditioned Image Transformer on CIFAR 10 achieving very similar log likelihoods as in un conditioned generation. The perceptual quality of generatedImage Transformer images however is signiÔ¨Åcantly higher than that of our unconditioned models. The samples from our 8 layer class conditioned models in Table 3 show that we can generate realistic looking images for some categories such as cars and trucks. 5.3. Image Super Resolution Super resolution is the process of recovering a high resolu tion image from a low resolution image while generating realistic and plausible details. Following Dahl et al.', '2015 . In visual planning tasks conditional image generation models could predict future frames of video con ditioned on previous frames and taken actions. In this work we hence also evaluate two different methods of performing conditional image generation with the Im age Transformer. In image class conditional generation we condition on an embedding of one of a small number of image classes. In super resolution with high magniÔ¨Åcation ratio 4x we condition on a very low resolution image employing the Image Transformer in an encoder decoder conÔ¨Åguration Kalchbrenner Blunsom 2013 . In com parison to recent work on autoregressive super resolution Dahl et al. 2017 a human evaluation study found im ages generated by our models to look convincingly natural signiÔ¨Åcantly more often. 2. Background There is a broad variety of types of image generation mod els in the literature.']",text-embedding-3-small,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.000000000000002,1.0,0.9069559804017532,0.0384615384615384
119,What challenges exist in evaluating conditioned language models?,"The challenges in evaluating conditioned language models include the cost of creating a large set of high-quality rationales for rationale-augmented training and fine-tuning methods, which is more complex than using simple input-output pairs in traditional machine learning. Additionally, traditional few-shot prompting methods used in some models can have limitations in terms of complexity and resource requirements.","['Recent efforts include RSR 19 RRS20 which Ô¨Åne tuned an 11 billion parameter language model and GLT 20 which focused on attending over a large corpus of data at test time. Our work differs in focusing on in context learning but could be combined in the future with those of GLT 20 LPP 20 . Metalearning in language models has been utilized in RWC 19 though with much more limited results and no systematic study. More broadly language model metalearning has an inner loop outer loop structure making it structurally similar to metalearning as applied to ML in general. Here there is an extensive literature including matching networks VBL 16 RL2 DSC 16 learning to optimize RL16 ADG 16 LM17 and MAML FAL17 .', '2021 in addition to neuro symbolic methods that use formal lan guages instead of natural language Roy and Roth 2015 Chiang and Chen 2019 Amini et al. 2019 Chen et al. 2019 . Second large language models offer the exciting prospect of in context few shot learning via prompting. That is instead of Ô¨Ånetuning a separate language model checkpoint for each new task one can simply prompt the model with a few input output exemplars demonstrating the task. Remarkably this has been successful for a range of simple question answering tasks Brown et al. 2020 . Both of the above ideas however have key limitations. For rationale augmented training and Ô¨Ånetuning methods it is costly to create a large set of high quality rationales which is much more complicated than simple input output pairs used in normal machine learning. For the traditional few shot prompting method used in Brown et al.', 'This approach includes ALBERT LCG 19 as well as general HVD15 and 39task speciÔ¨Åc SDCW19 JYS 19 KR16 approaches to distillation of language models. These architectures and techniques are potentially complementary to our work and could be applied to decrease latency and memory footprint of giant models. As Ô¨Åne tuned language models have neared human performance on many standard benchmark tasks considerable effort has been devoted to constructing more difÔ¨Åcult or open ended tasks including question answering KPR 19 IBGC 14 CCE 18 MCKS18 reading comprehension CHI 18 RCM19 and adversarially constructed datasets designed to be difÔ¨Åcult for existing language models SBBC19 NWD 19 . In this work we test our models on many of these datasets. Many previous efforts have focused speciÔ¨Åcally on question answering which constitutes a signiÔ¨Åcant fraction of the tasks we tested on.', 'Yizhong Wang Swaroop Mishra Pegah Alipoormolabashi Yeganeh Kordi Amirreza Mirzaei Anjana Arunkumar Arjun Ashok Arut Selvan Dhanasekaran Atharva Naik David Stap et al. 2022b. Benchmarking generalization via in context instructions on 1 600 language tasks. arXiv preprint arXiv 2204.07705. Jason Wei Maarten Bosma Vincent Y. Zhao Kelvin Guu Adams Wei Yu Brian Lester Nan Du Andrew M. Dai and Quoc V. Le. 2022a. Finetuned language models are zero shot learners. ICLR. 13Jason Wei Yi Tay Rishi Bommasani Colin Raffel Barret Zoph Sebastian Borgeaud Dani Yogatama Maarten Bosma Denny Zhou Donald Metzler et al. 2022b. Emergent abilities of large language models.']",text-embedding-3-small,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9940922212071212,1.0,0.8614580892560941,0.0384615384615384
120,What is the instructor's name for this course?,"The instructor for the course ""Introduction to Computational Natural Language Processing"" in Winter 2024 is Hamidreza Mahyar. 

Source: NLP Winter 2024 Course Outline.","['file name: NLP Winter 2024 Course Outline. Course Outline 1. COURSE INFORMATION Session Offered Winter 2024 Course Name Introduction to Computational Natural Language Processing Course Code SEP 775 Date s and Time s of lectures Lectures In person Wednesday 3 30 pm 6 30 pm Program Name System and Technology Calendar Description This graduate course introduces fundamental concepts in computational natural language processing NLP and their applications to teaching students how to deal with textual data in Artificial Intelligence. This course demonstrates how machines can learn different tasks in natural language such as language modeling text generation machine translation and language understanding. In this regard we go over the most promising methods in this literature and the most recent state of the art techniques. Moreover this course explores different real world applications of NLP and helps students get hands on experience in this field. Instructor Hamidreza Mahyar E Mail mahyarh@mcmaster.ca Office Hours upon request 2.', '. Students should be aware that when they access the electronic components of a course using these elements private information such as first and last names user names for the McMaster e mail accounts and program affiliation may become apparent to all other students in the same course. The available information is dependent on the technology used. Continuation in a course that uses on line elements will be deemed consent to this disclosure. If you have any questions or concerns about such disclosure please discuss this with the course instructor. ONLINE PROCTORING Some courses may use online proctoring software for tests and exams. This software may require students to turn on their video camera present identification monitor and record their computer activities and or lock restrict their browser or other applications software during tests or exams. This software may be required to be installed before the test exam begins. COMMUNICATIONS It is the student s responsibility to Maintain current contact information with the University including address phone numbers and emergency contact information. Use the University provided e mail address or maintain a valid forwarding e mail address.', 'COURSE SPECIFICS Course Description This course covers some fundamental concepts in natural language processing NLP and how they apply in real world use cases. Instruction Type Code Type Hours per term C Classroom instruction 39 L Laboratory workshop or fieldwork T Tutorial DE Distance education Total Hours 39 Resources ISBN Textbook Title Edition Author Publisher Natural Language Processing Jacob Eisenstein Speech and Language Processing Dan Jurafsky and James H. Martin Deep Learning Ian Goodfellow Yoshua Bengio and Aaron Courville Prerequisite s Proficiency in Python programming language Some Machine Learning and or Deep Learning knowledge is a plus Corequisite s Antirequisite s Course Specific Policies All work must be shown to get full credit. Specific policy of Projects Assignments submissionsProjects Assignments will be conducted online by using related software. Completed Projects Assignments should be uploaded to the drop box before midnight of the due date. Projects Assignments missed due to legitimate reasons must be completed later mutually agreed with the instructor.', 'Students should submit their request to their Faculty Office normally within 10 working days of the beginning of term in which they anticipate a need for accommodation or to the Registrar s Office prior to their examinations. Students should also contact their instructors as soon as possible to make alternative arrangements for classes assignments and tests. http www.mcmaster.ca policy Students AcademicStudies Studentcode.pdf COPYRIGHT AND RECORDING Students are advised that lectures demonstrations performances and any other course material provided by an instructor include copyright protected works. The Copyright Act and copyright law protect every original literary dramatic musical and artistic work including lectures by University instructors The recording of lectures tutorials or other methods of instruction may occur during a course. Recording may be done by either the instructor for the purpose of authorized distribution or by a student for the purpose of personal study. Students should be aware that their voice and or image may be recorded by others during the class. Please speak with the instructor if this is a concern for you.']",text-embedding-3-small,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.8698387664883226,1.0,0.8841454665343133,0.1
121,What is the email address of the instructor for this course?,"The email address of the instructor for this course is mahyarh@mcmaster.ca. 

Source: NLP Winter 2024 Course Outline.","['. Students should be aware that when they access the electronic components of a course using these elements private information such as first and last names user names for the McMaster e mail accounts and program affiliation may become apparent to all other students in the same course. The available information is dependent on the technology used. Continuation in a course that uses on line elements will be deemed consent to this disclosure. If you have any questions or concerns about such disclosure please discuss this with the course instructor. ONLINE PROCTORING Some courses may use online proctoring software for tests and exams. This software may require students to turn on their video camera present identification monitor and record their computer activities and or lock restrict their browser or other applications software during tests or exams. This software may be required to be installed before the test exam begins. COMMUNICATIONS It is the student s responsibility to Maintain current contact information with the University including address phone numbers and emergency contact information. Use the University provided e mail address or maintain a valid forwarding e mail address.', 'Students should submit their request to their Faculty Office normally within 10 working days of the beginning of term in which they anticipate a need for accommodation or to the Registrar s Office prior to their examinations. Students should also contact their instructors as soon as possible to make alternative arrangements for classes assignments and tests. http www.mcmaster.ca policy Students AcademicStudies Studentcode.pdf COPYRIGHT AND RECORDING Students are advised that lectures demonstrations performances and any other course material provided by an instructor include copyright protected works. The Copyright Act and copyright law protect every original literary dramatic musical and artistic work including lectures by University instructors The recording of lectures tutorials or other methods of instruction may occur during a course. Recording may be done by either the instructor for the purpose of authorized distribution or by a student for the purpose of personal study. Students should be aware that their voice and or image may be recorded by others during the class. Please speak with the instructor if this is a concern for you.', 'file name: NLP Winter 2024 Course Outline. Course Outline 1. COURSE INFORMATION Session Offered Winter 2024 Course Name Introduction to Computational Natural Language Processing Course Code SEP 775 Date s and Time s of lectures Lectures In person Wednesday 3 30 pm 6 30 pm Program Name System and Technology Calendar Description This graduate course introduces fundamental concepts in computational natural language processing NLP and their applications to teaching students how to deal with textual data in Artificial Intelligence. This course demonstrates how machines can learn different tasks in natural language such as language modeling text generation machine translation and language understanding. In this regard we go over the most promising methods in this literature and the most recent state of the art techniques. Moreover this course explores different real world applications of NLP and helps students get hands on experience in this field. Instructor Hamidreza Mahyar E Mail mahyarh@mcmaster.ca Office Hours upon request 2.', 'Regularly check the official University communications channels. Official University communications are considered received if sent by postal mail by fax or by e mail to the student s designated primary e mail account via their @mcmaster.ca alias. Accept that forwarded e mails may be lost and that e mail is considered received if sent via the student s @mcmaster.ca alias. Check the McMaster Avenue email and course websites on a regular basis during the term. CONDUCT EXPECTATIONS As a McMaster student you have the right to experience and the responsibility to demonstrate respectful and dignified interactions within all of our living learning and working communities. These expectations are described in the Code of Student Rights Responsibilities the Code . All students share the responsibility of maintaining a positive environment for the academic and personal growth of all McMaster community members whether in person or online. It is essential that students be mindful of their interactions online as the Code remains in effect in virtual learning environments. The Code applies to any interactions that adversely affect disrupt or interfere with reasonable participation in University activities.']",text-embedding-3-small,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.0000000000000002,1.0,0.9360940961522098,0.0
122,When is this course held every week?,"This course is held in person on Wednesdays from 3:30 pm to 6:30 pm every week. 
Source: NLP Winter 2024 Course Outline.","['3. SUB TOPIC S Week 1 Overview Human language Word meanings Word2Vec and Optimization basics Week 2 Intro to neural networks Gradients and backpropagation Week 3 Recurrent Neural Networks and Language Models Week 4 Conditioned Generation Week 5 Part 1 Self attention and transformers Part 2 Seq2Seq models Week 6 Midterm Test Week 7 Midterm Recess No class Week 8 Prompting Instruction Finetuning and RLHF Week 9 Question Answering systems Week 10 Multimodal models Week 11 Project presentation and Q A Week 12 Project presentation and Q A Week 13 Project presentation and Q A Classes end April 10th 4. ASSESSMENT OF LEARNING Weight 4 Assignments 20 4 Quizzes 10 Midterm Test 35 Group Project 35 TOTAL 100Note that the percentage distribution of each module may be adjusted. Percentage grades will be converted to letter grades and grade points per the University calendar. 5.', 'file name: NLP Winter 2024 Course Outline. Course Outline 1. COURSE INFORMATION Session Offered Winter 2024 Course Name Introduction to Computational Natural Language Processing Course Code SEP 775 Date s and Time s of lectures Lectures In person Wednesday 3 30 pm 6 30 pm Program Name System and Technology Calendar Description This graduate course introduces fundamental concepts in computational natural language processing NLP and their applications to teaching students how to deal with textual data in Artificial Intelligence. This course demonstrates how machines can learn different tasks in natural language such as language modeling text generation machine translation and language understanding. In this regard we go over the most promising methods in this literature and the most recent state of the art techniques. Moreover this course explores different real world applications of NLP and helps students get hands on experience in this field. Instructor Hamidreza Mahyar E Mail mahyarh@mcmaster.ca Office Hours upon request 2.', 'COURSE SPECIFICS Course Description This course covers some fundamental concepts in natural language processing NLP and how they apply in real world use cases. Instruction Type Code Type Hours per term C Classroom instruction 39 L Laboratory workshop or fieldwork T Tutorial DE Distance education Total Hours 39 Resources ISBN Textbook Title Edition Author Publisher Natural Language Processing Jacob Eisenstein Speech and Language Processing Dan Jurafsky and James H. Martin Deep Learning Ian Goodfellow Yoshua Bengio and Aaron Courville Prerequisite s Proficiency in Python programming language Some Machine Learning and or Deep Learning knowledge is a plus Corequisite s Antirequisite s Course Specific Policies All work must be shown to get full credit. Specific policy of Projects Assignments submissionsProjects Assignments will be conducted online by using related software. Completed Projects Assignments should be uploaded to the drop box before midnight of the due date. Projects Assignments missed due to legitimate reasons must be completed later mutually agreed with the instructor.', 'Students should submit their request to their Faculty Office normally within 10 working days of the beginning of term in which they anticipate a need for accommodation or to the Registrar s Office prior to their examinations. Students should also contact their instructors as soon as possible to make alternative arrangements for classes assignments and tests. http www.mcmaster.ca policy Students AcademicStudies Studentcode.pdf COPYRIGHT AND RECORDING Students are advised that lectures demonstrations performances and any other course material provided by an instructor include copyright protected works. The Copyright Act and copyright law protect every original literary dramatic musical and artistic work including lectures by University instructors The recording of lectures tutorials or other methods of instruction may occur during a course. Recording may be done by either the instructor for the purpose of authorized distribution or by a student for the purpose of personal study. Students should be aware that their voice and or image may be recorded by others during the class. Please speak with the instructor if this is a concern for you.']",text-embedding-3-small,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.8914695535350897,0.8,0.902218920774404,0.1428571428571428
123,What is the name of the TA for this course?,"The TA for this course is not mentioned in the provided context information. If you need specific information about the TA, you may want to reach out to the course instructor or the department offering the course for more details.
Source: NLP Winter 2024 Course Outline","['file name: NLP Winter 2024 Course Outline. Course Outline 1. COURSE INFORMATION Session Offered Winter 2024 Course Name Introduction to Computational Natural Language Processing Course Code SEP 775 Date s and Time s of lectures Lectures In person Wednesday 3 30 pm 6 30 pm Program Name System and Technology Calendar Description This graduate course introduces fundamental concepts in computational natural language processing NLP and their applications to teaching students how to deal with textual data in Artificial Intelligence. This course demonstrates how machines can learn different tasks in natural language such as language modeling text generation machine translation and language understanding. In this regard we go over the most promising methods in this literature and the most recent state of the art techniques. Moreover this course explores different real world applications of NLP and helps students get hands on experience in this field. Instructor Hamidreza Mahyar E Mail mahyarh@mcmaster.ca Office Hours upon request 2.', '. Students should be aware that when they access the electronic components of a course using these elements private information such as first and last names user names for the McMaster e mail accounts and program affiliation may become apparent to all other students in the same course. The available information is dependent on the technology used. Continuation in a course that uses on line elements will be deemed consent to this disclosure. If you have any questions or concerns about such disclosure please discuss this with the course instructor. ONLINE PROCTORING Some courses may use online proctoring software for tests and exams. This software may require students to turn on their video camera present identification monitor and record their computer activities and or lock restrict their browser or other applications software during tests or exams. This software may be required to be installed before the test exam begins. COMMUNICATIONS It is the student s responsibility to Maintain current contact information with the University including address phone numbers and emergency contact information. Use the University provided e mail address or maintain a valid forwarding e mail address.', 'COURSE SPECIFICS Course Description This course covers some fundamental concepts in natural language processing NLP and how they apply in real world use cases. Instruction Type Code Type Hours per term C Classroom instruction 39 L Laboratory workshop or fieldwork T Tutorial DE Distance education Total Hours 39 Resources ISBN Textbook Title Edition Author Publisher Natural Language Processing Jacob Eisenstein Speech and Language Processing Dan Jurafsky and James H. Martin Deep Learning Ian Goodfellow Yoshua Bengio and Aaron Courville Prerequisite s Proficiency in Python programming language Some Machine Learning and or Deep Learning knowledge is a plus Corequisite s Antirequisite s Course Specific Policies All work must be shown to get full credit. Specific policy of Projects Assignments submissionsProjects Assignments will be conducted online by using related software. Completed Projects Assignments should be uploaded to the drop box before midnight of the due date. Projects Assignments missed due to legitimate reasons must be completed later mutually agreed with the instructor.', '3. SUB TOPIC S Week 1 Overview Human language Word meanings Word2Vec and Optimization basics Week 2 Intro to neural networks Gradients and backpropagation Week 3 Recurrent Neural Networks and Language Models Week 4 Conditioned Generation Week 5 Part 1 Self attention and transformers Part 2 Seq2Seq models Week 6 Midterm Test Week 7 Midterm Recess No class Week 8 Prompting Instruction Finetuning and RLHF Week 9 Question Answering systems Week 10 Multimodal models Week 11 Project presentation and Q A Week 12 Project presentation and Q A Week 13 Project presentation and Q A Classes end April 10th 4. ASSESSMENT OF LEARNING Weight 4 Assignments 20 4 Quizzes 10 Midterm Test 35 Group Project 35 TOTAL 100Note that the percentage distribution of each module may be adjusted. Percentage grades will be converted to letter grades and grade points per the University calendar. 5.']",text-embedding-3-small,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.0,1.0,0.8231968127546736,0.0
124,What is Word2vec?,"Word2vec is a model that represents each word in a fixed vocabulary as a low-dimensional vector, much smaller than the vocabulary size. It learns the values of these word vectors to be predictive based on the distribution of words in a short context. The specific model described in the context is the skipgram word2vec algorithm. It aims to capture complex linguistic patterns beyond word similarity by making predictions in local context windows. The word vectors are sometimes referred to as distributed representations. Source: SEP 775 - Computational Natural Language Processing, lecture01_notes_Introduction.txt, Efficient_Estimation_of_Word_Representations_in_Vector_Space.txt, lecture01_notes_wordvecs2.txt, Contextual_Word_Representations_A_Contextual_Introduction.txt.","['However many of the details of word2vec will hold true in methods that we ll proceed to further in the course so we ll focus our time on that. 3.2 Word2vec model and objective The word2vec model represents each word in a fixed vocabulary as a low dimensional much smaller than vocabulary size vector. It learns the value of each word s vector to be predictive via a simple function of the distribution of words in a usually short 2 4 words context. The model we ll describe here is called the skipgram word2vec algo rithm. Skipgram word2vec. As usual we have a finite vocabulary V. Let C O be random variables representing an unknown pair of C V a center word and O V an outside word appearing in the context of the center word . We ll use c o to refer to specific values of the random variables.', 'file name: Efficient Estimation of Word Representations in Vector Space. EfÔ¨Åcient Estimation of Word Representations in Vector Space Tomas Mikolov Google Inc. Mountain View CA tmikolov@google.com Kai Chen Google Inc. Mountain View CA kaichen@google.com Greg Corrado Google Inc. Mountain View CA gcorrado@google.com Jeffrey Dean Google Inc. Mountain View CA jeff@google.com Abstract We propose two novel model architectures for computing continuous vector repre sentations of words from very large data sets. The quality of these representations is measured in a word similarity task and the results are compared to the previ ously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set.', 'Jeffrey Pennington Richard Socher and Christopher D. Manning. 2014. GloVe Global Vectors for Word Repre sentation 1.1 Comparison with Previous Methods So far we have looked at two main classes of methods to Ô¨Ånd word embeddings. The Ô¨Årst set are count based and rely on matrix factor ization e.g. LSA HAL . While these methods effectively leverage global statistical information they are primarily used to capture word similarities and do poorly on tasks such as word analogy indi cating a sub optimal vector space structure. The other set of methods are shallow window based e.g. the skip gram and the CBOW mod els which learn word embeddings by making predictions in local context windows. These models demonstrate the capacity to capture complex linguistic patterns beyond word similarity but fail to make use of the global co occurrence statistics.', 'The word s meaning is distributed across the whole vector for this reason these vectors are sometimes called distributed representations.5 As corpora grew scalability became a challenge because the number of observable contexts grew as well. Underlying all word vector algorithms is the notion that the value placed in each dimension of each word type s vector is a parameter that will be optimized alongside all the other parameters to best Ô¨Åt the observed patterns of the words in the data. Since we view these parameters as continuous values and the notion of Ô¨Åtting the data can be operationalized as a smooth continuous objective function selecting the parameter values is done using iterative algorithms based on gradient descent. Using tools that had become popular in machine learning faster methods based on stochastic optimization were developed. One widely known collection of algorithms is available as the word2vec package Mikolov et al. 2013 . A common pattern arose in which industry researchers with large corpora and powerful computing infrastructure would construct word vectors using an established often expensive iterative method and then publish the vectors for anyone to use.']",text-embedding-3-small,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.973841792836291,1.0,0.9220444508525116,0.0344827586206896
125,What are the main learning goals of the course?,"The main learning goals of the course include understanding the foundations of effective modern methods for deep learning applied to NLP, gaining a big picture understanding of human languages and the challenges in processing them via computers, learning about various key methods used in NLP in 2024 such as word vectors, feed forward networks, recurrent networks, attention mechanisms, encoder-decoder models, transformers, and large pre-trained language models. Additionally, the course aims to provide knowledge on word representations, language models, pre-trained language models, text generation, Seq2Seq models, prompting, feedback systems, multimodal models, and various NLP applications like Q&A systems.","['learning goals 4 1. The foundations of the effective modern methods for deep learning applied to NLP Basics first then key methods used in NLP in 2024 Word vectors feed forward networks recurrent networks attention encoder decoder models transformers large pre trained language models etc. 2. A big picture understanding of human languages and the difficulties in understanding and producing them via computers 3.', '5. LEARNING OUTCOMES By the end of this course the students will know about Introduction to Deep Learning Word representations Language models Pre trained language models Text generation Seq2Seq models Prompting Feedback systems Multimodal models NLP applications e.g. Q A systems 6. COURSE OUTLINE APPROVED ADVISORY STATEMENTS ANTI DISCRIMINATION The Faculty of Engineering is concerned with ensuring an environment that is free of all discrimination. If there is a problem individuals are reminded that they should contact the Associate Director Graduate Studies the Sexual Harassment Officer or the Human Rights Consultant as soon as possible. http www.mcmaster.ca policy General HR Discrimination_Harassment_Sexual_Harassment Prevention Response.pdf ACADEMIC INTEGRITY You are expected to exhibit honesty and use ethical behaviour in all aspects of the learning process. Academic credentials you earn are rooted in principles of honesty and academic integrity. It is your responsibility to understand what constitutes academic dishonesty.', 'vision are also of interest. 1.2 Language and machines Human children interacting with a rich multi modality world and various forms of feedback acquire language with exceptional sample efficiency not observing that much language and compute efficiency brains are efficient computing machines With all the impressive advances in NLP in the last decades we are still nowhere close to developing learning machines that have a fraction of acquisition ability of children. One fundamental and still quite open problem in building language learning machines is the question of representa tion how should we represent language in a computer such that the computer can robustly process and or generate it This is where this course focuses on the tools provided by deep learning a highly effec tive toolkit for representing both the wild variety of natural language and some of the rules and structures it sometimes adheres to. Much of this course will be dedicated to this question of representation and the rest of this note will talk about a basic subquestion how do we represent words Before that though let s briefly discuss some of the applications you can hope to build after learning modern NLP techniques.', '3. SUB TOPIC S Week 1 Overview Human language Word meanings Word2Vec and Optimization basics Week 2 Intro to neural networks Gradients and backpropagation Week 3 Recurrent Neural Networks and Language Models Week 4 Conditioned Generation Week 5 Part 1 Self attention and transformers Part 2 Seq2Seq models Week 6 Midterm Test Week 7 Midterm Recess No class Week 8 Prompting Instruction Finetuning and RLHF Week 9 Question Answering systems Week 10 Multimodal models Week 11 Project presentation and Q A Week 12 Project presentation and Q A Week 13 Project presentation and Q A Classes end April 10th 4. ASSESSMENT OF LEARNING Weight 4 Assignments 20 4 Quizzes 10 Midterm Test 35 Group Project 35 TOTAL 100Note that the percentage distribution of each module may be adjusted. Percentage grades will be converted to letter grades and grade points per the University calendar. 5.']",text-embedding-3-small,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999999999999996,1.0,0.9622961122871172,0.0769230769230769
126,How is the course grade distributed?,"The course grade is distributed as follows: 4 Assignments worth 20%, 4 Quizzes worth 10%, Midterm Test worth 35%, and Group Project worth 35%, making the total grade percentage 100%. Please note that the percentage distribution of each module may be adjusted. Percentage grades will be converted to letter grades and grade points per the University calendar.","['3. SUB TOPIC S Week 1 Overview Human language Word meanings Word2Vec and Optimization basics Week 2 Intro to neural networks Gradients and backpropagation Week 3 Recurrent Neural Networks and Language Models Week 4 Conditioned Generation Week 5 Part 1 Self attention and transformers Part 2 Seq2Seq models Week 6 Midterm Test Week 7 Midterm Recess No class Week 8 Prompting Instruction Finetuning and RLHF Week 9 Question Answering systems Week 10 Multimodal models Week 11 Project presentation and Q A Week 12 Project presentation and Q A Week 13 Project presentation and Q A Classes end April 10th 4. ASSESSMENT OF LEARNING Weight 4 Assignments 20 4 Quizzes 10 Midterm Test 35 Group Project 35 TOTAL 100Note that the percentage distribution of each module may be adjusted. Percentage grades will be converted to letter grades and grade points per the University calendar. 5.', 'For courses using such software students will be expected to submit their work electronically either directly to Turnitin.com or via an online learning platform e.g. A2L etc. using plagiarism detection a service supported by Turnitin.com so it can be checked for academic dishonesty. Students who do not wish their work to be submitted through the plagiarism detection software must inform the Instructor before the assignment is due. No penalty will be assigned to a student who does not submit work to the plagiarism detection software. All submitted work is subject to normal verification that standards of academicintegrity have been upheld e.g. on line search other software etc. . For more details about McMaster s use of Turnitin.com please go to www.mcmaster.ca academicintegrity. COURSES WITH AN ON LINE ELEMENT Some courses may use on line elements e.g. e mail Avenue to Learn A2L LearnLink web pages capa Moodle ThinkingCap etc. .', '. Students should be aware that when they access the electronic components of a course using these elements private information such as first and last names user names for the McMaster e mail accounts and program affiliation may become apparent to all other students in the same course. The available information is dependent on the technology used. Continuation in a course that uses on line elements will be deemed consent to this disclosure. If you have any questions or concerns about such disclosure please discuss this with the course instructor. ONLINE PROCTORING Some courses may use online proctoring software for tests and exams. This software may require students to turn on their video camera present identification monitor and record their computer activities and or lock restrict their browser or other applications software during tests or exams. This software may be required to be installed before the test exam begins. COMMUNICATIONS It is the student s responsibility to Maintain current contact information with the University including address phone numbers and emergency contact information. Use the University provided e mail address or maintain a valid forwarding e mail address.', 'Students should submit their request to their Faculty Office normally within 10 working days of the beginning of term in which they anticipate a need for accommodation or to the Registrar s Office prior to their examinations. Students should also contact their instructors as soon as possible to make alternative arrangements for classes assignments and tests. http www.mcmaster.ca policy Students AcademicStudies Studentcode.pdf COPYRIGHT AND RECORDING Students are advised that lectures demonstrations performances and any other course material provided by an instructor include copyright protected works. The Copyright Act and copyright law protect every original literary dramatic musical and artistic work including lectures by University instructors The recording of lectures tutorials or other methods of instruction may occur during a course. Recording may be done by either the instructor for the purpose of authorized distribution or by a student for the purpose of personal study. Students should be aware that their voice and or image may be recorded by others during the class. Please speak with the instructor if this is a concern for you.']",text-embedding-3-small,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999999999999996,1.0,0.956396062735634,0.0344827586206896
127,What is the policy for late submissions in this course?,"The late submission policy for this course is that there are NO free late days, and there is a penalty of 1 off course grade per day late. Assignments will not be accepted if they are more than 3 days late unless permission is given in advance.","['For courses using such software students will be expected to submit their work electronically either directly to Turnitin.com or via an online learning platform e.g. A2L etc. using plagiarism detection a service supported by Turnitin.com so it can be checked for academic dishonesty. Students who do not wish their work to be submitted through the plagiarism detection software must inform the Instructor before the assignment is due. No penalty will be assigned to a student who does not submit work to the plagiarism detection software. All submitted work is subject to normal verification that standards of academicintegrity have been upheld e.g. on line search other software etc. . For more details about McMaster s use of Turnitin.com please go to www.mcmaster.ca academicintegrity. COURSES WITH AN ON LINE ELEMENT Some courses may use on line elements e.g. e mail Avenue to Learn A2L LearnLink web pages capa Moodle ThinkingCap etc. .', 'Students should submit their request to their Faculty Office normally within 10 working days of the beginning of term in which they anticipate a need for accommodation or to the Registrar s Office prior to their examinations. Students should also contact their instructors as soon as possible to make alternative arrangements for classes assignments and tests. http www.mcmaster.ca policy Students AcademicStudies Studentcode.pdf COPYRIGHT AND RECORDING Students are advised that lectures demonstrations performances and any other course material provided by an instructor include copyright protected works. The Copyright Act and copyright law protect every original literary dramatic musical and artistic work including lectures by University instructors The recording of lectures tutorials or other methods of instruction may occur during a course. Recording may be done by either the instructor for the purpose of authorized distribution or by a student for the purpose of personal study. Students should be aware that their voice and or image may be recorded by others during the class. Please speak with the instructor if this is a concern for you.', 'An understanding of and ability to build systems in PyTorch for some of the major problems in NLP Word meaning dependency parsing machine translation question answeringCourse work and grading policy 5 4 Assignments 20 4 Quizzes 10 Midterm 35 Final Group Project 1 3 people 34 Late day policy NO free late days 1 off course grade per day late Assignments not accepted more than 3 days late unless given permission in advance Collaboration policy Understand allowed collaboration and how to document it Don t take code off the web acknowledge working with other students write your own assignment solutionsTrained on text data neural machine translation is quite good https kiswahili.tuko.co.keThe SEC said Musk your tweets are a blight. They really could cost you your job if you don t stop all this tweeting at night. Then Musk cried Why The tweets I wrote are not mean I don t use all caps and I m sure that my tweets are clean. But your tweets can move markets and that s why we re sore.', 'Departmental Policies The use of cell phones iPods laptops and other personal electronic devices are prohibited from the classroom during the class time unless the instructor makes an explicit exception. Announcements made in class or placed on Avenue are considered to have been communicated to all students including those individuals that are not in class. Instructor has the right to submit work to software to identify plagiarism. 3.']",text-embedding-3-small,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9748806789661276,1.0,0.962355592217676,0.0434782608695652
128,What does the course hope to teach regarding word vectors?,The course hopes to teach that word meaning can be represented rather well by a high dimensional vector of real numbers.,"['file name: lecture01 wordvecs1. Hamidreza Mahyar mahyarh@mcmaster.ca Computational Natural Language Processing Overview of NLP and Word VectorsLecture Plan 2 Lecture 1 Introduction and Word Vectors 1. The course 2. Human language and word meaning 3. Word2vec introduction 4. Word2vec objective function gradients 5. Optimization basics 6. Looking at word vectors Key learning today The astounding result that word meaning can be represented rather well by a high dimensional vector of real numbersCourse logistics in brief 3 Instructor Hamidreza Mahyar Head TA Reza Namazi Time Wednesday 3 30pm 6 30pm We put a lot of other important information on Avenue to Learn. Please read itWhat do we hope to teach A.k.a. learning goals 4 1.', '8 CBOW 100B 68.4 79.6 75.4 59.4 45.5 Table 6 Here we see the correlations be tween of word vector similarities using different embedding techniques with different human judgment datasets 2.6 Further Reading Dealing With Ambiguity One might wonder how we handle the situation where we want to capture the same word with different vectors for its different uses in natural language. For instance run is both a noun and a verb and is used and interpreted differently based on the context. Improving Word Representations Via Global Context And Multi ple Word Prototypes Huang et al 2012 describes how such cases can also be handled in NLP. The essence of the method is the following 1. Gather Ô¨Åxed size context windows of all occurrences of the word for instance 5 before and 5 after 2. Each context is represented by a weighted average of the context words vectors using idf weighting 3. Apply spherical k means to cluster these context representations. 4.', 'We believe that word vectors trained on even larger data sets with larger dimensionality will perform signiÔ¨Åcantly better and will enable the development of new innovative applications. Another way to improve accuracy is to provide more than one example of the relationship. By using ten examples instead of one to form the relationship vector we average the individual vectors together we have observed improvement of accuracy of our best models by about 10 absolutely on the semantic syntactic test. It is also possible to apply the vector operations to solve different tasks. For example we have observed good accuracy for selecting out of the list words by computing average vector for a list of words and Ô¨Ånding the most distant word vector. This is a popular type of problems in certain human intelligence tests. Clearly there is still a lot of discoveries to be made using these techniques. 6 Conclusion In this paper we studied the quality of vector representations of words derived by various models on a collection of syntactic and semantic language tasks.', 'However many of the details of word2vec will hold true in methods that we ll proceed to further in the course so we ll focus our time on that. 3.2 Word2vec model and objective The word2vec model represents each word in a fixed vocabulary as a low dimensional much smaller than vocabulary size vector. It learns the value of each word s vector to be predictive via a simple function of the distribution of words in a usually short 2 4 words context. The model we ll describe here is called the skipgram word2vec algo rithm. Skipgram word2vec. As usual we have a finite vocabulary V. Let C O be random variables representing an unknown pair of C V a center word and O V an outside word appearing in the context of the center word . We ll use c o to refer to specific values of the random variables.']",text-embedding-3-small,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.8935368210332378,1.0,0.964419553866905,0.0434782608695652
129,How does Word2vec represent word meaning?,"Word2vec represents each word in a fixed vocabulary as a low-dimensional vector that is much smaller than the vocabulary size. It learns the value of each word's vector to be predictive based on the distribution of words in a short context (usually 2-4 words). The model, specifically the skipgram word2vec algorithm, focuses on predicting the context words given a center word. By training on a large corpus, Word2vec captures semantic relationships between words based on how they co-occur in the text. 

Source: lecture01_notes_Introduction.txt","['As the word vectors are trained to predict the surrounding words in the sentence the vectors can be seen as representing the distribution of the context in which a word appears. These values are related logarithmically to the probabilities computed by the output layer so the sum of two word vectors is related to the product of the two context distributions. The product works here as the AND function words that are assigned high probabilities by both word vectors will have high probability and the other words will have low probability. Thus if Volga River appears frequently in the same sentence together with the words Russian and river the sum of these two word vectors will result in such a feature vector that is close to the vector of Volga River . 6 Comparison to Published Word Representations Many authors who previously worked on the neural network based representations of words have published their resulting models for further use and comparison amongst the most well known au thors are Collobert and Weston 2 Turian et al. 17 and Mnih and Hinton 10 .', 'However many of the details of word2vec will hold true in methods that we ll proceed to further in the course so we ll focus our time on that. 3.2 Word2vec model and objective The word2vec model represents each word in a fixed vocabulary as a low dimensional much smaller than vocabulary size vector. It learns the value of each word s vector to be predictive via a simple function of the distribution of words in a usually short 2 4 words context. The model we ll describe here is called the skipgram word2vec algo rithm. Skipgram word2vec. As usual we have a finite vocabulary V. Let C O be random variables representing an unknown pair of C V a center word and O V an outside word appearing in the context of the center word . We ll use c o to refer to specific values of the random variables.', 'More recently a square root type transformation in the form of Hellinger PCA HPCA Lebret and Collobert 2014 has been suggested as an effec tive way of learning word representations. Shallow Window Based Methods. Another approach is to learn word representations that aid in making predictions within local context win dows. For example Bengio et al. 2003 intro duced a model that learns word vector representa tions as part of a simple neural network architec ture for language modeling. Collobert and Weston 2008 decoupled the word vector training from the downstream training objectives which paved the way for Collobert et al. 2011 to use the full context of a word for learning the word represen tations rather than just the preceding context as is the case with language models. Recently the importance of the full neural net work structure for learning useful word repre sentations has been called into question.', 'Recently Mikolov et al. 2013c introduced a new evalua tion scheme based on word analogies that probes the Ô¨Åner structure of the word vector space by ex amining not the scalar distance between word vec tors but rather their various dimensions of dif ference. For example the analogy king is to queen as man is to woman should be encoded in the vector space by the vector equation king queen man woman. This evaluation scheme favors models that produce dimensions of mean ing thereby capturing the multi clustering idea of distributed representations Bengio 2009 . The two main model families for learning word vectors are 1 global matrix factorization meth ods such as latent semantic analysis LSA Deer wester et al. 1990 and 2 local context window methods such as the skip gram model of Mikolov et al. 2013c . Currently both families suffer sig niÔ¨Åcant drawbacks.']",text-embedding-3-small,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9410424731100469,0.8333333333333334,0.9052935517972108,0.0434782608695652
130,What is the main focus of this lecture on neural networks?,"The main focus of this lecture on neural networks is on backpropagation, which is a technique used for training neural networks by efficiently applying the chain rule to compute gradients and update parameters. It also covers the basics of neural networks, neuron units, non-linear decision boundaries, and the importance of understanding the details of gradients for debugging and improving models. Additionally, the lecture introduces practical tips and tricks for training neural networks and motivates the use of recurrent neural networks as a language model.","['This is the prowess of neural networks. Fun Fact Neural networks are biologically in spired classiÔ¨Åers which is why they are often called artiÔ¨Åcial neural networks to distinguish them from the organic kind. However in reality human neural networks are so much more capable and complex from artiÔ¨Åcial neural net works that it is usually better to not draw too many parallels between the two. We established in our previous discussions the need for non linear classiÔ¨Åers since most data are not linearly separable and thus our classiÔ¨Åcation performance on them is limited. Neural networks are a family of classiÔ¨Åers with non linear decision boundary as seen in Figure 1. Now that we know the sort of decision boundaries neural networks create let us see how they manage doing so. 1.1 A Neuron A neuron is a generic computational unit that takes n inputs and produces a single output. What differentiates the outputs of different neurons is their parameters also referred to as their weights .', 'everything doing this everywhere was the key test Now much less needed you can use it to check layers are correctly implemented 78Summary We ve mastered the core technology of neural nets Backpropagation recursively and hence efficiently apply the chain rule along computation graph downstream gradient upstream gradient x local gradient Forward pass compute results of operations and save intermediate values Backward pass apply chain rule to compute gradients 79Why learn all these details about gradients 80 Modern deep learning frameworks compute gradients for you But why take a class on compilers or systems when they are implemented for you Understanding what is going on under the hood is useful Backpropagation doesn t always work perfectly out of the box Understanding why is crucial for debugging and improving models See Karpathy article https medium.com @karpathy yes you should understand backprop e2f06eab496b Example in future lecture exploding and vanishing gradients', 'file name: lecture03 RNNs and LLMs. Hamidreza Mahyar mahyarh@mcmaster.ca Computational Natural Language Processing Language Models and Recurrent Neural NetsLecture Plan 1. A bit more about neural networks Language modeling RNNs 2. A new NLP task Language Modeling 4. Problems with RNNs 5. Recap on RNNs LMs Reminders You should have started working on Assignment 1 2 motivates 3. A new family of neural networks Recurrent Neural Networks RNNs This is the most important concept in the class It leads to GPT 3 and ChatGPT Important and used in Ass1 but not the only way to build LMs0 model power Training error Test error Classic view Regularization works to prevent overfitting when we have a lot of features or later a very powerful deep model etc.', 'file name: lecture02 notes Backpropagation.  Neural networks. Forward computation. Backward propagation. Neuron Units. Max margin Loss. Gradient checks. Xavier parameter initialization. Learning rates. Adagrad. This set of notes introduces single and multilayer neural networks and how they can be used for classiÔ¨Åcation purposes. We then dis cuss how they can be trained using a distributed gradient descent technique known as backpropagation. We will see how the chain rule can be used to make parameter updates sequentially. After a rigorous mathematical discussion of neural networks we will discuss some practical tips and tricks in training neural networks involving neuron units non linearities gradient checks Xavier parameter ini tialization learning rates Adagrad etc. Lastly we will motivate the use of recurrent neural networks as a language model. 1 Neural Networks Foundations Figure 1 We see here how a non linear decision boundary separates the data very well. This is the prowess of neural networks.']",text-embedding-3-small,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.0000000000000002,0.8333333333333334,0.9329961916333837,0.0357142857142857
131,What are the non-linear functions mentioned for neural networks?,"The non-linear functions mentioned for neural networks include logistic sigmoid, tanh, hard tanh, Swish, Rectified Linear Unit (ReLU), Leaky ReLU, Parametric ReLU, GELU. These non-linear functions are essential for neural networks to approximate complex functions and enable them to perform tasks like regression or classification effectively. 

Source: Lecture02_Neural_Nets.txt","['Non linearities old and new logistic sigmoid tanh hard tanh 1 0 1 1 Swish arXiv 1710.05941 swish ùë• ùë•. logistic ùë• Rectified Linear Unit ReLU ReLU ùëß max ùëß 0 Leaky ReLU Parametric ReLU 0 0 0 GELU arXiv 1606.08415 GELU ùë• ùë•. ùëÉùëã ùë• ùëã ùëÅ 0 1 ùë•. logistic 1.702ùë•Non linearities i.e. f on previous slide Why they re needed Neural networks do function approximation e.g. regression or classification Without non linearities deep neural networks can t do anything more than a linear transform Extra layers could just be compiled down into a single linear transform W1 W2 x Wx But with more layers that include non linearities they can approximate any complex function 5Training with cross entropy loss you use this in', 'This is the prowess of neural networks. Fun Fact Neural networks are biologically in spired classiÔ¨Åers which is why they are often called artiÔ¨Åcial neural networks to distinguish them from the organic kind. However in reality human neural networks are so much more capable and complex from artiÔ¨Åcial neural net works that it is usually better to not draw too many parallels between the two. We established in our previous discussions the need for non linear classiÔ¨Åers since most data are not linearly separable and thus our classiÔ¨Åcation performance on them is limited. Neural networks are a family of classiÔ¨Åers with non linear decision boundary as seen in Figure 1. Now that we know the sort of decision boundaries neural networks create let us see how they manage doing so. 1.1 A Neuron A neuron is a generic computational unit that takes n inputs and produces a single output. What differentiates the outputs of different neurons is their parameters also referred to as their weights .', '3.5 Non linear ClassiÔ¨Åers Figure 9 Here we see that many exam ples are wrongly classiÔ¨Åed even though the best linear decision boundary is chosen. This is due linear decision boundaries have limited model capacity for this dataset. We now introduce the need for non linear classiÔ¨Åcation models such as neural networks. We see in Figure 9 that a linear classiÔ¨Åer mis classiÔ¨Åes many datapoints. Using a non linear decision boundary as shown in Figure 10 we manage to classify all training points accu rately. Although oversimpliÔ¨Åed this is a classic case demonstrating the need for non linear decision boundaries. In the next set of notes we study neural networks as a class of non linear models that have performed particularly well in deep learning applications. Figure 10 Here we see that the non linear decision boundary allows for much better classiÔ¨Åcation of datapoints.', 'One of the most popular choices for neurons is the sigmoid or binary lo gistic regression unit. This unit takes an n dimensional input vector x and produces the scalar activation output a. This neuron is also associated with an n dimensional weight vector w and a bias scalar b. The output of this neuron is then a 1 1 exp wTx b We can also combine the weights and bias term above to equiva Neuron A neuron is the fundamental building block of neural networks. We will see that a neuron can be one of many functions that allows for non linearities to accrue in the network. lently formulate a 1 1 exp wT b x 1 Figure 2 This image captures how in a sigmoid neuron the input vector x is Ô¨Årst scaled summed added to a bias unit and then passed to the squashing sigmoid function. This formulation can be visualized in the manner shown in Fig ure 2.']",text-embedding-3-small,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.990267515898372,1.0,0.9555473551689352,0.037037037037037
132,What is the purpose of non-linearities in neural networks?,"The purpose of non-linearities in neural networks is to enable them to approximate any complex function. Without non-linearities, deep neural networks can only perform a linear transform, limiting their ability to learn and represent complex patterns in data. By introducing non-linearities, neural networks can model and approximate more intricate relationships within the data, allowing for better performance in tasks such as regression or classification.","['Non linearities old and new logistic sigmoid tanh hard tanh 1 0 1 1 Swish arXiv 1710.05941 swish ùë• ùë•. logistic ùë• Rectified Linear Unit ReLU ReLU ùëß max ùëß 0 Leaky ReLU Parametric ReLU 0 0 0 GELU arXiv 1606.08415 GELU ùë• ùë•. ùëÉùëã ùë• ùëã ùëÅ 0 1 ùë•. logistic 1.702ùë•Non linearities i.e. f on previous slide Why they re needed Neural networks do function approximation e.g. regression or classification Without non linearities deep neural networks can t do anything more than a linear transform Extra layers could just be compiled down into a single linear transform W1 W2 x Wx But with more layers that include non linearities they can approximate any complex function 5Training with cross entropy loss you use this in', 'This is the prowess of neural networks. Fun Fact Neural networks are biologically in spired classiÔ¨Åers which is why they are often called artiÔ¨Åcial neural networks to distinguish them from the organic kind. However in reality human neural networks are so much more capable and complex from artiÔ¨Åcial neural net works that it is usually better to not draw too many parallels between the two. We established in our previous discussions the need for non linear classiÔ¨Åers since most data are not linearly separable and thus our classiÔ¨Åcation performance on them is limited. Neural networks are a family of classiÔ¨Åers with non linear decision boundary as seen in Figure 1. Now that we know the sort of decision boundaries neural networks create let us see how they manage doing so. 1.1 A Neuron A neuron is a generic computational unit that takes n inputs and produces a single output. What differentiates the outputs of different neurons is their parameters also referred to as their weights .', 'SpeciÔ¨Åcally it introduces a special set of units called LSTM units which are linear and have a recurrent connection to itself which is Ô¨Åxed to 1. The Ô¨Çow of information into the unit and from the unit is guarded by an input and output gates their behaviour is learned . There are several variations of this basic structure. This solution does not address explicitly the exploding gradients problem. Sutskever et al. 2011 use the Hessian Free opti mizer in conjunction with structural damping a spe ciÔ¨Åc damping strategy of the Hessian. This approach seems to deal very well with the vanishing gradient though more detailed analysis is still missing. Pre sumably this method works because in high dimen sional spaces there is a high probability for long term components to be orthogonal to short term ones. This would allow the Hessian to rescale these components independently. In practice one can not guarantee that this property holds.', '2 Background A feed forward neural network is a non linear mapping from a input pattern x to an output vector y. Consider the lth hidden layer in a deep feed forward neural network and let al be the vector representation of the summed inputs to the neurons in that layer. The summed inputs are computed through a linear projection with the weight matrix W l and the bottom up inputs hl given as follows al i wl i hl hl 1 i f al i bl i 1 where f is an element wise non linear function and wl i is the incoming weights to the ith hidden units and bl i is the scalar bias parameter. The parameters in the neural network are learnt using gradient based optimization algorithms with the gradients being computed by back propagation. One of the challenges of deep learning is that the gradients with respect to the weights in one layer are highly dependent on the outputs of the neurons in the previous layer especially if these outputs change in a highly correlated way. Batch normalization Ioffe and Szegedy 2015 was proposed to reduce such undesirable covariate shift .']",text-embedding-3-small,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.0000000000000004,0.8,0.932399954989905,0.037037037037037
133,What is cross entropy loss used for in PyTorch?,"Cross entropy loss is used in PyTorch to minimize the negative log probability of the correct class, which is equivalent to maximizing the probability of the correct class. It is a concept from information theory where the cross entropy is calculated between the true probability distribution (p) and the computed model probability (q). It is commonly used as the loss function in PyTorch for tasks like classification. Cross entropy can be used to measure the difference between two probability distributions and is particularly useful when the actual probability distribution generating the data is unknown, allowing us to use a model approximation instead. It serves as a way to train neural networks effectively by adjusting the model parameters to minimize this loss.

Source: lecture02_Neural_Nets.txt","['with cross entropy loss you use this in PyTorch Until now our objective was stated as to maximize the probability of the correct class y or equivalently we can minimize the negative log probability of that class Now restated in terms of cross entropy a concept from information theory Let the true probability distribution be p let our computed model probability be q The cross entropy is Assuming a ground truth or true or gold or target probability distribution that is 1 at the right class and 0 everywhere else p 0 0 1 0 0 then Because of one hot p the only term left is the negative log probability of the true class yi log ùëù ùë¶i ùë•i Cross entropy can be used in other ways with a more interesting p but for now just know that you ll want to use it as the loss in PyTorch 6Remember Stochastic Gradient Descent Update equation i.e. for each parameter In deep learning ùúÉincludes the data representation e.g.', 'Now we are ready to introduce cross entropy. The cross entropy is useful when cross entropy we don t know the actual probability distribution p that generated some data. It allows us to use some m which is a model of p i.e. an approximation to p . The cross entropy of m on p is deÔ¨Åned by H p m lim n 1 n X W L p w1 ... wn logm w1 ... wn 3.50 That is we draw sequences according to the probability distribution p but sum the log of their probabilities according to m. Again following the Shannon McMillan Breiman theorem for a stationary er godic process H p m lim n 1 n logm w1w2 ...wn 3.51 This means that as for entropy we can estimate the cross entropy of a model m on some distribution p by taking a single sequence that is long enough instead of summing over all possible sequences. What makes the cross entropy useful is that the cross entropy H p m is an up per bound on the entropy H p .', 'For any model m H p H p m 3.52 This means that we can use some simpliÔ¨Åed model m to help estimate the true en tropy of a sequence of symbols drawn according to probability p. The more accurate m is the closer the cross entropy H p m will be to the true entropy H p . Thus the difference between H p m and H p is a measure of how accurate a model is. Between two models m1 and m2 the more accurate model will be the one with the lower cross entropy. The cross entropy can never be lower than the true entropy so a model cannot err by underestimating the true entropy. We are Ô¨Ånally ready to see the relation between perplexity and cross entropy as we saw it in Eq. 3.51. Cross entropy is deÔ¨Åned in the limit as the length of the observed word sequence goes to inÔ¨Ånity. We will need an approximation to cross entropy relying on a sufÔ¨Åciently long sequence of Ô¨Åxed length.', 'Non linearities old and new logistic sigmoid tanh hard tanh 1 0 1 1 Swish arXiv 1710.05941 swish ùë• ùë•. logistic ùë• Rectified Linear Unit ReLU ReLU ùëß max ùëß 0 Leaky ReLU Parametric ReLU 0 0 0 GELU arXiv 1606.08415 GELU ùë• ùë•. ùëÉùëã ùë• ùëã ùëÅ 0 1 ùë•. logistic 1.702ùë•Non linearities i.e. f on previous slide Why they re needed Neural networks do function approximation e.g. regression or classification Without non linearities deep neural networks can t do anything more than a linear transform Extra layers could just be compiled down into a single linear transform W1 W2 x Wx But with more layers that include non linearities they can approximate any complex function 5Training with cross entropy loss you use this in']",text-embedding-3-small,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9900192016286976,1.0,0.959938700365386,0.05
134,What is the significance of the backpropagation algorithm?,"The significance of the backpropagation algorithm lies in its ability to efficiently calculate loss gradients for parameters in a neural network by utilizing the chain rule of differentiation. This allows for updating parameters using gradient descent or its variants, which is crucial for optimizing the network's performance during training. Backpropagation enables the network to learn from data, reduce overfitting, and achieve higher performance by propagating errors backward through the network layers, adjusting weights to minimize the loss function. It essentially allows for the network to iteratively improve its predictions by adjusting the parameters based on the calculated gradients. Additionally, backpropagation enables the training of deep neural networks by efficiently updating the weights through the layers, making it a fundamental algorithm in modern machine learning and artificial intelligence.

Source: Lecture02_notes_Backpropagation","['The result is that the network typically learns more meaningful informa tion from the data is less likely to overÔ¨Åt and usually obtains higher performance overall on the task at hand. One intuitive reason why this technique should be so effective is that what dropout is doing is essentially doing is training exponentially many smaller networks at once and averaging over their predictions. Dropout applied to an artiÔ¨Åcial neural network. Image credits to Srivastava et al. In practice the way we introduce dropout is that we take the out put h of each layer of neurons and keep each neuron with prob ability p and else set it to 0. Then during back propagation we only pass gradients through neurons that were kept alive during the forward pass. Finally during testing we compute the forward pass using all of the neurons in the network.', 'No parameter updates are necessary if the cost is 0. Since we typically update parameters using gradient descent or a variant such as SGD we typically need the gradient information for any parameter as required in the update equation Œ∏ t 1 Œ∏ t Œ± Œ∏ t J Backpropagation is technique that allows us to use the chain rule of differentiation to calculate loss gradients for any parameter used in the feed forward computation on the model. To understand this further let us understand the toy network shown in Figure 5 for which we will perform backpropagation. Figure 5 This is a 4 2 1 neural network where neuron j on layer k receives input z k j and produces activation output a k j . Here we use a neural network with a single hidden layer and a single unit output. Let us establish some notation that will make it easier to generalize this model later xi is an input to the neural network. s is the output of the neural network. Each layer including the input and output layers has neurons which receive an input and produce an output.', 'The intuition is that to get a better approximation of the derivative slope around a point we need to examine the func tion f s behavior both to the left and right of that point. It can also be shown using Taylor s theorem that the centered difference formula has an error proportional to œµ2 which is quite small whereas the derivative deÔ¨Ånition is more error prone. Now a natural question you might ask is if this method is so pre cise why do we not use it to compute all of our network gradients instead of applying back propagation The simple answer as hinted earlier is inefÔ¨Åciency recall that every time we want to compute the gradient with respect to an element we need to make two forward passes through the network which will be computationally expen sive. Furthermore many large scale neural networks can contain millions of parameters and computing two passes per parameter is clearly not optimal. And since in optimization techniques such as SGD we must compute the gradients once per iteration for several thousands of iterations it is obvious that this method quickly grows intractable.', 'essentially the error propagating backwards from the i th neu ron in layer 2. a 1 j is an input fed to i th neuron in layer 2 when scaled by Wij. Figure 6 This subnetwork shows the relevant parts of the network required to update W 1 ij Let us discuss the error sharing distribution interpretation of backpropagation better using Figure 6 as an example. Say we were to update W 1 14 1. We start with the an error signal of 1 propagating backwards from a 3 1 . 2. We then multiply this error by the local gradient of the neuron which maps z 3 1 to a 3 1 . This happens to be 1 in this case and thus the error is still 1. This is now known as Œ¥ 3 1 1. 3. At this point the error signal of 1 has reached z 3 1 .']",text-embedding-3-small,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.000000000000001,1.0,0.9504238773918652,0.0344827586206896
135,What does the Jacobian matrix represent in the context of neural networks?,"The Jacobian matrix in the context of neural networks represents a generalization of the gradient. It is an m x n matrix of partial derivatives, where m is the number of outputs and n is the number of inputs of a function. It helps in computing how small changes in inputs affect the outputs of the function. The Jacobian matrix is crucial for understanding the sensitivity of the outputs to changes in inputs in neural network computations. 

Source: Lecture02_Neural_Nets.txt","['So its Jacobian will be n m. Note that zi m X k 1 Wikxk So an entry z x ij of the Jacobian will be z x ij zi xj xj m X k 1 Wikxk m X k 1 Wik xj xk Wij because xj xk 1 if k j and 0 if otherwise. So we see that z x W 2 Row vector times matrix with respect to the row vector z xW what is z x A computation similar to 1 shows that z x W T . 3 A vector with itself z x what is z x We have zi xi. So z x ij zi xj xj xi 1 if i j 0 if otherwise So we see that the Jacobian z x is a diagonal matrix where the entry at i i is 1. This is just the identity matrix z x I . When applying the chain 2rule this term will disappear because a matrix or vector multiplied by the identity matrix does not change.', '21Other Jacobians Compute these at home for practice 22Back to our Neural Net x xmuseums xin xParis xare xamazing 23Back to our Neural Net Let s find Really we care about the gradient of the loss Jt but we will compute the gradient of the score for simplicity x xmuseums xin xParis xare xamazing 241. Break up equations into simple pieces Carefully define your variables and keep track of their dimensionality 252. Apply the chain rule 262. Apply the chain rule 272. Apply the chain rule 282. Apply the chain rule 293. Write out the Jacobians Useful Jacobians from previous slide 303. Write out the Jacobians ùíñùëá Useful Jacobians from previous slide 313. Write out the Jacobians ùíñùëá Useful Jacobians from previous slide 323. Write out the Jacobians ùíñùëá Useful Jacobians from previous slide 333.', '013 64.48Gradients Given a function with 1 output and n inputs Its gradient is a vector of partial derivatives with respect to each input 11Jacobian Matrix Generalization of the Gradient Given a function with m outputs and n inputs It s Jacobian is an m x n matrix of partial derivatives 12Chain Rule For composition of one variable functions multiply derivatives For multiple variables functions multiply Jacobians 13Example Jacobian Elementwise activation Function 14Example Jacobian Elementwise activation Function Function has n outputs and n inputs n by n Jacobian 15Example Jacobian Elementwise activation Function 16Example Jacobian Elementwise activation Function 17Example Jacobian Elementwise activation Function 18Other Jacobians Compute these at home for practice Check your answers with the lecture notes 19Other Jacobians Compute these at home for practice Check your answers with the lecture notes 20Other Jacobians Compute these at home for practice Check your answers with the lecture notes Fine print This is the correct Jacobian. Later we discuss the shape convention using it the answer would be h.', 'Write out the Jacobians ùíñùëá ùíñùëá Useful Jacobians from previous slide . 34 Hadamard product element wise multiplication of 2 vectors to give vectorRe using Computation Suppose we now want to compute Using the chain rule again 35Re using Computation Suppose we now want to compute Using the chain rule again The same Let s avoid duplicated computation 36Re using Computation Suppose we now want to compute Using the chain rule again ùõøis the upstream gradient error signal 37 ùíñùëáDerivative with respect to Matrix Output shape What does look like 1 output nm inputs 1 by nm Jacobian Inconvenient to then do 38Derivative with respect to Matrix Output shape What does look like 1 output nm inputs 1 by nm Jacobian Inconvenient to then do Instead we leave pure math and use the shape convention the shape of the gradient is the shape of the parameters So is n by m 39Derivative with respect to Matrix What is is going to be in our answer The other term should']",text-embedding-3-small,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999987372256388,1.0,0.9306658796347352,0.0476190476190476
136,How is the derivative with respect to a matrix described?,"The derivative with respect to a matrix is described using the Jacobian matrix, which allows us to apply the chain rule to a vector-valued function by multiplying Jacobians. This approach helps in computing gradients efficiently for functions that involve matrices.","['. f1 xn . . . ... . . . fm x1 . . . fm xn That is f x ij fi xj which is just a standard non vector derivative . The Jacobian matrix will be useful for us because we can apply the chain rule to a vector valued function just by multiplying Jacobians. As a little illustration of this suppose we have a function f x f1 x f2 x taking a scalar to a vector of size 2 and a function g y g1 y1 y2 g2 y1 y2 taking a vector of size two to a vector of size two. Now let s compose them to get g x g1 f1 x f2 x g2 f1 x f2 x .', '4 An elementwise function applied a vector z f x what is z x Since f is being applied elementwise we have zi f xi . So z x ij zi xj xj f xi f xi if i j 0 if otherwise So we see that the Jacobian z x is a diagonal matrix where the entry at i i is the derivative of f applied to xi. We can write this as z x diag f x . Since multiplication by a diagonal matrix is the same as doing elementwise multiplication by the diagonal we could also write f x when applying the chain rule. 5 Matrix times column vector with respect to the matrix z W x Œ¥ J z what is J W J z z W Œ¥ z W This is a bit more complicated than the other identities. The reason for in cluding J z in the above problem formulation will become clear in a moment.', 'First suppose we have a loss function J a scalar and are computing its gradient with respect to a matrix W Rn m. Then we could think of J as a function of W taking nm inputs the entries of W to a single output J . This means the Jacobian J W would be a 1 nm vector. But in practice this is not a very useful way of arranging the gradient. It would be much nicer if the derivatives were in a n m matrix like this J W J W11 . . . J W1m . . . ... . . . J Wn1 . . . J Wnm Since this matrix has the same shape as W we could just subtract it times the learning rate from W when doing gradient descent. So in a slight abuse of notation let s Ô¨Ånd this matrix as J W instead. This way of arranging the gradients becomes complicated when computing z W . Unlike J z is a vector.', 'Using the regular chain rule we can compute the derivative of g as the Jacobian g x xg1 f1 x f2 x xg2 f1 x f2 x g1 f1 f1 x g1 f2 f2 x g2 f1 f1 x g2 f2 f2 x 1And we see this is the same as multiplying the two Jacobians g x g f f x g1 f1 g1 f2 g2 f1 g2 f2 f1 x f2 x 3 Useful Identities This section will now go over how to compute the Jacobian for several simple functions. It will provide some useful identities you can apply when taking neu ral network gradients. 1 Matrix times column vector with respect to the column vector z W x what is z x Suppose W Rn m. Then we can think of z as a function of x taking an m dimensional vector to an n dimensional vector.']",text-embedding-3-small,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9724758581468964,1.0,0.8988479685811289,0.0434782608695652
137,What role does automatic differentiation play in modern deep learning frameworks?,"Automatic differentiation plays a crucial role in modern deep learning frameworks as it allows these frameworks to efficiently compute gradients automatically for complex neural network architectures. This capability simplifies the process of training deep learning models by automatically calculating the gradients needed for optimization algorithms like stochastic gradient descent. By handling the differentiation process automatically, deep learning practitioners can focus more on designing and experimenting with neural network architectures without having to manually derive and implement the gradients for each operation in the network.

Source: lecture02_Neural_Nets.txt","['Non differentiable points arise because we use a hard transfer function 5 and because we use a max layer 7 in the sentence approach network. Fortunately stochastic gradient still converges to a meaningful local minimum despite such minor differentiability problems Bottou 1991 1998 . Stochastic gradient iterations that hit a non differentiability are simply skipped. Remark 6 Modular Approach The well known back propagation algorithm LeCun 1985 Rumelhart et al. 1986 computes gradients using the chain rule. The chain rule can also be used in a modular implementation.11 Our modules correspond to the boxes in Figure 1 and Figure 2. Given derivatives with respect to its outputs each module can independently compute derivatives with respect to its inputs and with respect to its trainable parameters as proposed by Bottou and Gallinari 1991 . This allows us to easily build variants of our networks. For details about gradient computations see Appendix A. Remark 7 Tricks Many tricks have been reported for training neural networks LeCun et al.', 'everything doing this everywhere was the key test Now much less needed you can use it to check layers are correctly implemented 78Summary We ve mastered the core technology of neural nets Backpropagation recursively and hence efficiently apply the chain rule along computation graph downstream gradient upstream gradient x local gradient Forward pass compute results of operations and save intermediate values Backward pass apply chain rule to compute gradients 79Why learn all these details about gradients 80 Modern deep learning frameworks compute gradients for you But why take a class on compilers or systems when they are implemented for you Understanding what is going on under the hood is useful Backpropagation doesn t always work perfectly out of the box Understanding why is crucial for debugging and improving models See Karpathy article https medium.com @karpathy yes you should understand backprop e2f06eab496b Example in future lecture exploding and vanishing gradients', '2507COLLOBERT WESTON BOTTOU KARLEN KAVUKCUOGLU AND KUKSA 3.4.3 STOCHASTIC GRADIENT Maximizing 8 with stochastic gradient Bottou 1991 is achieved by iteratively selecting a random example x y and making a gradient step Œ∏ Œ∏ Œª log p y x Œ∏ Œ∏ 16 where Œª is a chosen learning rate. Our neural networks described in Figure 1 and Figure 2 are a succession of layers that correspond to successive composition of functions. The neural network is Ô¨Ånally composed with the word level log likelihood 11 or successively composed in the re cursion 14 if using the sentence level log likelihood 13 . Thus an analytical formulation of the derivative 16 can be computed by applying the differentiation chain rule through the network and through the word level log likelihood 11 or through the recurrence 14 . Remark 5 Differentiability Our cost functions are differentiable almost everywhere.', 'Another approach is to allow the learning rate to decrease over time such that Œ± t Œ±0œÑ max t œÑ In the above scheme Œ±0 is a tunable parameter and represents the starting learning rate. œÑ is also a tunable parameter and represents the time at which the learning rate should start reducing. In practice this method has been found to work quite well. In the next section we discuss another method for adaptive gradient descent which does not require hand set learning rates. 2.8 Momentum Updates Momentum methods a variant of gradient descent inspired by the study of dynamics and motion in physics attempt to use the veloc ity of updates as a more effective update scheme. Pseudocode for momentum updates is shown below Snippet 2.2 Computes a standard momentum update on parameters x v mu v alpha grad_x x v 2.9 Adaptive Optimization Methods AdaGrad is an implementation of standard stochastic gradient de scent SGD with one key difference the learning rate can vary for each parameter.']",text-embedding-3-small,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.0000000000000002,1.0,0.9135256653749226,0.0
138,What is the purpose of manual gradient checking?,"The purpose of manual gradient checking is to verify the correctness of analytic gradients quickly, as they are much quicker to compute compared to numerical gradients. This helps ensure that the gradients are calculated accurately and can be used in training machine learning models effectively. 

Source: lecture02_notes_Backpropagation.txt","['This inefÔ¨Åciency is why we only use gradient check to verify the correctness of our analytic gradients which are much quicker to compute. A standard implementation of gradient check is shown below Snippet 2.1 def eval_numerical_gradient f x a naive implementation of numerical gradient of f at x f should be a function that takes a single argument x is the point numpy array to evaluate the gradient at fx f x evaluate function value at original point grad np.zeros x.shape h 0.00001 iterate over all indexes in x it np.nditer x flags multi_index op_flags readwrite while not it.finished evaluate function at x h ix it.multi_index old_value x ix x ix old_value h increment by h fxh_left f x evaluate f x h x ix old_value h decrement by h fxh_right f x evaluate f x h x ix old_value restore to previous value very important compute the partial derivative grad ix fxh_left fxh_right 2 h the slope it.iternext step to next dimension return grad 2.', 'We can compute them as follows Œ¥1 J Œ∏ ÀÜ y y T this is just identity 7 Œ¥2 J z J Œ∏ Œ∏ h h z using the chain rule Œ¥1 Œ∏ h h z substituting in Œ¥1 Œ¥1 U h z using identity 1 Œ¥1 U ReLU z using identity 4 Œ¥1 U sgn h we computed this earlier A good way of checking our work is by looking at the dimensions of the Jaco bians J z Œ¥1 U sgn h 1 Dh 1 Nc Nc Dh Dh 6We see that the dimensions of all the terms in the gradient match up i.e. the number of columns in a term equals the number of rows in the next term . This will always be the case if we computed our gradients correctly. Now we can use the error terms to compute our gradients. Note that we trans pose out answers when computing the gradients for column vectors terms to follow the shape convention.', 'Briefly the gradient think derivative U f of a scalar function f with respect to a parameter matrix U represents the direction to locally move U in in order to maximally increase the value of f. So in practice we do something like drawing the initial U 0 and V 0 randomly as U V N 0 0.001 V d matrices of independent draws from a zero centered normal distribution with small variance and then perform some number of iterations of the following process U i 1 U i Œ± UL U i V i . 7 This should be read as setting the value of U at iteration i 1 as the value of U at the previous iteration plus a small Œ± small step in the direction that locally best improves U with respect to the objective L U V we specified in Eqn 6. Stochastic gradients. There s a crucial detail remaining here beyond how to compute the gradient function U which we ll discuss later computing L U V is exceptionally expensive as it walks over the entire dataset.', 'Some examples if you want practice dot product of two vectors elementwise product of two vectors 2 norm of a vector. Feel free to use these identities in the assignments. One option is just to memorize them. Another option is to Ô¨Ågure them out by looking at the dimensions. For example only one ordering orientation of Œ¥ and x will produce the correct shape for J W assuming W is not square . 4 Gradient Layout Jacobean formulation is great for applying the chain rule you just have to mul tiply the Jacobians. However when doing SGD it s more convenient to follow the convention the shape of the gradient equals the shape of the parameter as we did when computing J W . That way subtracting the gradient times the learning rate from the parameters is easy. We expect answers to homework questions to follow this convention. Therefore if you compute the gradient of a column vector using Jacobian formulation you should take the transpose when reporting your Ô¨Ånal answer so the gradient is a column vector. Another option is to always follow the convention.']",text-embedding-3-small,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999999999999996,0.5,0.9309474617565372,0.0454545454545454
139,Why is understanding gradients important despite automatic differentiation?,"Understanding gradients is important despite automatic differentiation because backpropagation doesn't always work perfectly out of the box, and knowing why is crucial for debugging and improving models. It helps in understanding what is going on under the hood, which can be useful for optimizing neural networks effectively. Source: SEP 775 - Computational Natural Language Processing, lecture02_Neural_Nets.txt","['everything doing this everywhere was the key test Now much less needed you can use it to check layers are correctly implemented 78Summary We ve mastered the core technology of neural nets Backpropagation recursively and hence efficiently apply the chain rule along computation graph downstream gradient upstream gradient x local gradient Forward pass compute results of operations and save intermediate values Backward pass apply chain rule to compute gradients 79Why learn all these details about gradients 80 Modern deep learning frameworks compute gradients for you But why take a class on compilers or systems when they are implemented for you Understanding what is going on under the hood is useful Backpropagation doesn t always work perfectly out of the box Understanding why is crucial for debugging and improving models See Karpathy article https medium.com @karpathy yes you should understand backprop e2f06eab496b Example in future lecture exploding and vanishing gradients', 'Non differentiable points arise because we use a hard transfer function 5 and because we use a max layer 7 in the sentence approach network. Fortunately stochastic gradient still converges to a meaningful local minimum despite such minor differentiability problems Bottou 1991 1998 . Stochastic gradient iterations that hit a non differentiability are simply skipped. Remark 6 Modular Approach The well known back propagation algorithm LeCun 1985 Rumelhart et al. 1986 computes gradients using the chain rule. The chain rule can also be used in a modular implementation.11 Our modules correspond to the boxes in Figure 1 and Figure 2. Given derivatives with respect to its outputs each module can independently compute derivatives with respect to its inputs and with respect to its trainable parameters as proposed by Bottou and Gallinari 1991 . This allows us to easily build variants of our networks. For details about gradient computations see Appendix A. Remark 7 Tricks Many tricks have been reported for training neural networks LeCun et al.', 'Briefly the gradient think derivative U f of a scalar function f with respect to a parameter matrix U represents the direction to locally move U in in order to maximally increase the value of f. So in practice we do something like drawing the initial U 0 and V 0 randomly as U V N 0 0.001 V d matrices of independent draws from a zero centered normal distribution with small variance and then perform some number of iterations of the following process U i 1 U i Œ± UL U i V i . 7 This should be read as setting the value of U at iteration i 1 as the value of U at the previous iteration plus a small Œ± small step in the direction that locally best improves U with respect to the objective L U V we specified in Eqn 6. Stochastic gradients. There s a crucial detail remaining here beyond how to compute the gradient function U which we ll discuss later computing L U V is exceptionally expensive as it walks over the entire dataset.', 'We note that this deÔ¨Ånition of the numerical gradient follows very naturally from the deÔ¨Ånition of the derivative where in the scalar case f x f x œµ f x œµ Gradient checks are a great way to compare analytical and numerical gradients. Analytical gradients should be close and numerical gradients can be calculated using f Œ∏ J Œ∏ i J Œ∏ i 2œµ J Œ∏ i and J Œ∏ i can be evalu ated using two forward passes. An implementation of this can be seen in Snippet 2.1. Of course there is a slight difference the deÔ¨Ånition above only perturbs x in the positive direction to compute the gradient. While it would have been perfectly acceptable to deÔ¨Åne the numerical gradi ent in this way in practice it is often more precise and stable to use the centered difference formula where we perturb a parameter in both directions.']",text-embedding-3-small,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.984563332089755,1.0,0.9441304695223156,0.05
140,What is the core concept introduced in this class that leads to GPT-3 and ChatGPT?,"The core concept introduced in this class that leads to GPT-3 and ChatGPT is the ability of large language models like GPT-3 to perform zero-shot learning, which means they can perform tasks without any examples or gradient updates by specifying the right sequence prediction problem. This allows them to excel at tasks like question answering without specific training data. This concept is a key feature of models like GPT-3 and ChatGPT.

Source: Lecture07_Prompting_Instruction_Finetuning_and_RLHF","['entailment START The man is in the doorway DELIM The person is near the door EXTRACT 14 DecoderEmergent abilities of large language models GPT 2 2019 Let s revisit the Generative Pretrained Transformer GPT models from OpenAI as an example GPT 2 1.5B parameters Radford et al. 2019 Same architecture as GPT just bigger 117M 1.5B But trained on much more data 4GB 40GB of internet text data WebText Scrape links posted on Reddit w at least 3 upvotes rough proxy of human quality GPT 2018 GPT 2 2019 117M 1.5B 15One key emergent ability in GPT 2 is zero shot learning the ability to do many tasks with no examples and no gradient updates by simply Specifying the right sequence prediction problem e.g. question answering Passage Tom Brady... Q Where was Tom Brady born A ... Comparing probabilities of sequences e.g.', 'Section 5 discusses limitations of GPT 3. Section 6 discusses broader impacts. Section 7 reviews related work and Section 8 concludes. 2 Approach Our basic pre training approach including model data and training is similar to the process described in RWC 19 with relatively straightforward scaling up of the model size dataset size and diversity and length of training. Our use of in context learning is also similar to RWC 19 but in this work we systematically explore different settings for learning within the context. Therefore we start this section by explicitly deÔ¨Åning and contrasting the different settings that we will be evaluating GPT 3 on or could in principle evaluate GPT 3 on. These settings can be seen as lying on a spectrum of how much task speciÔ¨Åc data they tend to rely on.', 'First despite the strong quantitative and qualitative improvements of GPT 3 particularly compared to its direct predecessor GPT 2 it still has notable weaknesses in text synthesis and several NLP tasks. On text synthesis although the overall quality is high GPT 3 samples still sometimes repeat themselves semantically at the document level start to lose coherence over sufÔ¨Åciently long passages contradict themselves and occasionally contain non sequitur sentences or paragraphs. We will release a collection of 500 uncurated unconditional samples to help provide a better sense of GPT 3 s limitations and strengths at text synthesis. Within the domain of discrete language tasks we have noticed informally that GPT 3 seems to have special difÔ¨Åculty with common sense physics despite doing well on some datasets such as PIQA BZB 19 that test this domain. SpeciÔ¨Åcally GPT 3 has difÔ¨Åculty with questions of the type If I put cheese into the fridge will it melt .', 'Overall in context learning with GPT 3 shows mixed results on commonsense reasoning tasks with only small and inconsistent gains observed in the one and few shot learning settings for both PIQA and ARC but a signiÔ¨Åcant improvement is observed on OpenBookQA. GPT 3 sets SOTA on the new PIQA dataset in all evaluation settings. 3.6 Reading Comprehension Next we evaluate GPT 3 on the task of reading comprehension. We use a suite of 5 datasets including abstractive multiple choice and span based answer formats in both dialog and single question settings. We observe a wide spread in GPT 3 s performance across these datasets suggestive of varying capability with different answer formats. In general we observe GPT 3 is on par with initial baselines and early results trained using contextual representations on each respective dataset.']",text-embedding-3-small,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999999999999996,0.0,0.8777786908496656,0.0
141,What is the purpose of Dropout as mentioned in the document?,"The purpose of Dropout, as mentioned in the document, is to act as a powerful technique for regularization in neural networks. Dropout helps prevent overfitting by randomly dropping a subset of neurons during each forward-backward pass during training, effectively training exponentially many smaller networks at once and averaging over their predictions. This helps the network learn more meaningful information from the data, reduce overfitting, and improve performance on the task at hand. During testing, the full network is used to compute predictions. Additionally, Dropout ensures that the expected output of a neuron during testing is approximately the same as during training to maintain well-defined network behavior. (Source: lecture02_notes_Backpropagation.txt)","['The result is that the network typically learns more meaningful informa tion from the data is less likely to overÔ¨Åt and usually obtains higher performance overall on the task at hand. One intuitive reason why this technique should be so effective is that what dropout is doing is essentially doing is training exponentially many smaller networks at once and averaging over their predictions. Dropout applied to an artiÔ¨Åcial neural network. Image credits to Srivastava et al. In practice the way we introduce dropout is that we take the out put h of each layer of neurons and keep each neuron with prob ability p and else set it to 0. Then during back propagation we only pass gradients through neurons that were kept alive during the forward pass. Finally during testing we compute the forward pass using all of the neurons in the network.', 'It must be noted that the bias terms are not regularized and do not contribute to the cost term above try thinking about why this is the case There are indeed other types of regularization that are sometimes used such as L1 regularization which sums over the absolute values rather than squares of parameter elements however this is less commonly applied in practice since it leads to sparsity of parameter weights. In the next section we discuss dropout which effectively acts as another form of regularization by randomly dropping i.e. setting to zero neurons in the forward pass. 2.3 Dropout Dropout is a powerful technique for regularization Ô¨Årst introduced by Srivastava et al. in Dropout A Simple Way to Prevent Neural Net works from OverÔ¨Åtting. The idea is simple yet effective during train ing we will randomly drop with some probability 1 p a subset of neurons during each forward backward pass or equivalently we will keep alive each neuron with a probability p . Then during testing we will use the full network to compute our predictions.', 'However a key sub tlety is that in order for dropout to work effectively the expected output of a neuron during testing should be approximately the same as it was during training else the magnitude of the outputs could be radically different and the behavior of the network is no longer well deÔ¨Åned. Thus we must typically divide the outputs of each neuron during testing by a certain value it is left as an exercise to the reader to determine what this value should be in order for the expected outputs during training and testing to be equivalent. 2.4 Neuron Units So far we have discussed neural networks that contain sigmoidal neurons to introduce nonlinearities however in many applications better networks can be designed using other activation functions. Some common choices are listed here with their function and gra dient deÔ¨Ånitions and these can be substituted with the sigmoidal functions discussed above.', 'like an ensemble model Nowadays usually thought of as strong feature dependent regularizer Wager Wang Liang 2013DropoutDropout Srivastava Hinton Krizhevsky Sutskever Salakhutdinov 2012 JMLR 2014 During training For each data point each time Randomly set input to 0 with probability ùëù dropout ratio often p 0.5 except p 0.15 for input layer via dropout mask During testing Multiply all weights by 1 ùëù No other dropout ùë•2 ùë•3 0 ùë§1 ùë§2 ùë§3 ùë§4 ùëè 1 ùë¶ ùë§1ùë•1 ùë§2ùë•2 ùë§3ùë•3 ùë• ùë• 1 1 Train 1 Train 2 Test ùë•3 0 1 ùë¶ ùë§1ùë•1 ùë§3ùë•3 ùëè ùë§1']",text-embedding-3-small,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9300597327790432,1.0,0.921714795444684,0.3529411764705882
142,What are some approaches to address the vanishing gradient problem in RNNs?,"Some approaches to address the vanishing gradient problem in RNNs include:
1. Using Long Short Term Memory RNNs (LSTMs) proposed by Hochreiter and Schmidhuber in 1997 as a solution to the problem of vanishing gradients.
2. Creating RNNs with separate memory which is added to LSTMs.
3. Introducing more direct and linear pass-through connections in the model, such as attention mechanisms and residual connections.
4. Clipping gradients to a small number whenever they explode, as suggested by Thomas Mikolov.

Source: Lecture03_RNNs_and_LLMs.txt","['Problems with RNNs Vanishing and Exploding Gradients 48Vanishing gradient intuition 49Vanishing gradient intuition chain rule 50Vanishing gradient intuition chain rule 51Vanishing gradient intuition chain rule 52Vanishing gradient intuition What happens if these are small Vanishing gradient problem When these are small the gradient signal gets smaller and smaller as it backpropagates further 53Vanishing gradient proof sketch linear case Recall What if were the identity function 53 Source On the difficulty of training recurrent neural networks Pascanu et al 2013. http proceedings.mlr.press v28 pascanu13.pdf and supplemental materials at http proceedings.mlr.press v28 pascanu13 supp.pdf If Wh is small then this term gets exponentially problematic as becomes large chain rule Consider the gradient of the loss on step with respect to the hidden state on some previous step .', 'NaN which is easily detectable at runtime this issue is called the Gradient Explosion Problem. When the gradient value goes to zero however it can go undetected while dras tically reducing the learning quality of the model for far away words in the corpus this issue is called the Vanishing Gradient Problem. Due to vanishing gradients we don t know whether there is no dependency between steps t and t n in the data or we just cannot capture the true dependency due to this issue. To gain practical intuition about the vanishing gradient problem you may visit the following example website. 2.4 Solution to the Exploding Vanishing Gradients Now that we gained intuition about the nature of the vanishing gradi ents problem and how it manifests itself in deep neural networks let us focus on a simple and practical heuristic to solve these problems. To solve the problem of exploding gradients Thomas Mikolov Ô¨Årst introduced a simple heuristic solution that clips gradients to a small number whenever they explode.', 'press v28 pascanu13.pdfHow to fix the vanishing gradient problem The main problem is that it s too difficult for the RNN to learn to preserve information over many timesteps. In a vanilla RNN the hidden state is constantly being rewritten First off next time How about an RNN with separate memory which is added to LSTMs And then Creating more direct and linear pass through connections in model Attention residual connections etc. 60Long Short Term Memory RNNs LSTMs A type of RNN proposed by Hochreiter and Schmidhuber in 1997 as a solution to the problem of vanishing gradients Everyone cites that paper but really a crucial part of the modern LSTM is from Gers et al. 2000 Only started to be recognized as promising through the work of S s student Alex Graves c.', 'Let chain rule value of ONLY READ IF INTERESTEDVanishing gradient proof sketch linear case What s wrong with Consider if the eigenvalues of are all less than 1 We can write as a basis What about nonlinear activations i.e. what we use Pretty much the same thing except the proof requires for some dependent on dimensionality and Source On the difficulty of training recurrent neural networks Pascanu et al 2013. http proceedings.mlr.press v28 pascanu13.pdf and supplemental materials at http proceedings.mlr.press v28 pascanu13 supp.pdf 54 eigenvectors using the eigenvectors of Approaches 0 as grows so gradient vanishes sufficient but not necessary ONLY READ IF INTERESTEDWhy is vanishing gradient a problem Gradient signal from far away is lost because it s much smaller than gradient signal from close by. So model weights are updated only with respect to near effects not long term effects.']",text-embedding-3-small,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999990825212728,1.0,0.8671224389943339,0.0769230769230769
143,"What is perplexity, and how is it used in evaluating language models?","Perplexity is a measure used to evaluate language models by calculating how well a probability distribution or probability model predicts a sample. In the context of language models, perplexity quantifies how well a model predicts a given text or set of words. A lower perplexity indicates that the model is better at predicting the text, while a higher perplexity suggests the model is less accurate. It is used to compare different n-gram models by measuring how well they predict a given text or set of words. Lower perplexity values indicate better predictive performance of the language model on the test data. Perplexity can be used as a quick check on the performance of an algorithm, but it is essential to confirm any improvements in perplexity with a thorough evaluation of the model's performance on real tasks. Sampling sentences from a language model is another technique used to visualize the knowledge embodied by the model, where sentences are generated based on their likelihood according to the model. Perplexity is related to entropy, which is a measure of information in a random variable, and the concept of cross entropy explains the properties of perplexity and its relationship to entropy.

Source: SEP 775 - Computational Natural Language Processing, Lecture on N-gram Language Models","['We mentioned above that perplexity is a function of both the text and the lan guage model given a text W different language models will have different perplex ities. Because of this perplexity can be used to compare different n gram models. Let s look at an example in which we trained unigram bigram and trigram gram mars on 38 million words including start of sentence tokens from the Wall Street Journal using a 19 979 word vocabulary. We then computed the perplexity of each of these models on a test set of 1.5 million words using Eq. 3.16 for unigrams Eq. 3.17 for bigrams and the corresponding equation for trigrams. The table below shows the perplexity of a 1.5 million word WSJ test set according to each of these grammars. Unigram Bigram Trigram Perplexity 962 170 109 As we see above the more information the n gram gives us about the word sequence the higher the probability the n gram will assign to the string.', 'Nonetheless because perplexity often correlates with such improvements it is commonly used as a quick check on an algorithm. But a model s improvement in perplexity should always be conÔ¨Årmed by an end to end evaluation of a real task before concluding the evaluation of the model. 3.3 Sampling sentences from a language model One important way to visualize what kind of knowledge a language model embodies is to sample from it. Sampling from a distribution means to choose random points sampling according to their likelihood. Thus sampling from a language model which rep resents a distribution over sentences means to generate some sentences choosing each sentence according to its likelihood as deÔ¨Åned by the model. Thus we are more likely to generate sentences that the model thinks have a high probability and less likely to generate sentences that the model thinks have a low probability. This technique of visualizing a language model by sampling was Ô¨Årst suggested very early on by Shannon 1951 and Miller and Selfridge 1950 . It s simplest to visualize how this works for the unigram case.', 'We also need to include the end of sentence marker s but not the beginning of sentence marker s in the total count of word to kens N. There is another way to think about perplexity as the weighted average branch ing factor of a language. The branching factor of a language is the number of possi ble next words that can follow any word. Consider the task of recognizing the digits3.2 EVALUATING LANGUAGE MODELS 9 in English zero one two ... nine given that both in some training set and in some test set each of the 10 digits occurs with equal probability P 1 10. The perplexity of this mini language is in fact 10. To see that imagine a test string of digits of length N and assume that in the training set all the digits occurred with equal probability. By Eq.', '2013 for the details.22 CHAPTER 3 N GRAM LANGUAGE MODELS 3.8 Advanced Perplexity s Relation to Entropy We introduced perplexity in Section 3.2.1 as a way to evaluate n gram models on a test set. A better n gram model is one that assigns a higher probability to the test data and perplexity is a normalized version of the probability of the test set. The perplexity measure actually arises from the information theoretic concept of cross entropy which explains otherwise mysterious properties of perplexity why the inverse probability for example and its relationship to entropy. Entropy is a Entropy measure of information. Given a random variable X ranging over whatever we are predicting words letters parts of speech the set of which we ll call œá and with a particular probability function call it p x the entropy of the random variable X is H X X x œá p x log2 p x 3.43 The log can in principle be computed in any base.']",text-embedding-3-small,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9077103422875468,0.8571428571428571,0.9435757491272608,0.0357142857142857
144,How does LSTM solve the vanishing gradient problem?,"LSTM (Long Short Term Memory) units, which are a special type of units in LSTM networks, have a recurrent connection to themselves that is fixed to 1. The flow of information into and from the unit is controlled by input and output gates, and their behavior is learned. This architecture helps in addressing the vanishing gradient problem by allowing the network to preserve long-term dependencies more effectively compared to traditional RNNs, where the gradient signal can diminish as it backpropagates through many time steps. Additionally, using techniques like the Hessian Free optimizer with structural damping can also help in dealing with the vanishing gradient problem in LSTM networks.","['NaN which is easily detectable at runtime this issue is called the Gradient Explosion Problem. When the gradient value goes to zero however it can go undetected while dras tically reducing the learning quality of the model for far away words in the corpus this issue is called the Vanishing Gradient Problem. Due to vanishing gradients we don t know whether there is no dependency between steps t and t n in the data or we just cannot capture the true dependency due to this issue. To gain practical intuition about the vanishing gradient problem you may visit the following example website. 2.4 Solution to the Exploding Vanishing Gradients Now that we gained intuition about the nature of the vanishing gradi ents problem and how it manifests itself in deep neural networks let us focus on a simple and practical heuristic to solve these problems. To solve the problem of exploding gradients Thomas Mikolov Ô¨Årst introduced a simple heuristic solution that clips gradients to a small number whenever they explode.', 'Problems with RNNs Vanishing and Exploding Gradients 48Vanishing gradient intuition 49Vanishing gradient intuition chain rule 50Vanishing gradient intuition chain rule 51Vanishing gradient intuition chain rule 52Vanishing gradient intuition What happens if these are small Vanishing gradient problem When these are small the gradient signal gets smaller and smaller as it backpropagates further 53Vanishing gradient proof sketch linear case Recall What if were the identity function 53 Source On the difficulty of training recurrent neural networks Pascanu et al 2013. http proceedings.mlr.press v28 pascanu13.pdf and supplemental materials at http proceedings.mlr.press v28 pascanu13 supp.pdf If Wh is small then this term gets exponentially problematic as becomes large chain rule Consider the gradient of the loss on step with respect to the hidden state on some previous step .', 'SpeciÔ¨Åcally it introduces a special set of units called LSTM units which are linear and have a recurrent connection to itself which is Ô¨Åxed to 1. The Ô¨Çow of information into the unit and from the unit is guarded by an input and output gates their behaviour is learned . There are several variations of this basic structure. This solution does not address explicitly the exploding gradients problem. Sutskever et al. 2011 use the Hessian Free opti mizer in conjunction with structural damping a spe ciÔ¨Åc damping strategy of the Hessian. This approach seems to deal very well with the vanishing gradient though more detailed analysis is still missing. Pre sumably this method works because in high dimen sional spaces there is a high probability for long term components to be orthogonal to short term ones. This would allow the Hessian to rescale these components independently. In practice one can not guarantee that this property holds.', 'press v28 pascanu13.pdfHow to fix the vanishing gradient problem The main problem is that it s too difficult for the RNN to learn to preserve information over many timesteps. In a vanilla RNN the hidden state is constantly being rewritten First off next time How about an RNN with separate memory which is added to LSTMs And then Creating more direct and linear pass through connections in model Attention residual connections etc. 60Long Short Term Memory RNNs LSTMs A type of RNN proposed by Hochreiter and Schmidhuber in 1997 as a solution to the problem of vanishing gradients Everyone cites that paper but really a crucial part of the modern LSTM is from Gers et al. 2000 Only started to be recognized as promising through the work of S s student Alex Graves c.']",text-embedding-3-small,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.8463434993400726,0.6,0.9177787463605728,0.2631578947368421
145,"What is the impact of bidirectional RNNs, and where are they applicable?","Bidirectional RNNs are impactful as they allow predictions based on future words by reading through the corpus backwards, maintaining two hidden layers for left to right and right to left propagation. They are not applicable to Language Modeling where only left context is available, but are powerful when the entire input sequence is accessible, such as in BERT for contextual representation. (Source: lecture03_RNNs_and_LLMs.txt)","['The Ô¨Årst technique is that instead of initializing W hh ran domly start off from an identity matrix initialization. The second technique is to use the RectiÔ¨Åed Linear Units ReLU in stead of the sigmoid function. The derivative for the ReLU is either 0 or 1. This way gradients would Ô¨Çow through the neurons whose deriva tive is 1 without getting attenuated while propagating back through time steps. 2.5 Deep Bidirectional RNNs So far we have focused on RNNs that condition on past words to predict the next word in the sequence. It is possible to make predic tions based on future words by having the RNN model read through the corpus backwards. Irsoy et al. shows a bi directional deep neu ral network at each time step t this network maintains two hidden layers one for the left to right propagation and another for the right to left propagation.', 'h h h Figure 8 A bi directional RNN model h t f W xt V h t 1 b 17 h t f W xt V h t 1 b 18 ÀÜ yt g Uht c g U h t h t c 19 RNNs can also be multi layered. Figure 9 shows a multi layer bi directional RNN where each lower layer feeds the next layer. As shown in this Ô¨Ågure in this network architecture at time step t each interme diate neuron receives one set of parameters from the previous time step in the same RNN layer and two sets of parameters from the previous RNN hidden layer one input comes from the left to right RNN and the other from the right to left RNN.', 'Answer German 76 lots of neural architecture lots of neural architectureRNN LMs can be used to generate text e.g. speech recognition machine translation summarization RNN LM what s the the weather what s This is an example of a conditional language model. We ll see Machine Translation in much more detail next lectures. Input audio START conditioning 77Bidirectional RNNs was terribly exciting the movie Forward RNN Backward RNN Concatenated hidden states This contextual representation of terribly has both left and right context 78Bidirectional RNNs 79 Note bidirectional RNNs are only applicable if you have access to the entire input sequence They are not applicable to Language Modeling because in LM you only have left context available. If you do have entire input sequence e.g. any kind of encoding bidirectionality is powerful you should use it by default . For example BERT Bidirectional Encoder Representations from Transformers is a powerful pretrained contextual representation system built on bidirectionality.', 'In contrast to these models BIDAF does not summarize the two modalities in the attention layer and instead lets the attention vectors Ô¨Çow into the modeling RNN layer. The third group considered as variants of Memory Network Weston et al. 2015 repeats comput ing an attention vector between the query and the context through multiple layers typically referred to as multi hop Sordoni et al. 2016 Dhingra et al. 2016 . Shen et al. 2016 combine Memory Networks with Reinforcement Learning in order to dynamically control the number of hops. One can also extend our BIDAF model to incorporate multiple hops. Visual question answering. The task of question answering has also gained a lot of interest in the computer vision community. Early works on visual question answering VQA involved encoding the question using an RNN encoding the image using a CNN and combining them to answer the question Antol et al. 2015 Malinowski et al. 2015 .']",text-embedding-3-small,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9570805755154708,1.0,0.947181831781522,0.0454545454545454
146,Describe the concept of multi-layer or stacked RNNs and their benefit.,"Multi-layer or stacked RNNs involve applying multiple RNNs on top of each other, allowing the network to compute more complex representations. The lower RNN layers compute lower-level features, while the higher RNN layers compute higher-level features. This architecture enables the network to capture intricate patterns and dependencies in the data, leading to improved performance compared to using just a single layer of high-dimensional encodings. In practice, high-performing RNNs are often multi-layered, with 2 to 4 layers being optimal for tasks like Neural Machine Translation. Skip connections or dense connections are typically used to facilitate training deeper RNNs. Transformer-based networks like BERT, on the other hand, are usually deeper, with 12 or 24 layers, showcasing the significance of depth in neural network architectures.

Source: Lecture03_RNNs_and_LLMs.","['You will learn more about transformers including BERT in a couple of weeksMulti layer RNNs RNNs are already deep on one dimension they unroll over many timesteps We can also make them deep in another dimension by applying multiple RNNs this is a multi layer RNN. This allows the network to compute more complex representations The lower RNNs should compute lower level features and the higher RNNs should compute higher level features. Multi layer RNNs are also called stacked RNNs. 80Multi layer RNNs was terribly exciting the movie RNN layer 1 RNN layer 2 81 RNN layer 3 The hidden states from RNN layer i are the inputs to RNN layer i 1Multi layer RNNs in practice 39 Multi layer or stacked RNNs allow a network to compute more complex representations they work better than just have one layer of high dimensional encodings The lower RNNs should compute lower level features and the higher RNNs should compute higher level features.', 'High performing RNNs are usually multi layer but aren t as deep as convolutional or feed forward networks For example In a 2017 paper Britz et al. find that for Neural Machine Translation 2 to 4 layers is best for the encoder RNN and 4 layers is best for the decoder RNN Often 2 layers is a lot better than 1 and 3 might be a little better than 2 Usually skip connections dense connections are needed to train deeper RNNs e.g. 8 layers Transformer based networks e.g. BERT are usually deeper like 12 or 24 layers.', 'h h h Figure 8 A bi directional RNN model h t f W xt V h t 1 b 17 h t f W xt V h t 1 b 18 ÀÜ yt g Uht c g U h t h t c 19 RNNs can also be multi layered. Figure 9 shows a multi layer bi directional RNN where each lower layer feeds the next layer. As shown in this Ô¨Ågure in this network architecture at time step t each interme diate neuron receives one set of parameters from the previous time step in the same RNN layer and two sets of parameters from the previous RNN hidden layer one input comes from the left to right RNN and the other from the right to left RNN.', 'I ll leave discussion of the amazing feats one can achieve with RNNs to Andrej Karpathy s excellent blog post The Unreasonable Effectiveness of A xt ht1 24 24 3 23 PM Understanding LSTM Networks colah s blog 2 7 Recurrent Neural Networks http karpathy.github.io 2015 05 21 rnn effectiveness . But they really are pretty amazing. Essential to these successes is the use of LSTMs a very special kind of recurrent neural network which works for many tasks much much better than the standard version. Almost all exciting results based on recurrent neural networks are achieved with them. It s these LSTMs that this essay will explore. The Problem of Long Term Dependencies One of the appeals of RNNs is the idea that they might be able to connect previous information to the present task such as using previous video frames might inform the understanding of the present frame. If RNNs could do this they d be extremely useful. But can they It depends.']",text-embedding-3-small,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.8239440667517633,1.0,0.9485940835570444,0.0625
147,How does Dropout work during training and testing?,"During training, Dropout works by randomly dropping a subset of neurons with a certain probability (1-p) during each forward-backward pass, or equivalently keeping each neuron alive with a probability (p). This process helps prevent overfitting by training exponentially many smaller networks at once and averaging their predictions. During testing, all neurons in the network are used to compute predictions to ensure the expected output of a neuron during testing is approximately the same as during training, maintaining the network's well-defined behavior. Additionally, during testing, all weights are multiplied by (1-p) to adjust for the dropout applied during training. 

Source: SEP 775 - Computational Natural Language Processing, lecture02_notes_Backpropagation.txt","['The result is that the network typically learns more meaningful informa tion from the data is less likely to overÔ¨Åt and usually obtains higher performance overall on the task at hand. One intuitive reason why this technique should be so effective is that what dropout is doing is essentially doing is training exponentially many smaller networks at once and averaging over their predictions. Dropout applied to an artiÔ¨Åcial neural network. Image credits to Srivastava et al. In practice the way we introduce dropout is that we take the out put h of each layer of neurons and keep each neuron with prob ability p and else set it to 0. Then during back propagation we only pass gradients through neurons that were kept alive during the forward pass. Finally during testing we compute the forward pass using all of the neurons in the network.', 'However a key sub tlety is that in order for dropout to work effectively the expected output of a neuron during testing should be approximately the same as it was during training else the magnitude of the outputs could be radically different and the behavior of the network is no longer well deÔ¨Åned. Thus we must typically divide the outputs of each neuron during testing by a certain value it is left as an exercise to the reader to determine what this value should be in order for the expected outputs during training and testing to be equivalent. 2.4 Neuron Units So far we have discussed neural networks that contain sigmoidal neurons to introduce nonlinearities however in many applications better networks can be designed using other activation functions. Some common choices are listed here with their function and gra dient deÔ¨Ånitions and these can be substituted with the sigmoidal functions discussed above.', 'like an ensemble model Nowadays usually thought of as strong feature dependent regularizer Wager Wang Liang 2013DropoutDropout Srivastava Hinton Krizhevsky Sutskever Salakhutdinov 2012 JMLR 2014 During training For each data point each time Randomly set input to 0 with probability ùëù dropout ratio often p 0.5 except p 0.15 for input layer via dropout mask During testing Multiply all weights by 1 ùëù No other dropout ùë•2 ùë•3 0 ùë§1 ùë§2 ùë§3 ùë§4 ùëè 1 ùë¶ ùë§1ùë•1 ùë§2ùë•2 ùë§3ùë•3 ùë• ùë• 1 1 Train 1 Train 2 Test ùë•3 0 1 ùë¶ ùë§1ùë•1 ùë§3ùë•3 ùëè ùë§1', 'It must be noted that the bias terms are not regularized and do not contribute to the cost term above try thinking about why this is the case There are indeed other types of regularization that are sometimes used such as L1 regularization which sums over the absolute values rather than squares of parameter elements however this is less commonly applied in practice since it leads to sparsity of parameter weights. In the next section we discuss dropout which effectively acts as another form of regularization by randomly dropping i.e. setting to zero neurons in the forward pass. 2.3 Dropout Dropout is a powerful technique for regularization Ô¨Årst introduced by Srivastava et al. in Dropout A Simple Way to Prevent Neural Net works from OverÔ¨Åtting. The idea is simple yet effective during train ing we will randomly drop with some probability 1 p a subset of neurons during each forward backward pass or equivalently we will keep alive each neuron with a probability p . Then during testing we will use the full network to compute our predictions.']",text-embedding-3-small,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999992973011532,0.75,0.9207732490037022,0.0588235294117647
148,What is the role of parameter initialization in neural networks?,"Parameter initialization in neural networks plays a crucial role in achieving optimal performance. Proper initialization of weights and biases can lead to faster convergence, lower error rates, and help prevent issues like vanishing or exploding gradients during training. It sets the starting point for the optimization process and can significantly impact the overall training dynamics of the neural network. Different initialization strategies, such as initializing weights to small random numbers or using specific distributions like uniform or normal, can influence how well the network learns the underlying patterns in the data. Overall, proper parameter initialization is essential for the successful training and performance of neural networks.

Source: lecture02_notes_Backpropagation.txt","['The empirical Ô¨Åndings suggest that for sigmoid and tanh activation units faster convergence and lower error rates are achieved when the weights of a matrix W Rn l 1 n l are initialized randomly with a uniform distribution as follows W U s 6 n l n l 1 s 6 n l n l 1 Where n l is the number of input units to W fan in and n l 1 is the number of output units from W fan out . In this parameter initialization scheme bias units are initialized to 0. This approach attempts to maintain activation variances as well as backpropagated gradient variances across layers. Without such initialization the gradient variances which are a proxy for information generally decrease with backpropagation across layers. 2.7 Learning Strategies The rate magnitude of model parameter updates during training can be controlled using the learning rate.', 'We should also note that such initialization does not prevent cross ing the boundary between basins of attraction which as shown could happen even though no bifurcation boundary is crossed. Teacher forcing is a more interesting yet a not very well understood solution. It can be seen as a way of initializing the model in the right regime and the rightOn the diÔ¨Éculty of training Recurrent Neural Networks region of space. It has been shown that in practice it can reduce the chance that gradients explode and even allow training generator models or models that work with unbounded amounts of memory Pascanu and Jaeger 2011 Doya and Yoshizawa 1991 . One important downside is that it requires a target to be deÔ¨Åned at every time step. In Hochreiter and Schmidhuber 1997 Graves et al. 2009 a solution is proposed for the vanishing gra dients problem where the structure of the model is changed.', 'We can then take the Singular Value Decomposition SVD of X to get matrices U S V. We then compute UX to project X into the basis deÔ¨Åned by the columns of U. We Ô¨Ånally divide each dimension of the result by the corresponding singular value in S to scale our data appropriately if a singular value is zero we can just divide by a small number instead . 2.6 Parameter Initialization A key step towards achieving superlative performance with a neu ral network is initializing the parameters in a reasonable way. A good starting strategy is to initialize the weights to small random numbers normally distributed around 0 and in practice this often words acceptably well. However in Understanding the dif ficulty of training deep feedforward neural networks 2010 Xavier et al study the effect of different weight and bias initialization schemes on training dynamics.', 'A very simple ex ample is a function from two dimensional inputs to two dimensional outputs such as output 1 p1 input 1 p2 input 2 p3 input 1 input 2 p4 output 2 p5 tanh output 1 p6 input 1 p7 input 2 p8 input 1 input 2 p9 Neural networks are almost always deÔ¨Åned in terms of parameters here denoted by p1 . . . p9 which are automatically chosen using standard machine learning algorithms. Typically they include at least one transformation that is not linear e.g. the hyperbolic tangent above . The illustration displays the toy neural network as a computation graph inside the black box with inputs on the left outputs on the right and each parameter corresponds to a gray box placed along the top and bottom. Round nodes inside the box correspond to intermediate operations addition multiplication and tanh .']",text-embedding-3-small,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9841035568142736,0.8333333333333334,0.92832762816829,0.0526315789473684
149,"What are the differences between Vanilla RNNs, LSTMs, and GRUs?","Vanilla RNNs suffer from the vanishing gradient problem, making it difficult for them to capture long-term dependencies. LSTMs (Long Short-Term Memory) address this by introducing a more complex architecture with forget, input, and output gates, allowing them to retain and forget information as needed. GRUs (Gated Recurrent Units) are a variation of LSTMs that combine the forget and input gates into a single update gate, simplifying the model while still capturing long-term dependencies effectively. GRUs also merge the cell state and hidden state, making them computationally more efficient compared to LSTMs. 
Source: lecture03_notes_LM_RNN.txt","['Which of these variants is best Do the differences matter Greff et al. 2015 http arxiv.org pdf 1503.04069.pdf do a nice comparison of popular variants finding that they re all about the same. Jozefowicz et al. 2015 http jmlr.org proceedings papers v37 jozefowicz15.pdf tested more than ten thousand RNN architectures finding some that worked better than LSTMs on certain tasks. Conclusion Earlier I mentioned the remarkable results people are achieving with RNNs. Essentially all of these are achieved using LSTMs. They really work a lot better for most tasks Written down as a set of equations LSTMs look pretty intimidating.', 'Figure 12 The detailed internals of a GRU It is important to note that to train a GRU we need to learn all the different parameters W U W r U r W z U z . These follow the same backpropagation procedure we have seen in the past. 4 Long Short Term Memories Long Short Term Memories are another type of complex activation unit that differ a little from GRUs. The motivation for using these is similar to those for GRUs however the architecture of such units does differ.', 'Gated recurrent units are designed in a manner to have more persistent memory thereby making it easier for RNNs to capture long term dependencies. Let us see mathematically how a GRU uses ht 1 and xt to generate the next hidden state ht. We will then dive into the intuition of this architecture. zt œÉ W z xt U z ht 1 Update gate rt œÉ W r xt U r ht 1 Reset gate ht tanh rt Uht 1 Wxt New memory ht 1 zt ht zt ht 1 Hidden state The above equations can be thought of a GRU s four fundamental op erational stages and they have intuitive interpretations that make this model much more intellectually satisfying see Figure 12 1. New memory generation A new memory ht is the consolidation of a new input word xt with the past hidden state ht 1.', 'We only forget when we re going to input something in its place. We only input new values to the state when we forget something older. A slightly more dramatic variation on the LSTM is the Gated Recurrent Unit or GRU introduced by Cho et al. 2014 http arxiv.org pdf 1406.1078v3.pdf . It combines the forget and input gates into a single update gate. It also merges the cell state and hidden state and makes some other changes. The resulting model is simpler than standard LSTM models and has been growing increasingly popular. These are only a few of the most notable LSTM variants. There are lots of others like Depth Gated RNNs by Yao et al. 2015 http arxiv.org pdf 1508.03790v2.pdf . There s also some completely different approach to tackling long term dependencies like Clockwork RNNs by Koutnik et al. 2014 http arxiv.org pdf 1402.3511v1.pdf .']",text-embedding-3-small,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9389560628645268,0.875,0.9468894537340312,0.0434782608695652
150,What is the difference between traditional language models and conditioned language models?,"Traditional language models are trained to predict the next word in a sequence based on the previous words, while conditioned language models take into account additional context or conditions when making predictions. This additional context can include information about the task at hand, specific styles of text, or other forms of conditioning that guide the model's predictions. Conditioned language models are designed to be more flexible and adaptable to different contexts compared to traditional language models.

Source: Language Models_are_Few_Shot_Learners.txt","['Recent efforts include RSR 19 RRS20 which Ô¨Åne tuned an 11 billion parameter language model and GLT 20 which focused on attending over a large corpus of data at test time. Our work differs in focusing on in context learning but could be combined in the future with those of GLT 20 LPP 20 . Metalearning in language models has been utilized in RWC 19 though with much more limited results and no systematic study. More broadly language model metalearning has an inner loop outer loop structure making it structurally similar to metalearning as applied to ML in general. Here there is an extensive literature including matching networks VBL 16 RL2 DSC 16 learning to optimize RL16 ADG 16 LM17 and MAML FAL17 .', 'file name: Fine Tuning and Masked Language Models. Speech and Language Processing. Daniel Jurafsky James H. Martin. Copyright 2023. All rights reserved. Draft of February 3 2024. CHAPTER 11 Fine Tuning and Masked Lan guage Models Larvatus prodeo Masked I go forward Descartes In the previous chapter we saw how to pretrain transformer language models and how these pretrained models can be used as a tool for many kinds of NLP tasks by casting the tasks as word prediction. The models we introduced in Chapter 10 to do this task are causal or left to right transformer models. In this chapter we ll introduce a second paradigm for pretrained language mod els called the bidirectional transformer encoder trained via masked language modeling a method that allows the model to see entire texts at a time including masked language modeling both the right and left context. We ll introduce the most widely used version of the masked language modeling architecture the BERT model Devlin et al. 2019 .', 'This approach includes ALBERT LCG 19 as well as general HVD15 and 39task speciÔ¨Åc SDCW19 JYS 19 KR16 approaches to distillation of language models. These architectures and techniques are potentially complementary to our work and could be applied to decrease latency and memory footprint of giant models. As Ô¨Åne tuned language models have neared human performance on many standard benchmark tasks considerable effort has been devoted to constructing more difÔ¨Åcult or open ended tasks including question answering KPR 19 IBGC 14 CCE 18 MCKS18 reading comprehension CHI 18 RCM19 and adversarially constructed datasets designed to be difÔ¨Åcult for existing language models SBBC19 NWD 19 . In this work we test our models on many of these datasets. Many previous efforts have focused speciÔ¨Åcally on question answering which constitutes a signiÔ¨Åcant fraction of the tasks we tested on.', 'These possibilities exist on a spectrum ranging from demonstrations in the training set that are drawn from exactly the same distribution as those at test time to recognizing the same task but in a different format to adapting to a speciÔ¨Åc style of a general task such as QA to learning a skill entirely de novo. Where GPT 3 is on this spectrum may also vary from task to task. Synthetic tasks such as wordscrambling or deÔ¨Åning nonsense words seem especially likely to be learned de novo whereas translation clearly must be learned during pretraining although possibly from data that is very different in organization and style than the test data. Ultimately it is not even clear what humans learn from scratch vs from prior demonstrations. Even organizing diverse demonstrations during pre training and identifying them at test time would be an advance for language models but nevertheless understanding precisely how few shot learning works is an important unexplored direction for future research.']",text-embedding-3-small,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.0000000000000002,0.6666666666666666,0.9302634036553248,0.04
151,How can LSTM networks be used in language modeling?,"LSTM networks can be used in language modeling by providing a way to model and predict sequences of words in text data. LSTM (Long Short-Term Memory) networks are a type of recurrent neural network (RNN) that can effectively capture long-term dependencies in sequential data, making them well-suited for tasks like language modeling where the context of words matters. By processing input sequences step by step and updating their internal state based on the current input and previous context, LSTM networks can learn to generate text or assign probabilities to sequences of words, which is essential in language modeling tasks.

Source: lecture03_RNNs_and_LLMs","['Recap 69 Language Model A system that predicts the next word Recurrent Neural Network A family of neural networks that Take sequential input of any length Apply the same weights on each step Can optionally produce output on each step Recurrent Neural Network Language Model We ve shown that RNNs are a great way to build a LM despite some problems RNNs are also useful for much moreWhy should we care about Language Modeling 70 Language Modeling is a benchmark task that helps us measure our progress on predicting language use Language Modeling is a subcomponent of many NLP tasks especially those involving generating text or estimating the probability of text Predictive typing Speech recognition Handwriting recognition Spelling grammar correction Authorship identification Machine translation Summarization Dialogue etc. Everything else in NLP has now been rebuilt upon Language Modeling GPT 3 is an LMOther RNN ArchitecturesOther RNN uses RNNs can be used for sequence tagging e.g.', 'Language Modeling exams minds laptops Language Modeling is the task of predicting what word comes next books the students opened their 10Language Modeling You can also think of a Language Model as a system that assigns a probability to a piece of text For example if we have some text then the probability of this text according to the Language Model is This is what our LM provides 11You use Language Models every day 12You use Language Models every day 13n gram Language Models 14 the students opened their Question How to learn a Language Model Answer pre Deep Learning learn an n gram Language Model Definition An n gram is a chunk of n consecutive words.', 'Recent efforts include RSR 19 RRS20 which Ô¨Åne tuned an 11 billion parameter language model and GLT 20 which focused on attending over a large corpus of data at test time. Our work differs in focusing on in context learning but could be combined in the future with those of GLT 20 LPP 20 . Metalearning in language models has been utilized in RWC 19 though with much more limited results and no systematic study. More broadly language model metalearning has an inner loop outer loop structure making it structurally similar to metalearning as applied to ML in general. Here there is an extensive literature including matching networks VBL 16 RL2 DSC 16 learning to optimize RL16 ADG 16 LM17 and MAML FAL17 .', 'Language models are unsupervised multitask learners 2019. Colin Raffel Noam Shazeer Adam Roberts Katherine Lee Sharan Narang Michael Matena Yanqi Zhou Wei Li and Peter J. Liu. Exploring the limits of transfer learning with a uniÔ¨Åed text to text transformer 2019. arXiv 1910.10683. Martin Sundermeyer Ralf Schl uter and Hermann Ney. LSTM neural networks for language modeling. In Proc. of Interspeech 2012. 14Peter D. Turney and Patrick Pantel. From frequency to meaning Vector space models of semantics. Journal of ArtiÔ¨Åcial Intelligence Research 37 1 141 188 2010. Zhilin Yang Zihang Dai Yiming Yang Jaime Carbonell Ruslan Salakhutdinov and Quoc V. Le. XLNet Generalized autoregressive pretraining for language understanding. In Proc. of NeurIPS 2019.']",text-embedding-3-small,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.0000000000000009,1.0,0.9531471437002648,0.0434782608695652
152,What are the methods of generation in conditioned language modeling?,"The methods of generation in conditioned language modeling include Sampling (trying to generate a random sentence according to the probability distribution) and Argmax (trying to generate the sentence with the highest probability). Ancestral Sampling involves randomly generating words one by one, and Greedy Search picks the single highest probability word one by one. Beam Search maintains several paths instead of picking one high probability word. Model Ensembling is also used, where predictions from multiple models are combined to smooth over idiosyncrasies.","['It was a lot cleaner down in London. Text Credit Max Deutsch https medium.com deep writingConditioned Language Models Not just generate text generate text according to some specification Input X Structured Data English Document Utterance Image Speech Output Y Text NL Description Japanese Short Description Response Text Transcript Task NL Generation Translation Summarization Response Generation Image Captioning Speech RecognitionFormulation and ModelingCalculating the Probability of a SentenceConditional Language ModelsOne Type of Language Model LSTM LSTM LSTM LSTM predict hate predict this predict movie predict s LSTM Mikolov et al. 2011 s I hate this movie predict ILSTM LSTM LSTM LSTM LSTM s LSTM LSTM LSTM LSTM argmax argmax argmax argmax argmax s One Type of Conditional Language Model I hate this movie kono eiga ga kirai hate this movie Sutskever et al. 2014 Encoder I DecoderHow to Pass Hidden State Initialize decoder w encoder Sutskever et al.', 'e.g. Kim et al. 2016 Problem parameter averaging only works for models within the same run Knowledge distillation trains a model to copy the ensemble Specifically it tries to match the description over predicted words Why We want the model to make the same mistakes as an ensemble Shown to increase accuracy notablyStacking What if we have two very different models where prediction of outputs is done in very different ways e.g. a phrase based translation model and a neural MT model Niehues et al. 2017 Stacking uses the output of one system in calculating features for another systemCase Studies in Conditional Language ModelingFrom Images e.g. Karpathy et al. 2015 Input is image features output is text Use standard image encoders e.g.', '2014 encoder decoder Transform can be different dimensions encoder transform decoder encoder Input at every time step Kalchbrenner Blunsom 2013 decoder decoder decoderMethods of GenerationThe Generation Problem We have a model of P Y X how do we use it to generate a sentence Two methods Sampling Try to generate a random sentence according to the probability distribution. Argmax Try to generate the sentence with the highest probability.Ancestral Sampling Randomly generate words one by one. An exact method for sampling from P X no further work needed.Greedy Search One by one pick the single highest probability word Not exact real problems Will often generate the easy words first Will prefer multiple common words to one rare wordBeam Search Instead of picking one high probability word maintain several paths Some in reading materials more in a later classModel EnsemblingEnsembling Why Multiple models make somewhat uncorrelated errors Models tend to be more uncertain when they are about to make errors Smooths over idiosyncrasies of the model LSTM1 predict1 I LSTM2 predict2 Combine predictions from multiple models s sLinear Interpolation Take', '1994. On structuring prob abilistic dependencies in stochastic language modelling. Computer Speech and Language 8 1 38. Schwenk H. 2007. Continuous space language models. Computer Speech Language 21 3 492 518. Shannon C. E. 1948. A mathematical theory of commu nication. Bell System Technical Journal 27 3 379 423. Continued in the following volume. Shannon C. E. 1951. Prediction and entropy of printed En glish. Bell System Technical Journal 30 50 64. Stolcke A. 1998. Entropy based pruning of backoff lan guage models. Proc. DARPA Broadcast News Transcrip tion and Understanding Workshop. Stolcke A. 2002. SRILM an extensible language modeling toolkit. ICSLP. Talbot D. and M. Osborne. 2007.']",text-embedding-3-small,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9915019668921122,1.0,0.929385686255948,0.0357142857142857
153,What is beam search and how is it used in language generation?,"Beam search is a decoding algorithm used in natural language processing for generating sequences like sentences. It works by keeping track of the k most probable partial translations, called hypotheses, at each step of the decoding process. The beam size, k, is typically set to around 5 to 10 in practice. Each hypothesis has a score, which is its log probability, and the algorithm searches for high-scoring hypotheses to generate the output sequence. Beam search is more efficient than exhaustive search as it narrows down the search space by considering only the top k possibilities at each step, although it is not guaranteed to find the optimal solution. It is commonly used in tasks like neural machine translation for generating fluent and accurate translations.

Source: lecture05_Seq2Seq.txt","['top k on each step Beam search is not guaranteed to find optimal solution But much more efficient than exhaustive search 18Beam search decoding example Beam size k 2. Blue numbers START Calculate prob dist of next word 19Beam search decoding example Beam size k 2. Blue numbers START he I 0.7 log PLM he START 0.9 log PLM I START Take top k words and compute scores 20Beam search decoding example Beam size k 2. Blue numbers hit struck was got START he I 10 0.7 0.9 2.9 log PLM struck START he 0.7 1.6 log PLM was START I 0.9 1.8 log PLM got START I 0.9 1.7 log PLM hit START he 0.7 For each of the k hypotheses find top k next words and calculate scoresBeam search decoding example Beam size k 2.', 'decoder he hit me with a pie END START he hit me with a pie This is greedy decoding take most probable word on each step argmax argmax argmax argmax 15 argmax argmax argmaxProblems with greedy decoding 16 Greedy decoding has no way to undo decisions Input il a m entart√© he he hit he hit a he hit me with a pie whoops no going back now How to fix thisExhaustive search decoding Ideally we want to find a length T translation y that maximizes We could try computing all possible sequences y This means that on each step t of the decoder we re tracking Vt possible partial translations where V is vocab size This O VT complexity is far too expensive 17Beam search decoding Core idea On each step of decoder keep track of the k most probable partial translations which we call hypotheses k is the beam size in practice around 5 to 10 in NMT A hypothesis has a score which is its log probability Scores are all negative and higher score is better We search for high scoring hypotheses tracking top k on each step Beam search is not guaranteed', 'For simplicity we use greedy search for all generative tasks compared with beam search used in Brown et al. 2020 . Unlike GPT 3 which chooses the number of few shot exemplars k via best dev set performance for few shot LaMDA PT we choose the highest k that Ô¨Åts in the context length of 1024 tokens from k 1 3 5 10 . For DROP Dua et al. 2019 and SQuADv2 Rajpurkar et al. 2018 based on email correspondence with Brown et al. 2020 their deÔ¨Ånition of zero shot differs from ours in that they actually use exemplars but only from the same passage as the inference question each passage has more than one question . Hence GPT 3 zero shot results are not directly comparable with ours for DROP and SQuADv2. We mark these results using the symbol.', '2014 encoder decoder Transform can be different dimensions encoder transform decoder encoder Input at every time step Kalchbrenner Blunsom 2013 decoder decoder decoderMethods of GenerationThe Generation Problem We have a model of P Y X how do we use it to generate a sentence Two methods Sampling Try to generate a random sentence according to the probability distribution. Argmax Try to generate the sentence with the highest probability.Ancestral Sampling Randomly generate words one by one. An exact method for sampling from P X no further work needed.Greedy Search One by one pick the single highest probability word Not exact real problems Will often generate the easy words first Will prefer multiple common words to one rare wordBeam Search Instead of picking one high probability word maintain several paths Some in reading materials more in a later classModel EnsemblingEnsembling Why Multiple models make somewhat uncorrelated errors Models tend to be more uncertain when they are about to make errors Smooths over idiosyncrasies of the model LSTM1 predict1 I LSTM2 predict2 Combine predictions from multiple models s sLinear Interpolation Take']",text-embedding-3-small,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9218209045611296,1.0,0.9152883967876928,0.2307692307692307
154,Explain the concept of model ensembling in language models.,"Model ensembling in language models involves combining the predictions of multiple individual models to improve the overall performance. This can be done by training different models with diverse architectures or training data and then aggregating their predictions in some way, such as averaging or using a weighted combination. Ensembling helps to reduce overfitting, increase robustness, and enhance the generalization ability of the models by leveraging the strengths of each individual model. It can be particularly useful when dealing with complex tasks or datasets where a single model may not perform optimally in all scenarios.","['The methods of Chapter 6 like word2vec or GloVe learned a single vector embedding for each unique word w in the vocabulary. By contrast with contextual embeddings such as those learned by masked language models like BERT each word w will be represented by a different vector each time it appears in a different context. While the causal language models of Chapter 10 also use contextual embeddings the embeddings created by masked language models seem to function particularly well as representations. 11.1 Bidirectional Transformer Encoders Let s begin by introducing the bidirectional transformer encoder that underlies mod els like BERT and its descendants like RoBERTa Liu et al. 2019 or SpanBERT Joshi et al. 2020 . In Chapter 10 we explored causal left to right transformers that can serve as the basis for powerful language models models that can eas ily be applied to autoregressive generation problems such as contextual generation summarization and machine translation.', 'Recent efforts include RSR 19 RRS20 which Ô¨Åne tuned an 11 billion parameter language model and GLT 20 which focused on attending over a large corpus of data at test time. Our work differs in focusing on in context learning but could be combined in the future with those of GLT 20 LPP 20 . Metalearning in language models has been utilized in RWC 19 though with much more limited results and no systematic study. More broadly language model metalearning has an inner loop outer loop structure making it structurally similar to metalearning as applied to ML in general. Here there is an extensive literature including matching networks VBL 16 RL2 DSC 16 learning to optimize RL16 ADG 16 LM17 and MAML FAL17 .', 'e.g. Kim et al. 2016 Problem parameter averaging only works for models within the same run Knowledge distillation trains a model to copy the ensemble Specifically it tries to match the description over predicted words Why We want the model to make the same mistakes as an ensemble Shown to increase accuracy notablyStacking What if we have two very different models where prediction of outputs is done in very different ways e.g. a phrase based translation model and a neural MT model Niehues et al. 2017 Stacking uses the output of one system in calculating features for another systemCase Studies in Conditional Language ModelingFrom Images e.g. Karpathy et al. 2015 Input is image features output is text Use standard image encoders e.g.', 'Here s a summary of the main points that we covered Bidirectional encoders can be used to generate contextualized representations of input embeddings using the entire input context. Pretrained language models based on bidirectional encoders can be learned using a masked language model objective where a model is trained to guess the missing information from an input. Pretrained language models can be Ô¨Åne tuned for speciÔ¨Åc applications by adding lightweight classiÔ¨Åer layers on top of the outputs of the pretrained model. Bibliographical and Historical Notes22 Chapter 11 Fine Tuning and Masked Language Models Chaplot D. S. and R. Salakhutdinov. 2018. Knowledge based word sense disambiguation using topic models. AAAI. Clark K. M. T. Luong Q. V. Le and C. D. Manning. 2020. Electra Pre training text encoders as discrimina tors rather than generators. ICLR.']",text-embedding-3-small,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9687264226750372,0.0,0.9319585863235486,0.0454545454545454
155,How does ensemble distillation differ from parameter averaging?,"Ensemble distillation differs from parameter averaging in that ensemble distillation trains a model to copy the ensemble, specifically trying to match the description over predicted words, while parameter averaging is a method where models are written out several times near the end of training and the average of parameters is taken. Ensemble distillation aims to make the model make the same mistakes as an ensemble, which has been shown to increase accuracy notably. On the other hand, parameter averaging is a way to get some benefits of ensembling without the need to use multiple models at test time. 

Source: SEP 775 - Computational Natural Language Processing, lecture04_Conditioned_Generation.","['predictions from multiple models s sLinear Interpolation Take a weighted average of the M model probabilities Second term often set to uniform distribution 1 MLog linear Interpolation Weighted combination of log probabilities normalize Interpolation coefficient often set to uniform distribution 1 MLinear or Log Linear Think of it in logic Linear Logical OR the interpolated model likes any choice that a model gives a high probability use models with models that capture different traits necessary when any model can assign zero probability Log Linear Logical AND interpolated model only likes choices where all models agree use when you want to restrict possible answersParameter Averaging Problem Ensembling means we have to use M models at test time increasing our time memory complexity Parameter averaging is a cheap way to get some good effects of ensembling Basically write out models several times near the end of training and take the average of parametersEnsemble Distillation e.g. Kim et al.', 'e.g. Kim et al. 2016 Problem parameter averaging only works for models within the same run Knowledge distillation trains a model to copy the ensemble Specifically it tries to match the description over predicted words Why We want the model to make the same mistakes as an ensemble Shown to increase accuracy notablyStacking What if we have two very different models where prediction of outputs is done in very different ways e.g. a phrase based translation model and a neural MT model Niehues et al. 2017 Stacking uses the output of one system in calculating features for another systemCase Studies in Conditional Language ModelingFrom Images e.g. Karpathy et al. 2015 Input is image features output is text Use standard image encoders e.g.', 'Note in the entry for labeler vs. labeler ensemble the ensembles are slightly smaller than for other comparisons because we need to exclude the labeler being predicted. All ensembles have at least 3 workers. 36TL DR 6.7B sup. T 0.7 labeler labeler ensem ble length copying ROUGE 1.3B sup logprob 1.3B RM 6.7B sup logprob 6.7B RM labeler 70.8 2.6 73.1 2.9 56.9 0.6 56.4 0.6 56.9 0.6 54.5 1.2 67.5 1.1 54.3 1.2 69.7 1.1 labeler ensemble 73.1 2.9 55.0 5.1 54.5 4.8 66.7 4.7 61.1 11.', 'Yet it is often difÔ¨Åcult to Ô¨Ågure out which ideas are most responsible for the state of the art performance of a large ensemble. Because neural networks are nonconvex training runs with different initial parameters usually give different solutions. Table 11 reports results obtained for the CHUNK and NER task after ten training runs with random initial parameters. Voting the ten network outputs on a per tag basis voting ensemble leads to a small improvement over the average network performance. We have also tried a more sophisticated ensemble approach the ten network output scores before sentence level likelihood were combined with an additional linear layer 4 and then fed to a new sentence level likelihood 13 . The parameters of the combining layers were then trained on the existing training set while keeping the ten networks Ô¨Åxed joined ensemble . This approach did not improve on simple voting. These ensembles come of course at the expense of a ten fold increase of the running time. On the other hand multiple training times could be improved using smart sampling strategies Neal 1996 .']",text-embedding-3-small,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9915270026834316,0.75,0.9673624600780512,0.0666666666666666
156,What are some common evaluation metrics for conditioned language models?,"Some common evaluation metrics for conditioned language models include BLEU, embedding-based metrics like BertScore, BLEURT, COMET, PRISM, and Perplexity. These metrics are used to assess the performance of language models in tasks such as translation, paraphrasing, and generation. 

Source: SEP 775 - Computational Natural Language Processing, lecture04_Conditioned_Generation.","['CNN Often pre trained on large databases such as ImageNetCheck out this review paperHow do we EvaluateBasic Evaluation Paradigm Use parallel test set Use system to generate translations Compare target translations w referenceHuman Evaluation Ask a human to do evaluation Final goal but slow expensive and sometimes inconsistentBLEU Works by comparing n gram overlap w reference Pros Easy to use good for measuring system improvement Cons Often doesn t match human eval bad for comparing very different systemsEmbedding based Metrics Recently many metrics based on neural models BertScore Find similarity between BERT embeddings unsupervised Zhang et al. 2020 BLEURT Train BERT to predict human evaluation scores Sellam et al. 2020 COMET Train model to predict human eval also using source sentence Rei et al. 2020 PRISM Model based on training paraphrasing model Thompson and Post 2020Perplexity Calculate the perplexity of the words in the held out set without doing generation Pros Naturally solves multiple reference problem Cons Doesn t consider decoding or actually generating output.', 'May be reasonable for problems with lots of ambiguity.Which One to Use Meta evaluation runs human evaluation and automatic evaluation on the same outputs calculates correlation Examples WMT Metrics Task for MT Mathur et al. 2021 RealSumm for summarization Evaluation is hard especially with good systems Most metrics had no correlation w human eval over best systems at some WMT 2019 tasksQuestions', 'Below we evaluate the 8 models described in Section 2 the 175 billion parameter parameter GPT 3 and 7 smaller models on a wide range of datasets. We group the datasets into 9 categories representing roughly similar tasks. In Section 3.1 we evaluate on traditional language modeling tasks and tasks that are similar to language modeling such as Cloze tasks and sentence paragraph completion tasks. In Section 3.2 we evaluate on closed book question answering tasks tasks which require using the information stored in the model s parameters to answer general knowledge questions. In Section 3.3 we evaluate the model s ability to translate between languages especially one shot and few shot . In Section 3.4 we evaluate the model s performance on Winograd Schema like tasks. In Section 3.5 we evaluate on datasets that involve commonsense reasoning or question answering.', 'This approach includes ALBERT LCG 19 as well as general HVD15 and 39task speciÔ¨Åc SDCW19 JYS 19 KR16 approaches to distillation of language models. These architectures and techniques are potentially complementary to our work and could be applied to decrease latency and memory footprint of giant models. As Ô¨Åne tuned language models have neared human performance on many standard benchmark tasks considerable effort has been devoted to constructing more difÔ¨Åcult or open ended tasks including question answering KPR 19 IBGC 14 CCE 18 MCKS18 reading comprehension CHI 18 RCM19 and adversarially constructed datasets designed to be difÔ¨Åcult for existing language models SBBC19 NWD 19 . In this work we test our models on many of these datasets. Many previous efforts have focused speciÔ¨Åcally on question answering which constitutes a signiÔ¨Åcant fraction of the tasks we tested on.']",text-embedding-3-small,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999988395584252,0.3333333333333333,0.9200618134989174,0.0714285714285714
157,What is the significance of stacking in model ensembling?,"Stacking is significant in model ensembling when we have two very different models that predict outputs in different ways. It involves using the output of one system to calculate features for another system, which can lead to improved accuracy by leveraging the strengths of each model. (Source: lecture04_Conditioned_Generation)","['e.g. Kim et al. 2016 Problem parameter averaging only works for models within the same run Knowledge distillation trains a model to copy the ensemble Specifically it tries to match the description over predicted words Why We want the model to make the same mistakes as an ensemble Shown to increase accuracy notablyStacking What if we have two very different models where prediction of outputs is done in very different ways e.g. a phrase based translation model and a neural MT model Niehues et al. 2017 Stacking uses the output of one system in calculating features for another systemCase Studies in Conditional Language ModelingFrom Images e.g. Karpathy et al. 2015 Input is image features output is text Use standard image encoders e.g.', 'Yet it is often difÔ¨Åcult to Ô¨Ågure out which ideas are most responsible for the state of the art performance of a large ensemble. Because neural networks are nonconvex training runs with different initial parameters usually give different solutions. Table 11 reports results obtained for the CHUNK and NER task after ten training runs with random initial parameters. Voting the ten network outputs on a per tag basis voting ensemble leads to a small improvement over the average network performance. We have also tried a more sophisticated ensemble approach the ten network output scores before sentence level likelihood were combined with an additional linear layer 4 and then fed to a new sentence level likelihood 13 . The parameters of the combining layers were then trained on the existing training set while keeping the ten networks Ô¨Åxed joined ensemble . This approach did not improve on simple voting. These ensembles come of course at the expense of a ten fold increase of the running time. On the other hand multiple training times could be improved using smart sampling strategies Neal 1996 .', 'Note in the entry for labeler vs. labeler ensemble the ensembles are slightly smaller than for other comparisons because we need to exclude the labeler being predicted. All ensembles have at least 3 workers. 36TL DR 6.7B sup. T 0.7 labeler labeler ensem ble length copying ROUGE 1.3B sup logprob 1.3B RM 6.7B sup logprob 6.7B RM labeler 70.8 2.6 73.1 2.9 56.9 0.6 56.4 0.6 56.9 0.6 54.5 1.2 67.5 1.1 54.3 1.2 69.7 1.1 labeler ensemble 73.1 2.9 55.0 5.1 54.5 4.8 66.7 4.7 61.1 11.', 'predictions from multiple models s sLinear Interpolation Take a weighted average of the M model probabilities Second term often set to uniform distribution 1 MLog linear Interpolation Weighted combination of log probabilities normalize Interpolation coefficient often set to uniform distribution 1 MLinear or Log Linear Think of it in logic Linear Logical OR the interpolated model likes any choice that a model gives a high probability use models with models that capture different traits necessary when any model can assign zero probability Log Linear Logical AND interpolated model only likes choices where all models agree use when you want to restrict possible answersParameter Averaging Problem Ensembling means we have to use M models at test time increasing our time memory complexity Parameter averaging is a cheap way to get some good effects of ensembling Basically write out models several times near the end of training and take the average of parametersEnsemble Distillation e.g. Kim et al.']",text-embedding-3-small,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9609509521143909,0.8,0.9304699138220088,0.0666666666666666
158,How are images used as input for conditioned language models?,"Images are used as input for conditioned language models by representing the image classes as learned d-dimensional embeddings per class and adding the respective embedding to the input representation of every input position together with the positional encodings. This allows the class-conditioned Image Transformer to achieve similar log likelihoods as in unconditioned generation, with significantly higher perceptual quality in the generated images, such as realistic looking images for categories like cars and trucks. Additionally, in tasks like super resolution with high magnification ratio, a very low-resolution image can be used as input in an encoder-decoder configuration with the Image Transformer. 

Source: Image_Transformer.txt","['Generated horses planes and birds also look reasonable. 5.1. Generative Image Modeling Our unconditioned and class conditioned image generation models both use 1D local attention with lq 256 and a total memory size of 512. On CIFAR 10 our best unconditional models achieve a perplexity of 2.90 bits dim on the test set using either DMOL or categorical. For categorical we use 12 layers with d 512 heads 4 feed forward dimension 2048 with a dropout of 0.3. In DMOL our best conÔ¨Åg uses 14 layers d 256 heads 8 feed forward dimension 512 and a dropout of 0.2. This is a considerable improvement over two baselines the PixelRNN van den Oord et al. 2016a and PixelCNN Salimans et al. . Introduced after the Image Transformer the also self attention based Pixel SNAIL model reaches a signiÔ¨Åcantly lower perplexity of 2.85 bits dim on CIFAR 10 Chen et al.', 'e.g. Kim et al. 2016 Problem parameter averaging only works for models within the same run Knowledge distillation trains a model to copy the ensemble Specifically it tries to match the description over predicted words Why We want the model to make the same mistakes as an ensemble Shown to increase accuracy notablyStacking What if we have two very different models where prediction of outputs is done in very different ways e.g. a phrase based translation model and a neural MT model Niehues et al. 2017 Stacking uses the output of one system in calculating features for another systemCase Studies in Conditional Language ModelingFrom Images e.g. Karpathy et al. 2015 Input is image features output is text Use standard image encoders e.g.', '5.2. Conditioning on Image Class We represent the image classes as learned d dimensional embeddings per class and simply add the respective em bedding to the input representation of every input position together with the positional encodings. We trained the class conditioned Image Transformer on CIFAR 10 achieving very similar log likelihoods as in un conditioned generation. The perceptual quality of generatedImage Transformer images however is signiÔ¨Åcantly higher than that of our unconditioned models. The samples from our 8 layer class conditioned models in Table 3 show that we can generate realistic looking images for some categories such as cars and trucks. 5.3. Image Super Resolution Super resolution is the process of recovering a high resolu tion image from a low resolution image while generating realistic and plausible details. Following Dahl et al.', '2015 . In visual planning tasks conditional image generation models could predict future frames of video con ditioned on previous frames and taken actions. In this work we hence also evaluate two different methods of performing conditional image generation with the Im age Transformer. In image class conditional generation we condition on an embedding of one of a small number of image classes. In super resolution with high magniÔ¨Åcation ratio 4x we condition on a very low resolution image employing the Image Transformer in an encoder decoder conÔ¨Åguration Kalchbrenner Blunsom 2013 . In com parison to recent work on autoregressive super resolution Dahl et al. 2017 a human evaluation study found im ages generated by our models to look convincingly natural signiÔ¨Åcantly more often. 2. Background There is a broad variety of types of image generation mod els in the literature.']",text-embedding-3-small,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9648371345076504,1.0,0.8786494700271706,0.0384615384615384
159,What challenges exist in evaluating conditioned language models?,"The challenges in evaluating conditioned language models include the cost of creating a large set of high-quality rationales for rationale augmented training and fine-tuning methods, which is more complex than using simple input-output pairs in normal machine learning. Additionally, traditional few-shot prompting methods can be limited in scalability and efficiency when dealing with complex tasks.","['Recent efforts include RSR 19 RRS20 which Ô¨Åne tuned an 11 billion parameter language model and GLT 20 which focused on attending over a large corpus of data at test time. Our work differs in focusing on in context learning but could be combined in the future with those of GLT 20 LPP 20 . Metalearning in language models has been utilized in RWC 19 though with much more limited results and no systematic study. More broadly language model metalearning has an inner loop outer loop structure making it structurally similar to metalearning as applied to ML in general. Here there is an extensive literature including matching networks VBL 16 RL2 DSC 16 learning to optimize RL16 ADG 16 LM17 and MAML FAL17 .', '2021 in addition to neuro symbolic methods that use formal lan guages instead of natural language Roy and Roth 2015 Chiang and Chen 2019 Amini et al. 2019 Chen et al. 2019 . Second large language models offer the exciting prospect of in context few shot learning via prompting. That is instead of Ô¨Ånetuning a separate language model checkpoint for each new task one can simply prompt the model with a few input output exemplars demonstrating the task. Remarkably this has been successful for a range of simple question answering tasks Brown et al. 2020 . Both of the above ideas however have key limitations. For rationale augmented training and Ô¨Ånetuning methods it is costly to create a large set of high quality rationales which is much more complicated than simple input output pairs used in normal machine learning. For the traditional few shot prompting method used in Brown et al.', 'This approach includes ALBERT LCG 19 as well as general HVD15 and 39task speciÔ¨Åc SDCW19 JYS 19 KR16 approaches to distillation of language models. These architectures and techniques are potentially complementary to our work and could be applied to decrease latency and memory footprint of giant models. As Ô¨Åne tuned language models have neared human performance on many standard benchmark tasks considerable effort has been devoted to constructing more difÔ¨Åcult or open ended tasks including question answering KPR 19 IBGC 14 CCE 18 MCKS18 reading comprehension CHI 18 RCM19 and adversarially constructed datasets designed to be difÔ¨Åcult for existing language models SBBC19 NWD 19 . In this work we test our models on many of these datasets. Many previous efforts have focused speciÔ¨Åcally on question answering which constitutes a signiÔ¨Åcant fraction of the tasks we tested on.', 'Yizhong Wang Swaroop Mishra Pegah Alipoormolabashi Yeganeh Kordi Amirreza Mirzaei Anjana Arunkumar Arjun Ashok Arut Selvan Dhanasekaran Atharva Naik David Stap et al. 2022b. Benchmarking generalization via in context instructions on 1 600 language tasks. arXiv preprint arXiv 2204.07705. Jason Wei Maarten Bosma Vincent Y. Zhao Kelvin Guu Adams Wei Yu Brian Lester Nan Du Andrew M. Dai and Quoc V. Le. 2022a. Finetuned language models are zero shot learners. ICLR. 13Jason Wei Yi Tay Rishi Bommasani Colin Raffel Barret Zoph Sebastian Borgeaud Dani Yogatama Maarten Bosma Denny Zhou Donald Metzler et al. 2022b. Emergent abilities of large language models.']",text-embedding-3-small,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9940819543024294,0.6666666666666666,0.8620086999487082,0.0384615384615384
160,What is the instructor's name for this course?,"The instructor for the course ""Introduction to Computational Natural Language Processing"" is Hamidreza Mahyar. 

Source: NLP_Winter_2024_Course_Outline.txt","['Instruction Type Code Type Hours per term C Classroom instruction 39 L Laboratory workshop or fieldwork T Tutorial DE Distance education Total Hours 39 Resources ISBN Textbook Title Edition Author Publisher Natural Language Processing Jacob Eisenstein Speech and Language Processing Dan Jurafsky and James H.', 'Course Outline 1. COURSE INFORMATION Session Offered Winter 2024 Course Name Introduction to Computational Natural Language Processing Course Code SEP 775 Date s and Time s of lectures Lectures In person Wednesday 3 30 pm 6 30 pm Program Name System and Technology Calendar Description This graduate course introduces fundamental concepts in computational natural language processing NLP and their applications to teaching students how to deal with textual data in Artificial Intelligence.', 'Continuation in a course that uses on line elements will be deemed consent to this disclosure. If you have any questions or concerns about such disclosure please discuss this with the course instructor. ONLINE PROCTORING Some courses may use online proctoring software for tests and exams.', 'http www.mcmaster.ca policy Students AcademicStudies Studentcode.pdf COPYRIGHT AND RECORDING Students are advised that lectures demonstrations performances and any other course material provided by an instructor include copyright protected works. The Copyright Act and copyright law protect every original literary dramatic musical and artistic work including lectures by University instructors The recording of lectures tutorials or other methods of instruction may occur during a course.', 'Recording may be done by either the instructor for the purpose of authorized distribution or by a student for the purpose of personal study. Students should be aware that their voice and or image may be recorded by others during the class. Please speak with the instructor if this is a concern for you.', 'e mail Avenue to Learn A2L LearnLink web pages capa Moodle ThinkingCap etc. . Students should be aware that when they access the electronic components of a course using these elements private information such as first and last names user names for the McMaster e mail accounts and program affiliation may become apparent to all other students in the same course. The available information is dependent on the technology used.', 'Instructor Hamidreza Mahyar E Mail mahyarh@mcmaster.ca Office Hours upon request 2. COURSE SPECIFICS Course Description This course covers some fundamental concepts in natural language processing NLP and how they apply in real world use cases.', 'and Q A Week 12 Project presentation and Q A Week 13 Project presentation and Q A Classes end April 10th 4. ASSESSMENT OF LEARNING Weight 4 Assignments 20 4 Quizzes 10 Midterm Test 35 Group Project 35 TOTAL 100Note that the percentage distribution of each module may be adjusted.']",text-embedding-3-small,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.8880133956910505,1.0,0.8992024611303702,0.0
161,What is the email address of the instructor for this course?,"The email address of the instructor for this course is mahyarh@mcmaster.ca. 

Source: NLP_Winter_2024_Course_Outline","['e mail Avenue to Learn A2L LearnLink web pages capa Moodle ThinkingCap etc. . Students should be aware that when they access the electronic components of a course using these elements private information such as first and last names user names for the McMaster e mail accounts and program affiliation may become apparent to all other students in the same course. The available information is dependent on the technology used.', 'Use the University provided e mail address or maintain a valid forwarding e mail address. Regularly check the official University communications channels. Official University communications are considered received if sent by postal mail by fax or by e mail to the student s designated primary e mail account via their @mcmaster.ca alias.', 'Accept that forwarded e mails may be lost and that e mail is considered received if sent via the student s @mcmaster.ca alias. Check the McMaster Avenue email and course websites on a regular basis during the term. CONDUCT EXPECTATIONS As a McMaster student you have the right to experience and the responsibility to demonstrate respectful and dignified interactions within all of our living learning and working communities.', 'Continuation in a course that uses on line elements will be deemed consent to this disclosure. If you have any questions or concerns about such disclosure please discuss this with the course instructor. ONLINE PROCTORING Some courses may use online proctoring software for tests and exams.', 'Recording may be done by either the instructor for the purpose of authorized distribution or by a student for the purpose of personal study. Students should be aware that their voice and or image may be recorded by others during the class. Please speak with the instructor if this is a concern for you.', 'Instructor Hamidreza Mahyar E Mail mahyarh@mcmaster.ca Office Hours upon request 2. COURSE SPECIFICS Course Description This course covers some fundamental concepts in natural language processing NLP and how they apply in real world use cases.', 'http www.mcmaster.ca policy Students AcademicStudies Studentcode.pdf COPYRIGHT AND RECORDING Students are advised that lectures demonstrations performances and any other course material provided by an instructor include copyright protected works. The Copyright Act and copyright law protect every original literary dramatic musical and artistic work including lectures by University instructors The recording of lectures tutorials or other methods of instruction may occur during a course.', 'Instruction Type Code Type Hours per term C Classroom instruction 39 L Laboratory workshop or fieldwork T Tutorial DE Distance education Total Hours 39 Resources ISBN Textbook Title Edition Author Publisher Natural Language Processing Jacob Eisenstein Speech and Language Processing Dan Jurafsky and James H.']",text-embedding-3-small,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.99999896172979,1.0,0.938974983799755,0.0
162,When is this course held every week?,"The course ""Introduction to Computational Natural Language Processing"" (Course Code: SEP 775) is held in person every Wednesday from 3:30 pm to 6:30 pm.","['Course Outline 1. COURSE INFORMATION Session Offered Winter 2024 Course Name Introduction to Computational Natural Language Processing Course Code SEP 775 Date s and Time s of lectures Lectures In person Wednesday 3 30 pm 6 30 pm Program Name System and Technology Calendar Description This graduate course introduces fundamental concepts in computational natural language processing NLP and their applications to teaching students how to deal with textual data in Artificial Intelligence.', 'and Q A Week 12 Project presentation and Q A Week 13 Project presentation and Q A Classes end April 10th 4. ASSESSMENT OF LEARNING Weight 4 Assignments 20 4 Quizzes 10 Midterm Test 35 Group Project 35 TOTAL 100Note that the percentage distribution of each module may be adjusted.', 'EXTREME CIRCUMSTANCES The University reserves the right to change the dates and deadlines for any or all courses in extreme circumstances e.g. severe weather labour disruptions etc. . Changes will be communicated through regular McMaster communication channels such as McMaster Daily News A2L and or McMaster email.', 'Announcements made in class or placed on Avenue are considered to have been communicated to all students including those individuals that are not in class. Instructor has the right to submit work to software to identify plagiarism. 3. SUB TOPIC S Week 1 Overview Human language Word meanings Word2Vec and Optimization basics Week 2 Intro to neural networks Gradients and backpropagation Week 3 Recurrent Neural Networks and Language Models Week', '3 Recurrent Neural Networks and Language Models Week 4 Conditioned Generation Week 5 Part 1 Self attention and transformers Part 2 Seq2Seq models Week 6 Midterm Test Week 7 Midterm Recess No class Week 8 Prompting Instruction Finetuning and RLHF Week 9 Question Answering systems Week 10 Multimodal models Week 11 Project presentation and Q A Week 12 Project presentation and Q', 'to build systems in PyTorch for some of the major problems in NLP Word meaning dependency parsing machine translation question answeringCourse work and grading policy 5 4 Assignments 20 4 Quizzes 10 Midterm 35 Final Group Project 1 3 people 34 Late day policy NO free late days 1 off course grade per day late Assignments not accepted more than 3 days late unless given permission in advance Collaboration policy Understand allowed collaboration and how', 'Instructor Hamidreza Mahyar E Mail mahyarh@mcmaster.ca Office Hours upon request 2. COURSE SPECIFICS Course Description This course covers some fundamental concepts in natural language processing NLP and how they apply in real world use cases.', 'Continuation in a course that uses on line elements will be deemed consent to this disclosure. If you have any questions or concerns about such disclosure please discuss this with the course instructor. ONLINE PROCTORING Some courses may use online proctoring software for tests and exams.']",text-embedding-3-small,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.8450261749585097,1.0,0.8788097786964015,0.0555555555555555
163,What is the name of the TA for this course?,"The name of the TA for this course is AI Teaching Assistant (AI TA). 

Source: SEP 775_FinalProjects_2.txt","['Instruction Type Code Type Hours per term C Classroom instruction 39 L Laboratory workshop or fieldwork T Tutorial DE Distance education Total Hours 39 Resources ISBN Textbook Title Edition Author Publisher Natural Language Processing Jacob Eisenstein Speech and Language Processing Dan Jurafsky and James H.', 'file name: SEP775 FinalProjects 2. McMaster University SEP 775 Final Projects Project 1 AI Teaching Assistant Project Overview The goal of this project is to create an AI Teaching Assistant AI TA . The AI TA aims to provide an interactive platform for students to ask questions and receive guidance on course materials mirroring the support traditionally offered by human teaching assistants.', 'Course Outline 1. COURSE INFORMATION Session Offered Winter 2024 Course Name Introduction to Computational Natural Language Processing Course Code SEP 775 Date s and Time s of lectures Lectures In person Wednesday 3 30 pm 6 30 pm Program Name System and Technology Calendar Description This graduate course introduces fundamental concepts in computational natural language processing NLP and their applications to teaching students how to deal with textual data in Artificial Intelligence.', 'and Q A Week 12 Project presentation and Q A Week 13 Project presentation and Q A Classes end April 10th 4. ASSESSMENT OF LEARNING Weight 4 Assignments 20 4 Quizzes 10 Midterm Test 35 Group Project 35 TOTAL 100Note that the percentage distribution of each module may be adjusted.', 'file name: SEP775 Assignment3. McMaster University SEP 775 Assignment 3 TANDA Approach for QA System Enhancement Objectives Explore TANDA Transfer And Adapt methodology to improve Question Answering QA systems using pre trained Transformer models focusing on sequential fine tuning te chniques. 1. Introduction and Theory 15 Summarize the TANDA methodology s principles focusing on its novelty and the rationale behind sequential fine tuning.', 'Continuation in a course that uses on line elements will be deemed consent to this disclosure. If you have any questions or concerns about such disclosure please discuss this with the course instructor. ONLINE PROCTORING Some courses may use online proctoring software for tests and exams.', 'e mail Avenue to Learn A2L LearnLink web pages capa Moodle ThinkingCap etc. . Students should be aware that when they access the electronic components of a course using these elements private information such as first and last names user names for the McMaster e mail accounts and program affiliation may become apparent to all other students in the same course. The available information is dependent on the technology used.', 'Instructor Hamidreza Mahyar E Mail mahyarh@mcmaster.ca Office Hours upon request 2. COURSE SPECIFICS Course Description This course covers some fundamental concepts in natural language processing NLP and how they apply in real world use cases.']",text-embedding-3-small,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999988105789236,1.0,0.8502803568448247,0.0
164,What is Word2vec?,"Word2vec is a framework for learning word vectors that represents each word in a fixed vocabulary as a low-dimensional vector. It is a model used in natural language processing to generate word embeddings by capturing semantic relationships between words based on their usage in context. The Word2vec model is trained on a large corpus of text data to learn continuous vector representations of words in a way that words with similar meanings are closer together in the vector space. It has different architectures such as Skip-gram and Continuous Bag of Words (CBOW) models for training word embeddings efficiently.

Source: Lecture notes and materials for SEP 775 - Computational Natural Language Processing.","['The word2vec model is a probabilistic model specified as follows where uw refers to the row of U corresponding to word w V and likewise for V pU V o c exp u o vc w V exp u wvc 4 This may be familiar to you as the softmax function which takes arbitrary scores here one for each word in the vocabulary resulting from dot products and produces a probability distribution where larger scored things get higher probability.', 'However many of the details of word2vec will hold true in methods that we ll proceed to further in the course so we ll focus our time on that. 3.2 Word2vec model and objective The word2vec model represents each word in a fixed vocabulary as a low dimensional much smaller than vocabulary size vector.', '3.3 Estimating a word2vec model from a corpus How do we train word2vec in practice Specifying the word2vec model is relatively transparent from the math we ve given above one constructs matrices U and V and can write out the math of the probability.', '718 0.147 0.051 19Word meaning as a neural word vector visualization 0.286 0.792 0.177 0.107 0.109 0.542 0.349 0.271 0.487 expect 203. Word2vec Overview 21 Word2vec Mikolov et al. 2013 is a framework for learning word vectors Idea We have a large corpus body of text', 'It consistently outperforms word2vec on the word analogy task given the same corpus vocabulary window size and training time. It achieves better results faster and also obtains the best results irrespective of speed. 2 Evaluation of Word Vectors So far we have discussed methods such as the Word2Vec and GloVe methods to train and discover latent vector representations of natural language words in a semantic space.', '4The code is available at https code.google.com p word2vec 1118 T. Mikolov A. Deoras D. Povey L. Burget J. Àá Cernock y. Strategies for Training Large Scale Neural Network Language Models In Proc. Automatic Speech Recognition and Understand ing 2011. 19 T. Mikolov.', 'With word2vec we train the skip gram SG and continuous bag of words CBOW models on the 6 billion token corpus Wikipedia 2014 Giga word 5 with a vocabulary of the top 400 000 most frequent words and a context window size of 10. We used 10 negative samples which we show in Section 4.6 to be a good choice for this corpus.', 'in window of size 2 outside context words in window of size 2 ùëÉùë§ùë° 2 ùë§ùë° ùëÉùë§ùë° 1 ùë§ùë° 23 ùëÉùë§ùë° 2 ùë§ùë° ùëÉùë§ùë° 1 ùë§ùë°Word2vec objective function For each position']",text-embedding-3-small,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999999999999994,1.0,0.9477978655755558,0.0625
165,What are the main learning goals of the course?,"The main learning goals of the course include:
1. Understanding the foundations of modern methods for deep learning applied to NLP, starting with basics and then covering key methods used in NLP in 2024 such as word vectors, feed-forward networks, recurrent networks, attention, encoder-decoder models, transformers, and large pre-trained language models.
2. Developing a big picture understanding of human languages and the challenges in understanding and producing them using computers.
3. Building systems in PyTorch for some NLP tasks.
(Source: SEP 775 - Computational Natural Language Processing)","['learning goals 4 1. The foundations of the effective modern methods for deep learning applied to NLP Basics first then key methods used in NLP in 2024 Word vectors feed forward networks recurrent networks attention encoder decoder models transformers large pre trained language models etc. 2. A big picture understanding of human languages and the difficulties in understanding and producing them via computers 3. An understanding of and ability to build systems in PyTorch for some of', 'Percentage grades will be converted to letter grades and grade points per the University calendar. 5. LEARNING OUTCOMES By the end of this course the students will know about Introduction to Deep Learning Word representations Language models Pre trained language models Text generation Seq2Seq models Prompting Feedback systems Multimodal models NLP applications e.g. Q A systems 6.', 'and Q A Week 12 Project presentation and Q A Week 13 Project presentation and Q A Classes end April 10th 4. ASSESSMENT OF LEARNING Weight 4 Assignments 20 4 Quizzes 10 Midterm Test 35 Group Project 35 TOTAL 100Note that the percentage distribution of each module may be adjusted.', 'One fundamental and still quite open problem in building language learning machines is the question of representa tion how should we represent language in a computer such that the computer can robustly process and or generate it This is where this course focuses on the tools provided by deep learning a highly effec tive toolkit for representing both the wild variety of natural language and some of the rules and structures it sometimes adheres to.', 'Optimization basics 6. Looking at word vectors Key learning today The astounding result that word meaning can be represented rather well by a high dimensional vector of real numbersCourse logistics in brief 3 Instructor Hamidreza Mahyar Head TA Reza Namazi Time Wednesday 3 30pm 6 30pm We put a lot of other important information on Avenue to Learn. Please read itWhat do we hope to teach A.k.a.', 'e mail Avenue to Learn A2L LearnLink web pages capa Moodle ThinkingCap etc. . Students should be aware that when they access the electronic components of a course using these elements private information such as first and last names user names for the McMaster e mail accounts and program affiliation may become apparent to all other students in the same course. The available information is dependent on the technology used.', 'Announcements made in class or placed on Avenue are considered to have been communicated to all students including those individuals that are not in class. Instructor has the right to submit work to software to identify plagiarism. 3. SUB TOPIC S Week 1 Overview Human language Word meanings Word2Vec and Optimization basics Week 2 Intro to neural networks Gradients and backpropagation Week 3 Recurrent Neural Networks and Language Models Week', 'Martin Deep Learning Ian Goodfellow Yoshua Bengio and Aaron Courville Prerequisite s Proficiency in Python programming language Some Machine Learning and or Deep Learning knowledge is a plus Corequisite s Antirequisite s Course Specific Policies All work must be shown to get full credit. Specific policy of Projects Assignments submissionsProjects Assignments will be conducted online by using related software.']",text-embedding-3-small,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999999999999996,1.0,0.9699384877554512,0.0588235294117647
166,How is the course grade distributed?,"The course grade is distributed as follows:
- 4 Assignments: 20%
- 4 Quizzes: 10%
- Midterm Test: 35%
- Group Project: 35%
(Source: lecture01_wordvecs1.txt)","['and Q A Week 12 Project presentation and Q A Week 13 Project presentation and Q A Classes end April 10th 4. ASSESSMENT OF LEARNING Weight 4 Assignments 20 4 Quizzes 10 Midterm Test 35 Group Project 35 TOTAL 100Note that the percentage distribution of each module may be adjusted.', 'Percentage grades will be converted to letter grades and grade points per the University calendar. 5. LEARNING OUTCOMES By the end of this course the students will know about Introduction to Deep Learning Word representations Language models Pre trained language models Text generation Seq2Seq models Prompting Feedback systems Multimodal models NLP applications e.g. Q A systems 6.', 'to build systems in PyTorch for some of the major problems in NLP Word meaning dependency parsing machine translation question answeringCourse work and grading policy 5 4 Assignments 20 4 Quizzes 10 Midterm 35 Final Group Project 1 3 people 34 Late day policy NO free late days 1 off course grade per day late Assignments not accepted more than 3 days late unless given permission in advance Collaboration policy Understand allowed collaboration and how', 'All submitted work is subject to normal verification that standards of academicintegrity have been upheld e.g. on line search other software etc. . For more details about McMaster s use of Turnitin.com please go to www.mcmaster.ca academicintegrity. COURSES WITH AN ON LINE ELEMENT Some courses may use on line elements e.g.', 'e mail Avenue to Learn A2L LearnLink web pages capa Moodle ThinkingCap etc. . Students should be aware that when they access the electronic components of a course using these elements private information such as first and last names user names for the McMaster e mail accounts and program affiliation may become apparent to all other students in the same course. The available information is dependent on the technology used.', 'Accept that forwarded e mails may be lost and that e mail is considered received if sent via the student s @mcmaster.ca alias. Check the McMaster Avenue email and course websites on a regular basis during the term. CONDUCT EXPECTATIONS As a McMaster student you have the right to experience and the responsibility to demonstrate respectful and dignified interactions within all of our living learning and working communities.', 'Academic dishonesty is to knowingly act or fail to act in a way that results or could result in unearned academic credit or advantage. This behaviour can result in serious consequences e.g. the grade of zero on an assignment loss of credit with a notation on the transcript notation reads Grade of F assigned for academic dishonesty and or suspension or expulsion from the university.', 'Q A systems 6. COURSE OUTLINE APPROVED ADVISORY STATEMENTS ANTI DISCRIMINATION The Faculty of Engineering is concerned with ensuring an environment that is free of all discrimination. If there is a problem individuals are reminded that they should contact the Associate Director Graduate Studies the Sexual Harassment Officer or the Human Rights Consultant as soon as possible.']",text-embedding-3-small,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9898862143010954,1.0,0.9575546741286832,0.0
167,What is the policy for late submissions in this course?,"The late submission policy for this course states that there are NO free late days, and there is a penalty of 1 off course grade per day late. Assignments are not accepted more than 3 days late unless permission is given in advance.","['to build systems in PyTorch for some of the major problems in NLP Word meaning dependency parsing machine translation question answeringCourse work and grading policy 5 4 Assignments 20 4 Quizzes 10 Midterm 35 Final Group Project 1 3 people 34 Late day policy NO free late days 1 off course grade per day late Assignments not accepted more than 3 days late unless given permission in advance Collaboration policy Understand allowed collaboration and how', 'Completed Projects Assignments should be uploaded to the drop box before midnight of the due date. Projects Assignments missed due to legitimate reasons must be completed later mutually agreed with the instructor. Departmental Policies The use of cell phones iPods laptops and other personal electronic devices are prohibited from the classroom during the class time unless the instructor makes an explicit exception.', 'Students should submit their request to their Faculty Office normally within 10 working days of the beginning of term in which they anticipate a need for accommodation or to the Registrar s Office prior to their examinations. Students should also contact their instructors as soon as possible to make alternative arrangements for classes assignments and tests.', 'Adherence to Guidelines 10 Submissions should follow the provided guidelines including format labeling and adherence to the deadline. McMaster University 2', 'Submission Guidelines Submit all code in a ZIP file including Jupyter Notebooks and a detailed PDF report with written explanations and visualizations. Clearly label each part and question in your submissions. Deadline Feb. 7 2024 Rubric and Expectations Code Quality and Functionality 40 Code should be well organized commented and functioning as intended. The use of Python and relevant libraries e.g.', 'EXTREME CIRCUMSTANCES The University reserves the right to change the dates and deadlines for any or all courses in extreme circumstances e.g. severe weather labour disruptions etc. . Changes will be communicated through regular McMaster communication channels such as McMaster Daily News A2L and or McMaster email.', 'and Q A Week 12 Project presentation and Q A Week 13 Project presentation and Q A Classes end April 10th 4. ASSESSMENT OF LEARNING Weight 4 Assignments 20 4 Quizzes 10 Midterm Test 35 Group Project 35 TOTAL 100Note that the percentage distribution of each module may be adjusted.', 'A2L etc. using plagiarism detection a service supported by Turnitin.com so it can be checked for academic dishonesty. Students who do not wish their work to be submitted through the plagiarism detection software must inform the Instructor before the assignment is due. No penalty will be assigned to a student who does not submit work to the plagiarism detection software.']",text-embedding-3-small,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9748806789661276,0.25,0.9632243232391188,0.0
168,What does the course hope to teach regarding word vectors?,"The course hopes to teach that word meaning can be represented rather well by a high-dimensional vector of real numbers, and it will cover topics such as word embeddings, intrinsic and extrinsic evaluation of word vectors, training model weights and parameters, and using expert-built data structures like WordNet as additional input for creating word vectors.","['Optimization basics 6. Looking at word vectors Key learning today The astounding result that word meaning can be represented rather well by a high dimensional vector of real numbersCourse logistics in brief 3 Instructor Hamidreza Mahyar Head TA Reza Namazi Time Wednesday 3 30pm 6 30pm We put a lot of other important information on Avenue to Learn. Please read itWhat do we hope to teach A.k.a.', 'Then it extends our discussion of word vectors in terchangeably called word embeddings by seeing how they can be evaluated intrinsically and extrinsically. As we proceed we discuss the example of word analogies as an intrinsic evaluation technique and how it can be used to tune word embedding techniques. We then discuss training model weights parameters and word vectors for ex trinsic tasks.', 'The word vectors can also be initialized to random values then estimated solely through task learning which we might call learning from scratch. 6 We can use expert built data structures like WordNet as additional input to creating word vectors.', 'LSA HAL . While these methods effectively leverage global statistical information they are primarily used to capture word similarities and do poorly on tasks such as word analogy indi cating a sub optimal vector space structure. The other set of methods are shallow window based e.g. the skip gram and the CBOW mod els which learn word embeddings by making predictions in local context windows.', 'However many of the details of word2vec will hold true in methods that we ll proceed to further in the course so we ll focus our time on that. 3.2 Word2vec model and objective The word2vec model represents each word in a fixed vocabulary as a low dimensional much smaller than vocabulary size vector.', 'Here we see that the word vectors are classiÔ¨Åed correctly on some extrinsic classiÔ¨Åcation task. Now if we retrain only two of those vectors because of a limited training set size then we see in Figure 7 that one of the words gets misclassiÔ¨Åed because the boundary shifts as a result of word vector updates. Thus word vectors should not be retrained if the training data set is small.', 'file name: lecture01 wordvecs1. Hamidreza Mahyar mahyarh@mcmaster.ca Computational Natural Language Processing Overview of NLP and Word VectorsLecture Plan 2 Lecture 1 Introduction and Word Vectors 1. The course 2. Human language and word meaning 3. Word2vec introduction 4. Word2vec objective function gradients 5. Optimization basics 6.', 'vectors Idea We have a large corpus body of text a long list of words Every word in a fixed vocabulary is represented by a vector Go through each position t in the text which has a center word c and context outside words o Use the similarity of the word vectors for c and o to calculate the probability of o given c or vice versa Keep adjusting the word vectors to maximize this probabilityWord2Vec Overview Example windows and process for computing']",text-embedding-3-small,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.8936836759636598,1.0,0.9482233776159504,0.0555555555555555
169,How does Word2vec represent word meaning?,"Word2vec represents word meaning by training word vectors to predict the surrounding words in a sentence. These word vectors can be seen as representing the distribution of the context in which a word appears. The values in these vectors are related logarithmically to the probabilities computed by the output layer, and the sum of two word vectors is related to the product of the two context distributions.","['The word2vec model is a probabilistic model specified as follows where uw refers to the row of U corresponding to word w V and likewise for V pU V o c exp u o vc w V exp u wvc 4 This may be familiar to you as the softmax function which takes arbitrary scores here one for each word in the vocabulary resulting from dot products and produces a probability distribution where larger scored things get higher probability.', 'As the word vectors are trained to predict the surrounding words in the sentence the vectors can be seen as representing the distribution of the context in which a word appears. These values are related logarithmically to the probabilities computed by the output layer so the sum of two word vectors is related to the product of the two context distributions.', '718 0.147 0.051 19Word meaning as a neural word vector visualization 0.286 0.792 0.177 0.107 0.109 0.542 0.349 0.271 0.487 expect 203. Word2vec Overview 21 Word2vec Mikolov et al. 2013 is a framework for learning word vectors Idea We have a large corpus body of text', 'However many of the details of word2vec will hold true in methods that we ll proceed to further in the course so we ll focus our time on that. 3.2 Word2vec model and objective The word2vec model represents each word in a fixed vocabulary as a low dimensional much smaller than vocabulary size vector.', 'It consistently outperforms word2vec on the word analogy task given the same corpus vocabulary window size and training time. It achieves better results faster and also obtains the best results irrespective of speed. 2 Evaluation of Word Vectors So far we have discussed methods such as the Word2Vec and GloVe methods to train and discover latent vector representations of natural language words in a semantic space.', '3.3 Estimating a word2vec model from a corpus How do we train word2vec in practice Specifying the word2vec model is relatively transparent from the math we ve given above one constructs matrices U and V and can write out the math of the probability.', 'The word s meaning is distributed across the whole vector for this reason these vectors are sometimes called distributed representations.5 As corpora grew scalability became a challenge because the number of observable contexts grew as well.', 'This evaluation scheme favors models that produce dimensions of mean ing thereby capturing the multi clustering idea of distributed representations Bengio 2009 . The two main model families for learning word vectors are 1 global matrix factorization meth ods such as latent semantic analysis LSA Deer wester et al. 1990 and 2 local context window methods such as the skip gram model of Mikolov et al. 2013c .']",text-embedding-3-small,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.999223832433065,0.75,0.9244447182434832,0.0666666666666666
170,What is the main focus of this lecture on neural networks?,"The main focus of this lecture on neural networks is on understanding the core technology of neural nets, including backpropagation, forward pass, backward pass, and the importance of non-linearities in neural networks for function approximation and classification. It also discusses the motivation for using recurrent neural networks as a language model.","['for checking your implementation In the old days we hand wrote everything doing this everywhere was the key test Now much less needed you can use it to check layers are correctly implemented 78Summary We ve mastered the core technology of neural nets Backpropagation recursively and hence efficiently apply the chain rule along computation graph downstream gradient upstream gradient x local gradient Forward pass compute results of operations and save intermediate values Backward pass apply chain rule to compute gradients 79Why learn all these details', 'In the next set of notes we study neural networks as a class of non linear models that have performed particularly well in deep learning applications. Figure 10 Here we see that the non linear decision boundary allows for much better classiÔ¨Åcation of datapoints.', 'Neural networks are a family of classiÔ¨Åers with non linear decision boundary as seen in Figure 1. Now that we know the sort of decision boundaries neural networks create let us see how they manage doing so. 1.1 A Neuron A neuron is a generic computational unit that takes n inputs and produces a single output. What differentiates the outputs of different neurons is their parameters also referred to as their weights .', 'Lastly we will motivate the use of recurrent neural networks as a language model. 1 Neural Networks Foundations Figure 1 We see here how a non linear decision boundary separates the data very well. This is the prowess of neural networks. Fun Fact Neural networks are biologically in spired classiÔ¨Åers which is why they are often called artiÔ¨Åcial neural networks to distinguish them from the organic kind.', 'However in reality human neural networks are so much more capable and complex from artiÔ¨Åcial neural net works that it is usually better to not draw too many parallels between the two. We established in our previous discussions the need for non linear classiÔ¨Åers since most data are not linearly separable and thus our classiÔ¨Åcation performance on them is limited.', 'logistic 1.702ùë•Non linearities i.e. f on previous slide Why they re needed Neural networks do function approximation e.g. regression or classification Without non linearities deep neural networks can t do anything more than a linear transform Extra layers could just be compiled down into a single linear transform W1 W2 x Wx But with more layers that include non linearities they can approximate any complex function 5Training with cross', 'Figure 1 The Ô¨Årst deep neural network architecture model for NLP presented by Bengio et al. Figure 2 A simpliÔ¨Åed representation of Figure 1.', 'A new family of neural networks Recurrent Neural Networks RNNs This is the most important concept in the class It leads to GPT 3 and ChatGPT Important and used in Ass1 but not the only way to build LMs0 model power Training error Test error Classic view Regularization works to prevent overfitting when we have a lot of features or later a very powerful deep model etc.']",text-embedding-3-small,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.0000000000000002,0.75,0.9198067207583388,0.0588235294117647
171,What are the non-linear functions mentioned for neural networks?,"The non-linear functions mentioned for neural networks are sigmoid, tanh, and ReLU. 

Source: lecture02_Neural_Nets.txt","['logistic 1.702ùë•Non linearities i.e. f on previous slide Why they re needed Neural networks do function approximation e.g. regression or classification Without non linearities deep neural networks can t do anything more than a linear transform Extra layers could just be compiled down into a single linear transform W1 W2 x Wx But with more layers that include non linearities they can approximate any complex function 5Training with cross', 'We will see that a neuron can be one of many functions that allows for non linearities to accrue in the network. lently formulate a 1 1 exp wT b x 1 Figure 2 This image captures how in a sigmoid neuron the input vector x is Ô¨Årst scaled summed added to a bias unit and then passed to the squashing sigmoid function.', 'In the next set of notes we study neural networks as a class of non linear models that have performed particularly well in deep learning applications. Figure 10 Here we see that the non linear decision boundary allows for much better classiÔ¨Åcation of datapoints.', 'Neural networks are a family of classiÔ¨Åers with non linear decision boundary as seen in Figure 1. Now that we know the sort of decision boundaries neural networks create let us see how they manage doing so. 1.1 A Neuron A neuron is a generic computational unit that takes n inputs and produces a single output. What differentiates the outputs of different neurons is their parameters also referred to as their weights .', 'We now introduce the need for non linear classiÔ¨Åcation models such as neural networks. We see in Figure 9 that a linear classiÔ¨Åer mis classiÔ¨Åes many datapoints. Using a non linear decision boundary as shown in Figure 10 we manage to classify all training points accu rately. Although oversimpliÔ¨Åed this is a classic case demonstrating the need for non linear decision boundaries.', 'Thus we must typically divide the outputs of each neuron during testing by a certain value it is left as an exercise to the reader to determine what this value should be in order for the expected outputs during training and testing to be equivalent. 2.4 Neuron Units So far we have discussed neural networks that contain sigmoidal neurons to introduce nonlinearities however in many applications better networks can be designed using other activation functions.', 'However in reality human neural networks are so much more capable and complex from artiÔ¨Åcial neural net works that it is usually better to not draw too many parallels between the two. We established in our previous discussions the need for non linear classiÔ¨Åers since most data are not linearly separable and thus our classiÔ¨Åcation performance on them is limited.', 'and shifted sigmoid 2 as steep 1 1 tanh z 2logistic 2z 1 Logistic and tanh are still used e.g. logistic to get a probability However now for deep networks the first thing to try is ReLU it trains quickly and performs well due to good gradient backflow.']",text-embedding-3-small,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.98617138278318,0.6666666666666666,0.952406610320598,0.0555555555555555
172,What is the purpose of non-linearities in neural networks?,"The purpose of non-linearities in neural networks is to allow for function approximation, such as regression or classification. Without non-linearities, deep neural networks can only perform a linear transform, but with non-linearities, they can approximate complex functions effectively. Non-linearities are essential for neural networks to handle non-linear relationships in data and achieve better performance in tasks like classification and regression.","['logistic 1.702ùë•Non linearities i.e. f on previous slide Why they re needed Neural networks do function approximation e.g. regression or classification Without non linearities deep neural networks can t do anything more than a linear transform Extra layers could just be compiled down into a single linear transform W1 W2 x Wx But with more layers that include non linearities they can approximate any complex function 5Training with cross', 'However in reality human neural networks are so much more capable and complex from artiÔ¨Åcial neural net works that it is usually better to not draw too many parallels between the two. We established in our previous discussions the need for non linear classiÔ¨Åers since most data are not linearly separable and thus our classiÔ¨Åcation performance on them is limited.', 'We will see that a neuron can be one of many functions that allows for non linearities to accrue in the network. lently formulate a 1 1 exp wT b x 1 Figure 2 This image captures how in a sigmoid neuron the input vector x is Ô¨Årst scaled summed added to a bias unit and then passed to the squashing sigmoid function.', 'Neural networks are a family of classiÔ¨Åers with non linear decision boundary as seen in Figure 1. Now that we know the sort of decision boundaries neural networks create let us see how they manage doing so. 1.1 A Neuron A neuron is a generic computational unit that takes n inputs and produces a single output. What differentiates the outputs of different neurons is their parameters also referred to as their weights .', 'We now introduce the need for non linear classiÔ¨Åcation models such as neural networks. We see in Figure 9 that a linear classiÔ¨Åer mis classiÔ¨Åes many datapoints. Using a non linear decision boundary as shown in Figure 10 we manage to classify all training points accu rately. Although oversimpliÔ¨Åed this is a classic case demonstrating the need for non linear decision boundaries.', 'and shifted sigmoid 2 as steep 1 1 tanh z 2logistic 2z 1 Logistic and tanh are still used e.g. logistic to get a probability However now for deep networks the first thing to try is ReLU it trains quickly and performs well due to good gradient backflow.', 'Pre sumably this method works because in high dimen sional spaces there is a high probability for long term components to be orthogonal to short term ones. This would allow the Hessian to rescale these components independently. In practice one can not guarantee that this property holds. As discussed in section 2.3 this method is able to deal with the exploding gradient as well.', 'Thus we must typically divide the outputs of each neuron during testing by a certain value it is left as an exercise to the reader to determine what this value should be in order for the expected outputs during training and testing to be equivalent. 2.4 Neuron Units So far we have discussed neural networks that contain sigmoidal neurons to introduce nonlinearities however in many applications better networks can be designed using other activation functions.']",text-embedding-3-small,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.0000000000000004,1.0,0.9177981127096436,0.05
173,What is cross entropy loss used for in PyTorch?,Cross entropy loss is used in PyTorch to minimize the negative log probability of the correct class in order to train neural networks effectively by comparing the true probability distribution with the computed model probability distribution. It helps in optimizing the model's performance by adjusting the parameters to reduce the difference between the predicted output and the actual output.,"['can approximate any complex function 5Training with cross entropy loss you use this in PyTorch Until now our objective was stated as to maximize the probability of the correct class y or equivalently we can minimize the negative log probability of that class Now restated in terms of cross entropy a concept from information theory Let the true probability distribution be p let our computed model probability be q The cross entropy is Assuming a ground truth or true or gold or target probability distribution that', 'What makes the cross entropy useful is that the cross entropy H p m is an up per bound on the entropy H p .', 'Using the Cross entropy loss function we calculate the loss of such a training example as C j 1 yj log p yj 1 x C j 1 yj log exp Wj x C c 1 exp Wc x Of course the above summation will be a sum over C 1 zero values since yj is 1 only at a single index at least for now implying that x belongs to only 1 correct class.', 'Between two models m1 and m2 the more accurate model will be the one with the lower cross entropy. The cross entropy can never be lower than the true entropy so a model cannot err by underestimating the true entropy. We are Ô¨Ånally ready to see the relation between perplexity and cross entropy as we saw it in Eq. 3.51.', '7 Cross entropy loss with respect to logits ÀÜ y softmax Œ∏ J CE y ÀÜ y what is J Œ∏ The gradient is J Œ∏ ÀÜ y y or ÀÜ y y T if y is a column vector . 4These identities will be enough to let you quickly compute the gradients for many neural networks. However it s important to know how to compute Jacobians for other functions as well in case they show up.', 'After extending this trend by two more orders of magnitude we observe only a slight if any departure from the power law. One might worry that these improvements in cross entropy loss come only from modeling spurious details of our training corpus. However we will see in the following sections that improvements in cross entropy loss lead to consistent performance gains across a broad spectrum of natural language tasks.', '3.51. Cross entropy is deÔ¨Åned in the limit as the length of the observed word sequence goes to inÔ¨Ånity. We will need an approximation to cross entropy relying on a sufÔ¨Åciently long sequence of Ô¨Åxed length.', 'Performance measured in terms of cross entropy validation loss follows a power law trend with the amount of compute used for training. The power law behavior observed in KMH 20 continues for an additional two orders of magnitude with only small deviations from the predicted curve. For this Ô¨Ågure we exclude embedding parameters from compute and parameter counts.']",text-embedding-3-small,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.969708349024699,0.75,0.9802715673311097,0.0526315789473684
174,What is the significance of the backpropagation algorithm?,"The significance of the backpropagation algorithm is that it allows for the efficient calculation of error gradients and updates for parameters in a neural network model through the use of the chain rule of differentiation. This technique enables the network to learn from data more effectively, reduce overfitting, and improve overall performance on the given task by propagating errors backward through the network layers during training. Additionally, backpropagation allows for the training of neural networks by updating parameters using gradient descent or its variants, such as stochastic gradient descent (SGD), based on the calculated loss gradients for each parameter in the feedforward computation of the model.

Source: Lecture02 - Computational Natural Language Processing","['. . 1 . Such a recursive procedure is what makes backpropa gation a computationally affordable procedure. 2 Neural Networks Tips and Tricks Having discussed the mathematical foundations of neural networks we will now dive into some tips and tricks commonly employed when using neural networks in practice. 2.1 Gradient Check In the last section we discussed in detail how to calculate error gradients updates for parameters in a neural network model via calculus based analytic methods.', 'Image credits to Srivastava et al. In practice the way we introduce dropout is that we take the out put h of each layer of neurons and keep each neuron with prob ability p and else set it to 0. Then during back propagation we only pass gradients through neurons that were kept alive during the forward pass. Finally during testing we compute the forward pass using all of the neurons in the network.', 'The result is that the network typically learns more meaningful informa tion from the data is less likely to overÔ¨Åt and usually obtains higher performance overall on the task at hand. One intuitive reason why this technique should be so effective is that what dropout is doing is essentially doing is training exponentially many smaller networks at once and averaging over their predictions. Dropout applied to an artiÔ¨Åcial neural network.', 'i f z 2 i a 1 j Œ¥ 2 i a 1 j We see above that the gradient reduces to the product Œ¥ 2 i a 1 j where Œ¥ 2 i is essentially the error propagating backwards from the i th neu ron in layer 2. a 1 j is an input fed to i th neuron in layer 2 when scaled by Wij.', 'Since we typically update parameters using gradient descent or a variant such as SGD we typically need the gradient information for any parameter as required in the update equation Œ∏ t 1 Œ∏ t Œ± Œ∏ t J Backpropagation is technique that allows us to use the chain rule of differentiation to calculate loss gradients for any parameter used in the feed forward computation on the model.', 'We then dis cuss how they can be trained using a distributed gradient descent technique known as backpropagation. We will see how the chain rule can be used to make parameter updates sequentially. After a rigorous mathematical discussion of neural networks we will discuss some practical tips and tricks in training neural networks involving neuron units non linearities gradient checks Xavier parameter ini tialization learning rates Adagrad etc.', 'Thus we should use vectorized implementation in prac tice. Furthermore we should also reduce redundant calculations in backpropagation for instance notice that Œ¥ k depends directly on Œ¥ k 1 . Thus we should ensure that when we update W k using Œ¥ k 1 we save Œ¥ k 1 to later derive Œ¥ k and we then repeat this for k 1 . . . 1 .', 'W k is the transfer matrix that maps the output from the k th layer to the input to the k 1 th Thus W 1 W and W 2 U to put this new generalized notation in perspective of Section 1.3. Backpropagation Notation xi is an input to the neural network. s is the output of the neural net work.']",text-embedding-3-small,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.000000000000001,1.0,0.9435822999087664,0.0416666666666666
175,What does the Jacobian matrix represent in the context of neural networks?,"The Jacobian matrix in the context of neural networks represents a matrix of partial derivatives. It is used to describe how the outputs of a function change with respect to its inputs, particularly in the context of multi-variable functions and the chain rule in calculus. The Jacobian matrix helps in understanding the sensitivity of the outputs to changes in the inputs, which is crucial for optimizing neural network models.

Source: Lecture02_Neural_Nets","['activation Function 17Example Jacobian Elementwise activation Function 18Other Jacobians Compute these at home for practice Check your answers with the lecture notes 19Other Jacobians Compute these at home for practice Check your answers with the lecture notes 20Other Jacobians Compute these at home for practice Check your answers with the lecture notes Fine print This is the correct Jacobian. Later we discuss the shape convention using it the answer would be h.', 'Equation 5 also provides the form of Jacobian matrix xi xi 1 for the speciÔ¨Åc parametrization given in equation 11 where diag converts a vector into a diagonal matrix and œÉ computes the derivative of œÉ in an element wise fash ion.', 'm outputs and n inputs It s Jacobian is an m x n matrix of partial derivatives 12Chain Rule For composition of one variable functions multiply derivatives For multiple variables functions multiply Jacobians 13Example Jacobian Elementwise activation Function 14Example Jacobian Elementwise activation Function Function has n outputs and n inputs n by n Jacobian 15Example Jacobian Elementwise activation Function 16Example Jacobian Elementwise activation Function 17Example Jacobian Elementwise', 'So z x ij zi xj xj xi 1 if i j 0 if otherwise So we see that the Jacobian z x is a diagonal matrix where the entry at i i is 1. This is just the identity matrix z x I . When applying the chain 2rule this term will disappear because a matrix or vector multiplied by the identity matrix does not change.', 'Write out the Jacobians ùíñùëá Useful Jacobians from previous slide 323. Write out the Jacobians ùíñùëá Useful Jacobians from previous slide 333. Write out the Jacobians ùíñùëá ùíñùëá Useful Jacobians from previous slide . 34 Hadamard product element wise multiplication of 2 vectors to give vectorRe using Computation Suppose we now want to compute', 'So its Jacobian will be n m. Note that zi m X k 1 Wikxk So an entry z x ij of the Jacobian will be z x ij zi xj xj m X k 1 Wikxk m X k 1 Wik xj xk Wij because xj xk 1 if k j and 0 if otherwise.', 'Therefore if you compute the gradient of a column vector using Jacobian formulation you should take the transpose when reporting your Ô¨Ånal answer so the gradient is a column vector. Another option is to always follow the convention. In this case the identities may not work but you can still Ô¨Ågure out the answer by making sure the dimensions of your derivatives match up.', 'fm x1 . . . fm xn That is f x ij fi xj which is just a standard non vector derivative . The Jacobian matrix will be useful for us because we can apply the chain rule to a vector valued function just by multiplying Jacobians.']",text-embedding-3-small,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999978464652374,0.8,0.938398009812368,0.0555555555555555
176,How is the derivative with respect to a matrix described?,"The derivative with respect to a matrix is described using the Jacobian matrix, which allows us to apply the chain rule to a vector-valued function by multiplying Jacobians. The Jacobian matrix is useful because it provides a way to represent derivatives in a matrix form, making it easier to perform operations like gradient descent and chain rule applications. Additionally, when computing the gradient of a column vector using the Jacobian formulation, it is important to take the transpose when reporting the final answer to ensure that the gradient remains a column vector. Another approach is to follow the shape convention, where the shape of the gradient matches the shape of the parameters, simplifying the computation process.

Source: gradient_notes.txt","['fm x1 . . . fm xn That is f x ij fi xj which is just a standard non vector derivative . The Jacobian matrix will be useful for us because we can apply the chain rule to a vector valued function just by multiplying Jacobians.', 'It would be much nicer if the derivatives were in a n m matrix like this J W J W11 . . . J W1m . . . ... . . . J Wn1 . . . J Wnm Since this matrix has the same shape as W we could just subtract it times the learning rate from W when doing gradient descent.', 'Since multiplication by a diagonal matrix is the same as doing elementwise multiplication by the diagonal we could also write f x when applying the chain rule. 5 Matrix times column vector with respect to the matrix z W x Œ¥ J z what is J W J z z W Œ¥ z W This is a bit more complicated than the other identities. The reason for in cluding J z in the above problem formulation will become clear in a moment.', 'Therefore if you compute the gradient of a column vector using Jacobian formulation you should take the transpose when reporting your Ô¨Ånal answer so the gradient is a column vector. Another option is to always follow the convention. In this case the identities may not work but you can still Ô¨Ågure out the answer by making sure the dimensions of your derivatives match up.', 'look like 1 output nm inputs 1 by nm Jacobian Inconvenient to then do 38Derivative with respect to Matrix Output shape What does look like 1 output nm inputs 1 by nm Jacobian Inconvenient to then do Instead we leave pure math and use the shape convention the shape of the gradient is the shape of the parameters So is n by m 39Derivative with respect to Matrix What is is going to be in our', 'Briefly the gradient think derivative U f of a scalar function f with respect to a parameter matrix U represents the direction to locally move U in in order to maximally increase the value of f. So in practice we do something like drawing the initial U 0 and V 0 randomly as U V N 0 0.', 'First suppose we have a loss function J a scalar and are computing its gradient with respect to a matrix W Rn m. Then we could think of J as a function of W taking nm inputs the entries of W to a single output J . This means the Jacobian J W would be a 1 nm vector. But in practice this is not a very useful way of arranging the gradient.', 'So z x ij zi xj xj xi 1 if i j 0 if otherwise So we see that the Jacobian z x is a diagonal matrix where the entry at i i is 1. This is just the identity matrix z x I . When applying the chain 2rule this term will disappear because a matrix or vector multiplied by the identity matrix does not change.']",text-embedding-3-small,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.8729650796935989,1.0,0.926472887527714,0.0555555555555555
177,What role does automatic differentiation play in modern deep learning frameworks?,"Automatic Differentiation plays a crucial role in modern deep learning frameworks by automatically inferring gradient computations from the symbolic expressions of the forward propagation. This allows for efficient computation of gradients with respect to the inputs and trainable parameters of the neural networks, enabling easier building of network variants and aiding in debugging and improving models. Source: SEP 775 - Computational Natural Language Processing, lecture02_Neural_Nets.","['is the same In general our nets have regular layer structure and so we can use matrices and Jacobians Back Prop in General Computation Graph Inputs Single scalar output 73Automatic Differentiation The gradient computation can be automatically inferred from the symbolic expression of the fprop Each node type needs to know how to compute its output and how to compute the gradient wrt its inputs given the gradient wrt its output Modern DL frameworks Tensorflow PyTorch etc. do', 'Given derivatives with respect to its outputs each module can independently compute derivatives with respect to its inputs and with respect to its trainable parameters as proposed by Bottou and Gallinari 1991 . This allows us to easily build variants of our networks. For details about gradient computations see Appendix A. Remark 7 Tricks Many tricks have been reported for training neural networks LeCun et al. 1998 .', 'to compute gradients 79Why learn all these details about gradients 80 Modern deep learning frameworks compute gradients for you But why take a class on compilers or systems when they are implemented for you Understanding what is going on under the hood is useful Backpropagation doesn t always work perfectly out of the box Understanding why is crucial for debugging and improving models See Karpathy article https medium.', 'frameworks Tensorflow PyTorch etc. do backpropagation for you but mainly leave layer node writer to hand calculate the local derivative 74Backprop Implementations 75Implementation forward backward API 76Implementation forward backward API 77Manual Gradient checking Numeric Gradient For small h 1e 4 Easy to implement correctly But approximate and very slow You have to recompute f for every parameter of our model Useful for checking your implementation In the old days we hand', 'Remark 5 Differentiability Our cost functions are differentiable almost everywhere. Non differentiable points arise because we use a hard transfer function 5 and because we use a max layer 7 in the sentence approach network. Fortunately stochastic gradient still converges to a meaningful local minimum despite such minor differentiability problems Bottou 1991 1998 . Stochastic gradient iterations that hit a non differentiability are simply skipped.', 'This makes the math a bit cumbersome. Even worse if we re implementing the model without automatic diÔ¨Äerentiation computing J Œ∏ twice will be ineÔ¨Écient. So it will help us to deÔ¨Åne some variables to represent the intermediate derivatives Œ¥1 J Œ∏ Œ¥2 J z These can be thought as the error signals passed down to Œ∏ and z when doing backpropagation. We can compute them as follows Œ¥1 J', '1 Introduction Deep neural networks trained with some version of Stochastic Gradient Descent have been shown to substantially outperform previous approaches on various supervised learning tasks in computer vision Krizhevsky et al. 2012 and speech processing Hinton et al. 2012 . But state of the art deep neural networks often require many days of training.', 'The algorithm can also be thought of as adapting the learning rate based on the norm of the gradient. Compared to other learning rate adaptation strate gies which focus on improving convergence by col lecting statistics on the gradient as for example inOn the diÔ¨Éculty of training Recurrent Neural Networks Duchi et al.']",text-embedding-3-small,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9938458574725552,,0.948500775903374,0.0555555555555555
178,What is the purpose of manual gradient checking?,"The purpose of manual gradient checking is to compare analytical gradients (computed using mathematical formulas) with numerical gradients (computed using numerical approximation methods) to ensure the correctness of the gradient computation process. This helps in verifying the implementation of backpropagation and gradient descent algorithms by confirming that the gradients are calculated accurately, which is crucial for training neural networks effectively.","['4 Gradient Layout Jacobean formulation is great for applying the chain rule you just have to mul tiply the Jacobians. However when doing SGD it s more convenient to follow the convention the shape of the gradient equals the shape of the parameter as we did when computing J W . That way subtracting the gradient times the learning rate from the parameters is easy. We expect answers to homework questions to follow this convention.', 'is by looking at the dimensions of the Jaco bians J z Œ¥1 U sgn h 1 Dh 1 Nc Nc Dh Dh 6We see that the dimensions of all the terms in the gradient match up i.e. the number of columns in a term equals the number of rows in the next term . This will always be the case if we computed our gradients correctly. Now we can use the error terms to compute our gradients.', 'a single element in a weight matrix in practice this tends to be quite slow. Instead it is more eÔ¨Écient to keep everything in ma trix vector form. The basic building block of vectorized gradients is the Jacobian Matrix.', 'Therefore if you compute the gradient of a column vector using Jacobian formulation you should take the transpose when reporting your Ô¨Ånal answer so the gradient is a column vector. Another option is to always follow the convention. In this case the identities may not work but you can still Ô¨Ågure out the answer by making sure the dimensions of your derivatives match up.', 'an earlier checkpoint 58Gradient clipping solution for exploding gradient 58 Gradient clipping if the norm of the gradient is greater than some threshold scale it down before applying SGD update Intuition take a step in the same direction but a smaller step In practice remembering to clip gradients is important but exploding gradients are an easy problem to solve Source On the difficulty of training recurrent neural networks Pascanu et al 2013. http proceedings.mlr.', 'This is why just clipping the gradient might be suÔ¨Écient not requiring the use a second order method. Note that this algo rithm should work even when the rate of growth of the gradient is not the same as the one of the curvature a case for which a second order method would fail as the ratio between the gradient and curvature could still explode .', 'We note that this deÔ¨Ånition of the numerical gradient follows very naturally from the deÔ¨Ånition of the derivative where in the scalar case f x f x œµ f x œµ Gradient checks are a great way to compare analytical and numerical gradients.', 'The key insight is that all the steps taken when the gradient explodes are aligned with v and ignore other descent direction i.e. the model moves perpendicular to the wall . At the wall a small norm step in the direction of the gradient there fore merely pushes us back inside the smoother low curvature region besides the wall whereas a regular gradient step would bring us very far thus slowing or preventing further training.']",text-embedding-3-small,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999999999999996,0.25,0.9471256294864916,0.05
179,Why is understanding gradients important despite automatic differentiation?,Understanding gradients is important despite automatic differentiation because backpropagation doesn't always work perfectly out of the box. Knowing the details about gradients and how they are computed is crucial for debugging and improving models. It helps in understanding why certain issues may arise and how to address them effectively. Source: Lecture02_Neural_Nets.txt,"['to compute gradients 79Why learn all these details about gradients 80 Modern deep learning frameworks compute gradients for you But why take a class on compilers or systems when they are implemented for you Understanding what is going on under the hood is useful Backpropagation doesn t always work perfectly out of the box Understanding why is crucial for debugging and improving models See Karpathy article https medium.', 'This makes the math a bit cumbersome. Even worse if we re implementing the model without automatic diÔ¨Äerentiation computing J Œ∏ twice will be ineÔ¨Écient. So it will help us to deÔ¨Åne some variables to represent the intermediate derivatives Œ¥1 J Œ∏ Œ¥2 J z These can be thought as the error signals passed down to Œ∏ and z when doing backpropagation. We can compute them as follows Œ¥1 J', 'We note that this deÔ¨Ånition of the numerical gradient follows very naturally from the deÔ¨Ånition of the derivative where in the scalar case f x f x œµ f x œµ Gradient checks are a great way to compare analytical and numerical gradients.', '4 Gradient Layout Jacobean formulation is great for applying the chain rule you just have to mul tiply the Jacobians. However when doing SGD it s more convenient to follow the convention the shape of the gradient equals the shape of the parameter as we did when computing J W . That way subtracting the gradient times the learning rate from the parameters is easy. We expect answers to homework questions to follow this convention.', 'Given derivatives with respect to its outputs each module can independently compute derivatives with respect to its inputs and with respect to its trainable parameters as proposed by Bottou and Gallinari 1991 . This allows us to easily build variants of our networks. For details about gradient computations see Appendix A. Remark 7 Tricks Many tricks have been reported for training neural networks LeCun et al. 1998 .', 'is the same In general our nets have regular layer structure and so we can use matrices and Jacobians Back Prop in General Computation Graph Inputs Single scalar output 73Automatic Differentiation The gradient computation can be automatically inferred from the symbolic expression of the fprop Each node type needs to know how to compute its output and how to compute the gradient wrt its inputs given the gradient wrt its output Modern DL frameworks Tensorflow PyTorch etc. do', 'non vectorized gradients But doing a non vectorized gradient can be good for intuition recall the first lecture for an exampleGradients Given a function with 1 output and 1 input ùëìùë• ùë•3 It s gradient slope is its derivative ùëëf 3ùë•2 10 ùëëùë• How much will the output change if we change the input a bit At x 1 it changes about', 'Therefore if you compute the gradient of a column vector using Jacobian formulation you should take the transpose when reporting your Ô¨Ånal answer so the gradient is a column vector. Another option is to always follow the convention. In this case the identities may not work but you can still Ô¨Ågure out the answer by making sure the dimensions of your derivatives match up.']",text-embedding-3-small,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999989850365602,1.0,0.9433746746736532,0.0555555555555555
180,What is the core concept introduced in this class that leads to GPT-3 and ChatGPT?,"The core concept introduced in this class that leads to GPT-3 and ChatGPT is the development of advanced language models based on deep learning techniques, particularly leveraging transformer architectures for natural language processing tasks. These models are designed to understand and generate human-like text, enabling them to perform a wide range of language-related tasks with high accuracy and efficiency. They have the ability to learn from vast amounts of text data and generalize to new tasks with minimal fine-tuning, making them powerful tools for various applications in NLP.","['Finally given the broad spectrum of capabilities displayed by GPT 3 we discuss concerns about bias fairness and broader societal impacts and attempt a preliminary analysis of GPT 3 s characteristics in this regard. The remainder of this paper is organized as follows. In Section 2 we describe our approach and methods for training GPT 3 and evaluating it.', 'The ability of GPT 3 to generate several paragraphs of synthetic content that people Ô¨Ånd difÔ¨Åcult to distinguish from human written text in 3.9.4 represents a concerning milestone in this regard.', 'First despite the strong quantitative and qualitative improvements of GPT 3 particularly compared to its direct predecessor GPT 2 it still has notable weaknesses in text synthesis and several NLP tasks. On text synthesis although the overall quality is high GPT 3 samples still sometimes repeat themselves semantically at the document level start to lose coherence over sufÔ¨Åciently long passages contradict themselves and occasionally contain non sequitur sentences or paragraphs.', 'However it is very likely that incorporating these algorithmic advances could improve GPT 3 s performance on downstream tasks especially in the Ô¨Åne tuning setting and combining GPT 3 s scale with these algorithmic techniques is a promising direction for future work.', 'In general we observe GPT 3 is on par with initial baselines and early results trained using contextual representations on each respective dataset.', 'By presenting a broad characterization of GPT 3 s strengths and weaknesses including these limitations we hope to stimulate study of few shot learning in language models and draw attention to where progress is most needed. A heuristic sense of the overall results can be seen in Figure 1.3 which aggregates the various tasks though it should not be seen as a rigorous or meaningful benchmark in itself .', 'We discuss broader societal impacts of this Ô¨Ånding and of GPT 3 in general. Equal contribution Johns Hopkins University OpenAI Author contributions listed at end of paper. arXiv 2005.14165v4 cs.CL 22 Jul 2020Contents 1 Introduction 3 2 Approach 6 2.1 Model and Architectures . . . . . .', 'First we test GPT 3 s ability to perform arithmetic. Second we create several tasks that involve rearranging or unscrambling the letters in a word tasks which are unlikely to have been exactly seen during training. Third we test GPT 3 s ability to solve SAT style analogy problems few shot.']",text-embedding-3-small,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9988607910943532,0.3333333333333333,0.9107880125135877,0.0588235294117647
181,What is the purpose of Dropout as mentioned in the document?,"The purpose of Dropout, as mentioned in the document, is to act as a powerful technique for regularization in neural networks by randomly dropping neurons in the forward pass. This helps prevent overfitting, allows the network to learn more meaningful information from the data, and typically leads to higher performance on the task at hand. Dropout essentially trains exponentially many smaller networks at once and averages over their predictions, improving the overall performance of the neural network. Additionally, during testing, Dropout ensures that the expected output of a neuron remains consistent with its training phase to maintain network behavior.","['In the next section we discuss dropout which effectively acts as another form of regularization by randomly dropping i.e. setting to zero neurons in the forward pass. 2.3 Dropout Dropout is a powerful technique for regularization Ô¨Årst introduced by Srivastava et al. in Dropout A Simple Way to Prevent Neural Net works from OverÔ¨Åtting.', 'The result is that the network typically learns more meaningful informa tion from the data is less likely to overÔ¨Åt and usually obtains higher performance overall on the task at hand. One intuitive reason why this technique should be so effective is that what dropout is doing is essentially doing is training exponentially many smaller networks at once and averaging over their predictions. Dropout applied to an artiÔ¨Åcial neural network.', 'Image credits to Srivastava et al. In practice the way we introduce dropout is that we take the out put h of each layer of neurons and keep each neuron with prob ability p and else set it to 0. Then during back propagation we only pass gradients through neurons that were kept alive during the forward pass. Finally during testing we compute the forward pass using all of the neurons in the network.', 'However a key sub tlety is that in order for dropout to work effectively the expected output of a neuron during testing should be approximately the same as it was during training else the magnitude of the outputs could be radically different and the behavior of the network is no longer well deÔ¨Åned.', 'Dropout a simple way to prevent neural networks from overÔ¨Åtting. JMLR 2014. Rupesh Kumar Srivastava Klaus Greff and J urgen Schmidhuber. Highway networks. arXiv preprint arXiv 1505.00387 2015.', '7 1019 1.2 1021 Transformer base model 27.3 38.1 3.3 1018 Transformer big 28.4 41.8 2.3 1019 Residual Dropout We apply dropout 33 to the output of each sub layer before it is added to the sub layer input and normalized.', 'form of model bagging i.e. like an ensemble model Nowadays usually thought of as strong feature dependent regularizer Wager Wang Liang 2013DropoutDropout Srivastava Hinton Krizhevsky Sutskever Salakhutdinov 2012 JMLR 2014 During training For each data point each time Randomly set input to 0 with probability ùëù dropout ratio often p 0.', 'The idea is simple yet effective during train ing we will randomly drop with some probability 1 p a subset of neurons during each forward backward pass or equivalently we will keep alive each neuron with a probability p . Then during testing we will use the full network to compute our predictions.']",text-embedding-3-small,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9300597327790432,0.8,0.9265990947691318,0.0526315789473684
182,What are some approaches to address the vanishing gradient problem in RNNs?,"Some approaches to address the vanishing gradient problem in RNNs include:
1. Using a regularization term that represents a preference for parameter values such that backpropagated gradients neither increase nor decrease too much in magnitude.
2. Initializing the weight matrix W_hh from an identity matrix instead of random initialization.
3. Using Rectified Linear Units (ReLU) instead of the sigmoid function.

Source: lecture03_notes_LM_RNN.txt","['2013. http proceedings.mlr.press v28 pascanu13.pdfHow to fix the vanishing gradient problem The main problem is that it s too difficult for the RNN to learn to preserve information over many timesteps.', 'with RNNs Vanishing and Exploding Gradients 48Vanishing gradient intuition 49Vanishing gradient intuition chain rule 50Vanishing gradient intuition chain rule 51Vanishing gradient intuition chain rule 52Vanishing gradient intuition What happens if these are small Vanishing gradient problem When these are small the gradient signal gets smaller and smaller as it backpropagates further 53Vanishing gradient proof sketch linear case Recall What if were the identity', 'To gain practical intuition about the vanishing gradient problem you may visit the following example website. 2.4 Solution to the Exploding Vanishing Gradients Now that we gained intuition about the nature of the vanishing gradi ents problem and how it manifests itself in deep neural networks let us focus on a simple and practical heuristic to solve these problems.', 'Below we discuss the math ematical reasoning behind the vanishing gradient problem. Consider Equations 5 and 6 at a time step t to compute the RNN error dE dW we sum the error at each time step. That is dEt dW for every time step t is computed and accumulated.', 'When the gradient value goes to zero however it can go undetected while dras tically reducing the learning quality of the model for far away words in the corpus this issue is called the Vanishing Gradient Problem. Due to vanishing gradients we don t know whether there is no dependency between steps t and t n in the data or we just cannot capture the true dependency due to this issue.', '3.3. Vanishing gradient regularization We opt to address the vanishing gradients problem us ing a regularization term that represents a preference for parameter values such that back propagated gra dients neither increase or decrease too much in mag nitude.', 'press v28 pascanu13 supp.pdf 54 eigenvectors using the eigenvectors of Approaches 0 as grows so gradient vanishes sufficient but not necessary ONLY READ IF INTERESTEDWhy is vanishing gradient a problem Gradient signal from far away is lost because it s much smaller than gradient signal from close by. So model weights are updated only with respect to near effects not long term effects.', 'This a ple regime with a where any informa Figure 7 Gradient explosion clipping visualization To solve the problem of vanishing gradients we introduce two tech niques. The Ô¨Årst technique is that instead of initializing W hh ran domly start off from an identity matrix initialization. The second technique is to use the RectiÔ¨Åed Linear Units ReLU in stead of the sigmoid function.']",text-embedding-3-small,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999986551950828,1.0,0.7942085825380413,0.0588235294117647
183,"What is perplexity, and how is it used in evaluating language models?","Perplexity is a metric used to evaluate language models in natural language processing. It is the inverse probability of a test set normalized by the number of words in the set. Perplexity helps measure how well a language model predicts a given text. Lower perplexity values indicate that the model is more confident in predicting the next word in a sequence compared to the ground truth outcome. It is closely related to the information theoretic notion of entropy and can be used to compare different n-gram models. Perplexity is calculated without any prior knowledge of the test set or its vocabulary to ensure fair comparisons between language models. A lower perplexity signifies a better language model in terms of predicting words in the test set, but it does not guarantee improved performance in tasks like speech recognition or machine translation. Despite this, perplexity is commonly used as a quick check on the effectiveness of language processing algorithms.

Source: Lecture notes on N-gram Language Models in SEP 775 - Computational Natural Language Processing.","['3.2.1 Perplexity In practice we don t use raw probability as our metric for evaluating language mod els but a variant called perplexity. The perplexity sometimes called PPL for short perplexity of a language model on a test set is the inverse probability of the test set normalized by the number of words. For a test set W w1w2 .wN perplexity W P w1w2 .', 'We leave this exact calculation as exercise 3.12. We see in Section 3.8 that perplexity is also closely related to the information theoretic notion of entropy. We mentioned above that perplexity is a function of both the text and the lan guage model given a text W different language models will have different perplex ities. Because of this perplexity can be used to compare different n gram models.', '9 is called the perplexity relationship it is basically 2 to the power of the negative log probability of the cross entropy error function shown in Equation 8. Perplexity is a measure of confusion where lower values imply more conÔ¨Ådence in predicting the next word in the sequence compared to the ground truth outcome .', 'Note that in computing perplexities the n gram model P must be constructed without any knowledge of the test set or any prior knowledge of the vocabulary of the test set. Any kind of knowledge of the test set can cause the perplexity to be artiÔ¨Åcially low. The perplexity of two language models is only comparable if they use identical vocabularies.', 'The perplexity of a test set according to a language model is the geometric mean of the inverse test set probability computed by the model. Smoothing algorithms provide a more sophisticated way to estimate the prob ability of n grams. Commonly used smoothing algorithms for n grams rely on lower order n gram counts through backoff or interpolation. Both backoff and interpolation require discounting to create a probability dis tribution.', 'A better n gram model is one that assigns a higher probability to the test data and perplexity is a normalized version of the probability of the test set. The perplexity measure actually arises from the information theoretic concept of cross entropy which explains otherwise mysterious properties of perplexity why the inverse probability for example and its relationship to entropy. Entropy is a Entropy measure of information.', 'And the higher the probability the lower the perplexity since as Eq. 3.15 showed perplexity is related inversely to the likelihood of the test sequence according to the model . So a lower perplexity can tell us that a language model is a better predictor of the words in the test set.', 'An intrinsic improvement in perplexity does not guarantee an extrinsic im provement in the performance of a language processing task like speech recognition10 CHAPTER 3 N GRAM LANGUAGE MODELS or machine translation. Nonetheless because perplexity often correlates with such improvements it is commonly used as a quick check on an algorithm.']",text-embedding-3-small,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.93773732797214,1.0,0.950286821304382,0.32
184,How does LSTM solve the vanishing gradient problem?,"LSTM solves the vanishing gradient problem by using its architecture to make it easier for an RNN to preserve information over many timesteps. For example, if the forget gate is set to 1 for a cell dimension and the input gate set to 0, then the information of that cell is preserved indefinitely, unlike in a vanilla RNN where it is harder to preserve information over long sequences. This helps in mitigating the vanishing gradient problem.","['github.io posts 2015 08 Understanding LSTMs 65How does LSTM solve vanishing gradients 66 The LSTM architecture makes it much easier for an RNN to preserve information over many timesteps e.g. if the forget gate is set to 1 for a cell dimension and the input gate set to 0 then the information of that cell is preserved indefinitely. In contrast it s harder for a vanilla RNN to', 'When the gradient value goes to zero however it can go undetected while dras tically reducing the learning quality of the model for far away words in the corpus this issue is called the Vanishing Gradient Problem. Due to vanishing gradients we don t know whether there is no dependency between steps t and t n in the data or we just cannot capture the true dependency due to this issue.', 'To gain practical intuition about the vanishing gradient problem you may visit the following example website. 2.4 Solution to the Exploding Vanishing Gradients Now that we gained intuition about the nature of the vanishing gradi ents problem and how it manifests itself in deep neural networks let us focus on a simple and practical heuristic to solve these problems.', 'Below we discuss the math ematical reasoning behind the vanishing gradient problem. Consider Equations 5 and 6 at a time step t to compute the RNN error dE dW we sum the error at each time step. That is dEt dW for every time step t is computed and accumulated.', 'with RNNs Vanishing and Exploding Gradients 48Vanishing gradient intuition 49Vanishing gradient intuition chain rule 50Vanishing gradient intuition chain rule 51Vanishing gradient intuition chain rule 52Vanishing gradient intuition What happens if these are small Vanishing gradient problem When these are small the gradient signal gets smaller and smaller as it backpropagates further 53Vanishing gradient proof sketch linear case Recall What if were the identity', '2013. http proceedings.mlr.press v28 pascanu13.pdfHow to fix the vanishing gradient problem The main problem is that it s too difficult for the RNN to learn to preserve information over many timesteps.', 'contrast it s harder for a vanilla RNN to learn a recurrent weight matrix Wh that preserves info in the hidden state In practice you get about 100 timesteps rather than about 7 However there are alternative ways of creating more direct and linear pass through connections in models for long distance dependenciesIs vanishing exploding gradient just an RNN problem For example Residual connections aka ResNet Also known as skip connections The identity connection preserves information by default This makes deep', 'press v28 pascanu13 supp.pdf 54 eigenvectors using the eigenvectors of Approaches 0 as grows so gradient vanishes sufficient but not necessary ONLY READ IF INTERESTEDWhy is vanishing gradient a problem Gradient signal from far away is lost because it s much smaller than gradient signal from close by. So model weights are updated only with respect to near effects not long term effects.']",text-embedding-3-small,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.0000000000000009,1.0,0.9709972885441448,0.0666666666666666
185,"What is the impact of bidirectional RNNs, and where are they applicable?","The impact of bidirectional RNNs is that they provide a contextual representation that includes both left and right context, making them powerful for tasks where access to the entire input sequence is available. They are not applicable to Language Modeling because in LM only left context is available. However, if you do have the entire input sequence, bidirectionality is powerful and should be used by default. For example, BERT (Bidirectional Encoder Representations from Transformers) is a powerful pretrained contextual representation system built on bidirectionality. Additionally, bidirectional RNNs can also be multi-layered, where each lower layer feeds the next layer, enhancing their capabilities.

Source: Lecture03_RNNs_and_LLMs.txt","['Input audio START conditioning 77Bidirectional RNNs was terribly exciting the movie Forward RNN Backward RNN Concatenated hidden states This contextual representation of terribly has both left and right context 78Bidirectional RNNs 79 Note bidirectional RNNs are only applicable if you have access to the entire input sequence They are not applicable to Language Modeling because in LM you only have left context available.', 'If you do have entire input sequence e.g. any kind of encoding bidirectionality is powerful you should use it by default . For example BERT Bidirectional Encoder Representations from Transformers is a powerful pretrained contextual representation system built on bidirectionality.', 'h h h Figure 8 A bi directional RNN model h t f W xt V h t 1 b 17 h t f W xt V h t 1 b 18 ÀÜ yt g Uht c g U h t h t c 19 RNNs can also be multi layered. Figure 9 shows a multi layer bi directional RNN where each lower layer feeds the next layer.', 'Inspired by the window based method Hill et al. 2016 we split each article into short sentences where each sentence is a 19 word window around each entity hence the same word might appear in multiple sentences . The RNNs in BIDAF are not feed forwarded or back propagated across sentences which speed up the training process by par allelization.', 'Chen et al. 2016 show that simply using bilinear term for computing the attention weights in the same model drastically improves the accuracy. Wang Jiang 2016 reverse the direction of the attention attending on query words as the context RNN progresses for SQuAD. In contrast to these models BIDAF uses a memory less attention mechanism.', 'The derivative for the ReLU is either 0 or 1. This way gradients would Ô¨Çow through the neurons whose deriva tive is 1 without getting attenuated while propagating back through time steps. 2.5 Deep Bidirectional RNNs So far we have focused on RNNs that condition on past words to predict the next word in the sequence.', '3. Contextual Embedding Layer. We use a Long Short Term Memory Network LSTM Hochreiter Schmidhuber 1997 on top of the embeddings provided by the previous layers to model the temporal interactions between words. We place an LSTM in both directions and concatenate the outputs of the two LSTMs.', 'Figure 3. Bifurcation diagram of a single hidden unit RNN with Ô¨Åxed recurrent weight of 5.0 and adjustable bias b example introduced in Doya 1993 . See text. We show that there are two types of events that could lead to a large change in xt with t .']",text-embedding-3-small,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9668585653326168,1.0,0.9404790209130832,0.05
186,Describe the concept of multi-layer or stacked RNNs and their benefit.,"Multi-layer or stacked RNNs allow a network to compute more complex representations by having lower RNNs compute lower level features and higher RNNs compute higher level features. This architecture works better than just having one layer of high-dimensional encodings. The hidden states from one RNN layer serve as inputs to the next layer, enabling the network to learn hierarchical representations. Additionally, deeper layers in RNNs can improve prediction accuracy due to their higher learning capacity, although training such models may require a large corpus. 

Source: SEP775 - Computational Natural Language Processing, Lecture 03.","['This allows the network to compute more complex representations The lower RNNs should compute lower level features and the higher RNNs should compute higher level features. Multi layer RNNs are also called stacked RNNs. 80Multi layer RNNs was terribly exciting the movie RNN layer 1 RNN layer 2 81 RNN layer 3 The hidden states from RNN layer i are the inputs to RNN layer i', 'layer i are the inputs to RNN layer i 1Multi layer RNNs in practice 39 Multi layer or stacked RNNs allow a network to compute more complex representations they work better than just have one layer of high dimensional encodings The lower RNNs should compute lower level features and the higher RNNs should compute higher level features.', 'h h h Figure 8 A bi directional RNN model h t f W xt V h t 1 b 17 h t f W xt V h t 1 b 18 ÀÜ yt g Uht c g U h t h t c 19 RNNs can also be multi layered. Figure 9 shows a multi layer bi directional RNN where each lower layer feeds the next layer.', 'Perplexity 2J 9 2.2 Advantages Disadvantages and Applications of RNNs RNNs have several advantages 1. They can process input sequences of any length 2. The model size does not increase for longer input sequence lengths 3. Computation for step t can in theory use information from many steps back. 4.', 'You will learn more about transformers including BERT in a couple of weeksMulti layer RNNs RNNs are already deep on one dimension they unroll over many timesteps We can also make them deep in another dimension by applying multiple RNNs this is a multi layer RNN.', 'Figure 11 illustrates this model. ht œÜ ht 1 c yt 1 27 Extension III train deep recurrent neural networks using multiple RNN layers as discussed earlier in this chapter. Deeper layers often improve prediction accuracy due to their higher learning capacity. Of course this implies a large training corpus must be used to train the model.', 'RNNs should compute higher level features. High performing RNNs are usually multi layer but aren t as deep as convolutional or feed forward networks For example In a 2017 paper Britz et al.', 'contrast it s harder for a vanilla RNN to learn a recurrent weight matrix Wh that preserves info in the hidden state In practice you get about 100 timesteps rather than about 7 However there are alternative ways of creating more direct and linear pass through connections in models for long distance dependenciesIs vanishing exploding gradient just an RNN problem For example Residual connections aka ResNet Also known as skip connections The identity connection preserves information by default This makes deep']",text-embedding-3-small,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.950586506203946,1.0,0.9638006431112882,0.0526315789473684
187,How does Dropout work during training and testing?,"During training, Dropout works by randomly dropping a subset of neurons with a probability \( p \) during each forward and backward pass. This means that some neurons are set to zero. During testing, all neurons are used in the network to compute predictions, without any dropout applied. This ensures that the expected output of a neuron during testing is approximately the same as it was during training, maintaining the network's well-defined behavior.

Source: Lecture02_Notes_Backpropagation","['In the next section we discuss dropout which effectively acts as another form of regularization by randomly dropping i.e. setting to zero neurons in the forward pass. 2.3 Dropout Dropout is a powerful technique for regularization Ô¨Årst introduced by Srivastava et al. in Dropout A Simple Way to Prevent Neural Net works from OverÔ¨Åtting.', 'However a key sub tlety is that in order for dropout to work effectively the expected output of a neuron during testing should be approximately the same as it was during training else the magnitude of the outputs could be radically different and the behavior of the network is no longer well deÔ¨Åned.', 'Image credits to Srivastava et al. In practice the way we introduce dropout is that we take the out put h of each layer of neurons and keep each neuron with prob ability p and else set it to 0. Then during back propagation we only pass gradients through neurons that were kept alive during the forward pass. Finally during testing we compute the forward pass using all of the neurons in the network.', 'The result is that the network typically learns more meaningful informa tion from the data is less likely to overÔ¨Åt and usually obtains higher performance overall on the task at hand. One intuitive reason why this technique should be so effective is that what dropout is doing is essentially doing is training exponentially many smaller networks at once and averaging over their predictions. Dropout applied to an artiÔ¨Åcial neural network.', 'The idea is simple yet effective during train ing we will randomly drop with some probability 1 p a subset of neurons during each forward backward pass or equivalently we will keep alive each neuron with a probability p . Then during testing we will use the full network to compute our predictions.', '7 1019 1.2 1021 Transformer base model 27.3 38.1 3.3 1018 Transformer big 28.4 41.8 2.3 1019 Residual Dropout We apply dropout 33 to the output of each sub layer before it is added to the sub layer input and normalized.', 'form of model bagging i.e. like an ensemble model Nowadays usually thought of as strong feature dependent regularizer Wager Wang Liang 2013DropoutDropout Srivastava Hinton Krizhevsky Sutskever Salakhutdinov 2012 JMLR 2014 During training For each data point each time Randomly set input to 0 with probability ùëù dropout ratio often p 0.', 'Dropout a simple way to prevent neural networks from overÔ¨Åtting. JMLR 2014. Rupesh Kumar Srivastava Klaus Greff and J urgen Schmidhuber. Highway networks. arXiv preprint arXiv 1505.00387 2015.']",text-embedding-3-small,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.0000000000000002,1.0,0.9240701676069524,0.6842105263157895
188,What is the role of parameter initialization in neural networks?,"Parameter initialization in neural networks plays a crucial role in achieving superlative performance. It involves setting the initial values of weights and biases in a reasonable way to avoid issues like gradient variances and symmetries that can hinder learning. Proper initialization helps in faster convergence, lower error rates, and overall improved training dynamics. It is recommended to initialize weights to small random values around 0 and biases appropriately to ensure effective learning and specialization. Additionally, initializing parameters correctly can help control the magnitude of model parameter updates during training, contributing to the overall success of the neural network training process.

Source: SEP 775 - Computational Natural Language Processing, lecture02_notes_Backpropagation.txt, lecture03_RNNs_and_LLMs.txt","['2.6 Parameter Initialization A key step towards achieving superlative performance with a neu ral network is initializing the parameters in a reasonable way. A good starting strategy is to initialize the weights to small random numbers normally distributed around 0 and in practice this often words acceptably well.', '8 ¬µs per loop Matrices are awesome Always try to use vectors and matrices rather than for loops The speed gain goes from 1 to 2 orders of magnitude with GPUs 7Parameter Initialization You normally must initialize weights to small random values i.e. not zero matrices To avoid symmetries that prevent learning specialization Initialize hidden layer biases to 0 and output or reconstruction biases to optimal value if weights were 0 e.g.', 'Without such initialization the gradient variances which are a proxy for information generally decrease with backpropagation across layers. 2.7 Learning Strategies The rate magnitude of model parameter updates during training can be controlled using the learning rate.', 'Within each line child networks are initialized with the embeddings of their parents and trained on increasingly rich data sets with sometimes different parameters. That is suppose we have k processors which is much less than the possible set of parameters one would like to try. One chooses k initial parameter choices from the large set and trains these on the k processors.', 'p3 input 1 input 2 p4 output 2 p5 tanh output 1 p6 input 1 p7 input 2 p8 input 1 input 2 p9 Neural networks are almost always deÔ¨Åned in terms of parameters here denoted by p1 . . . p9 which are automatically chosen using standard machine learning algorithms.', 'However in Understanding the dif ficulty of training deep feedforward neural networks 2010 Xavier et al study the effect of different weight and bias initialization schemes on training dynamics. The empirical Ô¨Åndings suggest that for sigmoid and tanh activation units faster convergence and lower error rates are achieved when the weights of a matrix W Rn l 1 n l are initialized randomly with a uniform distribution as follows W U s 6 n l n l 1 s', 's 6 n l n l 1 s 6 n l n l 1 Where n l is the number of input units to W fan in and n l 1 is the number of output units from W fan out . In this parameter initialization scheme bias units are initialized to 0. This approach attempts to maintain activation variances as well as backpropagated gradient variances across layers.', 'For any parameter assignment Œ∏ depending on the ini tial state x0 the state xt of an autonomous dynamical system converges under the repeated application of the map F to one of several possible diÔ¨Äerent attrac tor states e.g. point attractors though other type of attractors exist .']",text-embedding-3-small,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.98836767902194,1.0,0.9402979547437342,0.1333333333333333
189,"What are the differences between Vanilla RNNs, LSTMs, and GRUs?","Vanilla RNNs constantly rewrite the hidden state, making it harder to learn long-term dependencies. LSTMs are explicitly designed to avoid the long-term dependency problem by preserving information over many timesteps using gates like input, forget, and output gates. GRUs are designed to have more persistent memory than RNNs, making it easier to capture long-term dependencies. Additionally, LSTMs can have peephole connections that allow gate layers to look at the cell state. These differences in architecture and design make LSTMs and GRUs more effective in capturing and preserving long-term dependencies compared to Vanilla RNNs.

Source: Lecture03_RNNs_and_LLMs.txt","['Figure 12 The detailed internals of a GRU It is important to note that to train a GRU we need to learn all the different parameters W U W r U r W z U z . These follow the same backpropagation procedure we have seen in the past. 4 Long Short Term Memories Long Short Term Memories are another type of complex activation unit that differ a little from GRUs.', 'github.io posts 2015 08 Understanding LSTMs 65How does LSTM solve vanishing gradients 66 The LSTM architecture makes it much easier for an RNN to preserve information over many timesteps e.g. if the forget gate is set to 1 for a cell dimension and the input gate set to 0 then the information of that cell is preserved indefinitely. In contrast it s harder for a vanilla RNN to', 'What motivates this Well although RNNs can theoretically capture long term dependencies they are very hard to actually train to do this. Gated recurrent units are designed in a manner to have more persistent memory thereby making it easier for RNNs to capture long term dependencies. Let us see mathematically how a GRU uses ht 1 and xt to generate the next hidden state ht.', 'The motivation for using these is similar to those for GRUs however the architecture of such units does differ. Let us Ô¨Årst take a look at the mathematical formulation of LSTM units before diving into the intuition behind this design it œÉ W i xt U i ht 1 Input gate ft œÉ W f xt U f ht 1 Forget gate ot œÉ W o xt U o ht 1 Output Exposure gate ct tanh W c xt U c ht', 'The differences are minor but it s worth mentioning some of them. One popular LSTM variant introduced by Gers Schmidhuber 2000 ftp ftp.idsia.ch pub juergen TimeCount IJCNN2000.pdf is adding peephole connections. This means that we let the gate layers look at the cell state.', 'contrast it s harder for a vanilla RNN to learn a recurrent weight matrix Wh that preserves info in the hidden state In practice you get about 100 timesteps rather than about 7 However there are alternative ways of creating more direct and linear pass through connections in models for long distance dependenciesIs vanishing exploding gradient just an RNN problem For example Residual connections aka ResNet Also known as skip connections The identity connection preserves information by default This makes deep', 'In a vanilla RNN the hidden state is constantly being rewritten First off next time How about an RNN with separate memory which is added to LSTMs And then Creating more direct and linear pass through connections in model Attention residual connections etc.', 'They work tremendously well on a large variety of problems and are now widely used. LSTMs are explicitly designed to avoid the long term dependency problem. Remembering information for long periods of time is practically their default behavior not something they struggle to learn 11 24 24 3 23 PM Understanding LSTM Networks colah s blog 3 7 All recurrent neural networks have the form of a chain of repeating modules of neural network.']",text-embedding-3-small,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9461546766612026,1.0,0.9351097073788,0.0555555555555555
190,What is the difference between traditional language models and conditioned language models?,"Traditional language models generate text based on the probability of a word given the previous words in a sentence, without considering any specific conditioning information. On the other hand, conditioned language models take into account additional information or context when generating text, allowing them to produce more relevant and accurate outputs based on the given conditions or prompts.","['a phrase based translation model and a neural MT model Niehues et al. 2017 Stacking uses the output of one system in calculating features for another systemCase Studies in Conditional Language ModelingFrom Images e.g. Karpathy et al. 2015 Input is image features output is text Use standard image encoders e.g. CNN Often pre trained on large databases such as ImageNetCheck out this review paperHow do we EvaluateBasic', 'Our work differs in focusing on in context learning but could be combined in the future with those of GLT 20 LPP 20 . Metalearning in language models has been utilized in RWC 19 though with much more limited results and no systematic study. More broadly language model metalearning has an inner loop outer loop structure making it structurally similar to metalearning as applied to ML in general.', 'The models we introduced in Chapter 10 to do this task are causal or left to right transformer models. In this chapter we ll introduce a second paradigm for pretrained language mod els called the bidirectional transformer encoder trained via masked language modeling a method that allows the model to see entire texts at a time including masked language modeling both the right and left context.', 'RRS20 recently demonstrated that a large language model can perform surprisingly well directly answering the questions without conditioning on auxilliary information. They denote this more restrictive evaluation setting as closed book . Their work suggests that even higher capacity models could perform even better and we test this hypothesis with GPT 3.', 'Pretrained language models based on bidirectional encoders can be learned using a masked language model objective where a model is trained to guess the missing information from an input. Pretrained language models can be Ô¨Åne tuned for speciÔ¨Åc applications by adding lightweight classiÔ¨Åer layers on top of the outputs of the pretrained model.', 'All models used the same context to condition outputs on and were pre trained with the same context size and the same article titles and subtitles were used as prompts for each model. However we also ran an experiment to control for participant effort and attention that followed the same format but involved intentionally bad model generated articles. This was done by generating articles from a control model a 160M parameter model with no context and increased output randomness.', 'Modern masked language models are now trained on much larger datasets of web text Ô¨Åltered a bit and augmented by higher quality data like Wikipedia the same as those we discussed for the causal large language models of Chapter 10. Multilingual models similarity use webtext and multilingual Wikipedia.', 'Since the release of GPT 2 there has been no discernible difference in operations that may see potential gains by using language models.']",text-embedding-3-small,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.0000000000000002,1.0,0.963885967580562,0.0588235294117647
191,How can LSTM networks be used in language modeling?,"LSTM networks can be used in language modeling by scaling them to over a billion parameters, which has been shown to improve generative or task performance. LSTM-based language models have been scaled up to increase parameter count and computation, which can enhance the model's ability to predict the next word based on the previous ones. LSTM networks are effective in scenarios where the relevant information needed for prediction is close in proximity to the prediction point, allowing the network to learn to use past information effectively.","['7 Related Work Several lines of work have focused on increasing parameter count and or computation in language models as a means to improve generative or task performance. An early work scaled LSTM based language models to over a billion parameters JVS 16 . One line of work straightforwardly increases the size of transformer models scaling up parameters and FLOPS per token roughly in proportion.', 'minds laptops Language Modeling is the task of predicting what word comes next books the students opened their 10Language Modeling You can also think of a Language Model as a system that assigns a probability to a piece of text For example if we have some text then the probability of this text according to the Language Model is This is what our LM provides 11You use Language Models every day 12You use Language Models every day 13n gram Language Models', 'For example consider a language model trying to predict the next word based on the previous ones. If we are trying to predict the last word in the clouds are in the sky we don t need any further context it s pretty obvious the next word is going to be sky. In such cases where the gap between the relevant information and the place that it s needed is small RNNs can learn to use the past information.', 'of any length Apply the same weights on each step Can optionally produce output on each step Recurrent Neural Network Language Model We ve shown that RNNs are a great way to build a LM despite some problems RNNs are also useful for much moreWhy should we care about Language Modeling 70 Language Modeling is a benchmark task that helps us measure our progress on predicting language use Language Modeling is a subcomponent of many NLP tasks especially those involving generating', 'Surprisingly grammatical but incoherent. We need to consider more than three words at a time if we want to model language well. But increasing n worsens sparsity problem and increases model sizeHow to build a neural language model Recall the Language Modeling task Input sequence of words Output prob. dist. of the next word How about a window based neural model We saw this applied to Named Entity Recognition in Lecture 2 LOCATION', '2.2 Recurrent Neural Net Language Model RNNLM Recurrent neural network based language model has been proposed to overcome certain limitations of the feedforward NNLM such as the need to specify the context length the order of the model N and because theoretically RNNs can efÔ¨Åciently represent more complex patterns than the shallow neural networks 15 2 .', 'Another recent trend in language modeling may offer a way forward. In recent years the capacity of transformer language models has increased substantially from 100 million parameters RNSS18 to 300 million parameters DCLT18 to 1.5 billion parameters RWC 19 to 8 billion parameters SPP 19 11 billion parameters RSR 19 and Ô¨Ånally 17 billion parameters Tur20 .', 'Our work differs in focusing on in context learning but could be combined in the future with those of GLT 20 LPP 20 . Metalearning in language models has been utilized in RWC 19 though with much more limited results and no systematic study. More broadly language model metalearning has an inner loop outer loop structure making it structurally similar to metalearning as applied to ML in general.']",text-embedding-3-small,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.0000000000000009,1.0,0.9350612584548028,0.0555555555555555
192,What are the methods of generation in conditioned language modeling?,The methods of generation in conditioned language modeling are Sampling and Try to generate a random sentence according to the probability distribution.,"['file name: lecture04 Conditioned Generation. Hamidreza Mahyar mahyarh@mcmaster.ca Computational Natural Language Processing Conditioned GenerationLanguage Models Language models are generative models of text s P x The Malfoys said Hermione. Harry was watching him. He looked like Madame Maxime. When she strode up the wrong staircase to visit himself.', 'generate text according to some specification Input X Structured Data English Document Utterance Image Speech Output Y Text NL Description Japanese Short Description Response Text Transcript Task NL Generation Translation Summarization Response Generation Image Captioning Speech RecognitionFormulation and ModelingCalculating the Probability of a SentenceConditional Language ModelsOne Type of Language Model LSTM LSTM LSTM LSTM predict hate predict this predict movie predict s LSTM Mikolov et al.', 'a phrase based translation model and a neural MT model Niehues et al. 2017 Stacking uses the output of one system in calculating features for another systemCase Studies in Conditional Language ModelingFrom Images e.g. Karpathy et al. 2015 Input is image features output is text Use standard image encoders e.g. CNN Often pre trained on large databases such as ImageNetCheck out this review paperHow do we EvaluateBasic', '2014 encoder decoder Transform can be different dimensions encoder transform decoder encoder Input at every time step Kalchbrenner Blunsom 2013 decoder decoder decoderMethods of GenerationThe Generation Problem We have a model of P Y X how do we use it to generate a sentence Two methods Sampling Try to generate a random sentence according to the probability distribution.', 'Thus sampling from a language model which rep resents a distribution over sentences means to generate some sentences choosing each sentence according to its likelihood as deÔ¨Åned by the model. Thus we are more likely to generate sentences that the model thinks have a high probability and less likely to generate sentences that the model thinks have a low probability.', '2020 . In Chapter 10 we explored causal left to right transformers that can serve as the basis for powerful language models models that can eas ily be applied to autoregressive generation problems such as contextual generation summarization and machine translation.', 'These approaches rely on the conditional computation framework BLC13 and speciÔ¨Åcally the mixture of experts method SMM 17 has been used to produce 100 billion parameter models and more recently 50 billion parameter translation models AJF19 though only a small fraction of the parameters are actually used on each forward pass.', '2020 PRISM Model based on training paraphrasing model Thompson and Post 2020Perplexity Calculate the perplexity of the words in the held out set without doing generation Pros Naturally solves multiple reference problem Cons Doesn t consider decoding or actually generating output.']",text-embedding-3-small,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999995421016048,,0.933751205643742,0.0714285714285714
193,What is beam search and how is it used in language generation?,"Beam search is a technique used in natural language processing for generating sequences of words, commonly used in tasks like machine translation and text generation. In beam search, the algorithm keeps track of the k most probable partial translations or sequences (referred to as hypotheses) at each step of the decoding process. The parameter k is known as the beam size, typically set to a value between 5 to 10 in neural machine translation tasks. Each hypothesis is assigned a score based on its log probability, and the algorithm aims to find high-scoring hypotheses by exploring different possible sequences of words. Beam search is more efficient than exhaustive search methods, although it does not guarantee finding the optimal solution. By maintaining a beam of top-k hypotheses, beam search helps in generating more coherent and fluent sequences of words in language generation tasks.

Source: lecture05_Seq2Seq.txt","['search 18Beam search decoding example Beam size k 2. Blue numbers START Calculate prob dist of next word 19Beam search decoding example Beam size k 2. Blue numbers START he I 0.7 log PLM he START 0.9 log PLM I START Take top k words and compute scores 20Beam search decoding example Beam size k 2. Blue numbers hit struck was got START he I 10 0.', '6 For each of the k hypotheses find top k next words and calculate scoresBeam search decoding example Beam size k 2.', 'On tasks with free form completion we use beam search with the same parameters as RSR 19 a beam width of 4 and a length penalty of Œ± 0.6. We score the model using F1 similarity score BLEU or exact match depending on what is standard for the dataset at hand. Final results are reported on the test set when publicly available for each model size and learning setting zero one and few shot .', '8 1.7 2.9 2.5 2.8 3.8 2.9 3.5 3.4 3.3 4.0 4.5 3.7 4.3 4.8 For each of the k hypotheses find top k next words and calculate scoresBeam search decoding example Beam size k 2.', 'Core idea On each step of decoder keep track of the k most probable partial translations which we call hypotheses k is the beam size in practice around 5 to 10 in NMT A hypothesis has a score which is its log probability Scores are all negative and higher score is better We search for high scoring hypotheses tracking top k on each step Beam search is not guaranteed to find optimal solution But much more efficient than exhaustive search 18Beam search decoding example Beam size k', '7 0.9 2.9 log PLM struck START he 0.7 1.6 log PLM was START I 0.9 1.8 log PLM got START I 0.9 1.7 log PLM hit START he 0.7 For each of the k hypotheses find top k next words and calculate scoresBeam search decoding example Beam size k 2.', '9 2.5 2.8 3.8 2.9 3.5 3.4 3.3 4.0 4.5 3.7 4.3 4.8 4.3 4.6 5.0 5.3 For each of the k hypotheses find top k next words and calculate scoresBeam search decoding example Beam size k 2.', '4 3.3 4.0 For each of the k hypotheses find top k next words and calculate scoresBeam search decoding example Beam size k 2. Blue numbers hit struck was got a me hit struck tart pie with on START he I 15 0.7 0.9 1.6 1.8 1.7 2.9 2.5 2.8 3.8 2.']",text-embedding-3-small,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9219124010345832,1.0,0.9173934275454227,0.2142857142857142
194,Explain the concept of model ensembling in language models.,"Model ensembling in language models refers to the technique of combining multiple individual models to improve overall performance. This can be done by training different models with varying architectures, hyperparameters, or training data, and then combining their predictions in some way, such as averaging or taking a majority vote. By leveraging the strengths of different models, ensembling can help reduce overfitting, improve generalization, and enhance the overall robustness of the language model.

Source: Fine_Tuning_and_Masked_Language_Models.txt","['Our work differs in focusing on in context learning but could be combined in the future with those of GLT 20 LPP 20 . Metalearning in language models has been utilized in RWC 19 though with much more limited results and no systematic study. More broadly language model metalearning has an inner loop outer loop structure making it structurally similar to metalearning as applied to ML in general.', 'Nonetheless when the number of languages grows very large multilingual models exhibit what has been called the curse of multilinguality Conneau et al. 2020 the performance on each language degrades compared to a model training on fewer languages.', 'During unsupervised pre training a language model develops a broad set of skills and pattern recognition abilities. It then uses these abilities at inference time to rapidly adapt to or recognize the desired task. We use the term in context learning to describe the inner loop of this process which occurs within the forward pass upon each sequence.', 'While the causal language models of Chapter 10 also use contextual embeddings the embeddings created by masked language models seem to function particularly well as representations. 11.1 Bidirectional Transformer Encoders Let s begin by introducing the bidirectional transformer encoder that underlies mod els like BERT and its descendants like RoBERTa Liu et al. 2019 or SpanBERT Joshi et al. 2020 .', 'This is because contextual embeddings whether from masked language11.4 FINE TUNING LANGUAGE MODELS 13 models or from autoregressive ones have the property that the vectors for all words are extremely similar.', 'But the span based approach can nat urally label both since the spans are labeled separately. 11.6 Summary This chapter has introduced the topic of transfer learning from pretrained language models. Here s a summary of the main points that we covered Bidirectional encoders can be used to generate contextualized representations of input embeddings using the entire input context.', 'Conneau et al. 2020 show that Œ± 0.3 works well to give rare languages more inclusion in the tokenization resulting in better multilingual performance overall. The result of this pretraining process consists of both learned word embeddings as well as all the parameters of the bidirectional encoder that are used to produce contextual embeddings for novel inputs.', 'Multilingual models have an additional decision to make what data to use to build the vocabulary Recall that all language models use subword tokenization BPE or SentencePiece Unigram LM are the two most common algorithms .']",text-embedding-3-small,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9582825376977878,0.0,0.910129617588691,0.0625
195,How does ensemble distillation differ from parameter averaging?,"Ensemble distillation differs from parameter averaging in that ensemble distillation trains a model to copy the ensemble by matching the description over predicted words, aiming for the model to make the same mistakes as an ensemble, which has been shown to increase accuracy notably. On the other hand, parameter averaging involves writing out models several times near the end of training and taking the average of parameters, which is a cheaper way to get some good effects of ensembling, but it only works for models within the same run. 

Source: SEP775 - Computational Natural Language Processing, lecture04_Conditioned_Generation.","['agree use when you want to restrict possible answersParameter Averaging Problem Ensembling means we have to use M models at test time increasing our time memory complexity Parameter averaging is a cheap way to get some good effects of ensembling Basically write out models several times near the end of training and take the average of parametersEnsemble Distillation e.g. Kim et al.', 'e.g. Kim et al. 2016 Problem parameter averaging only works for models within the same run Knowledge distillation trains a model to copy the ensemble Specifically it tries to match the description over predicted words Why We want the model to make the same mistakes as an ensemble Shown to increase accuracy notablyStacking What if we have two very different models where prediction of outputs is done in very different ways e.g.', 'Note in the entry for labeler vs. labeler ensemble the ensembles are slightly smaller than for other comparisons because we need to exclude the labeler being predicted. All ensembles have at least 3 workers. TL DR 6.7B RL T 0.7 labeler labeler ensem ble length copying ROUGE 1.3B sup logprob 1.3B RM 6.', 'The parameters of the combining layers were then trained on the existing training set while keeping the ten networks Ô¨Åxed joined ensemble . This approach did not improve on simple voting. These ensembles come of course at the expense of a ten fold increase of the running time. On the other hand multiple training times could be improved using smart sampling strategies Neal 1996 .', 'eig 0.0 . The results in this scenario are quite different than those of the vanilla scenario with better performance in many cases. However this change is not uniform as we observe that differ ent settings boost different algorithms. In fact the question Which method is best might have a completely different answer when comparing on the same task but with different hyperparameter values.', 'We can also evaluate the model as a mixture by averaging its losses over multiple random orderings. This is a lower bound on log likelihood. It is intractable to sample from exactly but can be approximated through Gibbs sampling. 7Figure 4 Comparing how models continue a prime top left .', 'We then counted the number of times each hyperparameter setting was chosen in these conÔ¨Ågurations Table 7 . Some trends emerge such as PPMI and SVD s prefer ence towards shorter context windows10 win 2 and that SGNS always prefers numerous nega tive samples neg 1 .', 'However this may be computationally expensive. We thus provide some rules of thumb which we found to work well in our setting Always use context distribution smoothing cds 0.75 to modify PMI as described in Section 3.2. It consistently improves performance and is applicable to PPMI SVD and SGNS. Do not use SVD correctly eig 1 .']",text-embedding-3-small,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.987278043698859,1.0,0.9658765632550164,0.0434782608695652
196,What are some common evaluation metrics for conditioned language models?,"Some common evaluation metrics for conditioned language models include BLEU, BertScore, BLEURT, and COMET. These metrics are based on neural models and aim to evaluate the performance of conditioned language models by comparing n-gram overlap with reference, finding similarity between BERT embeddings, training models to predict human evaluation scores, and training models to predict human evaluation using source sentences.

Source: lecture04_Conditioned_Generation","['out this review paperHow do we EvaluateBasic Evaluation Paradigm Use parallel test set Use system to generate translations Compare target translations w referenceHuman Evaluation Ask a human to do evaluation Final goal but slow expensive and sometimes inconsistentBLEU Works by comparing n gram overlap w reference Pros Easy to use good for measuring system improvement Cons Often doesn t match human eval bad for comparing very different systemsEmbedding based Metrics Recently many metrics based on neural models BertScore Find similarity between', 'May be reasonable for problems with lots of ambiguity.Which One to Use Meta evaluation runs human evaluation and automatic evaluation on the same outputs calculates correlation Examples WMT Metrics Task for MT Mathur et al. 2021 RealSumm for summarization Evaluation is hard especially with good systems Most metrics had no correlation w human eval over best systems at some WMT 2019 tasksQuestions', 'metrics based on neural models BertScore Find similarity between BERT embeddings unsupervised Zhang et al. 2020 BLEURT Train BERT to predict human evaluation scores Sellam et al. 2020 COMET Train model to predict human eval also using source sentence Rei et al.', '13 3.2 Evaluating Language Models The best way to evaluate the performance of a language model is to embed it in an application and measure how much the application improves. Such end to end evaluation is called extrinsic evaluation. Extrinsic evaluation is the only way to extrinsic evaluation know if a particular improvement in a component is really going to help the task at hand.', 'For Acc F1 BLEU we use the metric speciÔ¨Åed in Metric . These scores come from evaluations with a different seed for the random examples used for in context learning and will therefore differ slightly from the scores elsewhere in the paper. 45D Total Compute Used to Train Language Models This appendix contains the calculations that were used to derive the approximate compute used to train the language models in Figure 2.2.', 'RRS20 recently demonstrated that a large language model can perform surprisingly well directly answering the questions without conditioning on auxilliary information. They denote this more restrictive evaluation setting as closed book . Their work suggests that even higher capacity models could perform even better and we test this hypothesis with GPT 3.', 'Thus for speech recognition we can compare the performance of two language models by running the speech recognizer twice once with each language model and seeing which gives the more accurate transcription. Unfortunately running big NLP systems end to end is often very expensive. In stead it would be nice to have a metric that can be used to quickly evaluate potential improvements in a language model.', 'a phrase based translation model and a neural MT model Niehues et al. 2017 Stacking uses the output of one system in calculating features for another systemCase Studies in Conditional Language ModelingFrom Images e.g. Karpathy et al. 2015 Input is image features output is text Use standard image encoders e.g. CNN Often pre trained on large databases such as ImageNetCheck out this review paperHow do we EvaluateBasic']",text-embedding-3-small,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.0000000000000002,1.0,0.9256614054710058,0.0625
197,What is the significance of stacking in model ensembling?,"Stacking in model ensembling is significant because it allows us to combine predictions from multiple models that may have different ways of predicting outputs. By stacking the predictions, we can leverage the strengths of each model and potentially improve overall accuracy by smoothing over the idiosyncrasies of individual models. This approach helps to create a more robust and reliable ensemble model by incorporating diverse perspectives and reducing errors.","['agree use when you want to restrict possible answersParameter Averaging Problem Ensembling means we have to use M models at test time increasing our time memory complexity Parameter averaging is a cheap way to get some good effects of ensembling Basically write out models several times near the end of training and take the average of parametersEnsemble Distillation e.g. Kim et al.', 'e.g. Kim et al. 2016 Problem parameter averaging only works for models within the same run Knowledge distillation trains a model to copy the ensemble Specifically it tries to match the description over predicted words Why We want the model to make the same mistakes as an ensemble Shown to increase accuracy notablyStacking What if we have two very different models where prediction of outputs is done in very different ways e.g.', 'The parameters of the combining layers were then trained on the existing training set while keeping the ten networks Ô¨Åxed joined ensemble . This approach did not improve on simple voting. These ensembles come of course at the expense of a ten fold increase of the running time. On the other hand multiple training times could be improved using smart sampling strategies Neal 1996 .', 'Some in reading materials more in a later classModel EnsemblingEnsembling Why Multiple models make somewhat uncorrelated errors Models tend to be more uncertain when they are about to make errors Smooths over idiosyncrasies of the model LSTM1 predict1 I LSTM2 predict2 Combine predictions from multiple models s sLinear Interpolation Take a weighted average of the M model probabilities Second term often set to uniform distribution 1 MLog linear Interpolation Weighted', 'Note in the entry for labeler vs. labeler ensemble the ensembles are slightly smaller than for other comparisons because we need to exclude the labeler being predicted. All ensembles have at least 3 workers. TL DR 6.7B RL T 0.7 labeler labeler ensem ble length copying ROUGE 1.3B sup logprob 1.3B RM 6.', 'In some sense it s odd that this works but interesting 2.3 Elementwise nonlinearity Imagine if we were to stack self attention layers. Would this be sufficient for a replacement for stacked LSTM layers Intuitively there s one thing that s missing the elementwise nonlinearities that we ve come to expect in standard deep learning architectures. In fact if we stack two self attention layers we get something that looks a lot like a single self attention layer', 'This suggests we could get more reliable labels by aggregating labels from multiple workers on the same comparison. Indeed on the subset of the training data for which we have enough shared comparisons taking the modal label from 3 labelers increases their agreement rate with researchers from 72 to 77 . However we usually collect only one label per comparison in order to maximize label throughput.', 'All ensembles have at least 3 workers. 36TL DR 6.7B sup. T 0.7 labeler labeler ensem ble length copying ROUGE 1.3B sup logprob 1.3B RM 6.7B sup logprob 6.7B RM labeler 70.8 2.6 73.1 2.9 56.9 0.']",text-embedding-3-small,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9829441240064096,0.75,0.9032208920727968,0.0526315789473684
198,How are images used as input for conditioned language models?,"Images can be used as input for conditioned language models by representing the image features as input and generating text as output. This process involves using standard image encoders like CNN, which are often pre-trained on large databases such as ImageNet. The image features are then used as input to the model, which generates text based on the image content. This approach is commonly used in tasks like image captioning, where the model generates descriptive text based on the content of the input image.

Source: lecture04_Conditioned_Generation","['a phrase based translation model and a neural MT model Niehues et al. 2017 Stacking uses the output of one system in calculating features for another systemCase Studies in Conditional Language ModelingFrom Images e.g. Karpathy et al. 2015 Input is image features output is text Use standard image encoders e.g. CNN Often pre trained on large databases such as ImageNetCheck out this review paperHow do we EvaluateBasic', 'For DMOL we only evaluated learned coordinate embeddings. 5.2. Conditioning on Image Class We represent the image classes as learned d dimensional embeddings per class and simply add the respective em bedding to the input representation of every input position together with the positional encodings. We trained the class conditioned Image Transformer on CIFAR 10 achieving very similar log likelihoods as in un conditioned generation.', 'Many applications of image density models require condi tioning on additional information of various kinds from im ages in enhancement or reconstruction tasks such as super resolution in painting and denoising to text when synthesiz ing images from natural language descriptions Mansimov et al. 2015 . In visual planning tasks conditional image generation models could predict future frames of video con ditioned on previous frames and taken actions.', 'Both models are able to generate convincing cars trucks and ships. Generated horses planes and birds also look reasonable. 5.1. Generative Image Modeling Our unconditioned and class conditioned image generation models both use 1D local attention with lq 256 and a total memory size of 512. On CIFAR 10 our best unconditional models achieve a perplexity of 2.90 bits dim on the test set using either DMOL or categorical.', 'generate text according to some specification Input X Structured Data English Document Utterance Image Speech Output Y Text NL Description Japanese Short Description Response Text Transcript Task NL Generation Translation Summarization Response Generation Image Captioning Speech RecognitionFormulation and ModelingCalculating the Probability of a SentenceConditional Language ModelsOne Type of Language Model LSTM LSTM LSTM LSTM predict hate predict this predict movie predict s LSTM Mikolov et al.', '4. Inference Across all of the presented experiments we use categorical sampling during decoding with a tempered softmax Dahl et al. 2017 . We adjust the concentration of the distribution we sample from with a temperature œÑ 0 by which we divide the logits for the channel intensities. We tuned œÑ between 0.8 and 1.0 observing the highest perceptual quality in unconditioned and class conditional image generation with œÑ 1.0.', 'Model Architecture 3.1. Image Representation We treat pixel intensities as either discrete categories or ordi nal values this setting depends on the distribution Section 3.4 . For categories each of the input pixels three color channels is encoded using a channel speciÔ¨Åc set of 256Image Transformer Input Gen Truth Input Gen Truth Table 2. On the left are image completions from our best conditional generation model where we sample the second half.', '2016 Vinyals et al. 2015 Xu et al. 2016Attention as visual semantic alignmentText to image Conditional image synthesis Generative adversarial nets Goodfellow et al. 2014 Source Google Source Google Reed et al. 2016Outline 1. Early models 2. Features and fusion 3. Contrastive models 4. Multimodal foundation models 5.']",text-embedding-3-small,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9918367662783686,1.0,0.9500276108645668,0.05
199,What challenges exist in evaluating conditioned language models?,"Some challenges in evaluating conditioned language models include determining the effectiveness of using one system's output as features for another system, evaluating the performance of models that do not rely on auxiliary information, exploring the limitations of language models in terms of sample efficiency during pre-training, and investigating the potential benefits of incorporating additional modalities like images to enhance the model's understanding of the world.

Source: Lecture04_Conditioned_Generation","['a phrase based translation model and a neural MT model Niehues et al. 2017 Stacking uses the output of one system in calculating features for another systemCase Studies in Conditional Language ModelingFrom Images e.g. Karpathy et al. 2015 Input is image features output is text Use standard image encoders e.g. CNN Often pre trained on large databases such as ImageNetCheck out this review paperHow do we EvaluateBasic', 'RRS20 recently demonstrated that a large language model can perform surprisingly well directly answering the questions without conditioning on auxilliary information. They denote this more restrictive evaluation setting as closed book . Their work suggests that even higher capacity models could perform even better and we test this hypothesis with GPT 3.', 'Promising future directions in this vein might include learning the objective function from humans ZSW 19a Ô¨Åne tuning with reinforcement learning or adding additional modalities such as images to provide grounding and a better model of the world CLY 19 . Another limitation broadly shared by language models is poor sample efÔ¨Åciency during pre training.', '2022a. Finetuned language models are zero shot learners. ICLR. 13Jason Wei Yi Tay Rishi Bommasani Colin Raffel Barret Zoph Sebastian Borgeaud Dani Yogatama Maarten Bosma Denny Zhou Donald Metzler et al. 2022b. Emergent abilities of large language models.', 'And yet small models still fail the ability to perform abstract manipulations on unseen symbols for these three tasks only arises at the scale of 100B model parameters. As for the OOD evaluations standard prompting fails for both tasks. With chain of thought prompting language models achieve upward scaling curves though performance is lower than in the in domain setting .', 'Scaling PaLM to 540B Ô¨Åxed a substantial portion of errors in all categories. There are also three notable points regarding why small language models fail. The Ô¨Årst observation is that small language models fail at even relatively easy symbol mapping tasks.', 'The assessment was that language models may not be worth investing signiÔ¨Åcant resources in because there has been no convincing demonstration that current language models are signiÔ¨Åcantly better than current methods for generating text and because methods for targeting or controlling the content of language models are still at a very early stage.', 'As Ô¨Åne tuned language models have neared human performance on many standard benchmark tasks considerable effort has been devoted to constructing more difÔ¨Åcult or open ended tasks including question answering KPR 19 IBGC 14 CCE 18 MCKS18 reading comprehension CHI 18 RCM19 and adversarially constructed datasets designed to be difÔ¨Åcult for existing language models SBBC19 NWD 19 .']",text-embedding-3-small,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9932392273138442,0.75,0.8256684335451179,0.0526315789473684
200,What is the instructor's name for this course?,"The instructor for the course ""Introduction to Computational Natural Language Processing"" in Winter 2024 is Hamidreza Mahyar. 

Source: Course Outline - NLP_Winter_2024_Course_Outline.txt","['Instruction Type Code Type Hours per term C Classroom instruction 39 L Laboratory workshop or fieldwork T Tutorial DE Distance education Total Hours 39 Resources ISBN Textbook Title Edition Author Publisher Natural Language Processing Jacob Eisenstein Speech and Language Processing Dan Jurafsky and James H.', 'Course Outline 1. COURSE INFORMATION Session Offered Winter 2024 Course Name Introduction to Computational Natural Language Processing Course Code SEP 775 Date s and Time s of lectures Lectures In person Wednesday 3 30 pm 6 30 pm Program Name System and Technology Calendar Description This graduate course introduces fundamental concepts in computational natural language processing NLP and their applications to teaching students how to deal with textual data in Artificial Intelligence.', 'Continuation in a course that uses on line elements will be deemed consent to this disclosure. If you have any questions or concerns about such disclosure please discuss this with the course instructor. ONLINE PROCTORING Some courses may use online proctoring software for tests and exams.', 'http www.mcmaster.ca policy Students AcademicStudies Studentcode.pdf COPYRIGHT AND RECORDING Students are advised that lectures demonstrations performances and any other course material provided by an instructor include copyright protected works. The Copyright Act and copyright law protect every original literary dramatic musical and artistic work including lectures by University instructors The recording of lectures tutorials or other methods of instruction may occur during a course.', 'Recording may be done by either the instructor for the purpose of authorized distribution or by a student for the purpose of personal study. Students should be aware that their voice and or image may be recorded by others during the class. Please speak with the instructor if this is a concern for you.', 'e mail Avenue to Learn A2L LearnLink web pages capa Moodle ThinkingCap etc. . Students should be aware that when they access the electronic components of a course using these elements private information such as first and last names user names for the McMaster e mail accounts and program affiliation may become apparent to all other students in the same course. The available information is dependent on the technology used.', 'Instructor Hamidreza Mahyar E Mail mahyarh@mcmaster.ca Office Hours upon request 2. COURSE SPECIFICS Course Description This course covers some fundamental concepts in natural language processing NLP and how they apply in real world use cases.', 'and Q A Week 12 Project presentation and Q A Week 13 Project presentation and Q A Classes end April 10th 4. ASSESSMENT OF LEARNING Weight 4 Assignments 20 4 Quizzes 10 Midterm Test 35 Group Project 35 TOTAL 100Note that the percentage distribution of each module may be adjusted.']",text-embedding-3-small,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.8698155415208402,1.0,0.8807607490504835,0.0
201,What is the email address of the instructor for this course?,"The email address of the instructor for this course is mahyarh@mcmaster.ca. 
Source: NLP_Winter_2024_Course_Outline.txt","['e mail Avenue to Learn A2L LearnLink web pages capa Moodle ThinkingCap etc. . Students should be aware that when they access the electronic components of a course using these elements private information such as first and last names user names for the McMaster e mail accounts and program affiliation may become apparent to all other students in the same course. The available information is dependent on the technology used.', 'Use the University provided e mail address or maintain a valid forwarding e mail address. Regularly check the official University communications channels. Official University communications are considered received if sent by postal mail by fax or by e mail to the student s designated primary e mail account via their @mcmaster.ca alias.', 'Accept that forwarded e mails may be lost and that e mail is considered received if sent via the student s @mcmaster.ca alias. Check the McMaster Avenue email and course websites on a regular basis during the term. CONDUCT EXPECTATIONS As a McMaster student you have the right to experience and the responsibility to demonstrate respectful and dignified interactions within all of our living learning and working communities.', 'Continuation in a course that uses on line elements will be deemed consent to this disclosure. If you have any questions or concerns about such disclosure please discuss this with the course instructor. ONLINE PROCTORING Some courses may use online proctoring software for tests and exams.', 'Recording may be done by either the instructor for the purpose of authorized distribution or by a student for the purpose of personal study. Students should be aware that their voice and or image may be recorded by others during the class. Please speak with the instructor if this is a concern for you.', 'Instructor Hamidreza Mahyar E Mail mahyarh@mcmaster.ca Office Hours upon request 2. COURSE SPECIFICS Course Description This course covers some fundamental concepts in natural language processing NLP and how they apply in real world use cases.', 'http www.mcmaster.ca policy Students AcademicStudies Studentcode.pdf COPYRIGHT AND RECORDING Students are advised that lectures demonstrations performances and any other course material provided by an instructor include copyright protected works. The Copyright Act and copyright law protect every original literary dramatic musical and artistic work including lectures by University instructors The recording of lectures tutorials or other methods of instruction may occur during a course.', 'Instruction Type Code Type Hours per term C Classroom instruction 39 L Laboratory workshop or fieldwork T Tutorial DE Distance education Total Hours 39 Resources ISBN Textbook Title Edition Author Publisher Natural Language Processing Jacob Eisenstein Speech and Language Processing Dan Jurafsky and James H.']",text-embedding-3-small,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.999999125165964,1.0,0.938724518291746,0.0
202,When is this course held every week?,"The course ""Introduction to Computational Natural Language Processing"" (Course Code: SEP 775) is held in person every Wednesday from 3:30 pm to 6:30 pm. 
Source: Course Outline - NLP_Winter_2024_Course_Outline.txt","['Course Outline 1. COURSE INFORMATION Session Offered Winter 2024 Course Name Introduction to Computational Natural Language Processing Course Code SEP 775 Date s and Time s of lectures Lectures In person Wednesday 3 30 pm 6 30 pm Program Name System and Technology Calendar Description This graduate course introduces fundamental concepts in computational natural language processing NLP and their applications to teaching students how to deal with textual data in Artificial Intelligence.', 'and Q A Week 12 Project presentation and Q A Week 13 Project presentation and Q A Classes end April 10th 4. ASSESSMENT OF LEARNING Weight 4 Assignments 20 4 Quizzes 10 Midterm Test 35 Group Project 35 TOTAL 100Note that the percentage distribution of each module may be adjusted.', 'EXTREME CIRCUMSTANCES The University reserves the right to change the dates and deadlines for any or all courses in extreme circumstances e.g. severe weather labour disruptions etc. . Changes will be communicated through regular McMaster communication channels such as McMaster Daily News A2L and or McMaster email.', 'Announcements made in class or placed on Avenue are considered to have been communicated to all students including those individuals that are not in class. Instructor has the right to submit work to software to identify plagiarism. 3. SUB TOPIC S Week 1 Overview Human language Word meanings Word2Vec and Optimization basics Week 2 Intro to neural networks Gradients and backpropagation Week 3 Recurrent Neural Networks and Language Models Week', '3 Recurrent Neural Networks and Language Models Week 4 Conditioned Generation Week 5 Part 1 Self attention and transformers Part 2 Seq2Seq models Week 6 Midterm Test Week 7 Midterm Recess No class Week 8 Prompting Instruction Finetuning and RLHF Week 9 Question Answering systems Week 10 Multimodal models Week 11 Project presentation and Q A Week 12 Project presentation and Q', 'to build systems in PyTorch for some of the major problems in NLP Word meaning dependency parsing machine translation question answeringCourse work and grading policy 5 4 Assignments 20 4 Quizzes 10 Midterm 35 Final Group Project 1 3 people 34 Late day policy NO free late days 1 off course grade per day late Assignments not accepted more than 3 days late unless given permission in advance Collaboration policy Understand allowed collaboration and how', 'Instructor Hamidreza Mahyar E Mail mahyarh@mcmaster.ca Office Hours upon request 2. COURSE SPECIFICS Course Description This course covers some fundamental concepts in natural language processing NLP and how they apply in real world use cases.', 'Continuation in a course that uses on line elements will be deemed consent to this disclosure. If you have any questions or concerns about such disclosure please discuss this with the course instructor. ONLINE PROCTORING Some courses may use online proctoring software for tests and exams.']",text-embedding-3-small,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.8450103822635677,1.0,0.8528368903648232,0.0555555555555555
203,What is the name of the TA for this course?,"The Teaching Assistant for this course is an AI Teaching Assistant (AI TA). 

Source: SEP775_FinalProjects_2","['Instruction Type Code Type Hours per term C Classroom instruction 39 L Laboratory workshop or fieldwork T Tutorial DE Distance education Total Hours 39 Resources ISBN Textbook Title Edition Author Publisher Natural Language Processing Jacob Eisenstein Speech and Language Processing Dan Jurafsky and James H.', 'file name: SEP775 FinalProjects 2. McMaster University SEP 775 Final Projects Project 1 AI Teaching Assistant Project Overview The goal of this project is to create an AI Teaching Assistant AI TA . The AI TA aims to provide an interactive platform for students to ask questions and receive guidance on course materials mirroring the support traditionally offered by human teaching assistants.', 'Course Outline 1. COURSE INFORMATION Session Offered Winter 2024 Course Name Introduction to Computational Natural Language Processing Course Code SEP 775 Date s and Time s of lectures Lectures In person Wednesday 3 30 pm 6 30 pm Program Name System and Technology Calendar Description This graduate course introduces fundamental concepts in computational natural language processing NLP and their applications to teaching students how to deal with textual data in Artificial Intelligence.', 'and Q A Week 12 Project presentation and Q A Week 13 Project presentation and Q A Classes end April 10th 4. ASSESSMENT OF LEARNING Weight 4 Assignments 20 4 Quizzes 10 Midterm Test 35 Group Project 35 TOTAL 100Note that the percentage distribution of each module may be adjusted.', 'file name: SEP775 Assignment3. McMaster University SEP 775 Assignment 3 TANDA Approach for QA System Enhancement Objectives Explore TANDA Transfer And Adapt methodology to improve Question Answering QA systems using pre trained Transformer models focusing on sequential fine tuning te chniques. 1. Introduction and Theory 15 Summarize the TANDA methodology s principles focusing on its novelty and the rationale behind sequential fine tuning.', 'Continuation in a course that uses on line elements will be deemed consent to this disclosure. If you have any questions or concerns about such disclosure please discuss this with the course instructor. ONLINE PROCTORING Some courses may use online proctoring software for tests and exams.', 'e mail Avenue to Learn A2L LearnLink web pages capa Moodle ThinkingCap etc. . Students should be aware that when they access the electronic components of a course using these elements private information such as first and last names user names for the McMaster e mail accounts and program affiliation may become apparent to all other students in the same course. The available information is dependent on the technology used.', 'Instructor Hamidreza Mahyar E Mail mahyarh@mcmaster.ca Office Hours upon request 2. COURSE SPECIFICS Course Description This course covers some fundamental concepts in natural language processing NLP and how they apply in real world use cases.']",text-embedding-3-small,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9419672359654538,1.0,0.84011288057954,0.0
204,What is Word2vec?,"Word2vec is a framework for learning word vectors that represents each word in a fixed vocabulary as a low-dimensional vector. It is a model used to train and discover latent vector representations of natural language words in a semantic space. The model is specified with matrices U and V to calculate probabilities based on dot products, resulting in a probability distribution where higher scored items have a higher probability. Word2vec outperforms other methods like GloVe on tasks like word analogy and achieves better results faster. It is commonly used in natural language processing for various applications. 

Source: SEP 775 - Computational Natural Language Processing, lecture01_notes_Introduction.txt","['The word2vec model is a probabilistic model specified as follows where uw refers to the row of U corresponding to word w V and likewise for V pU V o c exp u o vc w V exp u wvc 4 This may be familiar to you as the softmax function which takes arbitrary scores here one for each word in the vocabulary resulting from dot products and produces a probability distribution where larger scored things get higher probability.', 'However many of the details of word2vec will hold true in methods that we ll proceed to further in the course so we ll focus our time on that. 3.2 Word2vec model and objective The word2vec model represents each word in a fixed vocabulary as a low dimensional much smaller than vocabulary size vector.', '3.3 Estimating a word2vec model from a corpus How do we train word2vec in practice Specifying the word2vec model is relatively transparent from the math we ve given above one constructs matrices U and V and can write out the math of the probability.', '718 0.147 0.051 19Word meaning as a neural word vector visualization 0.286 0.792 0.177 0.107 0.109 0.542 0.349 0.271 0.487 expect 203. Word2vec Overview 21 Word2vec Mikolov et al. 2013 is a framework for learning word vectors Idea We have a large corpus body of text', 'It consistently outperforms word2vec on the word analogy task given the same corpus vocabulary window size and training time. It achieves better results faster and also obtains the best results irrespective of speed. 2 Evaluation of Word Vectors So far we have discussed methods such as the Word2Vec and GloVe methods to train and discover latent vector representations of natural language words in a semantic space.', '4The code is available at https code.google.com p word2vec 1118 T. Mikolov A. Deoras D. Povey L. Burget J. Àá Cernock y. Strategies for Training Large Scale Neural Network Language Models In Proc. Automatic Speech Recognition and Understand ing 2011. 19 T. Mikolov.', 'With word2vec we train the skip gram SG and continuous bag of words CBOW models on the 6 billion token corpus Wikipedia 2014 Giga word 5 with a vocabulary of the top 400 000 most frequent words and a context window size of 10. We used 10 negative samples which we show in Section 4.6 to be a good choice for this corpus.', 'in window of size 2 outside context words in window of size 2 ùëÉùë§ùë° 2 ùë§ùë° ùëÉùë§ùë° 1 ùë§ùë° 23 ùëÉùë§ùë° 2 ùë§ùë° ùëÉùë§ùë° 1 ùë§ùë°Word2vec objective function For each position']",text-embedding-3-small,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999999999999994,0.8571428571428571,0.9473151550128104,0.0625
205,What are the main learning goals of the course?,"The main learning goals of the course are:
1. Understanding the foundations of effective modern methods for deep learning applied to NLP, including word vectors, feed-forward networks, recurrent networks, attention, encoder-decoder models, transformers, and large pre-trained language models.
2. Developing a big picture understanding of human languages and the challenges in understanding and producing them using computers.
3. Building systems in PyTorch for some NLP methods.
4. Gaining knowledge about deep learning, word representations, language models, pre-trained language models, text generation, Seq2Seq models, prompting feedback systems, multimodal models, and NLP applications like Q&A systems.

Source: NLP_Winter_2024_Course_Outline.txt","['learning goals 4 1. The foundations of the effective modern methods for deep learning applied to NLP Basics first then key methods used in NLP in 2024 Word vectors feed forward networks recurrent networks attention encoder decoder models transformers large pre trained language models etc. 2. A big picture understanding of human languages and the difficulties in understanding and producing them via computers 3. An understanding of and ability to build systems in PyTorch for some of', 'Percentage grades will be converted to letter grades and grade points per the University calendar. 5. LEARNING OUTCOMES By the end of this course the students will know about Introduction to Deep Learning Word representations Language models Pre trained language models Text generation Seq2Seq models Prompting Feedback systems Multimodal models NLP applications e.g. Q A systems 6.', 'and Q A Week 12 Project presentation and Q A Week 13 Project presentation and Q A Classes end April 10th 4. ASSESSMENT OF LEARNING Weight 4 Assignments 20 4 Quizzes 10 Midterm Test 35 Group Project 35 TOTAL 100Note that the percentage distribution of each module may be adjusted.', 'One fundamental and still quite open problem in building language learning machines is the question of representa tion how should we represent language in a computer such that the computer can robustly process and or generate it This is where this course focuses on the tools provided by deep learning a highly effec tive toolkit for representing both the wild variety of natural language and some of the rules and structures it sometimes adheres to.', 'Optimization basics 6. Looking at word vectors Key learning today The astounding result that word meaning can be represented rather well by a high dimensional vector of real numbersCourse logistics in brief 3 Instructor Hamidreza Mahyar Head TA Reza Namazi Time Wednesday 3 30pm 6 30pm We put a lot of other important information on Avenue to Learn. Please read itWhat do we hope to teach A.k.a.', 'e mail Avenue to Learn A2L LearnLink web pages capa Moodle ThinkingCap etc. . Students should be aware that when they access the electronic components of a course using these elements private information such as first and last names user names for the McMaster e mail accounts and program affiliation may become apparent to all other students in the same course. The available information is dependent on the technology used.', 'Announcements made in class or placed on Avenue are considered to have been communicated to all students including those individuals that are not in class. Instructor has the right to submit work to software to identify plagiarism. 3. SUB TOPIC S Week 1 Overview Human language Word meanings Word2Vec and Optimization basics Week 2 Intro to neural networks Gradients and backpropagation Week 3 Recurrent Neural Networks and Language Models Week', 'Martin Deep Learning Ian Goodfellow Yoshua Bengio and Aaron Courville Prerequisite s Proficiency in Python programming language Some Machine Learning and or Deep Learning knowledge is a plus Corequisite s Antirequisite s Course Specific Policies All work must be shown to get full credit. Specific policy of Projects Assignments submissionsProjects Assignments will be conducted online by using related software.']",text-embedding-3-small,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999999999999996,1.0,0.9537934661800512,0.0588235294117647
206,How is the course grade distributed?,"The course grade is distributed as follows: 4 Assignments worth 20%, 4 Quizzes worth 10%, Midterm Test worth 35%, and Group Project worth 35%. Please note that the percentage distribution of each module may be adjusted.","['and Q A Week 12 Project presentation and Q A Week 13 Project presentation and Q A Classes end April 10th 4. ASSESSMENT OF LEARNING Weight 4 Assignments 20 4 Quizzes 10 Midterm Test 35 Group Project 35 TOTAL 100Note that the percentage distribution of each module may be adjusted.', 'Percentage grades will be converted to letter grades and grade points per the University calendar. 5. LEARNING OUTCOMES By the end of this course the students will know about Introduction to Deep Learning Word representations Language models Pre trained language models Text generation Seq2Seq models Prompting Feedback systems Multimodal models NLP applications e.g. Q A systems 6.', 'to build systems in PyTorch for some of the major problems in NLP Word meaning dependency parsing machine translation question answeringCourse work and grading policy 5 4 Assignments 20 4 Quizzes 10 Midterm 35 Final Group Project 1 3 people 34 Late day policy NO free late days 1 off course grade per day late Assignments not accepted more than 3 days late unless given permission in advance Collaboration policy Understand allowed collaboration and how', 'All submitted work is subject to normal verification that standards of academicintegrity have been upheld e.g. on line search other software etc. . For more details about McMaster s use of Turnitin.com please go to www.mcmaster.ca academicintegrity. COURSES WITH AN ON LINE ELEMENT Some courses may use on line elements e.g.', 'e mail Avenue to Learn A2L LearnLink web pages capa Moodle ThinkingCap etc. . Students should be aware that when they access the electronic components of a course using these elements private information such as first and last names user names for the McMaster e mail accounts and program affiliation may become apparent to all other students in the same course. The available information is dependent on the technology used.', 'Accept that forwarded e mails may be lost and that e mail is considered received if sent via the student s @mcmaster.ca alias. Check the McMaster Avenue email and course websites on a regular basis during the term. CONDUCT EXPECTATIONS As a McMaster student you have the right to experience and the responsibility to demonstrate respectful and dignified interactions within all of our living learning and working communities.', 'Academic dishonesty is to knowingly act or fail to act in a way that results or could result in unearned academic credit or advantage. This behaviour can result in serious consequences e.g. the grade of zero on an assignment loss of credit with a notation on the transcript notation reads Grade of F assigned for academic dishonesty and or suspension or expulsion from the university.', 'Q A systems 6. COURSE OUTLINE APPROVED ADVISORY STATEMENTS ANTI DISCRIMINATION The Faculty of Engineering is concerned with ensuring an environment that is free of all discrimination. If there is a problem individuals are reminded that they should contact the Associate Director Graduate Studies the Sexual Harassment Officer or the Human Rights Consultant as soon as possible.']",text-embedding-3-small,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9898862143010956,1.0,0.9726906242479878,0.0
207,What is the policy for late submissions in this course?,"The policy for late submissions in this course is that there are NO free late days, and there is a penalty of 1 off course grade per day late. Assignments are not accepted more than 3 days late unless given permission in advance.","['to build systems in PyTorch for some of the major problems in NLP Word meaning dependency parsing machine translation question answeringCourse work and grading policy 5 4 Assignments 20 4 Quizzes 10 Midterm 35 Final Group Project 1 3 people 34 Late day policy NO free late days 1 off course grade per day late Assignments not accepted more than 3 days late unless given permission in advance Collaboration policy Understand allowed collaboration and how', 'Completed Projects Assignments should be uploaded to the drop box before midnight of the due date. Projects Assignments missed due to legitimate reasons must be completed later mutually agreed with the instructor. Departmental Policies The use of cell phones iPods laptops and other personal electronic devices are prohibited from the classroom during the class time unless the instructor makes an explicit exception.', 'Students should submit their request to their Faculty Office normally within 10 working days of the beginning of term in which they anticipate a need for accommodation or to the Registrar s Office prior to their examinations. Students should also contact their instructors as soon as possible to make alternative arrangements for classes assignments and tests.', 'Adherence to Guidelines 10 Submissions should follow the provided guidelines including format labeling and adherence to the deadline. McMaster University 2', 'Submission Guidelines Submit all code in a ZIP file including Jupyter Notebooks and a detailed PDF report with written explanations and visualizations. Clearly label each part and question in your submissions. Deadline Feb. 7 2024 Rubric and Expectations Code Quality and Functionality 40 Code should be well organized commented and functioning as intended. The use of Python and relevant libraries e.g.', 'EXTREME CIRCUMSTANCES The University reserves the right to change the dates and deadlines for any or all courses in extreme circumstances e.g. severe weather labour disruptions etc. . Changes will be communicated through regular McMaster communication channels such as McMaster Daily News A2L and or McMaster email.', 'and Q A Week 12 Project presentation and Q A Week 13 Project presentation and Q A Classes end April 10th 4. ASSESSMENT OF LEARNING Weight 4 Assignments 20 4 Quizzes 10 Midterm Test 35 Group Project 35 TOTAL 100Note that the percentage distribution of each module may be adjusted.', 'A2L etc. using plagiarism detection a service supported by Turnitin.com so it can be checked for academic dishonesty. Students who do not wish their work to be submitted through the plagiarism detection software must inform the Instructor before the assignment is due. No penalty will be assigned to a student who does not submit work to the plagiarism detection software.']",text-embedding-3-small,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9832537859774184,0.6666666666666666,0.962528560032566,0.0
208,What does the course hope to teach regarding word vectors?,"The course hopes to teach about how word meaning can be represented rather well by a high-dimensional vector of real numbers, as well as discussing word vectors, word embeddings, intrinsic and extrinsic evaluation techniques, training model weights, parameters, and word vectors for extrinsic tasks, and the importance of not retraining word vectors if the training dataset is small.","['Optimization basics 6. Looking at word vectors Key learning today The astounding result that word meaning can be represented rather well by a high dimensional vector of real numbersCourse logistics in brief 3 Instructor Hamidreza Mahyar Head TA Reza Namazi Time Wednesday 3 30pm 6 30pm We put a lot of other important information on Avenue to Learn. Please read itWhat do we hope to teach A.k.a.', 'Then it extends our discussion of word vectors in terchangeably called word embeddings by seeing how they can be evaluated intrinsically and extrinsically. As we proceed we discuss the example of word analogies as an intrinsic evaluation technique and how it can be used to tune word embedding techniques. We then discuss training model weights parameters and word vectors for ex trinsic tasks.', 'The word vectors can also be initialized to random values then estimated solely through task learning which we might call learning from scratch. 6 We can use expert built data structures like WordNet as additional input to creating word vectors.', 'LSA HAL . While these methods effectively leverage global statistical information they are primarily used to capture word similarities and do poorly on tasks such as word analogy indi cating a sub optimal vector space structure. The other set of methods are shallow window based e.g. the skip gram and the CBOW mod els which learn word embeddings by making predictions in local context windows.', 'However many of the details of word2vec will hold true in methods that we ll proceed to further in the course so we ll focus our time on that. 3.2 Word2vec model and objective The word2vec model represents each word in a fixed vocabulary as a low dimensional much smaller than vocabulary size vector.', 'Here we see that the word vectors are classiÔ¨Åed correctly on some extrinsic classiÔ¨Åcation task. Now if we retrain only two of those vectors because of a limited training set size then we see in Figure 7 that one of the words gets misclassiÔ¨Åed because the boundary shifts as a result of word vector updates. Thus word vectors should not be retrained if the training data set is small.', 'file name: lecture01 wordvecs1. Hamidreza Mahyar mahyarh@mcmaster.ca Computational Natural Language Processing Overview of NLP and Word VectorsLecture Plan 2 Lecture 1 Introduction and Word Vectors 1. The course 2. Human language and word meaning 3. Word2vec introduction 4. Word2vec objective function gradients 5. Optimization basics 6.', 'vectors Idea We have a large corpus body of text a long list of words Every word in a fixed vocabulary is represented by a vector Go through each position t in the text which has a center word c and context outside words o Use the similarity of the word vectors for c and o to calculate the probability of o given c or vice versa Keep adjusting the word vectors to maximize this probabilityWord2Vec Overview Example windows and process for computing']",text-embedding-3-small,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.8875024758015191,0.3333333333333333,0.9332630159984052,0.0555555555555555
209,How does Word2vec represent word meaning?,"Word2vec represents word meaning by training word vectors to predict the surrounding words in a sentence, which allows the vectors to capture the distribution of the context in which a word appears. These word vectors are related logarithmically to the probabilities computed by the output layer, and the sum of two word vectors is related to the product of the two context distributions.","['The word2vec model is a probabilistic model specified as follows where uw refers to the row of U corresponding to word w V and likewise for V pU V o c exp u o vc w V exp u wvc 4 This may be familiar to you as the softmax function which takes arbitrary scores here one for each word in the vocabulary resulting from dot products and produces a probability distribution where larger scored things get higher probability.', 'As the word vectors are trained to predict the surrounding words in the sentence the vectors can be seen as representing the distribution of the context in which a word appears. These values are related logarithmically to the probabilities computed by the output layer so the sum of two word vectors is related to the product of the two context distributions.', '718 0.147 0.051 19Word meaning as a neural word vector visualization 0.286 0.792 0.177 0.107 0.109 0.542 0.349 0.271 0.487 expect 203. Word2vec Overview 21 Word2vec Mikolov et al. 2013 is a framework for learning word vectors Idea We have a large corpus body of text', 'However many of the details of word2vec will hold true in methods that we ll proceed to further in the course so we ll focus our time on that. 3.2 Word2vec model and objective The word2vec model represents each word in a fixed vocabulary as a low dimensional much smaller than vocabulary size vector.', 'It consistently outperforms word2vec on the word analogy task given the same corpus vocabulary window size and training time. It achieves better results faster and also obtains the best results irrespective of speed. 2 Evaluation of Word Vectors So far we have discussed methods such as the Word2Vec and GloVe methods to train and discover latent vector representations of natural language words in a semantic space.', '3.3 Estimating a word2vec model from a corpus How do we train word2vec in practice Specifying the word2vec model is relatively transparent from the math we ve given above one constructs matrices U and V and can write out the math of the probability.', 'The word s meaning is distributed across the whole vector for this reason these vectors are sometimes called distributed representations.5 As corpora grew scalability became a challenge because the number of observable contexts grew as well.', 'This evaluation scheme favors models that produce dimensions of mean ing thereby capturing the multi clustering idea of distributed representations Bengio 2009 . The two main model families for learning word vectors are 1 global matrix factorization meth ods such as latent semantic analysis LSA Deer wester et al. 1990 and 2 local context window methods such as the skip gram model of Mikolov et al. 2013c .']",text-embedding-3-small,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999999999999986,1.0,0.9254202799938088,0.0666666666666666
210,What is the main focus of this lecture on neural networks?,"The main focus of this lecture on neural networks is on understanding the core technology of neural nets, specifically backpropagation, which involves recursively and efficiently applying the chain rule along the computation graph to compute gradients for training neural networks. Additionally, the lecture emphasizes the importance of non-linearities in neural networks for function approximation, regression, and classification tasks. Lastly, the lecture touches upon the use of recurrent neural networks as a language model.","['for checking your implementation In the old days we hand wrote everything doing this everywhere was the key test Now much less needed you can use it to check layers are correctly implemented 78Summary We ve mastered the core technology of neural nets Backpropagation recursively and hence efficiently apply the chain rule along computation graph downstream gradient upstream gradient x local gradient Forward pass compute results of operations and save intermediate values Backward pass apply chain rule to compute gradients 79Why learn all these details', 'In the next set of notes we study neural networks as a class of non linear models that have performed particularly well in deep learning applications. Figure 10 Here we see that the non linear decision boundary allows for much better classiÔ¨Åcation of datapoints.', 'Neural networks are a family of classiÔ¨Åers with non linear decision boundary as seen in Figure 1. Now that we know the sort of decision boundaries neural networks create let us see how they manage doing so. 1.1 A Neuron A neuron is a generic computational unit that takes n inputs and produces a single output. What differentiates the outputs of different neurons is their parameters also referred to as their weights .', 'Lastly we will motivate the use of recurrent neural networks as a language model. 1 Neural Networks Foundations Figure 1 We see here how a non linear decision boundary separates the data very well. This is the prowess of neural networks. Fun Fact Neural networks are biologically in spired classiÔ¨Åers which is why they are often called artiÔ¨Åcial neural networks to distinguish them from the organic kind.', 'However in reality human neural networks are so much more capable and complex from artiÔ¨Åcial neural net works that it is usually better to not draw too many parallels between the two. We established in our previous discussions the need for non linear classiÔ¨Åers since most data are not linearly separable and thus our classiÔ¨Åcation performance on them is limited.', 'logistic 1.702ùë•Non linearities i.e. f on previous slide Why they re needed Neural networks do function approximation e.g. regression or classification Without non linearities deep neural networks can t do anything more than a linear transform Extra layers could just be compiled down into a single linear transform W1 W2 x Wx But with more layers that include non linearities they can approximate any complex function 5Training with cross', 'Figure 1 The Ô¨Årst deep neural network architecture model for NLP presented by Bengio et al. Figure 2 A simpliÔ¨Åed representation of Figure 1.', 'A new family of neural networks Recurrent Neural Networks RNNs This is the most important concept in the class It leads to GPT 3 and ChatGPT Important and used in Ass1 but not the only way to build LMs0 model power Training error Test error Classic view Regularization works to prevent overfitting when we have a lot of features or later a very powerful deep model etc.']",text-embedding-3-small,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9980300028805336,1.0,0.9303458682219792,0.0588235294117647
211,What are the non-linear functions mentioned for neural networks?,"The non-linear functions mentioned for neural networks are sigmoid, tanh, and ReLU. Sigmoid and tanh are still used, for example, logistic to get a probability. However, for deep networks, the first thing to try is ReLU as it trains quickly and performs well due to good gradient backflow.","['logistic 1.702ùë•Non linearities i.e. f on previous slide Why they re needed Neural networks do function approximation e.g. regression or classification Without non linearities deep neural networks can t do anything more than a linear transform Extra layers could just be compiled down into a single linear transform W1 W2 x Wx But with more layers that include non linearities they can approximate any complex function 5Training with cross', 'We will see that a neuron can be one of many functions that allows for non linearities to accrue in the network. lently formulate a 1 1 exp wT b x 1 Figure 2 This image captures how in a sigmoid neuron the input vector x is Ô¨Årst scaled summed added to a bias unit and then passed to the squashing sigmoid function.', 'In the next set of notes we study neural networks as a class of non linear models that have performed particularly well in deep learning applications. Figure 10 Here we see that the non linear decision boundary allows for much better classiÔ¨Åcation of datapoints.', 'Neural networks are a family of classiÔ¨Åers with non linear decision boundary as seen in Figure 1. Now that we know the sort of decision boundaries neural networks create let us see how they manage doing so. 1.1 A Neuron A neuron is a generic computational unit that takes n inputs and produces a single output. What differentiates the outputs of different neurons is their parameters also referred to as their weights .', 'We now introduce the need for non linear classiÔ¨Åcation models such as neural networks. We see in Figure 9 that a linear classiÔ¨Åer mis classiÔ¨Åes many datapoints. Using a non linear decision boundary as shown in Figure 10 we manage to classify all training points accu rately. Although oversimpliÔ¨Åed this is a classic case demonstrating the need for non linear decision boundaries.', 'Thus we must typically divide the outputs of each neuron during testing by a certain value it is left as an exercise to the reader to determine what this value should be in order for the expected outputs during training and testing to be equivalent. 2.4 Neuron Units So far we have discussed neural networks that contain sigmoidal neurons to introduce nonlinearities however in many applications better networks can be designed using other activation functions.', 'However in reality human neural networks are so much more capable and complex from artiÔ¨Åcial neural net works that it is usually better to not draw too many parallels between the two. We established in our previous discussions the need for non linear classiÔ¨Åers since most data are not linearly separable and thus our classiÔ¨Åcation performance on them is limited.', 'and shifted sigmoid 2 as steep 1 1 tanh z 2logistic 2z 1 Logistic and tanh are still used e.g. logistic to get a probability However now for deep networks the first thing to try is ReLU it trains quickly and performs well due to good gradient backflow.']",text-embedding-3-small,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9902575281246188,1.0,0.9318978849463933,0.0555555555555555
212,What is the purpose of non-linearities in neural networks?,"The purpose of non-linearities in neural networks is to allow for function approximation, such as regression or classification. Without non-linearities, deep neural networks can only perform a linear transform, limiting their capabilities. By including non-linearities in more layers, neural networks can approximate complex functions effectively.","['logistic 1.702ùë•Non linearities i.e. f on previous slide Why they re needed Neural networks do function approximation e.g. regression or classification Without non linearities deep neural networks can t do anything more than a linear transform Extra layers could just be compiled down into a single linear transform W1 W2 x Wx But with more layers that include non linearities they can approximate any complex function 5Training with cross', 'However in reality human neural networks are so much more capable and complex from artiÔ¨Åcial neural net works that it is usually better to not draw too many parallels between the two. We established in our previous discussions the need for non linear classiÔ¨Åers since most data are not linearly separable and thus our classiÔ¨Åcation performance on them is limited.', 'We will see that a neuron can be one of many functions that allows for non linearities to accrue in the network. lently formulate a 1 1 exp wT b x 1 Figure 2 This image captures how in a sigmoid neuron the input vector x is Ô¨Årst scaled summed added to a bias unit and then passed to the squashing sigmoid function.', 'Neural networks are a family of classiÔ¨Åers with non linear decision boundary as seen in Figure 1. Now that we know the sort of decision boundaries neural networks create let us see how they manage doing so. 1.1 A Neuron A neuron is a generic computational unit that takes n inputs and produces a single output. What differentiates the outputs of different neurons is their parameters also referred to as their weights .', 'We now introduce the need for non linear classiÔ¨Åcation models such as neural networks. We see in Figure 9 that a linear classiÔ¨Åer mis classiÔ¨Åes many datapoints. Using a non linear decision boundary as shown in Figure 10 we manage to classify all training points accu rately. Although oversimpliÔ¨Åed this is a classic case demonstrating the need for non linear decision boundaries.', 'and shifted sigmoid 2 as steep 1 1 tanh z 2logistic 2z 1 Logistic and tanh are still used e.g. logistic to get a probability However now for deep networks the first thing to try is ReLU it trains quickly and performs well due to good gradient backflow.', 'Pre sumably this method works because in high dimen sional spaces there is a high probability for long term components to be orthogonal to short term ones. This would allow the Hessian to rescale these components independently. In practice one can not guarantee that this property holds. As discussed in section 2.3 this method is able to deal with the exploding gradient as well.', 'Thus we must typically divide the outputs of each neuron during testing by a certain value it is left as an exercise to the reader to determine what this value should be in order for the expected outputs during training and testing to be equivalent. 2.4 Neuron Units So far we have discussed neural networks that contain sigmoidal neurons to introduce nonlinearities however in many applications better networks can be designed using other activation functions.']",text-embedding-3-small,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.999997284737303,1.0,0.9208509764316138,0.05
213,What is cross entropy loss used for in PyTorch?,"Cross entropy loss is used in PyTorch to minimize the negative log probability of the correct class, which is equivalent to maximizing the probability of the correct class. It is a concept from information theory where the true probability distribution is denoted as p and the computed model probability is denoted as q. The cross entropy is calculated based on these distributions.

Source: lecture02_Neural_Nets.txt","['can approximate any complex function 5Training with cross entropy loss you use this in PyTorch Until now our objective was stated as to maximize the probability of the correct class y or equivalently we can minimize the negative log probability of that class Now restated in terms of cross entropy a concept from information theory Let the true probability distribution be p let our computed model probability be q The cross entropy is Assuming a ground truth or true or gold or target probability distribution that', 'What makes the cross entropy useful is that the cross entropy H p m is an up per bound on the entropy H p .', 'Using the Cross entropy loss function we calculate the loss of such a training example as C j 1 yj log p yj 1 x C j 1 yj log exp Wj x C c 1 exp Wc x Of course the above summation will be a sum over C 1 zero values since yj is 1 only at a single index at least for now implying that x belongs to only 1 correct class.', 'Between two models m1 and m2 the more accurate model will be the one with the lower cross entropy. The cross entropy can never be lower than the true entropy so a model cannot err by underestimating the true entropy. We are Ô¨Ånally ready to see the relation between perplexity and cross entropy as we saw it in Eq. 3.51.', '7 Cross entropy loss with respect to logits ÀÜ y softmax Œ∏ J CE y ÀÜ y what is J Œ∏ The gradient is J Œ∏ ÀÜ y y or ÀÜ y y T if y is a column vector . 4These identities will be enough to let you quickly compute the gradients for many neural networks. However it s important to know how to compute Jacobians for other functions as well in case they show up.', 'After extending this trend by two more orders of magnitude we observe only a slight if any departure from the power law. One might worry that these improvements in cross entropy loss come only from modeling spurious details of our training corpus. However we will see in the following sections that improvements in cross entropy loss lead to consistent performance gains across a broad spectrum of natural language tasks.', '3.51. Cross entropy is deÔ¨Åned in the limit as the length of the observed word sequence goes to inÔ¨Ånity. We will need an approximation to cross entropy relying on a sufÔ¨Åciently long sequence of Ô¨Åxed length.', 'Performance measured in terms of cross entropy validation loss follows a power law trend with the amount of compute used for training. The power law behavior observed in KMH 20 continues for an additional two orders of magnitude with only small deviations from the predicted curve. For this Ô¨Ågure we exclude embedding parameters from compute and parameter counts.']",text-embedding-3-small,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.96742580703847,0.8,0.9525411525120326,0.0526315789473684
214,What is the significance of the backpropagation algorithm?,"The significance of the backpropagation algorithm lies in its ability to efficiently calculate error gradients for updating parameters in a neural network model through the chain rule of differentiation. This technique allows for the training of neural networks by propagating errors backward from the output layer to the input layer, enabling the model to learn from the data, reduce overfitting, and improve overall performance on tasks. Additionally, backpropagation enables the training of exponentially many smaller networks at once through techniques like dropout, leading to more robust and effective learning outcomes.","['. . 1 . Such a recursive procedure is what makes backpropa gation a computationally affordable procedure. 2 Neural Networks Tips and Tricks Having discussed the mathematical foundations of neural networks we will now dive into some tips and tricks commonly employed when using neural networks in practice. 2.1 Gradient Check In the last section we discussed in detail how to calculate error gradients updates for parameters in a neural network model via calculus based analytic methods.', 'Image credits to Srivastava et al. In practice the way we introduce dropout is that we take the out put h of each layer of neurons and keep each neuron with prob ability p and else set it to 0. Then during back propagation we only pass gradients through neurons that were kept alive during the forward pass. Finally during testing we compute the forward pass using all of the neurons in the network.', 'The result is that the network typically learns more meaningful informa tion from the data is less likely to overÔ¨Åt and usually obtains higher performance overall on the task at hand. One intuitive reason why this technique should be so effective is that what dropout is doing is essentially doing is training exponentially many smaller networks at once and averaging over their predictions. Dropout applied to an artiÔ¨Åcial neural network.', 'i f z 2 i a 1 j Œ¥ 2 i a 1 j We see above that the gradient reduces to the product Œ¥ 2 i a 1 j where Œ¥ 2 i is essentially the error propagating backwards from the i th neu ron in layer 2. a 1 j is an input fed to i th neuron in layer 2 when scaled by Wij.', 'Since we typically update parameters using gradient descent or a variant such as SGD we typically need the gradient information for any parameter as required in the update equation Œ∏ t 1 Œ∏ t Œ± Œ∏ t J Backpropagation is technique that allows us to use the chain rule of differentiation to calculate loss gradients for any parameter used in the feed forward computation on the model.', 'We then dis cuss how they can be trained using a distributed gradient descent technique known as backpropagation. We will see how the chain rule can be used to make parameter updates sequentially. After a rigorous mathematical discussion of neural networks we will discuss some practical tips and tricks in training neural networks involving neuron units non linearities gradient checks Xavier parameter ini tialization learning rates Adagrad etc.', 'Thus we should use vectorized implementation in prac tice. Furthermore we should also reduce redundant calculations in backpropagation for instance notice that Œ¥ k depends directly on Œ¥ k 1 . Thus we should ensure that when we update W k using Œ¥ k 1 we save Œ¥ k 1 to later derive Œ¥ k and we then repeat this for k 1 . . . 1 .', 'W k is the transfer matrix that maps the output from the k th layer to the input to the k 1 th Thus W 1 W and W 2 U to put this new generalized notation in perspective of Section 1.3. Backpropagation Notation xi is an input to the neural network. s is the output of the neural net work.']",text-embedding-3-small,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.000000000000001,0.8,0.9419325507216396,0.0416666666666666
215,What does the Jacobian matrix represent in the context of neural networks?,"In the context of neural networks, the Jacobian matrix represents a matrix of partial derivatives. It is used to compute how small changes in input variables affect the outputs of the network, helping in understanding the sensitivity of the network to changes in its parameters or inputs. The Jacobian matrix is crucial for tasks like gradient computation during training and optimization processes in neural networks. 

Source: Lecture02_Neural_Nets","['activation Function 17Example Jacobian Elementwise activation Function 18Other Jacobians Compute these at home for practice Check your answers with the lecture notes 19Other Jacobians Compute these at home for practice Check your answers with the lecture notes 20Other Jacobians Compute these at home for practice Check your answers with the lecture notes Fine print This is the correct Jacobian. Later we discuss the shape convention using it the answer would be h.', 'Equation 5 also provides the form of Jacobian matrix xi xi 1 for the speciÔ¨Åc parametrization given in equation 11 where diag converts a vector into a diagonal matrix and œÉ computes the derivative of œÉ in an element wise fash ion.', 'm outputs and n inputs It s Jacobian is an m x n matrix of partial derivatives 12Chain Rule For composition of one variable functions multiply derivatives For multiple variables functions multiply Jacobians 13Example Jacobian Elementwise activation Function 14Example Jacobian Elementwise activation Function Function has n outputs and n inputs n by n Jacobian 15Example Jacobian Elementwise activation Function 16Example Jacobian Elementwise activation Function 17Example Jacobian Elementwise', 'So z x ij zi xj xj xi 1 if i j 0 if otherwise So we see that the Jacobian z x is a diagonal matrix where the entry at i i is 1. This is just the identity matrix z x I . When applying the chain 2rule this term will disappear because a matrix or vector multiplied by the identity matrix does not change.', 'Write out the Jacobians ùíñùëá Useful Jacobians from previous slide 323. Write out the Jacobians ùíñùëá Useful Jacobians from previous slide 333. Write out the Jacobians ùíñùëá ùíñùëá Useful Jacobians from previous slide . 34 Hadamard product element wise multiplication of 2 vectors to give vectorRe using Computation Suppose we now want to compute', 'So its Jacobian will be n m. Note that zi m X k 1 Wikxk So an entry z x ij of the Jacobian will be z x ij zi xj xj m X k 1 Wikxk m X k 1 Wik xj xk Wij because xj xk 1 if k j and 0 if otherwise.', 'Therefore if you compute the gradient of a column vector using Jacobian formulation you should take the transpose when reporting your Ô¨Ånal answer so the gradient is a column vector. Another option is to always follow the convention. In this case the identities may not work but you can still Ô¨Ågure out the answer by making sure the dimensions of your derivatives match up.', 'fm x1 . . . fm xn That is f x ij fi xj which is just a standard non vector derivative . The Jacobian matrix will be useful for us because we can apply the chain rule to a vector valued function just by multiplying Jacobians.']",text-embedding-3-small,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9855696680554974,1.0,0.9313581734779144,0.0555555555555555
216,How is the derivative with respect to a matrix described?,"The derivative with respect to a matrix is described using the Jacobian matrix, which is useful for applying the chain rule to a vector-valued function by multiplying Jacobians. It is also mentioned that having the derivatives in an n x m matrix allows for easier computation during gradient descent by subtracting the matrix multiplied by the learning rate from the original matrix. Additionally, the derivative with respect to a matrix involves considering the Jacobian matrix and the shape of the gradient being the same as the parameters. The Jacobian matrix helps in determining the direction to locally move the matrix in order to maximize the value of the function. Furthermore, the Jacobian matrix can be represented as a diagonal matrix where the entry at i, i is 1, which simplifies calculations when applying the chain rule.

Source: gradient_notes.txt, lecture01_notes_Introduction.txt, lecture02_Neural_Nets.txt","['fm x1 . . . fm xn That is f x ij fi xj which is just a standard non vector derivative . The Jacobian matrix will be useful for us because we can apply the chain rule to a vector valued function just by multiplying Jacobians.', 'It would be much nicer if the derivatives were in a n m matrix like this J W J W11 . . . J W1m . . . ... . . . J Wn1 . . . J Wnm Since this matrix has the same shape as W we could just subtract it times the learning rate from W when doing gradient descent.', 'Since multiplication by a diagonal matrix is the same as doing elementwise multiplication by the diagonal we could also write f x when applying the chain rule. 5 Matrix times column vector with respect to the matrix z W x Œ¥ J z what is J W J z z W Œ¥ z W This is a bit more complicated than the other identities. The reason for in cluding J z in the above problem formulation will become clear in a moment.', 'Therefore if you compute the gradient of a column vector using Jacobian formulation you should take the transpose when reporting your Ô¨Ånal answer so the gradient is a column vector. Another option is to always follow the convention. In this case the identities may not work but you can still Ô¨Ågure out the answer by making sure the dimensions of your derivatives match up.', 'look like 1 output nm inputs 1 by nm Jacobian Inconvenient to then do 38Derivative with respect to Matrix Output shape What does look like 1 output nm inputs 1 by nm Jacobian Inconvenient to then do Instead we leave pure math and use the shape convention the shape of the gradient is the shape of the parameters So is n by m 39Derivative with respect to Matrix What is is going to be in our', 'Briefly the gradient think derivative U f of a scalar function f with respect to a parameter matrix U represents the direction to locally move U in in order to maximally increase the value of f. So in practice we do something like drawing the initial U 0 and V 0 randomly as U V N 0 0.', 'First suppose we have a loss function J a scalar and are computing its gradient with respect to a matrix W Rn m. Then we could think of J as a function of W taking nm inputs the entries of W to a single output J . This means the Jacobian J W would be a 1 nm vector. But in practice this is not a very useful way of arranging the gradient.', 'So z x ij zi xj xj xi 1 if i j 0 if otherwise So we see that the Jacobian z x is a diagonal matrix where the entry at i i is 1. This is just the identity matrix z x I . When applying the chain 2rule this term will disappear because a matrix or vector multiplied by the identity matrix does not change.']",text-embedding-3-small,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9302511168041164,1.0,0.8984343860477051,0.0555555555555555
217,What role does automatic differentiation play in modern deep learning frameworks?,"Automatic Differentiation plays a crucial role in modern deep learning frameworks as it allows for the automatic computation of gradients during the backpropagation process. This means that instead of manually deriving and implementing the gradients for each operation in a neural network, automatic differentiation handles this process efficiently, saving time and reducing the chances of errors in gradient computations. It enables the frameworks like TensorFlow and PyTorch to automatically infer and compute the gradients needed for training neural networks effectively.

Source: lecture02_Neural_Nets.txt","['is the same In general our nets have regular layer structure and so we can use matrices and Jacobians Back Prop in General Computation Graph Inputs Single scalar output 73Automatic Differentiation The gradient computation can be automatically inferred from the symbolic expression of the fprop Each node type needs to know how to compute its output and how to compute the gradient wrt its inputs given the gradient wrt its output Modern DL frameworks Tensorflow PyTorch etc. do', 'Given derivatives with respect to its outputs each module can independently compute derivatives with respect to its inputs and with respect to its trainable parameters as proposed by Bottou and Gallinari 1991 . This allows us to easily build variants of our networks. For details about gradient computations see Appendix A. Remark 7 Tricks Many tricks have been reported for training neural networks LeCun et al. 1998 .', 'to compute gradients 79Why learn all these details about gradients 80 Modern deep learning frameworks compute gradients for you But why take a class on compilers or systems when they are implemented for you Understanding what is going on under the hood is useful Backpropagation doesn t always work perfectly out of the box Understanding why is crucial for debugging and improving models See Karpathy article https medium.', 'frameworks Tensorflow PyTorch etc. do backpropagation for you but mainly leave layer node writer to hand calculate the local derivative 74Backprop Implementations 75Implementation forward backward API 76Implementation forward backward API 77Manual Gradient checking Numeric Gradient For small h 1e 4 Easy to implement correctly But approximate and very slow You have to recompute f for every parameter of our model Useful for checking your implementation In the old days we hand', 'Remark 5 Differentiability Our cost functions are differentiable almost everywhere. Non differentiable points arise because we use a hard transfer function 5 and because we use a max layer 7 in the sentence approach network. Fortunately stochastic gradient still converges to a meaningful local minimum despite such minor differentiability problems Bottou 1991 1998 . Stochastic gradient iterations that hit a non differentiability are simply skipped.', 'This makes the math a bit cumbersome. Even worse if we re implementing the model without automatic diÔ¨Äerentiation computing J Œ∏ twice will be ineÔ¨Écient. So it will help us to deÔ¨Åne some variables to represent the intermediate derivatives Œ¥1 J Œ∏ Œ¥2 J z These can be thought as the error signals passed down to Œ∏ and z when doing backpropagation. We can compute them as follows Œ¥1 J', '1 Introduction Deep neural networks trained with some version of Stochastic Gradient Descent have been shown to substantially outperform previous approaches on various supervised learning tasks in computer vision Krizhevsky et al. 2012 and speech processing Hinton et al. 2012 . But state of the art deep neural networks often require many days of training.', 'The algorithm can also be thought of as adapting the learning rate based on the norm of the gradient. Compared to other learning rate adaptation strate gies which focus on improving convergence by col lecting statistics on the gradient as for example inOn the diÔ¨Éculty of training Recurrent Neural Networks Duchi et al.']",text-embedding-3-small,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.99386413042925,1.0,0.9175494915196184,0.0555555555555555
218,What is the purpose of manual gradient checking?,"The purpose of manual gradient checking is to compare analytical gradients (computed using mathematical formulas) with numerical gradients (computed using finite differences) to ensure that the gradients are calculated correctly and the model is learning effectively. This process helps in verifying the correctness of the gradient computation implementation and detecting any errors that could lead to suboptimal performance or incorrect updates to the model parameters. It is a crucial step in debugging and validating the training process of machine learning models. 

Source: lecture02_notes_Backpropagation.txt","['4 Gradient Layout Jacobean formulation is great for applying the chain rule you just have to mul tiply the Jacobians. However when doing SGD it s more convenient to follow the convention the shape of the gradient equals the shape of the parameter as we did when computing J W . That way subtracting the gradient times the learning rate from the parameters is easy. We expect answers to homework questions to follow this convention.', 'is by looking at the dimensions of the Jaco bians J z Œ¥1 U sgn h 1 Dh 1 Nc Nc Dh Dh 6We see that the dimensions of all the terms in the gradient match up i.e. the number of columns in a term equals the number of rows in the next term . This will always be the case if we computed our gradients correctly. Now we can use the error terms to compute our gradients.', 'a single element in a weight matrix in practice this tends to be quite slow. Instead it is more eÔ¨Écient to keep everything in ma trix vector form. The basic building block of vectorized gradients is the Jacobian Matrix.', 'Therefore if you compute the gradient of a column vector using Jacobian formulation you should take the transpose when reporting your Ô¨Ånal answer so the gradient is a column vector. Another option is to always follow the convention. In this case the identities may not work but you can still Ô¨Ågure out the answer by making sure the dimensions of your derivatives match up.', 'an earlier checkpoint 58Gradient clipping solution for exploding gradient 58 Gradient clipping if the norm of the gradient is greater than some threshold scale it down before applying SGD update Intuition take a step in the same direction but a smaller step In practice remembering to clip gradients is important but exploding gradients are an easy problem to solve Source On the difficulty of training recurrent neural networks Pascanu et al 2013. http proceedings.mlr.', 'This is why just clipping the gradient might be suÔ¨Écient not requiring the use a second order method. Note that this algo rithm should work even when the rate of growth of the gradient is not the same as the one of the curvature a case for which a second order method would fail as the ratio between the gradient and curvature could still explode .', 'We note that this deÔ¨Ånition of the numerical gradient follows very naturally from the deÔ¨Ånition of the derivative where in the scalar case f x f x œµ f x œµ Gradient checks are a great way to compare analytical and numerical gradients.', 'The key insight is that all the steps taken when the gradient explodes are aligned with v and ignore other descent direction i.e. the model moves perpendicular to the wall . At the wall a small norm step in the direction of the gradient there fore merely pushes us back inside the smoother low curvature region besides the wall whereas a regular gradient step would bring us very far thus slowing or preventing further training.']",text-embedding-3-small,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999999999999996,0.0,0.9291082943243076,0.05
219,Why is understanding gradients important despite automatic differentiation?,Understanding gradients is important despite automatic differentiation because backpropagation doesn't always work perfectly out of the box. Knowing the details about gradients can be crucial for debugging and improving models. It helps in understanding why certain issues may arise and how to address them effectively. Source: Lecture02_Neural_Nets.txt,"['to compute gradients 79Why learn all these details about gradients 80 Modern deep learning frameworks compute gradients for you But why take a class on compilers or systems when they are implemented for you Understanding what is going on under the hood is useful Backpropagation doesn t always work perfectly out of the box Understanding why is crucial for debugging and improving models See Karpathy article https medium.', 'This makes the math a bit cumbersome. Even worse if we re implementing the model without automatic diÔ¨Äerentiation computing J Œ∏ twice will be ineÔ¨Écient. So it will help us to deÔ¨Åne some variables to represent the intermediate derivatives Œ¥1 J Œ∏ Œ¥2 J z These can be thought as the error signals passed down to Œ∏ and z when doing backpropagation. We can compute them as follows Œ¥1 J', 'We note that this deÔ¨Ånition of the numerical gradient follows very naturally from the deÔ¨Ånition of the derivative where in the scalar case f x f x œµ f x œµ Gradient checks are a great way to compare analytical and numerical gradients.', '4 Gradient Layout Jacobean formulation is great for applying the chain rule you just have to mul tiply the Jacobians. However when doing SGD it s more convenient to follow the convention the shape of the gradient equals the shape of the parameter as we did when computing J W . That way subtracting the gradient times the learning rate from the parameters is easy. We expect answers to homework questions to follow this convention.', 'Given derivatives with respect to its outputs each module can independently compute derivatives with respect to its inputs and with respect to its trainable parameters as proposed by Bottou and Gallinari 1991 . This allows us to easily build variants of our networks. For details about gradient computations see Appendix A. Remark 7 Tricks Many tricks have been reported for training neural networks LeCun et al. 1998 .', 'is the same In general our nets have regular layer structure and so we can use matrices and Jacobians Back Prop in General Computation Graph Inputs Single scalar output 73Automatic Differentiation The gradient computation can be automatically inferred from the symbolic expression of the fprop Each node type needs to know how to compute its output and how to compute the gradient wrt its inputs given the gradient wrt its output Modern DL frameworks Tensorflow PyTorch etc. do', 'non vectorized gradients But doing a non vectorized gradient can be good for intuition recall the first lecture for an exampleGradients Given a function with 1 output and 1 input ùëìùë• ùë•3 It s gradient slope is its derivative ùëëf 3ùë•2 10 ùëëùë• How much will the output change if we change the input a bit At x 1 it changes about', 'Therefore if you compute the gradient of a column vector using Jacobian formulation you should take the transpose when reporting your Ô¨Ånal answer so the gradient is a column vector. Another option is to always follow the convention. In this case the identities may not work but you can still Ô¨Ågure out the answer by making sure the dimensions of your derivatives match up.']",text-embedding-3-small,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999999999999994,1.0,0.9467196630238972,0.0555555555555555
220,What is the core concept introduced in this class that leads to GPT-3 and ChatGPT?,"The core concept introduced in this class that leads to GPT-3 and ChatGPT is the use of large-scale language models based on transformer architectures, which have the ability to generate human-like text and perform various natural language processing tasks with high proficiency. These models leverage self-attention mechanisms to capture long-range dependencies in text data, enabling them to understand and generate coherent language patterns effectively. Additionally, fine-tuning these pre-trained models on specific tasks allows for improved performance on a wide range of natural language understanding and generation tasks.","['Finally given the broad spectrum of capabilities displayed by GPT 3 we discuss concerns about bias fairness and broader societal impacts and attempt a preliminary analysis of GPT 3 s characteristics in this regard. The remainder of this paper is organized as follows. In Section 2 we describe our approach and methods for training GPT 3 and evaluating it.', 'The ability of GPT 3 to generate several paragraphs of synthetic content that people Ô¨Ånd difÔ¨Åcult to distinguish from human written text in 3.9.4 represents a concerning milestone in this regard.', 'First despite the strong quantitative and qualitative improvements of GPT 3 particularly compared to its direct predecessor GPT 2 it still has notable weaknesses in text synthesis and several NLP tasks. On text synthesis although the overall quality is high GPT 3 samples still sometimes repeat themselves semantically at the document level start to lose coherence over sufÔ¨Åciently long passages contradict themselves and occasionally contain non sequitur sentences or paragraphs.', 'However it is very likely that incorporating these algorithmic advances could improve GPT 3 s performance on downstream tasks especially in the Ô¨Åne tuning setting and combining GPT 3 s scale with these algorithmic techniques is a promising direction for future work.', 'In general we observe GPT 3 is on par with initial baselines and early results trained using contextual representations on each respective dataset.', 'By presenting a broad characterization of GPT 3 s strengths and weaknesses including these limitations we hope to stimulate study of few shot learning in language models and draw attention to where progress is most needed. A heuristic sense of the overall results can be seen in Figure 1.3 which aggregates the various tasks though it should not be seen as a rigorous or meaningful benchmark in itself .', 'We discuss broader societal impacts of this Ô¨Ånding and of GPT 3 in general. Equal contribution Johns Hopkins University OpenAI Author contributions listed at end of paper. arXiv 2005.14165v4 cs.CL 22 Jul 2020Contents 1 Introduction 3 2 Approach 6 2.1 Model and Architectures . . . . . .', 'First we test GPT 3 s ability to perform arithmetic. Second we create several tasks that involve rearranging or unscrambling the letters in a word tasks which are unlikely to have been exactly seen during training. Third we test GPT 3 s ability to solve SAT style analogy problems few shot.']",text-embedding-3-small,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9982911866415304,0.6,0.9056503964045564,0.0588235294117647
221,What is the purpose of Dropout as mentioned in the document?,"The purpose of Dropout, as mentioned in the document, is to act as a powerful technique for regularization by randomly dropping neurons in the forward pass, which helps prevent neural networks from overfitting and typically leads to learning more meaningful information from the data, resulting in higher performance on the task at hand. Dropout essentially trains exponentially many smaller networks at once and averages over their predictions, contributing to its effectiveness in improving network performance. Additionally, during testing, all neurons in the network are used to compute predictions to ensure the expected output of a neuron remains consistent with training.","['In the next section we discuss dropout which effectively acts as another form of regularization by randomly dropping i.e. setting to zero neurons in the forward pass. 2.3 Dropout Dropout is a powerful technique for regularization Ô¨Årst introduced by Srivastava et al. in Dropout A Simple Way to Prevent Neural Net works from OverÔ¨Åtting.', 'The result is that the network typically learns more meaningful informa tion from the data is less likely to overÔ¨Åt and usually obtains higher performance overall on the task at hand. One intuitive reason why this technique should be so effective is that what dropout is doing is essentially doing is training exponentially many smaller networks at once and averaging over their predictions. Dropout applied to an artiÔ¨Åcial neural network.', 'Image credits to Srivastava et al. In practice the way we introduce dropout is that we take the out put h of each layer of neurons and keep each neuron with prob ability p and else set it to 0. Then during back propagation we only pass gradients through neurons that were kept alive during the forward pass. Finally during testing we compute the forward pass using all of the neurons in the network.', 'However a key sub tlety is that in order for dropout to work effectively the expected output of a neuron during testing should be approximately the same as it was during training else the magnitude of the outputs could be radically different and the behavior of the network is no longer well deÔ¨Åned.', 'Dropout a simple way to prevent neural networks from overÔ¨Åtting. JMLR 2014. Rupesh Kumar Srivastava Klaus Greff and J urgen Schmidhuber. Highway networks. arXiv preprint arXiv 1505.00387 2015.', '7 1019 1.2 1021 Transformer base model 27.3 38.1 3.3 1018 Transformer big 28.4 41.8 2.3 1019 Residual Dropout We apply dropout 33 to the output of each sub layer before it is added to the sub layer input and normalized.', 'form of model bagging i.e. like an ensemble model Nowadays usually thought of as strong feature dependent regularizer Wager Wang Liang 2013DropoutDropout Srivastava Hinton Krizhevsky Sutskever Salakhutdinov 2012 JMLR 2014 During training For each data point each time Randomly set input to 0 with probability ùëù dropout ratio often p 0.', 'The idea is simple yet effective during train ing we will randomly drop with some probability 1 p a subset of neurons during each forward backward pass or equivalently we will keep alive each neuron with a probability p . Then during testing we will use the full network to compute our predictions.']",text-embedding-3-small,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9653191454129152,1.0,0.926092050698141,0.0526315789473684
222,What are some approaches to address the vanishing gradient problem in RNNs?,"Some approaches to address the vanishing gradient problem in RNNs include using a regularization term that represents a preference for parameter values such that backpropagated gradients neither increase nor decrease too much in magnitude, initializing the weight matrix W_hh from an identity matrix instead of randomly, and using Rectified Linear Units (ReLU) instead of the sigmoid function. These techniques help mitigate the vanishing gradient issue and improve the learning quality of the model over many timesteps.","['2013. http proceedings.mlr.press v28 pascanu13.pdfHow to fix the vanishing gradient problem The main problem is that it s too difficult for the RNN to learn to preserve information over many timesteps.', 'with RNNs Vanishing and Exploding Gradients 48Vanishing gradient intuition 49Vanishing gradient intuition chain rule 50Vanishing gradient intuition chain rule 51Vanishing gradient intuition chain rule 52Vanishing gradient intuition What happens if these are small Vanishing gradient problem When these are small the gradient signal gets smaller and smaller as it backpropagates further 53Vanishing gradient proof sketch linear case Recall What if were the identity', 'To gain practical intuition about the vanishing gradient problem you may visit the following example website. 2.4 Solution to the Exploding Vanishing Gradients Now that we gained intuition about the nature of the vanishing gradi ents problem and how it manifests itself in deep neural networks let us focus on a simple and practical heuristic to solve these problems.', 'Below we discuss the math ematical reasoning behind the vanishing gradient problem. Consider Equations 5 and 6 at a time step t to compute the RNN error dE dW we sum the error at each time step. That is dEt dW for every time step t is computed and accumulated.', 'When the gradient value goes to zero however it can go undetected while dras tically reducing the learning quality of the model for far away words in the corpus this issue is called the Vanishing Gradient Problem. Due to vanishing gradients we don t know whether there is no dependency between steps t and t n in the data or we just cannot capture the true dependency due to this issue.', '3.3. Vanishing gradient regularization We opt to address the vanishing gradients problem us ing a regularization term that represents a preference for parameter values such that back propagated gra dients neither increase or decrease too much in mag nitude.', 'press v28 pascanu13 supp.pdf 54 eigenvectors using the eigenvectors of Approaches 0 as grows so gradient vanishes sufficient but not necessary ONLY READ IF INTERESTEDWhy is vanishing gradient a problem Gradient signal from far away is lost because it s much smaller than gradient signal from close by. So model weights are updated only with respect to near effects not long term effects.', 'This a ple regime with a where any informa Figure 7 Gradient explosion clipping visualization To solve the problem of vanishing gradients we introduce two tech niques. The Ô¨Årst technique is that instead of initializing W hh ran domly start off from an identity matrix initialization. The second technique is to use the RectiÔ¨Åed Linear Units ReLU in stead of the sigmoid function.']",text-embedding-3-small,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9995343873940332,1.0,0.8147539879133775,0.0588235294117647
223,"What is perplexity, and how is it used in evaluating language models?","Perplexity is a metric used for evaluating language models in natural language processing. It is the inverse probability of a test set normalized by the number of words in the set. Perplexity helps compare different n-gram models by measuring how well a language model predicts the next word in a sequence. Lower perplexity values indicate more confidence in predicting the next word compared to the ground truth outcome. It is important to construct the n-gram model without any prior knowledge of the test set or its vocabulary to ensure the perplexity is not artificially low. Perplexity is closely related to entropy and arises from the information theoretic concept of cross entropy. A lower perplexity suggests a better language model in predicting the words in the test set, but an intrinsic improvement in perplexity does not guarantee an extrinsic improvement in language processing tasks like speech recognition or machine translation, although it often correlates with such improvements. 

Source: N_gram_Language_Models.txt","['3.2.1 Perplexity In practice we don t use raw probability as our metric for evaluating language mod els but a variant called perplexity. The perplexity sometimes called PPL for short perplexity of a language model on a test set is the inverse probability of the test set normalized by the number of words. For a test set W w1w2 .wN perplexity W P w1w2 .', 'We leave this exact calculation as exercise 3.12. We see in Section 3.8 that perplexity is also closely related to the information theoretic notion of entropy. We mentioned above that perplexity is a function of both the text and the lan guage model given a text W different language models will have different perplex ities. Because of this perplexity can be used to compare different n gram models.', '9 is called the perplexity relationship it is basically 2 to the power of the negative log probability of the cross entropy error function shown in Equation 8. Perplexity is a measure of confusion where lower values imply more conÔ¨Ådence in predicting the next word in the sequence compared to the ground truth outcome .', 'Note that in computing perplexities the n gram model P must be constructed without any knowledge of the test set or any prior knowledge of the vocabulary of the test set. Any kind of knowledge of the test set can cause the perplexity to be artiÔ¨Åcially low. The perplexity of two language models is only comparable if they use identical vocabularies.', 'The perplexity of a test set according to a language model is the geometric mean of the inverse test set probability computed by the model. Smoothing algorithms provide a more sophisticated way to estimate the prob ability of n grams. Commonly used smoothing algorithms for n grams rely on lower order n gram counts through backoff or interpolation. Both backoff and interpolation require discounting to create a probability dis tribution.', 'A better n gram model is one that assigns a higher probability to the test data and perplexity is a normalized version of the probability of the test set. The perplexity measure actually arises from the information theoretic concept of cross entropy which explains otherwise mysterious properties of perplexity why the inverse probability for example and its relationship to entropy. Entropy is a Entropy measure of information.', 'And the higher the probability the lower the perplexity since as Eq. 3.15 showed perplexity is related inversely to the likelihood of the test sequence according to the model . So a lower perplexity can tell us that a language model is a better predictor of the words in the test set.', 'An intrinsic improvement in perplexity does not guarantee an extrinsic im provement in the performance of a language processing task like speech recognition10 CHAPTER 3 N GRAM LANGUAGE MODELS or machine translation. Nonetheless because perplexity often correlates with such improvements it is commonly used as a quick check on an algorithm.']",text-embedding-3-small,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9381099824882831,1.0,0.9475168319441384,0.48
224,How does LSTM solve the vanishing gradient problem?,"LSTM solves the vanishing gradient problem by using its architecture to make it easier for a recurrent neural network (RNN) to preserve information over many timesteps. For example, if the forget gate is set to 1 for a cell dimension and the input gate set to 0, then the information of that cell is preserved indefinitely, which is more challenging for a vanilla RNN to achieve. This helps LSTM in addressing the vanishing gradient problem by allowing the network to retain information over long sequences without it diminishing over time.","['github.io posts 2015 08 Understanding LSTMs 65How does LSTM solve vanishing gradients 66 The LSTM architecture makes it much easier for an RNN to preserve information over many timesteps e.g. if the forget gate is set to 1 for a cell dimension and the input gate set to 0 then the information of that cell is preserved indefinitely. In contrast it s harder for a vanilla RNN to', 'When the gradient value goes to zero however it can go undetected while dras tically reducing the learning quality of the model for far away words in the corpus this issue is called the Vanishing Gradient Problem. Due to vanishing gradients we don t know whether there is no dependency between steps t and t n in the data or we just cannot capture the true dependency due to this issue.', 'To gain practical intuition about the vanishing gradient problem you may visit the following example website. 2.4 Solution to the Exploding Vanishing Gradients Now that we gained intuition about the nature of the vanishing gradi ents problem and how it manifests itself in deep neural networks let us focus on a simple and practical heuristic to solve these problems.', 'Below we discuss the math ematical reasoning behind the vanishing gradient problem. Consider Equations 5 and 6 at a time step t to compute the RNN error dE dW we sum the error at each time step. That is dEt dW for every time step t is computed and accumulated.', 'with RNNs Vanishing and Exploding Gradients 48Vanishing gradient intuition 49Vanishing gradient intuition chain rule 50Vanishing gradient intuition chain rule 51Vanishing gradient intuition chain rule 52Vanishing gradient intuition What happens if these are small Vanishing gradient problem When these are small the gradient signal gets smaller and smaller as it backpropagates further 53Vanishing gradient proof sketch linear case Recall What if were the identity', '2013. http proceedings.mlr.press v28 pascanu13.pdfHow to fix the vanishing gradient problem The main problem is that it s too difficult for the RNN to learn to preserve information over many timesteps.', 'contrast it s harder for a vanilla RNN to learn a recurrent weight matrix Wh that preserves info in the hidden state In practice you get about 100 timesteps rather than about 7 However there are alternative ways of creating more direct and linear pass through connections in models for long distance dependenciesIs vanishing exploding gradient just an RNN problem For example Residual connections aka ResNet Also known as skip connections The identity connection preserves information by default This makes deep', 'press v28 pascanu13 supp.pdf 54 eigenvectors using the eigenvectors of Approaches 0 as grows so gradient vanishes sufficient but not necessary ONLY READ IF INTERESTEDWhy is vanishing gradient a problem Gradient signal from far away is lost because it s much smaller than gradient signal from close by. So model weights are updated only with respect to near effects not long term effects.']",text-embedding-3-small,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.0000000000000009,1.0,0.9685622386939384,0.0666666666666666
225,"What is the impact of bidirectional RNNs, and where are they applicable?","The impact of bidirectional RNNs is that they provide a contextual representation that includes both left and right context, making them powerful for tasks where access to the entire input sequence is available. They are not applicable to Language Modeling because in LM, only left context is available. However, if you do have the entire input sequence, bidirectionality is powerful and should be used by default. For example, models like BERT (Bidirectional Encoder Representations from Transformers) leverage bidirectionality to create powerful pretrained contextual representations. Additionally, bidirectional RNNs can also be multi-layered, where each lower layer feeds into the next layer, enhancing their representation capabilities.

Source: Lecture03_RNNs_and_LLMs.txt","['Input audio START conditioning 77Bidirectional RNNs was terribly exciting the movie Forward RNN Backward RNN Concatenated hidden states This contextual representation of terribly has both left and right context 78Bidirectional RNNs 79 Note bidirectional RNNs are only applicable if you have access to the entire input sequence They are not applicable to Language Modeling because in LM you only have left context available.', 'If you do have entire input sequence e.g. any kind of encoding bidirectionality is powerful you should use it by default . For example BERT Bidirectional Encoder Representations from Transformers is a powerful pretrained contextual representation system built on bidirectionality.', 'h h h Figure 8 A bi directional RNN model h t f W xt V h t 1 b 17 h t f W xt V h t 1 b 18 ÀÜ yt g Uht c g U h t h t c 19 RNNs can also be multi layered. Figure 9 shows a multi layer bi directional RNN where each lower layer feeds the next layer.', 'Inspired by the window based method Hill et al. 2016 we split each article into short sentences where each sentence is a 19 word window around each entity hence the same word might appear in multiple sentences . The RNNs in BIDAF are not feed forwarded or back propagated across sentences which speed up the training process by par allelization.', 'Chen et al. 2016 show that simply using bilinear term for computing the attention weights in the same model drastically improves the accuracy. Wang Jiang 2016 reverse the direction of the attention attending on query words as the context RNN progresses for SQuAD. In contrast to these models BIDAF uses a memory less attention mechanism.', 'The derivative for the ReLU is either 0 or 1. This way gradients would Ô¨Çow through the neurons whose deriva tive is 1 without getting attenuated while propagating back through time steps. 2.5 Deep Bidirectional RNNs So far we have focused on RNNs that condition on past words to predict the next word in the sequence.', '3. Contextual Embedding Layer. We use a Long Short Term Memory Network LSTM Hochreiter Schmidhuber 1997 on top of the embeddings provided by the previous layers to model the temporal interactions between words. We place an LSTM in both directions and concatenate the outputs of the two LSTMs.', 'Figure 3. Bifurcation diagram of a single hidden unit RNN with Ô¨Åxed recurrent weight of 5.0 and adjustable bias b example introduced in Doya 1993 . See text. We show that there are two types of events that could lead to a large change in xt with t .']",text-embedding-3-small,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9668585653326168,1.0,0.9401881159856248,0.05
226,Describe the concept of multi-layer or stacked RNNs and their benefit.,"Multi-layer or stacked RNNs allow a network to compute more complex representations by having lower RNN layers compute lower level features and higher RNN layers compute higher level features. This architecture works better than just having one layer of high-dimensional encodings. Each layer in a multi-layer RNN receives inputs from the previous layer, enabling the network to learn hierarchical representations of the input data, leading to improved performance in tasks such as sequence modeling and language processing.

Source: Lecture03_RNNs_and_LLMs, Lecture03_notes_LM_RNN","['This allows the network to compute more complex representations The lower RNNs should compute lower level features and the higher RNNs should compute higher level features. Multi layer RNNs are also called stacked RNNs. 80Multi layer RNNs was terribly exciting the movie RNN layer 1 RNN layer 2 81 RNN layer 3 The hidden states from RNN layer i are the inputs to RNN layer i', 'layer i are the inputs to RNN layer i 1Multi layer RNNs in practice 39 Multi layer or stacked RNNs allow a network to compute more complex representations they work better than just have one layer of high dimensional encodings The lower RNNs should compute lower level features and the higher RNNs should compute higher level features.', 'h h h Figure 8 A bi directional RNN model h t f W xt V h t 1 b 17 h t f W xt V h t 1 b 18 ÀÜ yt g Uht c g U h t h t c 19 RNNs can also be multi layered. Figure 9 shows a multi layer bi directional RNN where each lower layer feeds the next layer.', 'Perplexity 2J 9 2.2 Advantages Disadvantages and Applications of RNNs RNNs have several advantages 1. They can process input sequences of any length 2. The model size does not increase for longer input sequence lengths 3. Computation for step t can in theory use information from many steps back. 4.', 'You will learn more about transformers including BERT in a couple of weeksMulti layer RNNs RNNs are already deep on one dimension they unroll over many timesteps We can also make them deep in another dimension by applying multiple RNNs this is a multi layer RNN.', 'Figure 11 illustrates this model. ht œÜ ht 1 c yt 1 27 Extension III train deep recurrent neural networks using multiple RNN layers as discussed earlier in this chapter. Deeper layers often improve prediction accuracy due to their higher learning capacity. Of course this implies a large training corpus must be used to train the model.', 'RNNs should compute higher level features. High performing RNNs are usually multi layer but aren t as deep as convolutional or feed forward networks For example In a 2017 paper Britz et al.', 'contrast it s harder for a vanilla RNN to learn a recurrent weight matrix Wh that preserves info in the hidden state In practice you get about 100 timesteps rather than about 7 However there are alternative ways of creating more direct and linear pass through connections in models for long distance dependenciesIs vanishing exploding gradient just an RNN problem For example Residual connections aka ResNet Also known as skip connections The identity connection preserves information by default This makes deep']",text-embedding-3-small,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9457573221636204,1.0,0.963387373052975,0.0526315789473684
227,How does Dropout work during training and testing?,"During training, Dropout works by randomly dropping (setting to zero) neurons in the forward pass with a certain probability \( p \). This helps prevent overfitting by training multiple smaller networks simultaneously and averaging their predictions. During testing, all neurons in the network are used to compute predictions. This ensures that the expected output of a neuron during testing is approximately the same as during training, maintaining network behavior. 

Source: Lecture02_notes_Backpropagation","['In the next section we discuss dropout which effectively acts as another form of regularization by randomly dropping i.e. setting to zero neurons in the forward pass. 2.3 Dropout Dropout is a powerful technique for regularization Ô¨Årst introduced by Srivastava et al. in Dropout A Simple Way to Prevent Neural Net works from OverÔ¨Åtting.', 'However a key sub tlety is that in order for dropout to work effectively the expected output of a neuron during testing should be approximately the same as it was during training else the magnitude of the outputs could be radically different and the behavior of the network is no longer well deÔ¨Åned.', 'Image credits to Srivastava et al. In practice the way we introduce dropout is that we take the out put h of each layer of neurons and keep each neuron with prob ability p and else set it to 0. Then during back propagation we only pass gradients through neurons that were kept alive during the forward pass. Finally during testing we compute the forward pass using all of the neurons in the network.', 'The result is that the network typically learns more meaningful informa tion from the data is less likely to overÔ¨Åt and usually obtains higher performance overall on the task at hand. One intuitive reason why this technique should be so effective is that what dropout is doing is essentially doing is training exponentially many smaller networks at once and averaging over their predictions. Dropout applied to an artiÔ¨Åcial neural network.', 'The idea is simple yet effective during train ing we will randomly drop with some probability 1 p a subset of neurons during each forward backward pass or equivalently we will keep alive each neuron with a probability p . Then during testing we will use the full network to compute our predictions.', '7 1019 1.2 1021 Transformer base model 27.3 38.1 3.3 1018 Transformer big 28.4 41.8 2.3 1019 Residual Dropout We apply dropout 33 to the output of each sub layer before it is added to the sub layer input and normalized.', 'form of model bagging i.e. like an ensemble model Nowadays usually thought of as strong feature dependent regularizer Wager Wang Liang 2013DropoutDropout Srivastava Hinton Krizhevsky Sutskever Salakhutdinov 2012 JMLR 2014 During training For each data point each time Randomly set input to 0 with probability ùëù dropout ratio often p 0.', 'Dropout a simple way to prevent neural networks from overÔ¨Åtting. JMLR 2014. Rupesh Kumar Srivastava Klaus Greff and J urgen Schmidhuber. Highway networks. arXiv preprint arXiv 1505.00387 2015.']",text-embedding-3-small,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9935096588271314,1.0,0.9198824073135816,0.0526315789473684
228,What is the role of parameter initialization in neural networks?,"Parameter initialization in neural networks is a crucial step towards achieving optimal performance. By initializing the weights to small random numbers normally distributed around 0, and avoiding symmetries that prevent learning specialization, the network can start learning effectively. Proper initialization helps in maintaining activation variances and backpropagated gradient variances across layers, which in turn can lead to faster convergence and lower error rates during training. Additionally, initializing biases appropriately is important for the overall performance of the network. Overall, parameter initialization sets the foundation for successful training and optimization of neural networks.

Source: lecture02_notes_Backpropagation.txt","['2.6 Parameter Initialization A key step towards achieving superlative performance with a neu ral network is initializing the parameters in a reasonable way. A good starting strategy is to initialize the weights to small random numbers normally distributed around 0 and in practice this often words acceptably well.', '8 ¬µs per loop Matrices are awesome Always try to use vectors and matrices rather than for loops The speed gain goes from 1 to 2 orders of magnitude with GPUs 7Parameter Initialization You normally must initialize weights to small random values i.e. not zero matrices To avoid symmetries that prevent learning specialization Initialize hidden layer biases to 0 and output or reconstruction biases to optimal value if weights were 0 e.g.', 'Without such initialization the gradient variances which are a proxy for information generally decrease with backpropagation across layers. 2.7 Learning Strategies The rate magnitude of model parameter updates during training can be controlled using the learning rate.', 'Within each line child networks are initialized with the embeddings of their parents and trained on increasingly rich data sets with sometimes different parameters. That is suppose we have k processors which is much less than the possible set of parameters one would like to try. One chooses k initial parameter choices from the large set and trains these on the k processors.', 'p3 input 1 input 2 p4 output 2 p5 tanh output 1 p6 input 1 p7 input 2 p8 input 1 input 2 p9 Neural networks are almost always deÔ¨Åned in terms of parameters here denoted by p1 . . . p9 which are automatically chosen using standard machine learning algorithms.', 'However in Understanding the dif ficulty of training deep feedforward neural networks 2010 Xavier et al study the effect of different weight and bias initialization schemes on training dynamics. The empirical Ô¨Åndings suggest that for sigmoid and tanh activation units faster convergence and lower error rates are achieved when the weights of a matrix W Rn l 1 n l are initialized randomly with a uniform distribution as follows W U s 6 n l n l 1 s', 's 6 n l n l 1 s 6 n l n l 1 Where n l is the number of input units to W fan in and n l 1 is the number of output units from W fan out . In this parameter initialization scheme bias units are initialized to 0. This approach attempts to maintain activation variances as well as backpropagated gradient variances across layers.', 'For any parameter assignment Œ∏ depending on the ini tial state x0 the state xt of an autonomous dynamical system converges under the repeated application of the map F to one of several possible diÔ¨Äerent attrac tor states e.g. point attractors though other type of attractors exist .']",text-embedding-3-small,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9638291061609338,1.0,0.9417326437596776,0.1333333333333333
229,"What are the differences between Vanilla RNNs, LSTMs, and GRUs?","Vanilla RNNs constantly rewrite the hidden state, making it harder to learn long-term dependencies. LSTMs are explicitly designed to avoid the long-term dependency problem by preserving information for long periods of time as their default behavior. GRUs are designed to have more persistent memory compared to RNNs, making it easier for capturing long-term dependencies. LSTMs have additional features like peephole connections, allowing gate layers to look at the cell state. 

Source: Lecture03_RNNs_and_LLMs, Understanding_LSTM_Networks","['Figure 12 The detailed internals of a GRU It is important to note that to train a GRU we need to learn all the different parameters W U W r U r W z U z . These follow the same backpropagation procedure we have seen in the past. 4 Long Short Term Memories Long Short Term Memories are another type of complex activation unit that differ a little from GRUs.', 'github.io posts 2015 08 Understanding LSTMs 65How does LSTM solve vanishing gradients 66 The LSTM architecture makes it much easier for an RNN to preserve information over many timesteps e.g. if the forget gate is set to 1 for a cell dimension and the input gate set to 0 then the information of that cell is preserved indefinitely. In contrast it s harder for a vanilla RNN to', 'What motivates this Well although RNNs can theoretically capture long term dependencies they are very hard to actually train to do this. Gated recurrent units are designed in a manner to have more persistent memory thereby making it easier for RNNs to capture long term dependencies. Let us see mathematically how a GRU uses ht 1 and xt to generate the next hidden state ht.', 'The motivation for using these is similar to those for GRUs however the architecture of such units does differ. Let us Ô¨Årst take a look at the mathematical formulation of LSTM units before diving into the intuition behind this design it œÉ W i xt U i ht 1 Input gate ft œÉ W f xt U f ht 1 Forget gate ot œÉ W o xt U o ht 1 Output Exposure gate ct tanh W c xt U c ht', 'The differences are minor but it s worth mentioning some of them. One popular LSTM variant introduced by Gers Schmidhuber 2000 ftp ftp.idsia.ch pub juergen TimeCount IJCNN2000.pdf is adding peephole connections. This means that we let the gate layers look at the cell state.', 'contrast it s harder for a vanilla RNN to learn a recurrent weight matrix Wh that preserves info in the hidden state In practice you get about 100 timesteps rather than about 7 However there are alternative ways of creating more direct and linear pass through connections in models for long distance dependenciesIs vanishing exploding gradient just an RNN problem For example Residual connections aka ResNet Also known as skip connections The identity connection preserves information by default This makes deep', 'In a vanilla RNN the hidden state is constantly being rewritten First off next time How about an RNN with separate memory which is added to LSTMs And then Creating more direct and linear pass through connections in model Attention residual connections etc.', 'They work tremendously well on a large variety of problems and are now widely used. LSTMs are explicitly designed to avoid the long term dependency problem. Remembering information for long periods of time is practically their default behavior not something they struggle to learn 11 24 24 3 23 PM Understanding LSTM Networks colah s blog 3 7 All recurrent neural networks have the form of a chain of repeating modules of neural network.']",text-embedding-3-small,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9948495283046253,1.0,0.9246337170928308,0.0555555555555555
230,What is the difference between traditional language models and conditioned language models?,"Traditional language models are typically trained to predict the next word in a sequence based on the previous words, without considering any specific conditioning information. On the other hand, conditioned language models take into account additional information or context when generating text, such as images or specific features, to improve the quality and relevance of the generated output.

Source: Lecture04_Conditioned_Generation","['a phrase based translation model and a neural MT model Niehues et al. 2017 Stacking uses the output of one system in calculating features for another systemCase Studies in Conditional Language ModelingFrom Images e.g. Karpathy et al. 2015 Input is image features output is text Use standard image encoders e.g. CNN Often pre trained on large databases such as ImageNetCheck out this review paperHow do we EvaluateBasic', 'Our work differs in focusing on in context learning but could be combined in the future with those of GLT 20 LPP 20 . Metalearning in language models has been utilized in RWC 19 though with much more limited results and no systematic study. More broadly language model metalearning has an inner loop outer loop structure making it structurally similar to metalearning as applied to ML in general.', 'The models we introduced in Chapter 10 to do this task are causal or left to right transformer models. In this chapter we ll introduce a second paradigm for pretrained language mod els called the bidirectional transformer encoder trained via masked language modeling a method that allows the model to see entire texts at a time including masked language modeling both the right and left context.', 'RRS20 recently demonstrated that a large language model can perform surprisingly well directly answering the questions without conditioning on auxilliary information. They denote this more restrictive evaluation setting as closed book . Their work suggests that even higher capacity models could perform even better and we test this hypothesis with GPT 3.', 'Pretrained language models based on bidirectional encoders can be learned using a masked language model objective where a model is trained to guess the missing information from an input. Pretrained language models can be Ô¨Åne tuned for speciÔ¨Åc applications by adding lightweight classiÔ¨Åer layers on top of the outputs of the pretrained model.', 'All models used the same context to condition outputs on and were pre trained with the same context size and the same article titles and subtitles were used as prompts for each model. However we also ran an experiment to control for participant effort and attention that followed the same format but involved intentionally bad model generated articles. This was done by generating articles from a control model a 160M parameter model with no context and increased output randomness.', 'Modern masked language models are now trained on much larger datasets of web text Ô¨Åltered a bit and augmented by higher quality data like Wikipedia the same as those we discussed for the causal large language models of Chapter 10. Multilingual models similarity use webtext and multilingual Wikipedia.', 'Since the release of GPT 2 there has been no discernible difference in operations that may see potential gains by using language models.']",text-embedding-3-small,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.0000000000000002,0.5,0.9460912254389082,0.0588235294117647
231,How can LSTM networks be used in language modeling?,"LSTM networks can be used in language modeling by scaling them to over a billion parameters, which has been shown to improve generative or task performance. LSTM-based language models can predict the next word based on the previous ones, making them effective for tasks where context plays a crucial role in predicting the next word. Additionally, LSTM networks can efficiently represent more complex patterns compared to shallow neural networks, making them suitable for language modeling tasks that require capturing long-range dependencies in text data.","['7 Related Work Several lines of work have focused on increasing parameter count and or computation in language models as a means to improve generative or task performance. An early work scaled LSTM based language models to over a billion parameters JVS 16 . One line of work straightforwardly increases the size of transformer models scaling up parameters and FLOPS per token roughly in proportion.', 'minds laptops Language Modeling is the task of predicting what word comes next books the students opened their 10Language Modeling You can also think of a Language Model as a system that assigns a probability to a piece of text For example if we have some text then the probability of this text according to the Language Model is This is what our LM provides 11You use Language Models every day 12You use Language Models every day 13n gram Language Models', 'For example consider a language model trying to predict the next word based on the previous ones. If we are trying to predict the last word in the clouds are in the sky we don t need any further context it s pretty obvious the next word is going to be sky. In such cases where the gap between the relevant information and the place that it s needed is small RNNs can learn to use the past information.', 'of any length Apply the same weights on each step Can optionally produce output on each step Recurrent Neural Network Language Model We ve shown that RNNs are a great way to build a LM despite some problems RNNs are also useful for much moreWhy should we care about Language Modeling 70 Language Modeling is a benchmark task that helps us measure our progress on predicting language use Language Modeling is a subcomponent of many NLP tasks especially those involving generating', 'Surprisingly grammatical but incoherent. We need to consider more than three words at a time if we want to model language well. But increasing n worsens sparsity problem and increases model sizeHow to build a neural language model Recall the Language Modeling task Input sequence of words Output prob. dist. of the next word How about a window based neural model We saw this applied to Named Entity Recognition in Lecture 2 LOCATION', '2.2 Recurrent Neural Net Language Model RNNLM Recurrent neural network based language model has been proposed to overcome certain limitations of the feedforward NNLM such as the need to specify the context length the order of the model N and because theoretically RNNs can efÔ¨Åciently represent more complex patterns than the shallow neural networks 15 2 .', 'Another recent trend in language modeling may offer a way forward. In recent years the capacity of transformer language models has increased substantially from 100 million parameters RNSS18 to 300 million parameters DCLT18 to 1.5 billion parameters RWC 19 to 8 billion parameters SPP 19 11 billion parameters RSR 19 and Ô¨Ånally 17 billion parameters Tur20 .', 'Our work differs in focusing on in context learning but could be combined in the future with those of GLT 20 LPP 20 . Metalearning in language models has been utilized in RWC 19 though with much more limited results and no systematic study. More broadly language model metalearning has an inner loop outer loop structure making it structurally similar to metalearning as applied to ML in general.']",text-embedding-3-small,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999985275825996,1.0,0.9457708458038248,0.0555555555555555
232,What are the methods of generation in conditioned language modeling?,The methods of generation in conditioned language modeling are Sampling and Try to generate a random sentence according to the probability distribution.,"['file name: lecture04 Conditioned Generation. Hamidreza Mahyar mahyarh@mcmaster.ca Computational Natural Language Processing Conditioned GenerationLanguage Models Language models are generative models of text s P x The Malfoys said Hermione. Harry was watching him. He looked like Madame Maxime. When she strode up the wrong staircase to visit himself.', 'generate text according to some specification Input X Structured Data English Document Utterance Image Speech Output Y Text NL Description Japanese Short Description Response Text Transcript Task NL Generation Translation Summarization Response Generation Image Captioning Speech RecognitionFormulation and ModelingCalculating the Probability of a SentenceConditional Language ModelsOne Type of Language Model LSTM LSTM LSTM LSTM predict hate predict this predict movie predict s LSTM Mikolov et al.', 'a phrase based translation model and a neural MT model Niehues et al. 2017 Stacking uses the output of one system in calculating features for another systemCase Studies in Conditional Language ModelingFrom Images e.g. Karpathy et al. 2015 Input is image features output is text Use standard image encoders e.g. CNN Often pre trained on large databases such as ImageNetCheck out this review paperHow do we EvaluateBasic', '2014 encoder decoder Transform can be different dimensions encoder transform decoder encoder Input at every time step Kalchbrenner Blunsom 2013 decoder decoder decoderMethods of GenerationThe Generation Problem We have a model of P Y X how do we use it to generate a sentence Two methods Sampling Try to generate a random sentence according to the probability distribution.', 'Thus sampling from a language model which rep resents a distribution over sentences means to generate some sentences choosing each sentence according to its likelihood as deÔ¨Åned by the model. Thus we are more likely to generate sentences that the model thinks have a high probability and less likely to generate sentences that the model thinks have a low probability.', '2020 . In Chapter 10 we explored causal left to right transformers that can serve as the basis for powerful language models models that can eas ily be applied to autoregressive generation problems such as contextual generation summarization and machine translation.', 'These approaches rely on the conditional computation framework BLC13 and speciÔ¨Åcally the mixture of experts method SMM 17 has been used to produce 100 billion parameter models and more recently 50 billion parameter translation models AJF19 though only a small fraction of the parameters are actually used on each forward pass.', '2020 PRISM Model based on training paraphrasing model Thompson and Post 2020Perplexity Calculate the perplexity of the words in the held out set without doing generation Pros Naturally solves multiple reference problem Cons Doesn t consider decoding or actually generating output.']",text-embedding-3-small,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.0000000000000002,,0.933751205643742,0.0714285714285714
233,What is beam search and how is it used in language generation?,"Beam search is a technique used in natural language processing for generating sequences of words, such as in language generation tasks like machine translation or text generation. In beam search, the algorithm keeps track of the k most probable partial translations (hypotheses) at each step of the decoding process. These hypotheses are ranked based on their log probabilities, with higher scores indicating better likelihood. The beam size (k) is typically set to a value around 5 to 10 in neural machine translation. Beam search is more efficient than exhaustive search but does not guarantee finding the optimal solution. It helps explore the search space efficiently by focusing on the most promising candidates at each step of generation.

Source: Lecture05_Seq2Seq","['search 18Beam search decoding example Beam size k 2. Blue numbers START Calculate prob dist of next word 19Beam search decoding example Beam size k 2. Blue numbers START he I 0.7 log PLM he START 0.9 log PLM I START Take top k words and compute scores 20Beam search decoding example Beam size k 2. Blue numbers hit struck was got START he I 10 0.', '6 For each of the k hypotheses find top k next words and calculate scoresBeam search decoding example Beam size k 2.', 'On tasks with free form completion we use beam search with the same parameters as RSR 19 a beam width of 4 and a length penalty of Œ± 0.6. We score the model using F1 similarity score BLEU or exact match depending on what is standard for the dataset at hand. Final results are reported on the test set when publicly available for each model size and learning setting zero one and few shot .', '8 1.7 2.9 2.5 2.8 3.8 2.9 3.5 3.4 3.3 4.0 4.5 3.7 4.3 4.8 For each of the k hypotheses find top k next words and calculate scoresBeam search decoding example Beam size k 2.', 'Core idea On each step of decoder keep track of the k most probable partial translations which we call hypotheses k is the beam size in practice around 5 to 10 in NMT A hypothesis has a score which is its log probability Scores are all negative and higher score is better We search for high scoring hypotheses tracking top k on each step Beam search is not guaranteed to find optimal solution But much more efficient than exhaustive search 18Beam search decoding example Beam size k', '7 0.9 2.9 log PLM struck START he 0.7 1.6 log PLM was START I 0.9 1.8 log PLM got START I 0.9 1.7 log PLM hit START he 0.7 For each of the k hypotheses find top k next words and calculate scoresBeam search decoding example Beam size k 2.', '9 2.5 2.8 3.8 2.9 3.5 3.4 3.3 4.0 4.5 3.7 4.3 4.8 4.3 4.6 5.0 5.3 For each of the k hypotheses find top k next words and calculate scoresBeam search decoding example Beam size k 2.', '4 3.3 4.0 For each of the k hypotheses find top k next words and calculate scoresBeam search decoding example Beam size k 2. Blue numbers hit struck was got a me hit struck tart pie with on START he I 15 0.7 0.9 1.6 1.8 1.7 2.9 2.5 2.8 3.8 2.']",text-embedding-3-small,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9218209045611296,1.0,0.9205367680331072,0.2142857142857142
234,Explain the concept of model ensembling in language models.,"Model ensembling in language models involves combining the predictions of multiple individual models to improve the overall performance. This can be done by averaging the predictions of different models or using more complex techniques like stacking or blending to leverage the strengths of each model. By ensembling models, the final prediction can be more robust and accurate, as it reduces the risk of overfitting and captures a more diverse range of patterns and features present in the data. Ensembling is a common practice in machine learning to enhance the predictive power of models by leveraging the diversity of multiple models. 

Source: Fine Tuning and Masked Language Models","['Our work differs in focusing on in context learning but could be combined in the future with those of GLT 20 LPP 20 . Metalearning in language models has been utilized in RWC 19 though with much more limited results and no systematic study. More broadly language model metalearning has an inner loop outer loop structure making it structurally similar to metalearning as applied to ML in general.', 'Nonetheless when the number of languages grows very large multilingual models exhibit what has been called the curse of multilinguality Conneau et al. 2020 the performance on each language degrades compared to a model training on fewer languages.', 'During unsupervised pre training a language model develops a broad set of skills and pattern recognition abilities. It then uses these abilities at inference time to rapidly adapt to or recognize the desired task. We use the term in context learning to describe the inner loop of this process which occurs within the forward pass upon each sequence.', 'While the causal language models of Chapter 10 also use contextual embeddings the embeddings created by masked language models seem to function particularly well as representations. 11.1 Bidirectional Transformer Encoders Let s begin by introducing the bidirectional transformer encoder that underlies mod els like BERT and its descendants like RoBERTa Liu et al. 2019 or SpanBERT Joshi et al. 2020 .', 'This is because contextual embeddings whether from masked language11.4 FINE TUNING LANGUAGE MODELS 13 models or from autoregressive ones have the property that the vectors for all words are extremely similar.', 'But the span based approach can nat urally label both since the spans are labeled separately. 11.6 Summary This chapter has introduced the topic of transfer learning from pretrained language models. Here s a summary of the main points that we covered Bidirectional encoders can be used to generate contextualized representations of input embeddings using the entire input context.', 'Conneau et al. 2020 show that Œ± 0.3 works well to give rare languages more inclusion in the tokenization resulting in better multilingual performance overall. The result of this pretraining process consists of both learned word embeddings as well as all the parameters of the bidirectional encoder that are used to produce contextual embeddings for novel inputs.', 'Multilingual models have an additional decision to make what data to use to build the vocabulary Recall that all language models use subword tokenization BPE or SentencePiece Unigram LM are the two most common algorithms .']",text-embedding-3-small,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9687264226750372,0.0,0.921247654526876,0.0625
235,How does ensemble distillation differ from parameter averaging?,"Ensemble distillation differs from parameter averaging in that ensemble distillation trains a model to copy the ensemble, specifically trying to match the distribution over predicted words, while parameter averaging involves writing out models several times near the end of training and taking the average of parameters. Ensemble distillation aims to make the model make the same mistakes as an ensemble, which has been shown to increase accuracy notably. On the other hand, parameter averaging only works for models within the same run. (Source: lecture04_Conditioned_Generation.txt)","['agree use when you want to restrict possible answersParameter Averaging Problem Ensembling means we have to use M models at test time increasing our time memory complexity Parameter averaging is a cheap way to get some good effects of ensembling Basically write out models several times near the end of training and take the average of parametersEnsemble Distillation e.g. Kim et al.', 'e.g. Kim et al. 2016 Problem parameter averaging only works for models within the same run Knowledge distillation trains a model to copy the ensemble Specifically it tries to match the description over predicted words Why We want the model to make the same mistakes as an ensemble Shown to increase accuracy notablyStacking What if we have two very different models where prediction of outputs is done in very different ways e.g.', 'Note in the entry for labeler vs. labeler ensemble the ensembles are slightly smaller than for other comparisons because we need to exclude the labeler being predicted. All ensembles have at least 3 workers. TL DR 6.7B RL T 0.7 labeler labeler ensem ble length copying ROUGE 1.3B sup logprob 1.3B RM 6.', 'The parameters of the combining layers were then trained on the existing training set while keeping the ten networks Ô¨Åxed joined ensemble . This approach did not improve on simple voting. These ensembles come of course at the expense of a ten fold increase of the running time. On the other hand multiple training times could be improved using smart sampling strategies Neal 1996 .', 'eig 0.0 . The results in this scenario are quite different than those of the vanilla scenario with better performance in many cases. However this change is not uniform as we observe that differ ent settings boost different algorithms. In fact the question Which method is best might have a completely different answer when comparing on the same task but with different hyperparameter values.', 'We can also evaluate the model as a mixture by averaging its losses over multiple random orderings. This is a lower bound on log likelihood. It is intractable to sample from exactly but can be approximated through Gibbs sampling. 7Figure 4 Comparing how models continue a prime top left .', 'We then counted the number of times each hyperparameter setting was chosen in these conÔ¨Ågurations Table 7 . Some trends emerge such as PPMI and SVD s prefer ence towards shorter context windows10 win 2 and that SGNS always prefers numerous nega tive samples neg 1 .', 'However this may be computationally expensive. We thus provide some rules of thumb which we found to work well in our setting Always use context distribution smoothing cds 0.75 to modify PMI as described in Section 3.2. It consistently improves performance and is applicable to PPMI SVD and SGNS. Do not use SVD correctly eig 1 .']",text-embedding-3-small,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9957593478996196,1.0,0.9734601350928788,0.0434782608695652
236,What are some common evaluation metrics for conditioned language models?,"Some common evaluation metrics for conditioned language models include BLEU, BertScore, BLEURT, and COMET. These metrics are based on neural models and aim to evaluate the performance of conditioned language models by comparing n-gram overlap with reference translations, finding similarity between BERT embeddings, training models to predict human evaluation scores, and training models to predict human evaluation using the source sentence.

Source: lecture04_Conditioned_Generation","['out this review paperHow do we EvaluateBasic Evaluation Paradigm Use parallel test set Use system to generate translations Compare target translations w referenceHuman Evaluation Ask a human to do evaluation Final goal but slow expensive and sometimes inconsistentBLEU Works by comparing n gram overlap w reference Pros Easy to use good for measuring system improvement Cons Often doesn t match human eval bad for comparing very different systemsEmbedding based Metrics Recently many metrics based on neural models BertScore Find similarity between', 'May be reasonable for problems with lots of ambiguity.Which One to Use Meta evaluation runs human evaluation and automatic evaluation on the same outputs calculates correlation Examples WMT Metrics Task for MT Mathur et al. 2021 RealSumm for summarization Evaluation is hard especially with good systems Most metrics had no correlation w human eval over best systems at some WMT 2019 tasksQuestions', 'metrics based on neural models BertScore Find similarity between BERT embeddings unsupervised Zhang et al. 2020 BLEURT Train BERT to predict human evaluation scores Sellam et al. 2020 COMET Train model to predict human eval also using source sentence Rei et al.', '13 3.2 Evaluating Language Models The best way to evaluate the performance of a language model is to embed it in an application and measure how much the application improves. Such end to end evaluation is called extrinsic evaluation. Extrinsic evaluation is the only way to extrinsic evaluation know if a particular improvement in a component is really going to help the task at hand.', 'For Acc F1 BLEU we use the metric speciÔ¨Åed in Metric . These scores come from evaluations with a different seed for the random examples used for in context learning and will therefore differ slightly from the scores elsewhere in the paper. 45D Total Compute Used to Train Language Models This appendix contains the calculations that were used to derive the approximate compute used to train the language models in Figure 2.2.', 'RRS20 recently demonstrated that a large language model can perform surprisingly well directly answering the questions without conditioning on auxilliary information. They denote this more restrictive evaluation setting as closed book . Their work suggests that even higher capacity models could perform even better and we test this hypothesis with GPT 3.', 'Thus for speech recognition we can compare the performance of two language models by running the speech recognizer twice once with each language model and seeing which gives the more accurate transcription. Unfortunately running big NLP systems end to end is often very expensive. In stead it would be nice to have a metric that can be used to quickly evaluate potential improvements in a language model.', 'a phrase based translation model and a neural MT model Niehues et al. 2017 Stacking uses the output of one system in calculating features for another systemCase Studies in Conditional Language ModelingFrom Images e.g. Karpathy et al. 2015 Input is image features output is text Use standard image encoders e.g. CNN Often pre trained on large databases such as ImageNetCheck out this review paperHow do we EvaluateBasic']",text-embedding-3-small,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.0000000000000002,1.0,0.9256568475039524,0.0625
237,What is the significance of stacking in model ensembling?,"Stacking in model ensembling is significant when we have two very different models that predict outputs in different ways. By combining predictions from multiple models through stacking, we can leverage the strengths of each model and potentially improve overall performance by creating a more robust and accurate ensemble model. This approach helps address the issue of models making somewhat uncorrelated errors and can lead to increased accuracy by smoothing over idiosyncrasies of individual models.","['agree use when you want to restrict possible answersParameter Averaging Problem Ensembling means we have to use M models at test time increasing our time memory complexity Parameter averaging is a cheap way to get some good effects of ensembling Basically write out models several times near the end of training and take the average of parametersEnsemble Distillation e.g. Kim et al.', 'e.g. Kim et al. 2016 Problem parameter averaging only works for models within the same run Knowledge distillation trains a model to copy the ensemble Specifically it tries to match the description over predicted words Why We want the model to make the same mistakes as an ensemble Shown to increase accuracy notablyStacking What if we have two very different models where prediction of outputs is done in very different ways e.g.', 'The parameters of the combining layers were then trained on the existing training set while keeping the ten networks Ô¨Åxed joined ensemble . This approach did not improve on simple voting. These ensembles come of course at the expense of a ten fold increase of the running time. On the other hand multiple training times could be improved using smart sampling strategies Neal 1996 .', 'Some in reading materials more in a later classModel EnsemblingEnsembling Why Multiple models make somewhat uncorrelated errors Models tend to be more uncertain when they are about to make errors Smooths over idiosyncrasies of the model LSTM1 predict1 I LSTM2 predict2 Combine predictions from multiple models s sLinear Interpolation Take a weighted average of the M model probabilities Second term often set to uniform distribution 1 MLog linear Interpolation Weighted', 'Note in the entry for labeler vs. labeler ensemble the ensembles are slightly smaller than for other comparisons because we need to exclude the labeler being predicted. All ensembles have at least 3 workers. TL DR 6.7B RL T 0.7 labeler labeler ensem ble length copying ROUGE 1.3B sup logprob 1.3B RM 6.', 'In some sense it s odd that this works but interesting 2.3 Elementwise nonlinearity Imagine if we were to stack self attention layers. Would this be sufficient for a replacement for stacked LSTM layers Intuitively there s one thing that s missing the elementwise nonlinearities that we ve come to expect in standard deep learning architectures. In fact if we stack two self attention layers we get something that looks a lot like a single self attention layer', 'This suggests we could get more reliable labels by aggregating labels from multiple workers on the same comparison. Indeed on the subset of the training data for which we have enough shared comparisons taking the modal label from 3 labelers increases their agreement rate with researchers from 72 to 77 . However we usually collect only one label per comparison in order to maximize label throughput.', 'All ensembles have at least 3 workers. 36TL DR 6.7B sup. T 0.7 labeler labeler ensem ble length copying ROUGE 1.3B sup logprob 1.3B RM 6.7B sup logprob 6.7B RM labeler 70.8 2.6 73.1 2.9 56.9 0.']",text-embedding-3-small,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9579770805575656,1.0,0.9126685978850264,0.0526315789473684
238,How are images used as input for conditioned language models?,"Images can be used as input for conditioned language models in various ways, such as representing image classes as learned embeddings and adding them to the input representation of every input position along with positional encodings. This allows the models to generate text descriptions or responses based on the content of the images. Additionally, in tasks like visual planning, conditional image generation models can predict future frames of video based on previous frames and actions taken.","['a phrase based translation model and a neural MT model Niehues et al. 2017 Stacking uses the output of one system in calculating features for another systemCase Studies in Conditional Language ModelingFrom Images e.g. Karpathy et al. 2015 Input is image features output is text Use standard image encoders e.g. CNN Often pre trained on large databases such as ImageNetCheck out this review paperHow do we EvaluateBasic', 'For DMOL we only evaluated learned coordinate embeddings. 5.2. Conditioning on Image Class We represent the image classes as learned d dimensional embeddings per class and simply add the respective em bedding to the input representation of every input position together with the positional encodings. We trained the class conditioned Image Transformer on CIFAR 10 achieving very similar log likelihoods as in un conditioned generation.', 'Many applications of image density models require condi tioning on additional information of various kinds from im ages in enhancement or reconstruction tasks such as super resolution in painting and denoising to text when synthesiz ing images from natural language descriptions Mansimov et al. 2015 . In visual planning tasks conditional image generation models could predict future frames of video con ditioned on previous frames and taken actions.', 'Both models are able to generate convincing cars trucks and ships. Generated horses planes and birds also look reasonable. 5.1. Generative Image Modeling Our unconditioned and class conditioned image generation models both use 1D local attention with lq 256 and a total memory size of 512. On CIFAR 10 our best unconditional models achieve a perplexity of 2.90 bits dim on the test set using either DMOL or categorical.', 'generate text according to some specification Input X Structured Data English Document Utterance Image Speech Output Y Text NL Description Japanese Short Description Response Text Transcript Task NL Generation Translation Summarization Response Generation Image Captioning Speech RecognitionFormulation and ModelingCalculating the Probability of a SentenceConditional Language ModelsOne Type of Language Model LSTM LSTM LSTM LSTM predict hate predict this predict movie predict s LSTM Mikolov et al.', '4. Inference Across all of the presented experiments we use categorical sampling during decoding with a tempered softmax Dahl et al. 2017 . We adjust the concentration of the distribution we sample from with a temperature œÑ 0 by which we divide the logits for the channel intensities. We tuned œÑ between 0.8 and 1.0 observing the highest perceptual quality in unconditioned and class conditional image generation with œÑ 1.0.', 'Model Architecture 3.1. Image Representation We treat pixel intensities as either discrete categories or ordi nal values this setting depends on the distribution Section 3.4 . For categories each of the input pixels three color channels is encoded using a channel speciÔ¨Åc set of 256Image Transformer Input Gen Truth Input Gen Truth Table 2. On the left are image completions from our best conditional generation model where we sample the second half.', '2016 Vinyals et al. 2015 Xu et al. 2016Attention as visual semantic alignmentText to image Conditional image synthesis Generative adversarial nets Goodfellow et al. 2014 Source Google Source Google Reed et al. 2016Outline 1. Early models 2. Features and fusion 3. Contrastive models 4. Multimodal foundation models 5.']",text-embedding-3-small,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9918168868462132,1.0,0.9158231968815762,0.05
239,What challenges exist in evaluating conditioned language models?,"Some challenges in evaluating conditioned language models include poor sample efficiency during pre-training, the need for fine-tuning with reinforcement learning, and the limitation of adding additional modalities such as images for grounding and a better model of the world. Additionally, the evaluation setting of ""closed book"" where models directly answer questions without conditioning on auxiliary information poses a challenge in assessing the performance of conditioned language models.","['a phrase based translation model and a neural MT model Niehues et al. 2017 Stacking uses the output of one system in calculating features for another systemCase Studies in Conditional Language ModelingFrom Images e.g. Karpathy et al. 2015 Input is image features output is text Use standard image encoders e.g. CNN Often pre trained on large databases such as ImageNetCheck out this review paperHow do we EvaluateBasic', 'RRS20 recently demonstrated that a large language model can perform surprisingly well directly answering the questions without conditioning on auxilliary information. They denote this more restrictive evaluation setting as closed book . Their work suggests that even higher capacity models could perform even better and we test this hypothesis with GPT 3.', 'Promising future directions in this vein might include learning the objective function from humans ZSW 19a Ô¨Åne tuning with reinforcement learning or adding additional modalities such as images to provide grounding and a better model of the world CLY 19 . Another limitation broadly shared by language models is poor sample efÔ¨Åciency during pre training.', '2022a. Finetuned language models are zero shot learners. ICLR. 13Jason Wei Yi Tay Rishi Bommasani Colin Raffel Barret Zoph Sebastian Borgeaud Dani Yogatama Maarten Bosma Denny Zhou Donald Metzler et al. 2022b. Emergent abilities of large language models.', 'And yet small models still fail the ability to perform abstract manipulations on unseen symbols for these three tasks only arises at the scale of 100B model parameters. As for the OOD evaluations standard prompting fails for both tasks. With chain of thought prompting language models achieve upward scaling curves though performance is lower than in the in domain setting .', 'Scaling PaLM to 540B Ô¨Åxed a substantial portion of errors in all categories. There are also three notable points regarding why small language models fail. The Ô¨Årst observation is that small language models fail at even relatively easy symbol mapping tasks.', 'The assessment was that language models may not be worth investing signiÔ¨Åcant resources in because there has been no convincing demonstration that current language models are signiÔ¨Åcantly better than current methods for generating text and because methods for targeting or controlling the content of language models are still at a very early stage.', 'As Ô¨Åne tuned language models have neared human performance on many standard benchmark tasks considerable effort has been devoted to constructing more difÔ¨Åcult or open ended tasks including question answering KPR 19 IBGC 14 CCE 18 MCKS18 reading comprehension CHI 18 RCM19 and adversarially constructed datasets designed to be difÔ¨Åcult for existing language models SBBC19 NWD 19 .']",text-embedding-3-small,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9932392273138442,0.5,0.8562768066051475,0.0526315789473684
240,What is the instructor's name for this course?,"The instructor's name for the course ""Introduction to Computational Natural Language Processing"" is Hamidreza Mahyar.","['file name: NLP Winter 2024 Course Outline. Course Outline 1. COURSE INFORMATION Session Offered Winter 2024 Course Name Introduction to Computational Natural Language Processing Course Code SEP 775 Date s and Time s of lectures Lectures In person Wednesday 3 30 pm 6 30 pm Program Name System and Technology Calendar Description This graduate course introduces fundamental concepts in computational natural language processing NLP and their applications to teaching students how to deal with textual data in Artificial Intelligence. This course demonstrates how machines can learn different tasks in natural language such as language modeling text generation machine translation and language understanding. In this regard we go over the most promising methods in this literature and the most recent state of the art techniques. Moreover this course explores different real world applications of NLP and helps students get hands on experience in this field. Instructor Hamidreza Mahyar E Mail mahyarh@mcmaster.ca Office Hours upon request 2. COURSE SPECIFICS Course Description This course covers some fundamental concepts in natural language processing NLP and how they apply in real world use cases. Instruction Type Code Type Hours per term C Classroom instruction 39 L Laboratory workshop or fieldwork T Tutorial DE Distance education Total Hours 39 Resources ISBN Textbook Title Edition Author Publisher Natural Language Processing Jacob Eisenstein Speech and Language Processing Dan Jurafsky and James H. Martin Deep Learning Ian Goodfellow Yoshua Bengio and Aaron Courville Prerequisite s Proficiency in Python programming language Some Machine Learning and or Deep Learning knowledge is a plus Corequisite s Antirequisite s Course Specific Policies All work must be shown to get full credit. Specific policy of Projects Assignments submissionsProjects Assignments will be conducted online by using related software. Completed Projects Assignments should be uploaded to the drop box before midnight of the due date. Projects Assignments missed due to legitimate reasons must be completed later mutually agreed with the instructor. Departmental Policies The use of cell phones iPods laptops and other personal electronic devices are prohibited from the classroom during the class time unless the instructor makes an explicit exception. Announcements made in class or placed on Avenue are considered to have been communicated to all students including those individuals that are not in class. Instructor has the right to submit work to software to identify plagiarism.', 'file name: lecture07 Prompting Instruction Finetuning and RLHF . Hamidreza Mahyar mahyarh@mcmaster.ca Computational Natural Language Processing Prompting Instruction Finetuning and RLHFReminders Project proposals will be posted this week. A2 due Saturday 11 59PM We still recommend using Colab for the assignments in case you run into trouble e.g. you have exceeded Colab quota Please contact us 2Larger and larger models 3 https www.economist.com interactive briefing 2022 06 11 huge foundation models are turbo charging ai progress GPT 4 has 1.7 trillion parametersTrained on more and more data tokens seen during training https babylm.github.io 4 GPT 4 has seen 13 trillion tokensRecap What kinds of things does pretraining learn 5 Stanford University is located in California. Trivia I put fork down on the table. syntax The woman walked across the street checking for traffic over shoulder. coreference I went to the ocean to see the fish turtles seals and . lexical semantics topic Overall the value I got from the two hours watching it was the sum total of the popcorn and the drink. The movie was . sentiment Iroh went into the kitchen to make some tea. Standing next to Iroh Zuko pondered his destiny. Zuko left the .']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9156338799161428,1.0,0.9394654423691152,0.1923076923076923
241,What is the email address of the instructor for this course?,"The email address of the instructor for the course Computational Natural Language Processing is mahyarh@mcmaster.ca.
Source: lecture07_Prompting_Instruction_Finetuning_and_RLHF","['If you have any questions or concerns about such disclosure please discuss this with the course instructor. ONLINE PROCTORING Some courses may use online proctoring software for tests and exams. This software may require students to turn on their video camera present identification monitor and record their computer activities and or lock restrict their browser or other applications software during tests or exams. This software may be required to be installed before the test exam begins. COMMUNICATIONS It is the student s responsibility to Maintain current contact information with the University including address phone numbers and emergency contact information. Use the University provided e mail address or maintain a valid forwarding e mail address. Regularly check the official University communications channels. Official University communications are considered received if sent by postal mail by fax or by e mail to the student s designated primary e mail account via their @mcmaster.ca alias. Accept that forwarded e mails may be lost and that e mail is considered received if sent via the student s @mcmaster.ca alias. Check the McMaster Avenue email and course websites on a regular basis during the term. CONDUCT EXPECTATIONS As a McMaster student you have the right to experience and the responsibility to demonstrate respectful and dignified interactions within all of our living learning and working communities. These expectations are described in the Code of Student Rights Responsibilities the Code . All students share the responsibility of maintaining a positive environment for the academic and personal growth of all McMaster community members whether in person or online. It is essential that students be mindful of their interactions online as the Code remains in effect in virtual learning environments. The Code applies to any interactions that adversely affect disrupt or interfere with reasonable participation in University activities. Student disruptions or behaviours that interfere with university functions on online platforms e.g. use of Avenue 2 Learn WebEx or Zoom for delivery will be taken very seriously and will be investigated. Outcomes may include restriction or removal of the involved students access to these platforms. ACADEMIC ACCOMMODATION OF STUDENTS WITH DISABILITIES Students with disabilities who require academic accommodation must contact Student Accessibility Services SAS at 905 525 9140 ext. 28652 or sas@mcmaster.ca to make arrangements with a Program Coordinator.', 'file name: lecture07 Prompting Instruction Finetuning and RLHF . Hamidreza Mahyar mahyarh@mcmaster.ca Computational Natural Language Processing Prompting Instruction Finetuning and RLHFReminders Project proposals will be posted this week. A2 due Saturday 11 59PM We still recommend using Colab for the assignments in case you run into trouble e.g. you have exceeded Colab quota Please contact us 2Larger and larger models 3 https www.economist.com interactive briefing 2022 06 11 huge foundation models are turbo charging ai progress GPT 4 has 1.7 trillion parametersTrained on more and more data tokens seen during training https babylm.github.io 4 GPT 4 has seen 13 trillion tokensRecap What kinds of things does pretraining learn 5 Stanford University is located in California. Trivia I put fork down on the table. syntax The woman walked across the street checking for traffic over shoulder. coreference I went to the ocean to see the fish turtles seals and . lexical semantics topic Overall the value I got from the two hours watching it was the sum total of the popcorn and the drink. The movie was . sentiment Iroh went into the kitchen to make some tea. Standing next to Iroh Zuko pondered his destiny. Zuko left the .']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.927217552317146,1.0,0.8985133698664932,0.0344827586206896
242,When is this course held every week?,"The course ""Introduction to Computational Natural Language Processing"" (SEP 775) is held in person every Wednesday from 3:30 pm to 6:30 pm.","['Thus a pear would Ô¨Çoat. So the answer is no. Table 26 Few shot exemplars for full chain of thought prompt for Date Understanding. PROMPT FOR DATE UNDERSTANDING Q 2015 is coming in 36 hours. What is the date one week from today in MM DD YYYY A If 2015 is coming in 36 hours then it is coming in 2 days. 2 days before 01 01 2015 is 12 30 2014 so today is 12 30 2014. So one week from today will be 01 05 2015. So the answer is 01 05 2015. Q The Ô¨Årst day of 2019 is a Tuesday and today is the Ô¨Årst Monday of 2019. What is the date today in MM DD YYYY A If the Ô¨Årst day of 2019 was Tuesday then 01 01 2019 was a Tuesday. Today is the Ô¨Årst monday would be six days later. So today is 01 07 2019. So the answer is 01 07 2019. Q The concert was scheduled to be on 06 01 1943 but was delayed by one day to today. What is the date 10 days ago in MM DD YYYY A One day after 06 01 1943 is 06 02 1943 so today is 06 02 1943. 10 days before today is 05 23 1943. So the answer is 05 23 1943. Q It is 4 19 1969 today. What is the date 24 hours later in MM DD YYYY A Today is 04 19 1969. 24 hours later is one day after today which would be 04 20 1969. So the answer is 04 20 1969. Q Jane thought today is 3 11 2002 but today is in fact Mar 12 which is 1 day later.', 'file name: NLP Winter 2024 Course Outline. Course Outline 1. COURSE INFORMATION Session Offered Winter 2024 Course Name Introduction to Computational Natural Language Processing Course Code SEP 775 Date s and Time s of lectures Lectures In person Wednesday 3 30 pm 6 30 pm Program Name System and Technology Calendar Description This graduate course introduces fundamental concepts in computational natural language processing NLP and their applications to teaching students how to deal with textual data in Artificial Intelligence. This course demonstrates how machines can learn different tasks in natural language such as language modeling text generation machine translation and language understanding. In this regard we go over the most promising methods in this literature and the most recent state of the art techniques. Moreover this course explores different real world applications of NLP and helps students get hands on experience in this field. Instructor Hamidreza Mahyar E Mail mahyarh@mcmaster.ca Office Hours upon request 2. COURSE SPECIFICS Course Description This course covers some fundamental concepts in natural language processing NLP and how they apply in real world use cases. Instruction Type Code Type Hours per term C Classroom instruction 39 L Laboratory workshop or fieldwork T Tutorial DE Distance education Total Hours 39 Resources ISBN Textbook Title Edition Author Publisher Natural Language Processing Jacob Eisenstein Speech and Language Processing Dan Jurafsky and James H. Martin Deep Learning Ian Goodfellow Yoshua Bengio and Aaron Courville Prerequisite s Proficiency in Python programming language Some Machine Learning and or Deep Learning knowledge is a plus Corequisite s Antirequisite s Course Specific Policies All work must be shown to get full credit. Specific policy of Projects Assignments submissionsProjects Assignments will be conducted online by using related software. Completed Projects Assignments should be uploaded to the drop box before midnight of the due date. Projects Assignments missed due to legitimate reasons must be completed later mutually agreed with the instructor. Departmental Policies The use of cell phones iPods laptops and other personal electronic devices are prohibited from the classroom during the class time unless the instructor makes an explicit exception. Announcements made in class or placed on Avenue are considered to have been communicated to all students including those individuals that are not in class. Instructor has the right to submit work to software to identify plagiarism.']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.8459265441514411,1.0,0.8855559242760931,0.1081081081081081
243,What is the name of the TA for this course?,"The name of the TA for this course is the AI Teaching Assistant (AI TA). It is designed to provide an interactive platform for students to ask questions and receive guidance on course materials using NLP techniques to enhance the learning experience through dynamic interaction and content understanding. 

Source: SEP 775 - Computational Natural Language Processing.","['file name: SEP775 FinalProjects 2. McMaster University SEP 775 Final Projects Project 1 AI Teaching Assistant Project Overview The goal of this project is to create an AI Teaching Assistant AI TA . The AI TA aims to provide an interactive platform for students to ask questions and receive guidance on course materials mirroring the support traditionally offered by human teaching assistants. The project emphasizes the application of NLP techniques to create an educational tool that enhances the learning experience through dynamic interaction and content understanding. Project Objectives Automate FAQ Responses Develop the AI TA to autonomously address frequently asked questions related to course logistics assignments and content. Content Understanding and Query Handling Implement NLP strategies enabling the AI TA to interpret and respond to inquiries about the uploaded course material accurately. Dynamic Interaction Design the AI TA to interact with students conversationally providing information and resources pertinent to their queries. Technical Approach NLP Engine Components Language Model Integration Leverage pre trained NLP models e.g. BERT Llama for robust text understanding and generation tapping into their vast linguistic knowledge. Natural Language Understanding NLU Employ models like BERT for NLU tasks to accurately parse student inquiries determining their intent and extracting relevant details for precise responses. Retrieval Augmented Generation RAG Central to the NLP Engine RAG com bines retrieval methods to access information within a structured database of course materials with generative capabilities to formulate coherent and context aware respon ses . This approach ensures the AI TA delivers informative answers directly pulling from course content and synthesizing it into helpful responses. Response Generation Utilize the designed AI TA to generate responses that are not only accurate but also engaging and pedagogically effective. Interaction Interface Simplified Interface Development Construct an intuitive web or command line in terface that allows straightforward question submission and response retrieval ensuring ease of use for student interactions with the AI TA. McMaster University 1McMaster University SEP 775 Project 2 TV Show Script Generation Project Overview For this project you will harness the power of Natural Language Processing NLP and Machine Learning to create a script for a new episode of a TV show you enjoy. The challenge is to design a model that captures the essence style dialogue and storyline of the original series thereby generating a script that feels authentic and could seamlessly fit into the show.', '2015. Teaching ma chines to read and comprehend. In Advances in Neu ral Information Processing Systems NIPS . Daniel Hewlett Alexandre Lacoste Llion Jones Illia Polosukhin Andrew Fandrianto Jay Han Matthew Kelcey and David Berthelot. 2016. Wikireading A novel large scale language understanding task over wikipedia. In Association for Computational Lin guistics ACL . pages 1535 1545. Felix Hill Antoine Bordes Sumit Chopra and Jason Weston. 2016. The Goldilocks Principle Reading children s books with explicit memory representa tions. In International Conference on Learning Rep resentations ICLR . Minyoung Huh Pulkit Agrawal and Alexei A Efros. 2016. What makes ImageNet good for transfer learning arXiv preprint arXiv 1608.08614 . Mohit Iyyer Jordan L Boyd Graber Leonardo Max Batista Claudino Richard Socher and Hal Daum e III. 2014. A neural network for factoid ques tion answering over paragraphs. In Empirical Meth ods in Natural Language Processing EMNLP . pages 633 644. Rudolf Kadlec Ondrej Bajgar and Jan Kleindienst. 2016. From particular to general A preliminary case study of transfer learning in reading compre hension. Machine Intelligence Workshop NIPS . Diederik Kingma and Jimmy Ba. 2014. Adam A method for stochastic optimization. arXiv preprint arXiv 1412.6980 . Kenton Lee Tom Kwiatkowski Ankur Parikh and Di panjan Das. 2016. Learning recurrent span repre sentations for extractive question answering. arXiv preprint arXiv 1611.01436 . Christopher D Manning Mihai Surdeanu John Bauer Jenny Finkel Steven J Bethard and David Mc Closky. 2014. The stanford corenlp natural lan guage processing toolkit. In Association for Com putational Linguistics ACL . pages 55 60.']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.0000000000000002,1.0,0.8238646699748721,0.0192307692307692
244,What is Word2vec?,"Word2Vec is a popular model in natural language processing that is used to generate word embeddings. It is a probabilistic model that learns distributed representations of words in a continuous vector space based on the context in which words appear. The model is trained to predict the likelihood of a word appearing in the context of another word, capturing semantic relationships between words. Word2Vec has two main architectures: Continuous Bag of Words (CBOW) and Skip-gram. In Skip-gram Word2Vec, the model predicts the context words given a center word. By training on a large corpus of text data, Word2Vec can learn high-quality word embeddings that encode semantic information about words based on their usage in context.

Source: Lecture01_notes_Introduction.txt","['Skipgram word2vec. As usual we have a finite vocabulary V. Let C O be random variables representing an unknown pair of C V a center word and O V an outside word appearing in the context of the center word . We ll use c o to refer to specific values of the random variables. Let U R V d and V R V d. Note that each word in V is associated with a single row of U and one of V we think of this as resulting from an arbitrary ordering of V. The word2vec model is a probabilistic model specified as follows where uw refers to the row of U corresponding to word w V and likewise for V pU V o c exp u o vc w V exp u wvc 4 This may be familiar to you as the softmax function which takes arbitrary scores here one for each word in the vocabulary resulting from dot products and produces a probability distribution where larger scored things get higher probability. Note that the vector of probabilities over all words given a center word pU V c R V is a lot like a row of our old co occurrence matrix Xc. The story isn t over yet this is just a model. How do we estimate the values of the parameters U V We learn to minimize the cross entropy loss objective with the true distribution P O C min U V Eo c log pU V o c . 5 This equation should be read as minimize with respect to param eters U and V the expectation over values of o and c drawn from the distributions of O and V the negative log probability under the U V model of that value of o given that value of c . There is so much rich detail to get into here. How do we perform the min operation How do we get the random variables O and C Why the negative log of the probability Why is this so much better than co occurrence counting Can you tell why not all distributions over o given c can be represented by this model Should this be good Bad Surprising Obvious For now let s go through a few details about how to implement this in practice.', 'file name: SEP775 Assignment1. McMaster University SEP 775 Assignment 1 In Depth Word Vectors Analysis Total 50 Points Objectives This assignment focuses on a comprehensive understanding of word vector technologies specifically Word2Vec and GloVe. You will explore their applications visualize the results and analyze the semantic and syntactic relationships they capture. 1. Building and Analyzing Word Vectors with Word2Vec 20 Points Task Create word vectors using the Word2Vec model on a selected corpus. Visualization Utilize PCA or t SNE for visualizing these vectors in 2D. Analysis Discuss the word relationships and clusters formed. 2. GloVe Vectors Advanced Analysis 20 Points Task Analyze word relationships using pre trained GloVe vectors. Activity Conduct an analogy task e.g. king man woman with examples. Explanation Explain the results of the analogy tasks. 3. Semantic and Syntactic Word Relationships 10 Points Comparison Evaluate Word2Vec and GloVe for capturing semantic and syntactic relationships. Illustration Use specific word pairs or groups to demonstrate differences in repre sentation by each model. McMaster University 1McMaster University SEP 775 Resources for Word2Vec and GloVe Word2Vec Pre trained Word2Vec embeddings can be found on repositories such as HuggingFace or directly using Gensim. GloVe Pre trained GloVe embeddings are available at the GloVe website or through HuggingFace. Submission Guidelines Submit all code in a ZIP file including Jupyter Notebooks and a detailed PDF report with written explanations and visualizations. Clearly label each part and question in your submissions. Deadline Feb. 7 2024 Rubric and Expectations Code Quality and Functionality 40 Code should be well organized commented and functioning as intended. The use of Python and relevant libraries e.g. Gensim for Word2Vec should demonstrate a good grasp of the tools. Analysis and Interpretation 30 Written explanations should be insightful de monstrating a deep understanding of the word vector models. Analysis of visualizations analogy tasks and model comparisons should be thorough and reflective. Visualization 20 Visualizations should be clear accurately labeled and effectively convey the relationships or patterns discovered in the data. Adherence to Guidelines 10 Submissions should follow the provided guidelines including format labeling and adherence to the deadline. McMaster University 2']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9911034732306482,1.0,0.9132327809560185,0.027027027027027
245,What are the main learning goals of the course?,"The main learning goals of the course include experimenting with different values of K in language models, using natural language prompts, comparing LM likelihoods for completions, normalizing by unconditional probability, framing tasks for binary classification, utilizing beam search for free form completion tasks, scoring models using F1 similarity score, BLEU, or exact match, reporting final results on test sets when available, and displaying training curves for models.","['Larger values of K are usually but not always better so when a separate development and test set are available we experiment with a few values of K on the development set and then run the best value on the test set. For some tasks see Appendix G we also use a natural language prompt in addition to or for K 0 instead of demonstrations. On tasks that involve choosing one correct completion from several options multiple choice we provide K examples of context plus correct completion followed by one example of context only and compare the LM likelihood of each completion. For most tasks we compare the per token likelihood to normalize for length however on a small number of datasets ARC OpenBookQA and RACE we gain additional beneÔ¨Åt as measured on the development set by normalizing by the unconditional probability of each completion by computing P completion context P completion answer context where answer context is the string Answer or A and is used to prompt that the completion should be an answer but is otherwise generic. On tasks that involve binary classiÔ¨Åcation we give the options more semantically meaningful names e.g. True or False rather than 0 or 1 and then treat the task like multiple choice we also sometimes frame the task similar to what is done by RSR 19 see Appendix G for details. On tasks with free form completion we use beam search with the same parameters as RSR 19 a beam width of 4 and a length penalty of Œ± 0.6. We score the model using F1 similarity score BLEU or exact match depending on what is standard for the dataset at hand. Final results are reported on the test set when publicly available for each model size and learning setting zero one and few shot . When the test set is private our model is often too large to Ô¨Åt on the test server so we report results on the development set. We do submit to the test server on a small number of datasets SuperGLUE TriviaQA PiQa where we were able to make submission work and we submit only the 200B few shot results and report development set results for everything else. 3 Results In Figure 3.1 we display training curves for the 8 models described in Section 2.', '. . . . . . . . . . . . . . . . . . . . 10 3 Results 10 3.1 Language Modeling Cloze and Completion Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 3.2 Closed Book Question Answering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 3.3 Translation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 3.4 Winograd Style Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 3.5 Common Sense Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999999999999996,0.8571428571428571,0.8653942373961093,0.0
246,How is the course grade distributed?,"The course grade is distributed as follows:
- 4 Assignments: 20%
- 4 Quizzes: 10%
- Midterm Test: 35%
- Group Project: 35%
Total: 100%

Source: SEP 775 - Computational Natural Language Processing, Course Outline.","['Instructor has the right to submit work to software to identify plagiarism. 3. SUB TOPIC S Week 1 Overview Human language Word meanings Word2Vec and Optimization basics Week 2 Intro to neural networks Gradients and backpropagation Week 3 Recurrent Neural Networks and Language Models Week 4 Conditioned Generation Week 5 Part 1 Self attention and transformers Part 2 Seq2Seq models Week 6 Midterm Test Week 7 Midterm Recess No class Week 8 Prompting Instruction Finetuning and RLHF Week 9 Question Answering systems Week 10 Multimodal models Week 11 Project presentation and Q A Week 12 Project presentation and Q A Week 13 Project presentation and Q A Classes end April 10th 4. ASSESSMENT OF LEARNING Weight 4 Assignments 20 4 Quizzes 10 Midterm Test 35 Group Project 35 TOTAL 100Note that the percentage distribution of each module may be adjusted. Percentage grades will be converted to letter grades and grade points per the University calendar. 5. LEARNING OUTCOMES By the end of this course the students will know about Introduction to Deep Learning Word representations Language models Pre trained language models Text generation Seq2Seq models Prompting Feedback systems Multimodal models NLP applications e.g. Q A systems 6. COURSE OUTLINE APPROVED ADVISORY STATEMENTS ANTI DISCRIMINATION The Faculty of Engineering is concerned with ensuring an environment that is free of all discrimination. If there is a problem individuals are reminded that they should contact the Associate Director Graduate Studies the Sexual Harassment Officer or the Human Rights Consultant as soon as possible. http www.mcmaster.ca policy General HR Discrimination_Harassment_Sexual_Harassment Prevention Response.pdf ACADEMIC INTEGRITY You are expected to exhibit honesty and use ethical behaviour in all aspects of the learning process. Academic credentials you earn are rooted in principles of honesty and academic integrity. It is your responsibility to understand what constitutes academic dishonesty. Academic dishonesty is to knowingly act or fail to act in a way that results or could result in unearned academic credit or advantage. This behaviour can result in serious consequences e.g.', 'In comparison DPR is able to return Top 1 Top 5 Top 20 Top 100 Gold 44.9 66.8 78.1 85.0 Dist. Sup. 43.9 65.3 77.1 84.4 Table 5 Retrieval accuracy on the development set of Natural Questions trained on passages that match the gold context Gold or the top BM25 passage that con tains the answer Dist. Sup. . Sim Loss Retrieval Accuracy Top 1 Top 5 Top 20 Top 100 DP NLL 44.9 66.8 78.1 85.0 Triplet 41.6 65.0 77.2 84.5 L2 NLL 43.5 64.7 76.1 83.1 Triplet 42.2 66.0 78.1 84.9 Table 6 Retrieval Top k accuracy on the development set of Natural Questions using different similarity and loss functions. the correct answer presumably by matching body of water with semantic neighbors such as sea and channel even though no lexical overlap exists. The second example is one where BM25 does better. The salient phrase Thoros of Myr is critical and DPR is unable to capture it. D Joint Training of Retriever and Reader We Ô¨Åx the passage encoder in our joint training scheme while allowing only the question encoder to receive backpropagation signal from the com bined retriever reader loss function. This allows us to leverage the HNSW based FAISS index for efÔ¨Åcient low latency retrieving without reindexing the passages during model updates. Our loss func tion largely follows ORQA s approach which uses log probabilities of positive passages selected from the retriever model and correct spans and passages selected from the reader model. Since the passage encoder is Ô¨Åxed we could use larger amount of retrieved passages when calculating the retriever loss. SpeciÔ¨Åcally we get top 100 passages for each question in a mini batch and use the method similar to in batch negative training all retrieved passages vectors participate in the loss calculation for all questions in a batch.']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999986660437328,1.0,0.9436485716155828,0.0
247,What is the policy for late submissions in this course?,The policy for late submissions in this course is not explicitly mentioned in the provided context information. It is recommended to refer to the course syllabus or contact the instructor directly for specific details regarding late submissions.,"['28652 or sas@mcmaster.ca to make arrangements with a Program Coordinator. For further information consult McMaster University s Academic Accommodation of Students with Disabilities policy. ACADEMIC ACCOMMODATION FOR RELIGIOUS INDIGENOUS OR SPIRITUAL OBSERVANCES RISOStudents requiring academic accommodation based on religious indigenous or spiritual observances should follow the procedures set out in the RISO policy. Students should submit their request to their Faculty Office normally within 10 working days of the beginning of term in which they anticipate a need for accommodation or to the Registrar s Office prior to their examinations. Students should also contact their instructors as soon as possible to make alternative arrangements for classes assignments and tests. http www.mcmaster.ca policy Students AcademicStudies Studentcode.pdf COPYRIGHT AND RECORDING Students are advised that lectures demonstrations performances and any other course material provided by an instructor include copyright protected works. The Copyright Act and copyright law protect every original literary dramatic musical and artistic work including lectures by University instructors The recording of lectures tutorials or other methods of instruction may occur during a course. Recording may be done by either the instructor for the purpose of authorized distribution or by a student for the purpose of personal study. Students should be aware that their voice and or image may be recorded by others during the class. Please speak with the instructor if this is a concern for you. EXTREME CIRCUMSTANCES The University reserves the right to change the dates and deadlines for any or all courses in extreme circumstances e.g. severe weather labour disruptions etc. . Changes will be communicated through regular McMaster communication channels such as McMaster Daily News A2L and or McMaster email.', 'This behaviour can result in serious consequences e.g. the grade of zero on an assignment loss of credit with a notation on the transcript notation reads Grade of F assigned for academic dishonesty and or suspension or expulsion from the university. For information on the various types of academic dishonesty please refer to the Academic Integrity Policy located at https secretariat.mcmaster.ca university policies procedures guidelines The following illustrates only three forms of academic dishonesty plagiarism e.g. the submission of work that is not one s own or for which other credit has been obtained. improper collaboration in group work. copying or using unauthorized aids in tests and examinations. AUTHENTICITY PLAGIARISM DETECTION Some courses may use a web based service Turnitin.com to reveal authenticity and ownership of student submitted work. For courses using such software students will be expected to submit their work electronically either directly to Turnitin.com or via an online learning platform e.g. A2L etc. using plagiarism detection a service supported by Turnitin.com so it can be checked for academic dishonesty. Students who do not wish their work to be submitted through the plagiarism detection software must inform the Instructor before the assignment is due. No penalty will be assigned to a student who does not submit work to the plagiarism detection software. All submitted work is subject to normal verification that standards of academicintegrity have been upheld e.g. on line search other software etc. . For more details about McMaster s use of Turnitin.com please go to www.mcmaster.ca academicintegrity. COURSES WITH AN ON LINE ELEMENT Some courses may use on line elements e.g. e mail Avenue to Learn A2L LearnLink web pages capa Moodle ThinkingCap etc. . Students should be aware that when they access the electronic components of a course using these elements private information such as first and last names user names for the McMaster e mail accounts and program affiliation may become apparent to all other students in the same course. The available information is dependent on the technology used. Continuation in a course that uses on line elements will be deemed consent to this disclosure. If you have any questions or concerns about such disclosure please discuss this with the course instructor.']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.0,1.0,0.8690122643136096,0.0
248,What does the course hope to teach regarding word vectors?,"The course hopes to teach the foundations of effective modern methods for deep learning applied to NLP, including basics and key methods used in NLP in 2024 such as word vectors, feed-forward networks, recurrent networks, attention, encoder-decoder models, transformers, and large pre-trained language models. Additionally, the course aims to provide a big picture understanding of human languages and the challenges in understanding and producing them via computers. Lastly, students are expected to gain an understanding of and ability to build systems in PyTorch for major NLP problems like word meaning, dependency parsing, machine translation, and question answering.","['file name: lecture01 wordvecs1. Hamidreza Mahyar mahyarh@mcmaster.ca Computational Natural Language Processing Overview of NLP and Word VectorsLecture Plan 2 Lecture 1 Introduction and Word Vectors 1. The course 2. Human language and word meaning 3. Word2vec introduction 4. Word2vec objective function gradients 5. Optimization basics 6. Looking at word vectors Key learning today The astounding result that word meaning can be represented rather well by a high dimensional vector of real numbersCourse logistics in brief 3 Instructor Hamidreza Mahyar Head TA Reza Namazi Time Wednesday 3 30pm 6 30pm We put a lot of other important information on Avenue to Learn. Please read itWhat do we hope to teach A.k.a. learning goals 4 1. The foundations of the effective modern methods for deep learning applied to NLP Basics first then key methods used in NLP in 2024 Word vectors feed forward networks recurrent networks attention encoder decoder models transformers large pre trained language models etc. 2. A big picture understanding of human languages and the difficulties in understanding and producing them via computers 3. An understanding of and ability to build systems in PyTorch for some of the major problems in NLP Word meaning dependency parsing machine translation question answeringCourse work and grading policy 5 4 Assignments 20 4 Quizzes 10 Midterm 35 Final Group Project 1 3 people 34 Late day policy NO free late days 1 off course grade per day late Assignments not accepted more than 3 days late unless given permission in advance Collaboration policy Understand allowed collaboration and how to document it Don t take code off the web acknowledge working with other students write your own assignment solutionsTrained on text data neural machine translation is quite good https kiswahili.tuko.co.keThe SEC said Musk your tweets are a blight. They really could cost you your job if you don t stop all this tweeting at night. Then Musk cried Why The tweets I wrote are not mean I don t use all caps and I m sure that my tweets are clean. But your tweets can move markets and that s why we re sore.', 'Word2vec Overview 21 Word2vec Mikolov et al. 2013 is a framework for learning word vectors Idea We have a large corpus body of text a long list of words Every word in a fixed vocabulary is represented by a vector Go through each position t in the text which has a center word c and context outside words o Use the similarity of the word vectors for c and o to calculate the probability of o given c or vice versa Keep adjusting the word vectors to maximize this probabilityWord2Vec Overview Example windows and process for computing ùëÉùë§ùë° ùëó ùë§ùë° crises banking into turning problems as center word at position t outside context words in window of size 2 outside context words in window of size 2 ùëÉùë§ùë° 1 ùë§ùë° 22 ùëÉùë§ùë° 2 ùë§ùë° ùëÉùë§ùë° 1 ùë§ùë° ùëÉùë§ùë° 2 ùë§ùë°Word2Vec Overview Example windows and process for computing ùëÉùë§ùë° ùëó ùë§ùë° crises banking into turning problems as center word at position t outside context words in window of size 2 outside context words in window of size 2 ùëÉùë§ùë° 2 ùë§ùë° ùëÉùë§ùë° 1 ùë§ùë° 23 ùëÉùë§ùë° 2 ùë§ùë° ùëÉùë§ùë° 1 ùë§ùë°Word2vec objective function For each position ùë° 1 ùëá predict context words within a window of fixed size m given center word ùë§ùë°. Data likelihood The objective function ùêΩùúÉis the average negative log likelihood Minimizing objective function Maximizing predictive accuracy sometimes called a cost or loss function 24Word2vec objective function We want to minimize the objective function Question How to calculate ùëÉùë§ùë° ùëó ùë§ùë° ùúÉ Answer We will use two vectors per word w']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.8935645846756067,1.0,0.8957451172873727,0.0588235294117647
249,How does Word2vec represent word meaning?,"Word2vec represents each word in a fixed vocabulary as a low-dimensional vector, much smaller than the vocabulary size. It learns the value of each word's vector to be predictive based on the distribution of words in a usually short context (2-4 words). This model, specifically the skipgram Word2vec algorithm, aims to capture semantic relationships between words by training on a large corpus of text data to learn word embeddings that represent the meaning of words based on their context in the text.","['Here s an example where a few relevant windows for co occurrence are labeled Larger notions of co occurrence e.g. large windows or documents lead to more semantic or even topic encoding representations shorter windows lead to more syntax encoding representa tions It s hot and delicious. I poured the tea z center word for 1 my uncle 3. document In brief shorter windows like the one word window above labeled 1 seem to encode syntactic properties. For example nouns tend to appear right next to the or is. Plural nouns don t appear right next to a. Larger windows tend to encode more semantic and at extremes topic like properties. Note how poured or delicious may occur farther from tea but still be relevant. Document level windows for large documents 1000s of words intuitively represent words by what kinds of documents they appear in sports law medicine etc. Another design decision we made was to represent explicit counts of words in V sized vectors. This ends up being a big mistake. We ve already stated that high dimensional vectors tend to be un wieldy in today s neural systems. But another issue is that raw counts of words end up over emphasizing the importance of very common words like the. Taking the log token frequency ends up being much more useful. A very influential paper on word repre sentation taught us much more about what is wrong with the raw co occurrence method by introducing GloVe Pennington et al. 2014 a co occurence based word representation algorithm that works as well as word2vec the method we ll introduce in the next section. However many of the details of word2vec will hold true in methods that we ll proceed to further in the course so we ll focus our time on that. 3.2 Word2vec model and objective The word2vec model represents each word in a fixed vocabulary as a low dimensional much smaller than vocabulary size vector. It learns the value of each word s vector to be predictive via a simple function of the distribution of words in a usually short 2 4 words context. The model we ll describe here is called the skipgram word2vec algo rithm. Skipgram word2vec.', 'Skipgram word2vec. As usual we have a finite vocabulary V. Let C O be random variables representing an unknown pair of C V a center word and O V an outside word appearing in the context of the center word . We ll use c o to refer to specific values of the random variables. Let U R V d and V R V d. Note that each word in V is associated with a single row of U and one of V we think of this as resulting from an arbitrary ordering of V. The word2vec model is a probabilistic model specified as follows where uw refers to the row of U corresponding to word w V and likewise for V pU V o c exp u o vc w V exp u wvc 4 This may be familiar to you as the softmax function which takes arbitrary scores here one for each word in the vocabulary resulting from dot products and produces a probability distribution where larger scored things get higher probability. Note that the vector of probabilities over all words given a center word pU V c R V is a lot like a row of our old co occurrence matrix Xc. The story isn t over yet this is just a model. How do we estimate the values of the parameters U V We learn to minimize the cross entropy loss objective with the true distribution P O C min U V Eo c log pU V o c . 5 This equation should be read as minimize with respect to param eters U and V the expectation over values of o and c drawn from the distributions of O and V the negative log probability under the U V model of that value of o given that value of c . There is so much rich detail to get into here. How do we perform the min operation How do we get the random variables O and C Why the negative log of the probability Why is this so much better than co occurrence counting Can you tell why not all distributions over o given c can be represented by this model Should this be good Bad Surprising Obvious For now let s go through a few details about how to implement this in practice.']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9396030810584962,1.0,0.913031313106276,0.03125
250,What is the main focus of this lecture on neural networks?,"The main focus of this lecture on neural networks is to introduce single and multilayer neural networks and how they can be used for classification purposes, as well as discussing how they can be trained using backpropagation, which is a distributed gradient descent technique. The lecture also covers topics such as neuron units, non-linearities, gradient checks, Xavier parameter initialization, learning rates, and Adagrad. Additionally, the lecture touches on the motivation for using recurrent neural networks as a language model.","['file name: lecture02 notes Backpropagation.  Neural networks. Forward computation. Backward propagation. Neuron Units. Max margin Loss. Gradient checks. Xavier parameter initialization. Learning rates. Adagrad. This set of notes introduces single and multilayer neural networks and how they can be used for classiÔ¨Åcation purposes. We then dis cuss how they can be trained using a distributed gradient descent technique known as backpropagation. We will see how the chain rule can be used to make parameter updates sequentially. After a rigorous mathematical discussion of neural networks we will discuss some practical tips and tricks in training neural networks involving neuron units non linearities gradient checks Xavier parameter ini tialization learning rates Adagrad etc. Lastly we will motivate the use of recurrent neural networks as a language model. 1 Neural Networks Foundations Figure 1 We see here how a non linear decision boundary separates the data very well. This is the prowess of neural networks. Fun Fact Neural networks are biologically in spired classiÔ¨Åers which is why they are often called artiÔ¨Åcial neural networks to distinguish them from the organic kind. However in reality human neural networks are so much more capable and complex from artiÔ¨Åcial neural net works that it is usually better to not draw too many parallels between the two. We established in our previous discussions the need for non linear classiÔ¨Åers since most data are not linearly separable and thus our classiÔ¨Åcation performance on them is limited. Neural networks are a family of classiÔ¨Åers with non linear decision boundary as seen in Figure 1. Now that we know the sort of decision boundaries neural networks create let us see how they manage doing so. 1.1 A Neuron A neuron is a generic computational unit that takes n inputs and produces a single output. What differentiates the outputs of different neurons is their parameters also referred to as their weights . One of the most popular choices for neurons is the sigmoid or binary lo gistic regression unit.', 'One of the most popular choices for neurons is the sigmoid or binary lo gistic regression unit. This unit takes an n dimensional input vector x and produces the scalar activation output a. This neuron is also associated with an n dimensional weight vector w and a bias scalar b. The output of this neuron is then a 1 1 exp wTx b We can also combine the weights and bias term above to equiva Neuron A neuron is the fundamental building block of neural networks. We will see that a neuron can be one of many functions that allows for non linearities to accrue in the network. lently formulate a 1 1 exp wT b x 1 Figure 2 This image captures how in a sigmoid neuron the input vector x is Ô¨Årst scaled summed added to a bias unit and then passed to the squashing sigmoid function. This formulation can be visualized in the manner shown in Fig ure 2. 1.2 A Single Layer of Neurons We extend the idea above to multiple neurons by considering the case where the input x is fed as an input to multiple such neurons as shown in Figure 3. If we refer to the different neurons weights as w 1 w m and the biases as b1 bm we can say the respective activations are a1 am a1 1 1 exp w 1 Tx b1 . . . am 1 1 exp w m Tx bm Let us deÔ¨Åne the following abstractions to keep the notation simple and useful for more complex networks œÉ z 1 1 exp z1 . . . 1 1 exp zm b b1 . . . bm Rm W w 1 T w m T Rm n We can now write the output of scaling and biases as Figure 3 This image captures how multiple sigmoid units are stacked on the right all of which receive the same input x. z Wx b The activations of the sigmoid function can then be written as a 1 . . . a m œÉ z œÉ Wx b So what do these activations really tell us Well one can think of these activations as indicators of the presence of some weighted combination of features. We can then use a combination of these activations to perform classiÔ¨Åcation tasks.']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.0000000000000002,1.0,0.9272416794937224,0.0303030303030303
251,What are the non-linear functions mentioned for neural networks?,The non-linear functions mentioned for neural networks include sigmoid or binary logistic regression units.,"['file name: lecture02 notes Backpropagation.  Neural networks. Forward computation. Backward propagation. Neuron Units. Max margin Loss. Gradient checks. Xavier parameter initialization. Learning rates. Adagrad. This set of notes introduces single and multilayer neural networks and how they can be used for classiÔ¨Åcation purposes. We then dis cuss how they can be trained using a distributed gradient descent technique known as backpropagation. We will see how the chain rule can be used to make parameter updates sequentially. After a rigorous mathematical discussion of neural networks we will discuss some practical tips and tricks in training neural networks involving neuron units non linearities gradient checks Xavier parameter ini tialization learning rates Adagrad etc. Lastly we will motivate the use of recurrent neural networks as a language model. 1 Neural Networks Foundations Figure 1 We see here how a non linear decision boundary separates the data very well. This is the prowess of neural networks. Fun Fact Neural networks are biologically in spired classiÔ¨Åers which is why they are often called artiÔ¨Åcial neural networks to distinguish them from the organic kind. However in reality human neural networks are so much more capable and complex from artiÔ¨Åcial neural net works that it is usually better to not draw too many parallels between the two. We established in our previous discussions the need for non linear classiÔ¨Åers since most data are not linearly separable and thus our classiÔ¨Åcation performance on them is limited. Neural networks are a family of classiÔ¨Åers with non linear decision boundary as seen in Figure 1. Now that we know the sort of decision boundaries neural networks create let us see how they manage doing so. 1.1 A Neuron A neuron is a generic computational unit that takes n inputs and produces a single output. What differentiates the outputs of different neurons is their parameters also referred to as their weights . One of the most popular choices for neurons is the sigmoid or binary lo gistic regression unit.', 'f on previous slide Why they re needed Neural networks do function approximation e.g. regression or classification Without non linearities deep neural networks can t do anything more than a linear transform Extra layers could just be compiled down into a single linear transform W1 W2 x Wx But with more layers that include non linearities they can approximate any complex function 5Training with cross entropy loss you use this in PyTorch Until now our objective was stated as to maximize the probability of the correct class y or equivalently we can minimize the negative log probability of that class Now restated in terms of cross entropy a concept from information theory Let the true probability distribution be p let our computed model probability be q The cross entropy is Assuming a ground truth or true or gold or target probability distribution that is 1 at the right class and 0 everywhere else p 0 0 1 0 0 then Because of one hot p the only term left is the negative log probability of the true class yi log ùëù ùë¶i ùë•i Cross entropy can be used in other ways with a more interesting p but for now just know that you ll want to use it as the loss in PyTorch 6Remember Stochastic Gradient Descent Update equation i.e. for each parameter In deep learning ùúÉincludes the data representation e.g. word vectors too How can we compute ùúÉùêΩ ùúÉ 1. By hand 2. Algorithmically the backpropagation algorithm ùõº step size or learning rate 7Lecture Plan 8 Gradients 1. Introduction 2. Matrix calculus 3.']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9911262334187996,1.0,0.9296729030172588,0.04
252,What is the purpose of non-linearities in neural networks?,"The purpose of non-linearities in neural networks is to enable the network to learn and represent complex, non-linear relationships in the data, allowing for more sophisticated decision boundaries and improving the network's ability to model and classify data accurately. Non-linearities introduce flexibility and expressiveness to neural networks, enabling them to capture intricate patterns and dependencies in the data that linear models cannot handle effectively.

Source: SEP 775 - Computational Natural Language Processing, lecture02_notes_Backpropagation.txt","['file name: lecture02 notes Backpropagation.  Neural networks. Forward computation. Backward propagation. Neuron Units. Max margin Loss. Gradient checks. Xavier parameter initialization. Learning rates. Adagrad. This set of notes introduces single and multilayer neural networks and how they can be used for classiÔ¨Åcation purposes. We then dis cuss how they can be trained using a distributed gradient descent technique known as backpropagation. We will see how the chain rule can be used to make parameter updates sequentially. After a rigorous mathematical discussion of neural networks we will discuss some practical tips and tricks in training neural networks involving neuron units non linearities gradient checks Xavier parameter ini tialization learning rates Adagrad etc. Lastly we will motivate the use of recurrent neural networks as a language model. 1 Neural Networks Foundations Figure 1 We see here how a non linear decision boundary separates the data very well. This is the prowess of neural networks. Fun Fact Neural networks are biologically in spired classiÔ¨Åers which is why they are often called artiÔ¨Åcial neural networks to distinguish them from the organic kind. However in reality human neural networks are so much more capable and complex from artiÔ¨Åcial neural net works that it is usually better to not draw too many parallels between the two. We established in our previous discussions the need for non linear classiÔ¨Åers since most data are not linearly separable and thus our classiÔ¨Åcation performance on them is limited. Neural networks are a family of classiÔ¨Åers with non linear decision boundary as seen in Figure 1. Now that we know the sort of decision boundaries neural networks create let us see how they manage doing so. 1.1 A Neuron A neuron is a generic computational unit that takes n inputs and produces a single output. What differentiates the outputs of different neurons is their parameters also referred to as their weights . One of the most popular choices for neurons is the sigmoid or binary lo gistic regression unit.', 'In order to modify the previously discussed Softmax model to use windows of words for classiÔ¨Åcation we would simply substitute x i with x i window in the following manner x i window x i 2 x i 1 x i x i 1 x i 2 As a result when we evaluate the gradient of the loss with respect to the words we will receive gradients for the word vectors Œ¥window x i 2 x i 1 x i x i 1 x i 2 The gradient will of course need to be distributed to update the corresponding word vectors in implementation. 3.5 Non linear ClassiÔ¨Åers Figure 9 Here we see that many exam ples are wrongly classiÔ¨Åed even though the best linear decision boundary is chosen. This is due linear decision boundaries have limited model capacity for this dataset. We now introduce the need for non linear classiÔ¨Åcation models such as neural networks. We see in Figure 9 that a linear classiÔ¨Åer mis classiÔ¨Åes many datapoints. Using a non linear decision boundary as shown in Figure 10 we manage to classify all training points accu rately. Although oversimpliÔ¨Åed this is a classic case demonstrating the need for non linear decision boundaries. In the next set of notes we study neural networks as a class of non linear models that have performed particularly well in deep learning applications. Figure 10 Here we see that the non linear decision boundary allows for much better classiÔ¨Åcation of datapoints.']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.0000000000000004,1.0,0.9116414329668128,0.0294117647058823
253,What is cross entropy loss used for in PyTorch?,"Cross entropy loss is used in PyTorch to minimize the negative log probability of the correct class or target class in a classification task. It is a concept from information theory and is used as the loss function in PyTorch for training neural networks. By using cross entropy loss, the model aims to maximize the probability of the correct class during training.

Source: Lecture02_Neural_Nets.txt","['f on previous slide Why they re needed Neural networks do function approximation e.g. regression or classification Without non linearities deep neural networks can t do anything more than a linear transform Extra layers could just be compiled down into a single linear transform W1 W2 x Wx But with more layers that include non linearities they can approximate any complex function 5Training with cross entropy loss you use this in PyTorch Until now our objective was stated as to maximize the probability of the correct class y or equivalently we can minimize the negative log probability of that class Now restated in terms of cross entropy a concept from information theory Let the true probability distribution be p let our computed model probability be q The cross entropy is Assuming a ground truth or true or gold or target probability distribution that is 1 at the right class and 0 everywhere else p 0 0 1 0 0 then Because of one hot p the only term left is the negative log probability of the true class yi log ùëù ùë¶i ùë•i Cross entropy can be used in other ways with a more interesting p but for now just know that you ll want to use it as the loss in PyTorch 6Remember Stochastic Gradient Descent Update equation i.e. for each parameter In deep learning ùúÉincludes the data representation e.g. word vectors too How can we compute ùúÉùêΩ ùúÉ 1. By hand 2. Algorithmically the backpropagation algorithm ùõº step size or learning rate 7Lecture Plan 8 Gradients 1. Introduction 2. Matrix calculus 3.', 'The notation in this image is slightly different here the equivalent of W hh is Wh W hx is We and W S is U. E converts word inputs x t to word embeddings e t . The Ô¨Ånal softmax over the vocabulary shows us the probability of various options for token x 5 condi tioned on all previous tokens. The input could be much longer than 4 5 tokens. 2.1 RNN Loss and Perplexity The loss function used in RNNs is often the cross entropy error in troduced in earlier notes. Equation 7 shows this function as the sum over the entire vocabulary at time step t. J t Œ∏ V j 1 yt j log ÀÜ yt j 7 The cross entropy error over a corpus of size T is J 1 T T t 1 J t Œ∏ 1 T T t 1 V j 1 yt j log ÀÜ yt j 8 Equation 9 is called the perplexity relationship it is basically 2 to the power of the negative log probability of the cross entropy error function shown in Equation 8. Perplexity is a measure of confusion where lower values imply more conÔ¨Ådence in predicting the next word in the sequence compared to the ground truth outcome . Perplexity 2J 9 2.2 Advantages Disadvantages and Applications of RNNs RNNs have several advantages 1. They can process input sequences of any length 2. The model size does not increase for longer input sequence lengths 3. Computation for step t can in theory use information from many steps back. 4. The same weights are applied to every timestep of the input so there is symmetry in how inputs are processed However they also have some disadvantages 1. Computation is slow because it is sequential it cannot be paral lelized 2. In practice it is difÔ¨Åcult to access information from many steps back due to problems like vanishing and exploding gradients which we discuss in the following subsection The amount of memory required to run a layer of RNN is pro portional to the number of words in the corpus.']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999999999999988,0.75,0.9730814997847116,0.1
254,What is the significance of the backpropagation algorithm?,"The significance of the backpropagation algorithm lies in its ability to efficiently compute gradients in neural networks by recursively applying the chain rule along the computation graph. This allows for the calculation of how changing the parameters in the network affects the final output, enabling the model to learn and improve through adjusting its weights based on the error propagated backwards from the output layer to the input layer. Understanding backpropagation is crucial for debugging and improving models in deep learning. It helps in identifying issues such as exploding or vanishing gradients, which can impact the training process and the overall performance of the neural network.

Source: Lecture02_Neural_Nets.txt","['Thus W 1 W and W 2 UT using notation from Section 1.3. Let us begin Suppose the cost J 1 sc s is positive and we want to perform the update of parameter W 1 14 in Figure 5 and Figure 6 we must realize that W 1 14 only contributes to z 2 1 and thus a 2 1 . This fact is crucial to understanding backpropagation backpropagated gradients are only affected by values they contribute to. a 2 1 is consequently used in the forward computation of score by multiplication with W 2 1 . We can see from the max margin loss that J s J sc 1 Therefore we will work with s W 1 ij here for simplicity. Thus s W 1 ij W 2 a 2 W 1 ij W 2 i a 2 i W 1 ij W 2 i a 2 i W 1 ij W 2 i a 2 i W 1 ij W 2 i a 2 i z 2 i z 2 i W 1 ij W 2 i f z 2 i z 2 i z 2 i W 1 ij W 2 i f z 2 i z 2 i W 1 ij W 2 i f z 2 i W 1 ij b 1 i a 1 1 W 1 i1 a 1 2 W 1 i2 a 1 3 W 1 i3 a 1 4 W 1 i4 W 2 i f z 2 i W 1 ij b 1 i k a 1 k W 1 ik W 2 i f z 2 i a 1 j Œ¥ 2 i a 1 j We see above that the gradient reduces to the product Œ¥ 2 i a 1 j where Œ¥ 2 i is essentially the error propagating backwards from the i th neu ron in layer 2. a 1 j is an input fed to i th neuron in layer 2 when scaled by Wij.', 'do backpropagation for you but mainly leave layer node writer to hand calculate the local derivative 74Backprop Implementations 75Implementation forward backward API 76Implementation forward backward API 77Manual Gradient checking Numeric Gradient For small h 1e 4 Easy to implement correctly But approximate and very slow You have to recompute f for every parameter of our model Useful for checking your implementation In the old days we hand wrote everything doing this everywhere was the key test Now much less needed you can use it to check layers are correctly implemented 78Summary We ve mastered the core technology of neural nets Backpropagation recursively and hence efficiently apply the chain rule along computation graph downstream gradient upstream gradient x local gradient Forward pass compute results of operations and save intermediate values Backward pass apply chain rule to compute gradients 79Why learn all these details about gradients 80 Modern deep learning frameworks compute gradients for you But why take a class on compilers or systems when they are implemented for you Understanding what is going on under the hood is useful Backpropagation doesn t always work perfectly out of the box Understanding why is crucial for debugging and improving models See Karpathy article https medium.com @karpathy yes you should understand backprop e2f06eab496b Example in future lecture exploding and vanishing gradients']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.000000000000001,1.0,0.9400472733000752,0.125
255,What does the Jacobian matrix represent in the context of neural networks?,"In the context of neural networks, the Jacobian matrix represents a matrix of partial derivatives. It is used to describe how each output of a function changes with respect to each input parameter. It is a generalization of the gradient when dealing with functions that have multiple outputs and multiple inputs.","['The vanishing gradients problem refers to the opposite behaviour when long term components go exponentially fast to norm 0 making it impossible for the model to learn correlation between temporally dis tant events. 2.1. The mechanics To understand this phenomenon we need to look at the form of each temporal component and in particular at the matrix factors xt xk see equation 5 that take the form of a product of t k Jacobian matrices. In the same way a product of t k real numbers can shrink to zero or explode to inÔ¨Ånity so does this product of matrices along some direction v . In what follows we will try to formalize these intu itions extending a similar derivation done in Bengio et al. 1994 where only a single hidden unit case was considered . If we consider a linear version of the model i.e. set œÉ to the identity function in equation 11 we can use the power iteration method to formally analyze this prod uct of Jacobian matrices and obtain tight conditions for when the gradients explode or vanish see the sup plementary materials for a detailed derivation of these conditions . It is suÔ¨Écient for the largest eigenvalue Œª1 of the recurrent weight matrix to be smaller than 1 for long term components to vanish as t and necessary for it to be larger than 1 for gradients to explode. We can generalize these results for nonlinear functions œÉ where the absolute values of œÉ x is bounded say by a value Œ≥ R and therefore diag œÉ xk Œ≥. We Ô¨Årst prove that it is suÔ¨Écient for Œª1 1 Œ≥ where Œª1 is the absolute value of the largest eigenvalue of the recurrent weight matrix Wrec for the vanishing gradient problem to occur. Note that we assume the parametrization given by equation 11 . The Jacobian matrix xk 1 xk is given by WT recdiag œÉ xk . The 2 norm of this Jacobian is bounded by the product ofOn the diÔ¨Éculty of training Recurrent Neural Networks the norms of the two matrices see equation 6 .', 'Introduction 2. Matrix calculus 3. BackpropagationComputing Gradients by Hand 9 Matrix calculus Fully vectorized gradients Multivariable calculus is just like single variable calculus if you use matrices Much faster and more useful than non vectorized gradients But doing a non vectorized gradient can be good for intuition recall the first lecture for an exampleGradients Given a function with 1 output and 1 input ùëìùë• ùë•3 It s gradient slope is its derivative ùëëf 3ùë•2 10 ùëëùë• How much will the output change if we change the input a bit At x 1 it changes about 3 times as much 1.013 1.03 At x 4 it changes about 48 times as much 4.013 64.48Gradients Given a function with 1 output and n inputs Its gradient is a vector of partial derivatives with respect to each input 11Jacobian Matrix Generalization of the Gradient Given a function with m outputs and n inputs It s Jacobian is an m x n matrix of partial derivatives 12Chain Rule For composition of one variable functions multiply derivatives For multiple variables functions multiply Jacobians 13Example Jacobian Elementwise activation Function 14Example Jacobian Elementwise activation Function Function has n outputs and n inputs n by n Jacobian 15Example Jacobian Elementwise activation Function 16Example Jacobian Elementwise activation Function 17Example Jacobian Elementwise activation Function 18Other Jacobians Compute these at home for practice Check your answers with the lecture notes 19Other Jacobians Compute these at home for practice Check your answers with the lecture notes 20Other Jacobians Compute these at home for practice Check your answers with the lecture notes Fine print This is the correct Jacobian. Later we discuss the shape convention using it the answer would be h. 21Other Jacobians Compute these at home for practice 22Back to our Neural Net x xmuseums xin xParis xare xamazing 23Back to our Neural Net Let s find Really we care about the gradient of the loss Jt but we will compute the gradient of the score for simplicity x xmuseums xin xParis xare xamazing 241.']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999983910712938,1.0,0.9416513639883712,0.0526315789473684
256,How is the derivative with respect to a matrix described?,"The derivative with respect to a matrix is described using the Jacobian matrix. The Jacobian matrix is an m x n matrix of partial derivatives, where m represents the number of outputs and n represents the number of inputs. It allows us to compute the gradients of a function with multiple outputs and inputs efficiently.","['So z x ij zi xj xj f xi f xi if i j 0 if otherwise So we see that the Jacobian z x is a diagonal matrix where the entry at i i is the derivative of f applied to xi. We can write this as z x diag f x . Since multiplication by a diagonal matrix is the same as doing elementwise multiplication by the diagonal we could also write f x when applying the chain rule. 5 Matrix times column vector with respect to the matrix z W x Œ¥ J z what is J W J z z W Œ¥ z W This is a bit more complicated than the other identities. The reason for in cluding J z in the above problem formulation will become clear in a moment. First suppose we have a loss function J a scalar and are computing its gradient with respect to a matrix W Rn m. Then we could think of J as a function of W taking nm inputs the entries of W to a single output J . This means the Jacobian J W would be a 1 nm vector. But in practice this is not a very useful way of arranging the gradient. It would be much nicer if the derivatives were in a n m matrix like this J W J W11 . . . J W1m . . . ... . . . J Wn1 . . . J Wnm Since this matrix has the same shape as W we could just subtract it times the learning rate from W when doing gradient descent. So in a slight abuse of notation let s Ô¨Ånd this matrix as J W instead. This way of arranging the gradients becomes complicated when computing z W . Unlike J z is a vector. So if we are trying to rearrange the gradients like with J W z W would be an n m n tensor Luckily we can avoid the issue by taking the gradient with respect to a single weight Wij instead. 3z Wij is just a vector which is much easier to deal with. We have zk m X l 1 Wklxl zk Wij m X l 1 xl Wij Wkl Note that Wij Wkl 1 if i k and j l and 0 if otherwise. So if k i everything in the sum is zero and the gradient is zero.', 'Introduction 2. Matrix calculus 3. BackpropagationComputing Gradients by Hand 9 Matrix calculus Fully vectorized gradients Multivariable calculus is just like single variable calculus if you use matrices Much faster and more useful than non vectorized gradients But doing a non vectorized gradient can be good for intuition recall the first lecture for an exampleGradients Given a function with 1 output and 1 input ùëìùë• ùë•3 It s gradient slope is its derivative ùëëf 3ùë•2 10 ùëëùë• How much will the output change if we change the input a bit At x 1 it changes about 3 times as much 1.013 1.03 At x 4 it changes about 48 times as much 4.013 64.48Gradients Given a function with 1 output and n inputs Its gradient is a vector of partial derivatives with respect to each input 11Jacobian Matrix Generalization of the Gradient Given a function with m outputs and n inputs It s Jacobian is an m x n matrix of partial derivatives 12Chain Rule For composition of one variable functions multiply derivatives For multiple variables functions multiply Jacobians 13Example Jacobian Elementwise activation Function 14Example Jacobian Elementwise activation Function Function has n outputs and n inputs n by n Jacobian 15Example Jacobian Elementwise activation Function 16Example Jacobian Elementwise activation Function 17Example Jacobian Elementwise activation Function 18Other Jacobians Compute these at home for practice Check your answers with the lecture notes 19Other Jacobians Compute these at home for practice Check your answers with the lecture notes 20Other Jacobians Compute these at home for practice Check your answers with the lecture notes Fine print This is the correct Jacobian. Later we discuss the shape convention using it the answer would be h. 21Other Jacobians Compute these at home for practice 22Back to our Neural Net x xmuseums xin xParis xare xamazing 23Back to our Neural Net Let s find Really we care about the gradient of the loss Jt but we will compute the gradient of the score for simplicity x xmuseums xin xParis xare xamazing 241.']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.999998870980108,1.0,0.8922688652633788,0.0454545454545454
257,What role does automatic differentiation play in modern deep learning frameworks?,"Automatic differentiation plays a crucial role in modern deep learning frameworks by efficiently computing gradients for the optimization process. It allows for the gradients to be automatically calculated during backpropagation, which is essential for training neural networks effectively. This process simplifies the implementation of complex neural network architectures and optimization algorithms, making it easier for researchers and practitioners to experiment with different models and techniques in the field of deep learning.

Source: Lecture notes on Computational Natural Language Processing.","['The parameters in the neural network are learnt using gradient based optimization algorithms with the gradients being computed by back propagation. One of the challenges of deep learning is that the gradients with respect to the weights in one layer are highly dependent on the outputs of the neurons in the previous layer especially if these outputs change in a highly correlated way. Batch normalization Ioffe and Szegedy 2015 was proposed to reduce such undesirable covariate shift . The method normalizes the summed inputs to each hidden unit over the training cases. SpeciÔ¨Åcally for the ith summed input in the lth layer the batch normalization method rescales the summed inputs according to their variances under the distribution of the data al i gl i œÉl i al i ¬µl i ¬µl i E x P x al i œÉl i r E x P x h al i ¬µl i 2i 2 where al i is normalized summed inputs to the ith hidden unit in the lth layer and gi is a gain parame ter scaling the normalized activation before the non linear activation function. Note the expectation is under the whole training data distribution. It is typically impractical to compute the expectations in Eq. 2 exactly since it would require forward passes through the whole training dataset with the current set of weights. Instead ¬µ and œÉ are estimated using the empirical samples from the current mini batch. This puts constraints on the size of a mini batch and it is hard to apply to recurrent neural networks. 3 Layer normalization We now consider the layer normalization method which is designed to overcome the drawbacks of batch normalization. Notice that changes in the output of one layer will tend to cause highly correlated changes in the summed inputs to the next layer especially with ReLU units whose outputs can change by a lot. This suggests the covariate shift problem can be reduced by Ô¨Åxing the mean and the variance of the summed inputs within each layer. We thus compute the layer normalization statistics over all the hidden units in the same layer as follows ¬µl 1 H H X i 1 al i œÉl v u u t 1 H H X i 1 al i ¬µl 2 3 where H denotes the number of hidden units in a layer. The difference between Eq. 2 and Eq.', 'The regularizer we propose below prefers solutions for which the error signal preserves norm as it travels back in time ‚Ñ¶ X k ‚Ñ¶k X k E xk 1 xk 1 xk E xk 1 1 2 9 In order to be computationally eÔ¨Écient we only use the immediate partial derivative of ‚Ñ¶with respect to Wrec we consider that xk and E xk 1 as being constant with respect to Wrec when computing the derivative of ‚Ñ¶k as depicted in equation 10 . Note we use the parametrization of equation 11 . This can be done ef Ô¨Åciently because we get the values of E xk from BPTT. We use Theano to compute these gradients Bergstra et al. 2010 Bastien et al. 2012 . ‚Ñ¶ Wrec P k ‚Ñ¶k Wrec P k E xk 1 WT recdiag œÉ xk E xk 1 1 2 Wrec 10 Note that our regularization term only forces the Ja cobian matrices xk 1 xk to preserve norm in the relevant direction of the error E xk 1 not for any direction i.e. we do not enforce that all eigenvalues are close to 1 . The second observation is that we are using a soft con straint therefore we are not ensured the norm of the error signal is preserved. If it happens that these Jaco bian matrices are such that the norm explodes as t k increases then this could lead to the exploding gradi ents problem and we need to deal with it for example as described in section 3.2. This can be seen from the dynamical systems perspective as well preventing vanishing gradients implies that we are pushing the model such that it is further away from the attrac tor such that it does not converge to it case in which the gradients vanish and closer to boundaries between basins of attractions making it more probable for the gradients to explode. 4. Experiments and Results 4.1.']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.0000000000000002,0.2,0.9284826866531602,0.0384615384615384
258,What is the purpose of manual gradient checking?,"The purpose of manual gradient checking is to compare analytical gradients with numerical gradients to ensure they are close, by evaluating them using forward passes. This process helps verify the correctness of the analytic gradients and is useful for debugging implementations. It involves perturbing parameters to compute gradients and is done to ensure the accuracy of the gradient computations.","['do backpropagation for you but mainly leave layer node writer to hand calculate the local derivative 74Backprop Implementations 75Implementation forward backward API 76Implementation forward backward API 77Manual Gradient checking Numeric Gradient For small h 1e 4 Easy to implement correctly But approximate and very slow You have to recompute f for every parameter of our model Useful for checking your implementation In the old days we hand wrote everything doing this everywhere was the key test Now much less needed you can use it to check layers are correctly implemented 78Summary We ve mastered the core technology of neural nets Backpropagation recursively and hence efficiently apply the chain rule along computation graph downstream gradient upstream gradient x local gradient Forward pass compute results of operations and save intermediate values Backward pass apply chain rule to compute gradients 79Why learn all these details about gradients 80 Modern deep learning frameworks compute gradients for you But why take a class on compilers or systems when they are implemented for you Understanding what is going on under the hood is useful Backpropagation doesn t always work perfectly out of the box Understanding why is crucial for debugging and improving models See Karpathy article https medium.com @karpathy yes you should understand backprop e2f06eab496b Example in future lecture exploding and vanishing gradients', 'We note that this deÔ¨Ånition of the numerical gradient follows very naturally from the deÔ¨Ånition of the derivative where in the scalar case f x f x œµ f x œµ Gradient checks are a great way to compare analytical and numerical gradients. Analytical gradients should be close and numerical gradients can be calculated using f Œ∏ J Œ∏ i J Œ∏ i 2œµ J Œ∏ i and J Œ∏ i can be evalu ated using two forward passes. An implementation of this can be seen in Snippet 2.1. Of course there is a slight difference the deÔ¨Ånition above only perturbs x in the positive direction to compute the gradient. While it would have been perfectly acceptable to deÔ¨Åne the numerical gradi ent in this way in practice it is often more precise and stable to use the centered difference formula where we perturb a parameter in both directions. The intuition is that to get a better approximation of the derivative slope around a point we need to examine the func tion f s behavior both to the left and right of that point. It can also be shown using Taylor s theorem that the centered difference formula has an error proportional to œµ2 which is quite small whereas the derivative deÔ¨Ånition is more error prone. Now a natural question you might ask is if this method is so pre cise why do we not use it to compute all of our network gradients instead of applying back propagation The simple answer as hinted earlier is inefÔ¨Åciency recall that every time we want to compute the gradient with respect to an element we need to make two forward passes through the network which will be computationally expen sive. Furthermore many large scale neural networks can contain millions of parameters and computing two passes per parameter is clearly not optimal. And since in optimization techniques such as SGD we must compute the gradients once per iteration for several thousands of iterations it is obvious that this method quickly grows intractable. This inefÔ¨Åciency is why we only use gradient check to verify the correctness of our analytic gradients which are much quicker to compute.']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999999999999996,1.0,0.9320840105615464,0.0833333333333333
259,Why is understanding gradients important despite automatic differentiation?,"Understanding gradients is important despite automatic differentiation because it allows us to verify the correctness of our analytic gradients through methods like gradient checks. This verification is crucial in ensuring the accuracy of our models and their training process. Additionally, understanding gradients helps in gaining insights into how changes in input parameters affect the output of a model, which is essential for debugging and improving the performance of neural networks.","['We note that this deÔ¨Ånition of the numerical gradient follows very naturally from the deÔ¨Ånition of the derivative where in the scalar case f x f x œµ f x œµ Gradient checks are a great way to compare analytical and numerical gradients. Analytical gradients should be close and numerical gradients can be calculated using f Œ∏ J Œ∏ i J Œ∏ i 2œµ J Œ∏ i and J Œ∏ i can be evalu ated using two forward passes. An implementation of this can be seen in Snippet 2.1. Of course there is a slight difference the deÔ¨Ånition above only perturbs x in the positive direction to compute the gradient. While it would have been perfectly acceptable to deÔ¨Åne the numerical gradi ent in this way in practice it is often more precise and stable to use the centered difference formula where we perturb a parameter in both directions. The intuition is that to get a better approximation of the derivative slope around a point we need to examine the func tion f s behavior both to the left and right of that point. It can also be shown using Taylor s theorem that the centered difference formula has an error proportional to œµ2 which is quite small whereas the derivative deÔ¨Ånition is more error prone. Now a natural question you might ask is if this method is so pre cise why do we not use it to compute all of our network gradients instead of applying back propagation The simple answer as hinted earlier is inefÔ¨Åciency recall that every time we want to compute the gradient with respect to an element we need to make two forward passes through the network which will be computationally expen sive. Furthermore many large scale neural networks can contain millions of parameters and computing two passes per parameter is clearly not optimal. And since in optimization techniques such as SGD we must compute the gradients once per iteration for several thousands of iterations it is obvious that this method quickly grows intractable. This inefÔ¨Åciency is why we only use gradient check to verify the correctness of our analytic gradients which are much quicker to compute.', 'Introduction 2. Matrix calculus 3. BackpropagationComputing Gradients by Hand 9 Matrix calculus Fully vectorized gradients Multivariable calculus is just like single variable calculus if you use matrices Much faster and more useful than non vectorized gradients But doing a non vectorized gradient can be good for intuition recall the first lecture for an exampleGradients Given a function with 1 output and 1 input ùëìùë• ùë•3 It s gradient slope is its derivative ùëëf 3ùë•2 10 ùëëùë• How much will the output change if we change the input a bit At x 1 it changes about 3 times as much 1.013 1.03 At x 4 it changes about 48 times as much 4.013 64.48Gradients Given a function with 1 output and n inputs Its gradient is a vector of partial derivatives with respect to each input 11Jacobian Matrix Generalization of the Gradient Given a function with m outputs and n inputs It s Jacobian is an m x n matrix of partial derivatives 12Chain Rule For composition of one variable functions multiply derivatives For multiple variables functions multiply Jacobians 13Example Jacobian Elementwise activation Function 14Example Jacobian Elementwise activation Function Function has n outputs and n inputs n by n Jacobian 15Example Jacobian Elementwise activation Function 16Example Jacobian Elementwise activation Function 17Example Jacobian Elementwise activation Function 18Other Jacobians Compute these at home for practice Check your answers with the lecture notes 19Other Jacobians Compute these at home for practice Check your answers with the lecture notes 20Other Jacobians Compute these at home for practice Check your answers with the lecture notes Fine print This is the correct Jacobian. Later we discuss the shape convention using it the answer would be h. 21Other Jacobians Compute these at home for practice 22Back to our Neural Net x xmuseums xin xParis xare xamazing 23Back to our Neural Net Let s find Really we care about the gradient of the loss Jt but we will compute the gradient of the score for simplicity x xmuseums xin xParis xare xamazing 241.']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9536899962692664,1.0,0.9321810290017454,0.0625
260,What is the core concept introduced in this class that leads to GPT-3 and ChatGPT?,"The core concept introduced in this class that leads to GPT-3 and ChatGPT is the idea of in-context learning in autoregressive language models. This concept focuses on the ability of models like GPT-3 to perform well in tasks even with minimal training data, such as one-shot or few-shot learning scenarios. By leveraging in-context learning, these models can exhibit strong performance on various natural language processing tasks despite not being explicitly trained on those tasks. This approach allows models like GPT-3 to generalize and adapt to new tasks efficiently.

Source: Language_Models_are_Few_Shot_Learners.txt","['On the Easy version of the dataset questions which either of the mentioned baseline approaches answered correctly GPT 3 achieves 68.8 71.2 and 70.1 which slightly exceeds a Ô¨Åne tuned RoBERTa baseline from KKS 20 . However both of these results are still much worse than the overall SOTAs achieved by the UniÔ¨ÅedQA which exceeds GPT 3 s few shot results by 27 on the challenge set and 22 on the easy set. On OpenBookQA MCKS18 GPT 3 improves signiÔ¨Åcantly from zero to few shot settings but is still over 20 points short of the overall SOTA. GPT 3 s few shot performance is similar to a Ô¨Åne tuned BERT Large baseline on the leaderboard. Overall in context learning with GPT 3 shows mixed results on commonsense reasoning tasks with only small and inconsistent gains observed in the one and few shot learning settings for both PIQA and ARC but a signiÔ¨Åcant improvement is observed on OpenBookQA. GPT 3 sets SOTA on the new PIQA dataset in all evaluation settings. 3.6 Reading Comprehension Next we evaluate GPT 3 on the task of reading comprehension. We use a suite of 5 datasets including abstractive multiple choice and span based answer formats in both dialog and single question settings. We observe a wide spread in GPT 3 s performance across these datasets suggestive of varying capability with different answer formats. In general we observe GPT 3 is on par with initial baselines and early results trained using contextual representations on each respective dataset. GPT 3 performs best within 3 points of the human baseline on CoQA RCM19 a free form conversational dataset and performs worst 13 F1 below an ELMo baseline on QuAC CHI 18 a dataset which requires modeling structured dialog acts and answer span selections of teacher student interactions.', 'Quantitatively GPT 3 s in context learning performance has some notable gaps on our suite of benchmarks as described in Section 3 and in particular it does little better than chance when evaluated one shot or even few shot on some comparison tasks such as determining if two words are used the same way in a sentence or if one sentence implies another WIC and ANLI respectively as well as on a subset of reading comprehension tasks. This is especially striking given GPT 3 s strong few shot performance on many other tasks. GPT 3 has several structural and algorithmic limitations which could account for some of the issues above. We focused on exploring in context learning behavior in autoregressive language models because it is straightforward to both sample and compute likelihoods with this model class. As a result our experiments do not include any bidirectional architectures or other training objectives such as denoising. This is a noticeable difference from much of the recent literature which has documented improved Ô¨Åne tuning performance when using these approaches over standard language models RSR 19 . Thus our design decision comes at the cost of potentially worse performance on tasks which empirically beneÔ¨Åt from bidirectionality. This may include Ô¨Åll in the blank tasks tasks that involve looking back and comparing two pieces of content or tasks that require re reading or carefully considering a long passage and then generating a very short answer. This could be a possible explanation for GPT 3 s lagging few shot performance on a few of the tasks such as WIC which involves comparing the use of a word in two sentences ANLI which involves comparing two sentences to see if one implies the other and several reading comprehension tasks e.g. QuAC and RACE . We also conjecture based on past literature that a large bidirectional model would be stronger at Ô¨Åne tuning than GPT 3. Making a bidirectional model at the scale of GPT 3 and or trying to make bidirectional models work with few or zero shot learning is a promising direction for future research and could help achieve the best of both worlds .']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999999999999996,0.2,0.8942130870636218,0.0454545454545454
261,What is the purpose of Dropout as mentioned in the document?,"The purpose of Dropout, as mentioned in the document, is to act as a powerful technique for regularization in neural networks. During training, Dropout randomly drops a subset of neurons with a certain probability during each forward and backward pass, or keeps each neuron alive with a probability p. This process helps prevent overfitting, allows the network to learn more meaningful information from the data, and typically leads to higher performance on the task at hand by effectively training exponentially many smaller networks at once and averaging over their predictions. During testing, the full network is used to compute predictions.","['Imposing such a con straint can also be interpreted as the prior Bayesian belief that the optimal weights are close to zero how close depends on the value of Œª. Choosing the right value of Œª is critical and must be chosen via hyperparameter tuning. Too high a value of Œª causes most of the weights to be set too close to 0 and the model does not learn anything meaningful from the training data often obtaining poor ac curacy on training validation and testing sets. Too low a value and we fall into the domain of overÔ¨Åtting once again. It must be noted that the bias terms are not regularized and do not contribute to the cost term above try thinking about why this is the case There are indeed other types of regularization that are sometimes used such as L1 regularization which sums over the absolute values rather than squares of parameter elements however this is less commonly applied in practice since it leads to sparsity of parameter weights. In the next section we discuss dropout which effectively acts as another form of regularization by randomly dropping i.e. setting to zero neurons in the forward pass. 2.3 Dropout Dropout is a powerful technique for regularization Ô¨Årst introduced by Srivastava et al. in Dropout A Simple Way to Prevent Neural Net works from OverÔ¨Åtting. The idea is simple yet effective during train ing we will randomly drop with some probability 1 p a subset of neurons during each forward backward pass or equivalently we will keep alive each neuron with a probability p . Then during testing we will use the full network to compute our predictions. The result is that the network typically learns more meaningful informa tion from the data is less likely to overÔ¨Åt and usually obtains higher performance overall on the task at hand. One intuitive reason why this technique should be so effective is that what dropout is doing is essentially doing is training exponentially many smaller networks at once and averaging over their predictions. Dropout applied to an artiÔ¨Åcial neural network. Image credits to Srivastava et al. In practice the way we introduce dropout is that we take the out put h of each layer of neurons and keep each neuron with prob ability p and else set it to 0. Then during back propagation we only pass gradients through neurons that were kept alive during the forward pass.', '0.73 I ve become pathetic pining after a guy my ex. Would like to reach state of less pathetic. If more info is necessary please let me know. 0.69 I have body issues body acne scarring and weight issues that prevent me from having a normal life without shame and prevent me from having a better sex life with my BF. 1.0 Do you take someone back after they ve turned you down off even if you can t see them in person or are they just not worth the risk 0.52 Table 19 Qualitative examples showing the change in reward of the reward model on human generated edits to TL DR summaries that make the summaries better. Examples are randomly selected from the set where the edit distance was less than 5 and the magnitude of change in reward was greater than 0.5. Text in strike through was removed from the original summary in the edit and text in bold was added. The reward model is sensitive to small but semantically meaningful changes in the summary although it makes errors on occasion. TL DR 1.3B sup. T 0.7 researcher labeler labeler ensem ble length copying ROUGE 1.3B sup logprob 1.3B RM 6.7B sup logprob 6.7B RM researcher 73.4 4.1 77.7 2.1 84.4 3.3 55.5 4.3 62.3 4.1 59.1 4.2 61.8 4.8 72.2 4.5 62.8 4.7 78.0 3.9 labeler 77.7 2.1 68.6 1.7 74.4 2.0 54.4 1.3 58.0 1.2 57.7 1.3 58.7 2.0 65.8 2.0 61.9 2.1 70.8 1.8 labeler ensemble 84.4 3.3 74.4 2.0 60.6 4.0 62.7 3.8 59.']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9300597327790432,1.0,0.9231571609685568,0.04
262,What are some approaches to address the vanishing gradient problem in RNNs?,"Some approaches to address the vanishing gradient problem in RNNs include:
1. Initializing the weight matrix W_hh from an identity matrix instead of random initialization.
2. Using Rectified Linear Units (ReLU) instead of the sigmoid function, as the derivative for ReLU is either 0 or 1, allowing gradients to flow through neurons without getting attenuated.
(Source: lecture03_notes_LM_RNN.txt)","['av steps. 3. Dealing wi vanishing g 3.1. Previous so Using an L1 or L2 help with explodin ters initialized wit Wrec is probably that the gradient tion found in secti ensure that durin exceeds 1. This a ple regime with a where any informa Figure 7 Gradient explosion clipping visualization To solve the problem of vanishing gradients we introduce two tech niques. The Ô¨Årst technique is that instead of initializing W hh ran domly start off from an identity matrix initialization. The second technique is to use the RectiÔ¨Åed Linear Units ReLU in stead of the sigmoid function. The derivative for the ReLU is either 0 or 1. This way gradients would Ô¨Çow through the neurons whose deriva tive is 1 without getting attenuated while propagating back through time steps. 2.5 Deep Bidirectional RNNs So far we have focused on RNNs that condition on past words to predict the next word in the sequence. It is possible to make predic tions based on future words by having the RNN model read through the corpus backwards. Irsoy et al. shows a bi directional deep neu ral network at each time step t this network maintains two hidden layers one for the left to right propagation and another for the right to left propagation. To maintain two hidden layers at any time this network consumes twice as much memory space for its weight and bias parameters. The Ô¨Ånal classiÔ¨Åcation result ÀÜ yt is generated through combining the score results produced by both RNN hidden layers. Fig ure 8 shows the bi directional network architecture and Equations 17 and 18 show the mathematical formulation behind setting up the bi directional RNN hidden layer. The only difference between these two relationships is in the direction of recursing through the corpus. Equa tion 19 shows the classiÔ¨Åcation relationship used for predicting the next word via summarizing past and future word representations. Bidirectionality h t f W xt V h t f W xt V yt g U h t h t y h x now represents summarizes the past and around a single token.', 'Problems with RNNs Vanishing and Exploding Gradients 48Vanishing gradient intuition 49Vanishing gradient intuition chain rule 50Vanishing gradient intuition chain rule 51Vanishing gradient intuition chain rule 52Vanishing gradient intuition What happens if these are small Vanishing gradient problem When these are small the gradient signal gets smaller and smaller as it backpropagates further 53Vanishing gradient proof sketch linear case Recall What if were the identity function 53 Source On the difficulty of training recurrent neural networks Pascanu et al 2013. http proceedings.mlr.press v28 pascanu13.pdf and supplemental materials at http proceedings.mlr.press v28 pascanu13 supp.pdf If Wh is small then this term gets exponentially problematic as becomes large chain rule Consider the gradient of the loss on step with respect to the hidden state on some previous step . Let chain rule value of ONLY READ IF INTERESTEDVanishing gradient proof sketch linear case What s wrong with Consider if the eigenvalues of are all less than 1 We can write as a basis What about nonlinear activations i.e. what we use Pretty much the same thing except the proof requires for some dependent on dimensionality and Source On the difficulty of training recurrent neural networks Pascanu et al 2013. http proceedings.mlr.press v28 pascanu13.pdf and supplemental materials at http proceedings.mlr.press v28 pascanu13 supp.pdf 54 eigenvectors using the eigenvectors of Approaches 0 as grows so gradient vanishes sufficient but not necessary ONLY READ IF INTERESTEDWhy is vanishing gradient a problem Gradient signal from far away is lost because it s much smaller than gradient signal from close by. So model weights are updated only with respect to near effects not long term effects. 56Effect of vanishing gradient on RNN LM 57 LM task When she tried to print her tickets she found that the printer was out of toner. She went to the stationery store to buy more toner. It was very overpriced. After installing the toner into the printer she finally printed her To learn from this training example the RNN LM needs to model the dependency between tickets on the 7th step and the target word tickets at the end.']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.0000000000000009,0.6666666666666666,0.7854664844942174,0.037037037037037
263,"What is perplexity, and how is it used in evaluating language models?","Perplexity is a measurement used to evaluate language models based on how well they predict a sample of text. It quantifies how well a probability distribution or a language model predicts a sample. A lower perplexity indicates that the model is better at predicting the sample, as it means the model is less surprised by the text and assigns higher probabilities to the words in the sample. It is important to note that perplexity should be compared between models with identical vocabularies to be meaningful. While a lower perplexity often correlates with better performance in language processing tasks like speech recognition or machine translation, it is crucial to confirm any improvement in perplexity with an end-to-end evaluation of a real task before concluding the evaluation of the model.

Source: N-gram Language Models lecture","['Any kind of knowledge of the test set can cause the perplexity to be artiÔ¨Åcially low. The perplexity of two language models is only comparable if they use identical vocabularies. An intrinsic improvement in perplexity does not guarantee an extrinsic im provement in the performance of a language processing task like speech recognition10 CHAPTER 3 N GRAM LANGUAGE MODELS or machine translation. Nonetheless because perplexity often correlates with such improvements it is commonly used as a quick check on an algorithm. But a model s improvement in perplexity should always be conÔ¨Årmed by an end to end evaluation of a real task before concluding the evaluation of the model. 3.3 Sampling sentences from a language model One important way to visualize what kind of knowledge a language model embodies is to sample from it. Sampling from a distribution means to choose random points sampling according to their likelihood. Thus sampling from a language model which rep resents a distribution over sentences means to generate some sentences choosing each sentence according to its likelihood as deÔ¨Åned by the model. Thus we are more likely to generate sentences that the model thinks have a high probability and less likely to generate sentences that the model thinks have a low probability. This technique of visualizing a language model by sampling was Ô¨Årst suggested very early on by Shannon 1951 and Miller and Selfridge 1950 . It s simplest to visualize how this works for the unigram case. Imagine all the words of the English language covering the probability space between 0 and 1 each word covering an interval proportional to its frequency. Fig. 3.3 shows a visualization using a unigram LM computed from the text of this book. We choose a random value between 0 and 1 Ô¨Ånd that point on the probability line and print the word whose interval includes this chosen value. We continue choosing random numbers and generating words until we randomly generate the sentence Ô¨Ånal token s .', 'Now we see the following test set 0 0 0 0 0 3 0 0 0 0. We should expect the perplexity of this test set to be lower since most of the time the next number will be zero which is very predictable i.e. has a high probability. Thus although the branching factor is still 10 the perplexity or weighted branching factor is smaller. We leave this exact calculation as exercise 3.12. We see in Section 3.8 that perplexity is also closely related to the information theoretic notion of entropy. We mentioned above that perplexity is a function of both the text and the lan guage model given a text W different language models will have different perplex ities. Because of this perplexity can be used to compare different n gram models. Let s look at an example in which we trained unigram bigram and trigram gram mars on 38 million words including start of sentence tokens from the Wall Street Journal using a 19 979 word vocabulary. We then computed the perplexity of each of these models on a test set of 1.5 million words using Eq. 3.16 for unigrams Eq. 3.17 for bigrams and the corresponding equation for trigrams. The table below shows the perplexity of a 1.5 million word WSJ test set according to each of these grammars. Unigram Bigram Trigram Perplexity 962 170 109 As we see above the more information the n gram gives us about the word sequence the higher the probability the n gram will assign to the string. A trigram model is less surprised than a unigram model because it has a better idea of what words might come next and so it assigns them a higher probability. And the higher the probability the lower the perplexity since as Eq. 3.15 showed perplexity is related inversely to the likelihood of the test sequence according to the model . So a lower perplexity can tell us that a language model is a better predictor of the words in the test set. Note that in computing perplexities the n gram model P must be constructed without any knowledge of the test set or any prior knowledge of the vocabulary of the test set.']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9377451467611762,1.0,0.9505724214383208,0.0303030303030303
264,How does LSTM solve the vanishing gradient problem?,"LSTM (Long Short-Term Memory) units address the vanishing gradient problem by introducing a special set of units with a recurrent connection to itself fixed to 1. The flow of information into and out of the unit is controlled by input and output gates, the behavior of which is learned. Additionally, LSTM units have variations in their basic structure that help mitigate the vanishing gradient issue. Source: Lecture notes on Computational Natural Language Processing.","['SpeciÔ¨Åcally it introduces a special set of units called LSTM units which are linear and have a recurrent connection to itself which is Ô¨Åxed to 1. The Ô¨Çow of information into the unit and from the unit is guarded by an input and output gates their behaviour is learned . There are several variations of this basic structure. This solution does not address explicitly the exploding gradients problem. Sutskever et al. 2011 use the Hessian Free opti mizer in conjunction with structural damping a spe ciÔ¨Åc damping strategy of the Hessian. This approach seems to deal very well with the vanishing gradient though more detailed analysis is still missing. Pre sumably this method works because in high dimen sional spaces there is a high probability for long term components to be orthogonal to short term ones. This would allow the Hessian to rescale these components independently. In practice one can not guarantee that this property holds. As discussed in section 2.3 this method is able to deal with the exploding gradient as well. Structural damping is an enhancement that forces the change in the state to be small when the pa rameter changes by some small value Œ∏. This asks for the Jacobian matrices xt Œ∏ to have small norm hence further helping with the exploding gradients problem. The fact that it helps when training recurrent neural models on long sequences suggests that while the cur vature might explode at the same time with the gradi ent it might not grow at the same rate and hence not be suÔ¨Écient to deal with the exploding gradient. Echo State Networks LukoÀá seviÀá cius and Jaeger 2009 avoid the exploding and vanishing gradients problem by not learning the recurrent and input weights. They are sampled from hand crafted distributions. Because usually the largest eigenvalue of the recurrent weight is by construction smaller than 1 information fed in to the model has to die out exponentially fast. This means that these models can not easily deal with long term dependencies even though the reason is slightly diÔ¨Äerent from the vanishing gradients problem. An extension to the classical model is represented by leaky integration units Jaeger et al.', 'The vanishing gradients problem refers to the opposite behaviour when long term components go exponentially fast to norm 0 making it impossible for the model to learn correlation between temporally dis tant events. 2.1. The mechanics To understand this phenomenon we need to look at the form of each temporal component and in particular at the matrix factors xt xk see equation 5 that take the form of a product of t k Jacobian matrices. In the same way a product of t k real numbers can shrink to zero or explode to inÔ¨Ånity so does this product of matrices along some direction v . In what follows we will try to formalize these intu itions extending a similar derivation done in Bengio et al. 1994 where only a single hidden unit case was considered . If we consider a linear version of the model i.e. set œÉ to the identity function in equation 11 we can use the power iteration method to formally analyze this prod uct of Jacobian matrices and obtain tight conditions for when the gradients explode or vanish see the sup plementary materials for a detailed derivation of these conditions . It is suÔ¨Écient for the largest eigenvalue Œª1 of the recurrent weight matrix to be smaller than 1 for long term components to vanish as t and necessary for it to be larger than 1 for gradients to explode. We can generalize these results for nonlinear functions œÉ where the absolute values of œÉ x is bounded say by a value Œ≥ R and therefore diag œÉ xk Œ≥. We Ô¨Årst prove that it is suÔ¨Écient for Œª1 1 Œ≥ where Œª1 is the absolute value of the largest eigenvalue of the recurrent weight matrix Wrec for the vanishing gradient problem to occur. Note that we assume the parametrization given by equation 11 . The Jacobian matrix xk 1 xk is given by WT recdiag œÉ xk . The 2 norm of this Jacobian is bounded by the product ofOn the diÔ¨Éculty of training Recurrent Neural Networks the norms of the two matrices see equation 6 .']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9612453513455184,0.8,0.9208311971176883,0.0333333333333333
265,"What is the impact of bidirectional RNNs, and where are they applicable?","Bidirectional RNNs have the advantage of being able to capture information from both past and future contexts simultaneously, which can lead to better understanding of the overall sequence. They are applicable in tasks where understanding the context from both directions is important, such as machine translation, speech recognition, and sentiment analysis. 

Source: Lecture03_notes_LM_RNN","['The notation in this image is slightly different here the equivalent of W hh is Wh W hx is We and W S is U. E converts word inputs x t to word embeddings e t . The Ô¨Ånal softmax over the vocabulary shows us the probability of various options for token x 5 condi tioned on all previous tokens. The input could be much longer than 4 5 tokens. 2.1 RNN Loss and Perplexity The loss function used in RNNs is often the cross entropy error in troduced in earlier notes. Equation 7 shows this function as the sum over the entire vocabulary at time step t. J t Œ∏ V j 1 yt j log ÀÜ yt j 7 The cross entropy error over a corpus of size T is J 1 T T t 1 J t Œ∏ 1 T T t 1 V j 1 yt j log ÀÜ yt j 8 Equation 9 is called the perplexity relationship it is basically 2 to the power of the negative log probability of the cross entropy error function shown in Equation 8. Perplexity is a measure of confusion where lower values imply more conÔ¨Ådence in predicting the next word in the sequence compared to the ground truth outcome . Perplexity 2J 9 2.2 Advantages Disadvantages and Applications of RNNs RNNs have several advantages 1. They can process input sequences of any length 2. The model size does not increase for longer input sequence lengths 3. Computation for step t can in theory use information from many steps back. 4. The same weights are applied to every timestep of the input so there is symmetry in how inputs are processed However they also have some disadvantages 1. Computation is slow because it is sequential it cannot be paral lelized 2. In practice it is difÔ¨Åcult to access information from many steps back due to problems like vanishing and exploding gradients which we discuss in the following subsection The amount of memory required to run a layer of RNN is pro portional to the number of words in the corpus.', 'h h h Figure 8 A bi directional RNN model h t f W xt V h t 1 b 17 h t f W xt V h t 1 b 18 ÀÜ yt g Uht c g U h t h t c 19 RNNs can also be multi layered. Figure 9 shows a multi layer bi directional RNN where each lower layer feeds the next layer. As shown in this Ô¨Ågure in this network architecture at time step t each interme diate neuron receives one set of parameters from the previous time step in the same RNN layer and two sets of parameters from the previous RNN hidden layer one input comes from the left to right RNN and the other from the right to left RNN. To construct a Deep RNN with L layers the above relationships are modiÔ¨Åed to the relationships in Equations 20 and 21 where the input to each intermediate neuron at level i is the output of the RNN at layer i 1 at the same time step t. The output ÀÜ y at each time step is the result of propagating input parameters through all hidden layers Equation 22 . Going Deep h i t f W i ht i 1 h i t f W i ht i 1 yt g U h t L h y h 3 x Each memory layer passes an intermediate seq representation to the next. h 2 h 1 Figure 9 A deep bi directional RNN with three RNN layers. h i t f W i h i 1 t V i h i t 1 b i 20 h i t f W i h i 1 t V i h i t 1 b i 21 ÀÜ yt g Uht c g U h L t h L t c 22 2.6 Application RNN Translation Model Traditional translation models are quite complex they consist of nu merous machine learning algorithms applied to different stages of the language translation pipeline. In this section we discuss the potential for adopting RNNs as a replacement to traditional translation mod ules. Consider the RNN example model shown in Figure 10 here the German phrase Echt dicke Kiste is translated to Awesome sauce.']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9216181230273186,1.0,0.9389450440695772,0.0416666666666666
266,Describe the concept of multi-layer or stacked RNNs and their benefit.,"Multi-layer or stacked RNNs involve applying multiple RNNs on top of each other, allowing the network to compute more complex representations. The lower RNN layers compute lower-level features, while the higher RNN layers compute higher-level features. Stacked RNNs are also known as multi-layer RNNs, and they are beneficial because they enable the network to learn more intricate patterns and relationships in the data, leading to improved performance compared to using just a single layer of RNN. In practice, high-performing RNNs are often multi-layered, although not as deep as convolutional or feedforward networks. For example, in Neural Machine Translation, research has shown that having 2 to 4 layers for the encoder RNN and 4 layers for the decoder RNN is optimal. Skip connections or dense connections are typically used to facilitate the training of deeper RNNs.

Source: Lecture03_RNNs_and_LLMs","['You will learn more about transformers including BERT in a couple of weeksMulti layer RNNs RNNs are already deep on one dimension they unroll over many timesteps We can also make them deep in another dimension by applying multiple RNNs this is a multi layer RNN. This allows the network to compute more complex representations The lower RNNs should compute lower level features and the higher RNNs should compute higher level features. Multi layer RNNs are also called stacked RNNs. 80Multi layer RNNs was terribly exciting the movie RNN layer 1 RNN layer 2 81 RNN layer 3 The hidden states from RNN layer i are the inputs to RNN layer i 1Multi layer RNNs in practice 39 Multi layer or stacked RNNs allow a network to compute more complex representations they work better than just have one layer of high dimensional encodings The lower RNNs should compute lower level features and the higher RNNs should compute higher level features. High performing RNNs are usually multi layer but aren t as deep as convolutional or feed forward networks For example In a 2017 paper Britz et al. find that for Neural Machine Translation 2 to 4 layers is best for the encoder RNN and 4 layers is best for the decoder RNN Often 2 layers is a lot better than 1 and 3 might be a little better than 2 Usually skip connections dense connections are needed to train deeper RNNs e.g. 8 layers Transformer based networks e.g. BERT are usually deeper like 12 or 24 layers. You will learn about Transformers later they have a lot of skipping like connections Massive Exploration of Neural Machine Translation Architecutres Britz et al 2017. https arxiv.org pdf 1703.03906.pdfTerminology By the end of the course You will understand phrases like stacked bidirectional LSTMs with residual connections and self attention The RNN described in this lecture simple vanilla Elman RNN You learned about other RNN flavors like LSTM and GRU and multi layer RNNs 83', 'h h h Figure 8 A bi directional RNN model h t f W xt V h t 1 b 17 h t f W xt V h t 1 b 18 ÀÜ yt g Uht c g U h t h t c 19 RNNs can also be multi layered. Figure 9 shows a multi layer bi directional RNN where each lower layer feeds the next layer. As shown in this Ô¨Ågure in this network architecture at time step t each interme diate neuron receives one set of parameters from the previous time step in the same RNN layer and two sets of parameters from the previous RNN hidden layer one input comes from the left to right RNN and the other from the right to left RNN. To construct a Deep RNN with L layers the above relationships are modiÔ¨Åed to the relationships in Equations 20 and 21 where the input to each intermediate neuron at level i is the output of the RNN at layer i 1 at the same time step t. The output ÀÜ y at each time step is the result of propagating input parameters through all hidden layers Equation 22 . Going Deep h i t f W i ht i 1 h i t f W i ht i 1 yt g U h t L h y h 3 x Each memory layer passes an intermediate seq representation to the next. h 2 h 1 Figure 9 A deep bi directional RNN with three RNN layers. h i t f W i h i 1 t V i h i t 1 b i 20 h i t f W i h i 1 t V i h i t 1 b i 21 ÀÜ yt g Uht c g U h L t h L t c 22 2.6 Application RNN Translation Model Traditional translation models are quite complex they consist of nu merous machine learning algorithms applied to different stages of the language translation pipeline. In this section we discuss the potential for adopting RNNs as a replacement to traditional translation mod ules. Consider the RNN example model shown in Figure 10 here the German phrase Echt dicke Kiste is translated to Awesome sauce.']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.953641280146938,1.0,0.9574814937918092,0.0588235294117647
267,How does Dropout work during training and testing?,"During training, Dropout works by randomly dropping a subset of neurons with a certain probability (1-p) during each forward and backward pass. This means that some neurons are set to zero temporarily. During testing, all weights are multiplied by (1/p) to ensure that the full network is used for computing predictions without any dropout applied. This technique helps prevent overfitting, encourages the network to learn more meaningful information, and typically leads to higher performance on the given task.

Source: lecture02_notes_Backpropagation.txt","['Imposing such a con straint can also be interpreted as the prior Bayesian belief that the optimal weights are close to zero how close depends on the value of Œª. Choosing the right value of Œª is critical and must be chosen via hyperparameter tuning. Too high a value of Œª causes most of the weights to be set too close to 0 and the model does not learn anything meaningful from the training data often obtaining poor ac curacy on training validation and testing sets. Too low a value and we fall into the domain of overÔ¨Åtting once again. It must be noted that the bias terms are not regularized and do not contribute to the cost term above try thinking about why this is the case There are indeed other types of regularization that are sometimes used such as L1 regularization which sums over the absolute values rather than squares of parameter elements however this is less commonly applied in practice since it leads to sparsity of parameter weights. In the next section we discuss dropout which effectively acts as another form of regularization by randomly dropping i.e. setting to zero neurons in the forward pass. 2.3 Dropout Dropout is a powerful technique for regularization Ô¨Årst introduced by Srivastava et al. in Dropout A Simple Way to Prevent Neural Net works from OverÔ¨Åtting. The idea is simple yet effective during train ing we will randomly drop with some probability 1 p a subset of neurons during each forward backward pass or equivalently we will keep alive each neuron with a probability p . Then during testing we will use the full network to compute our predictions. The result is that the network typically learns more meaningful informa tion from the data is less likely to overÔ¨Åt and usually obtains higher performance overall on the task at hand. One intuitive reason why this technique should be so effective is that what dropout is doing is essentially doing is training exponentially many smaller networks at once and averaging over their predictions. Dropout applied to an artiÔ¨Åcial neural network. Image credits to Srivastava et al. In practice the way we introduce dropout is that we take the out put h of each layer of neurons and keep each neuron with prob ability p and else set it to 0. Then during back propagation we only pass gradients through neurons that were kept alive during the forward pass.', 'like an ensemble model Nowadays usually thought of as strong feature dependent regularizer Wager Wang Liang 2013DropoutDropout Srivastava Hinton Krizhevsky Sutskever Salakhutdinov 2012 JMLR 2014 During training For each data point each time Randomly set input to 0 with probability ùëù dropout ratio often p 0.5 except p 0.15 for input layer via dropout mask During testing Multiply all weights by 1 ùëù No other dropout ùë•2 ùë•3 0 ùë§1 ùë§2 ùë§3 ùë§4 ùëè 1 ùë¶ ùë§1ùë•1 ùë§2ùë•2 ùë§3ùë•3 ùë• ùë• 1 1 Train 1 Train 2 Test ùë•3 0 1 ùë¶ ùë§1ùë•1 ùë§3ùë•3 ùëè ùë§1 ùë§2 ùë§3 ùë§4 ùëè ùë•1 0 ùë•2 ùë•3 ùë•4 ùë§1 ùë§2 ùë§3 ùë§4 ùëè 1 ùë¶ 1 ùëù ùë§1ùë•1 ùë§2ùë•2 ùë§3ùë•3 ùë§4ùë•4 6Vectorization E.g. looping over word vectors versus concatenating them all into one large matrix and then multiplying the softmax weights with that matrix for loop 1000 loops best of 3 639 ¬µs per loop Using single a C x N matrix 10000 loops best of 3 53.8 ¬µs per loop Matrices are awesome Always try to use vectors and matrices rather than for loops The speed gain goes from 1 to 2 orders of magnitude with GPUs 7Parameter Initialization You normally must initialize weights to small random values i.e. not zero matrices To avoid symmetries that prevent learning specialization Initialize hidden layer biases to 0 and output or reconstruction biases to optimal value if weights were 0 e.g.']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9578600645051888,0.8,0.9297054211674108,0.375
268,What is the role of parameter initialization in neural networks?,"Parameter initialization in neural networks plays a crucial role in achieving superlative performance. A good starting strategy is to initialize the weights to small random numbers normally distributed around 0. Different weight and bias initialization schemes can impact training dynamics, affecting convergence speed and error rates. For sigmoid and tanh activation units, faster convergence and lower error rates are achieved when weights are initialized randomly with a uniform distribution. This initialization scheme aims to maintain activation variances and backpropagated gradient variances across layers, preventing gradient variances from decreasing with backpropagation. 

Source: Lecture02 notes Backpropagation","['This is useful since input features are often measured in different units but we often want to initially consider all features as equally important. The way we accomplish this is by simply dividing the features by their respective standard deviation calculated across the training set. Whitening Not as commonly used as mean subtraction normalization whiten ing essentially converts the data to a have an identity covariance matrix that is features become uncorrelated and have a variance of 1. This is done by Ô¨Årst mean subtracting the data as usual to get X . We can then take the Singular Value Decomposition SVD of X to get matrices U S V. We then compute UX to project X into the basis deÔ¨Åned by the columns of U. We Ô¨Ånally divide each dimension of the result by the corresponding singular value in S to scale our data appropriately if a singular value is zero we can just divide by a small number instead . 2.6 Parameter Initialization A key step towards achieving superlative performance with a neu ral network is initializing the parameters in a reasonable way. A good starting strategy is to initialize the weights to small random numbers normally distributed around 0 and in practice this often words acceptably well. However in Understanding the dif ficulty of training deep feedforward neural networks 2010 Xavier et al study the effect of different weight and bias initialization schemes on training dynamics. The empirical Ô¨Åndings suggest that for sigmoid and tanh activation units faster convergence and lower error rates are achieved when the weights of a matrix W Rn l 1 n l are initialized randomly with a uniform distribution as follows W U s 6 n l n l 1 s 6 n l n l 1 Where n l is the number of input units to W fan in and n l 1 is the number of output units from W fan out . In this parameter initialization scheme bias units are initialized to 0. This approach attempts to maintain activation variances as well as backpropagated gradient variances across layers. Without such initialization the gradient variances which are a proxy for information generally decrease with backpropagation across layers. 2.7 Learning Strategies The rate magnitude of model parameter updates during training can be controlled using the learning rate.', 'file name: lecture02 notes Backpropagation.  Neural networks. Forward computation. Backward propagation. Neuron Units. Max margin Loss. Gradient checks. Xavier parameter initialization. Learning rates. Adagrad. This set of notes introduces single and multilayer neural networks and how they can be used for classiÔ¨Åcation purposes. We then dis cuss how they can be trained using a distributed gradient descent technique known as backpropagation. We will see how the chain rule can be used to make parameter updates sequentially. After a rigorous mathematical discussion of neural networks we will discuss some practical tips and tricks in training neural networks involving neuron units non linearities gradient checks Xavier parameter ini tialization learning rates Adagrad etc. Lastly we will motivate the use of recurrent neural networks as a language model. 1 Neural Networks Foundations Figure 1 We see here how a non linear decision boundary separates the data very well. This is the prowess of neural networks. Fun Fact Neural networks are biologically in spired classiÔ¨Åers which is why they are often called artiÔ¨Åcial neural networks to distinguish them from the organic kind. However in reality human neural networks are so much more capable and complex from artiÔ¨Åcial neural net works that it is usually better to not draw too many parallels between the two. We established in our previous discussions the need for non linear classiÔ¨Åers since most data are not linearly separable and thus our classiÔ¨Åcation performance on them is limited. Neural networks are a family of classiÔ¨Åers with non linear decision boundary as seen in Figure 1. Now that we know the sort of decision boundaries neural networks create let us see how they manage doing so. 1.1 A Neuron A neuron is a generic computational unit that takes n inputs and produces a single output. What differentiates the outputs of different neurons is their parameters also referred to as their weights . One of the most popular choices for neurons is the sigmoid or binary lo gistic regression unit.']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.914254739826237,1.0,0.8969246338460815,0.0285714285714285
269,"What are the differences between Vanilla RNNs, LSTMs, and GRUs?","The main differences between Vanilla RNNs, LSTMs, and GRUs are as follows:

1. **Vanilla RNNs**:
   - Vanilla RNNs suffer from the vanishing/exploding gradient problem, which makes it difficult for them to capture long-term dependencies.
   - They have a simple structure with a single tanh activation function.
   - They struggle to retain information for a long time due to the vanishing gradient problem.

2. **LSTMs (Long Short-Term Memory)**:
   - LSTMs address the vanishing gradient problem by introducing a more complex structure with three gates: input gate, forget gate, and output gate.
   - They can retain information over long sequences, making them more suitable for tasks requiring memory of past inputs.
   - LSTMs have a separate cell state that allows them to control the flow of information more effectively.

3. **GRUs (Gated Recurrent Units)**:
   - GRUs are a variation of LSTMs that combine the forget and input gates into a single update gate.
   - They have a simpler structure compared to LSTMs, with fewer parameters to learn.
   - GRUs merge the cell state and hidden state, making them computationally more efficient than LSTMs.

In summary, Vanilla RNNs have a simple structure but struggle with capturing long-term dependencies, LSTMs address this issue with a more complex architecture including separate gates for controlling information flow, and GRUs simplify the LSTM structure by combining gates and merging states for improved efficiency.

Source: SEP 775 - Computational Natural Language Processing, lecture notes and provided context information.","['We only forget when we re going to input something in its place. We only input new values to the state when we forget something older. A slightly more dramatic variation on the LSTM is the Gated Recurrent Unit or GRU introduced by Cho et al. 2014 http arxiv.org pdf 1406.1078v3.pdf . It combines the forget and input gates into a single update gate. It also merges the cell state and hidden state and makes some other changes. The resulting model is simpler than standard LSTM models and has been growing increasingly popular. These are only a few of the most notable LSTM variants. There are lots of others like Depth Gated RNNs by Yao et al. 2015 http arxiv.org pdf 1508.03790v2.pdf . There s also some completely different approach to tackling long term dependencies like Clockwork RNNs by Koutnik et al. 2014 http arxiv.org pdf 1402.3511v1.pdf . Which of these variants is best Do the differences matter Greff et al. 2015 http arxiv.org pdf 1503.04069.pdf do a nice comparison of popular variants finding that they re all about the same. Jozefowicz et al. 2015 http jmlr.org proceedings papers v37 jozefowicz15.pdf tested more than ten thousand RNN architectures finding some that worked better than LSTMs on certain tasks. Conclusion Earlier I mentioned the remarkable results people are achieving with RNNs. Essentially all of these are achieved using LSTMs. They really work a lot better for most tasks Written down as a set of equations LSTMs look pretty intimidating. Hopefully walking through them step by step in this essay has made them a bit more approachable.1 24 24 3 23 PM Understanding LSTM Networks colah s blog 7 7 Attention and Augmented Recurrent Neural Networks On Distill Conv Nets A Modular Perspective Neural Networks Manifolds and Topology Deep Learning NLP and Representations LSTMs were a big step in what we can accomplish with RNNs.', 'Extension V given a word sequence A B C in German whose transla tion is X Y in English instead of training the RNN using A B C X Y train it using C B A X Y. The intutition behind this technique is that A is more likely to be translated to X. Thus given the vanishing gradient problem discussed earlier reversing the order of the input words can help reduce the error rate in generating the output phrase. 3 Gated Recurrent Units Beyond the extensions discussed so far RNNs have been found to per form better with the use of more complex units for activation. So far we have discussed methods that transition from hidden state ht 1 to ht using an afÔ¨Åne transformation and a point wise nonlinearity. Here we discuss the use of a gated activation function thereby modifying the RNN architecture. What motivates this Well although RNNs can theoretically capture long term dependencies they are very hard to actually train to do this. Gated recurrent units are designed in a manner to have more persistent memory thereby making it easier for RNNs to capture long term dependencies. Let us see mathematically how a GRU uses ht 1 and xt to generate the next hidden state ht. We will then dive into the intuition of this architecture. zt œÉ W z xt U z ht 1 Update gate rt œÉ W r xt U r ht 1 Reset gate ht tanh rt Uht 1 Wxt New memory ht 1 zt ht zt ht 1 Hidden state The above equations can be thought of a GRU s four fundamental op erational stages and they have intuitive interpretations that make this model much more intellectually satisfying see Figure 12 1. New memory generation A new memory ht is the consolidation of a new input word xt with the past hidden state ht 1. Anthropomor phically this stage is the one who knows the recipe of combining a newly observed word with the past hidden state ht 1 to summarize this new word in light of the contextual past as the vector ht. 2. Reset Gate The reset signal rt is responsible for determining how important ht 1 is to the summarization ht.']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9885228476530064,1.0,0.9417599208437222,0.037037037037037
270,What is the difference between traditional language models and conditioned language models?,"Traditional language models are generative models of text, while conditioned language models not only generate text but generate text according to some specification or input data. Conditioned language models take into account structured data, English documents, utterances, images, speech, or other forms of input to generate text output that is tailored to the given conditions or specifications.

Source: lecture04 Conditioned Generation. Hamidreza Mahyar mahyarh@mcmaster.ca Computational Natural Language Processing.","['file name: lecture04 Conditioned Generation. Hamidreza Mahyar mahyarh@mcmaster.ca Computational Natural Language Processing Conditioned GenerationLanguage Models Language models are generative models of text s P x The Malfoys said Hermione. Harry was watching him. He looked like Madame Maxime. When she strode up the wrong staircase to visit himself. I m afraid I ve definitely been suspended from power no chance indeed said Snape. He put his head back behind them and read groups as they crossed a corner and fluttered down onto their ink lamp and picked up his spoon. The doorbell rang. It was a lot cleaner down in London. Text Credit Max Deutsch https medium.com deep writingConditioned Language Models Not just generate text generate text according to some specification Input X Structured Data English Document Utterance Image Speech Output Y Text NL Description Japanese Short Description Response Text Transcript Task NL Generation Translation Summarization Response Generation Image Captioning Speech RecognitionFormulation and ModelingCalculating the Probability of a SentenceConditional Language ModelsOne Type of Language Model LSTM LSTM LSTM LSTM predict hate predict this predict movie predict s LSTM Mikolov et al. 2011 s I hate this movie predict ILSTM LSTM LSTM LSTM LSTM s LSTM LSTM LSTM LSTM argmax argmax argmax argmax argmax s One Type of Conditional Language Model I hate this movie kono eiga ga kirai hate this movie Sutskever et al. 2014 Encoder I DecoderHow to Pass Hidden State Initialize decoder w encoder Sutskever et al. 2014 encoder decoder Transform can be different dimensions encoder transform decoder encoder Input at every time step Kalchbrenner Blunsom 2013 decoder decoder decoderMethods of GenerationThe Generation Problem We have a model of P Y X how do we use it to generate a sentence Two methods Sampling Try to generate a random sentence according to the probability distribution. Argmax Try to generate the sentence with the highest probability.Ancestral Sampling Randomly generate words one by one.', 'For instance HLW 20 observe that larger models do not necessarily generalize better out of distribution. There is evidence that suggests that the generalization achieved under this paradigm can be poor because the model is overly speciÔ¨Åc to the training distribution and does not generalize well outside it YdC 19 MPL19 . Thus the performance of Ô¨Åne tuned models on speciÔ¨Åc benchmarks even when it is nominally at human level may exaggerate actual performance on the underlying task GSL 18 NK19 . Third humans do not require large supervised datasets to learn most language tasks a brief directive in natural language e.g. please tell me if this sentence describes something happy or something sad or at most a tiny number of demonstrations e.g. here are two examples of people acting brave please give a third example of bravery is often Figure 1.1 Language model meta learning. During unsupervised pre training a language model develops a broad set of skills and pattern recognition abilities. It then uses these abilities at inference time to rapidly adapt to or recognize the desired task. We use the term in context learning to describe the inner loop of this process which occurs within the forward pass upon each sequence. The sequences in this diagram are not intended to be representative of the data a model would see during pre training but are intended to show that there are sometimes repeated sub tasks embedded within a single sequence. 3Figure 1.2 Larger models make increasingly efÔ¨Åcient use of in context information. We show in context learning performance on a simple task requiring the model to remove random symbols from a word both with and without a natural language task description see Sec. 3.9.2 . The steeper in context learning curves for large models demonstrate improved ability to learn a task from contextual information. We see qualitatively similar behavior across a wide range of tasks. sufÔ¨Åcient to enable a human to perform a new task to at least a reasonable degree of competence. Aside from pointing to a conceptual limitation in our current NLP techniques this adaptability has practical advantages it allows humans to seamlessly mix together or switch between many tasks and skills for example performing addition during a lengthy dialogue.']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.0000000000000002,1.0,0.9596062166776196,0.0
271,How can LSTM networks be used in language modeling?,"LSTM networks can be used in language modeling by predicting the next word in a sequence of text. They are particularly effective for tasks such as handwriting recognition, speech recognition, machine translation, parsing, image captioning, and generating language models. LSTM networks have shown real-world success in achieving state-of-the-art results in various NLP tasks due to their ability to handle long-term dependencies and sequential data effectively.

Source: Lecture03_RNNs_and_LLMs","['are hard to train Another solution lots of new deep feedforward convolutional architectures add more direct connections thus allowing the gradient to flow Deep Residual Learning for Image Recognition He et al 2015. https arxiv.org pdf 1512.03385.pdfLSTMs real world success 68 In 2013 2015 LSTMs started achieving state of the art results Successful tasks include handwriting recognition speech recognition machine translation parsing and image captioning as well as language models LSTMs became the dominant approach for most NLP tasks Now 2019 2023 Transformers have become dominant for all tasks For example in WMT a Machine Translation conference competition In WMT 2014 there were 0 neural machine translation systems In WMT 2016 the summary report contains RNN 44 times and these systems won In WMT 2019 RNN 7 times Transformer 105 times Source Findings of the 2016 Conference on Machine Translation WMT16 Bojar et al. 2016 http www.statmt.org wmt16 pdf W16 2301.pdf Source Findings of the 2018 Conference on Machine Translation WMT18 Bojar et al. 2018 http www.statmt.org wmt18 pdf WMT028.pdf Source Findings of the 2019 Conference on Machine Translation WMT19 Barrault et al. 2019 http www.statmt.org wmt18 pdf WMT028.pdf5. Recap 69 Language Model A system that predicts the next word Recurrent Neural Network A family of neural networks that Take sequential input of any length Apply the same weights on each step Can optionally produce output on each step Recurrent Neural Network Language Model We ve shown that RNNs are a great way to build a LM despite some problems RNNs are also useful for much moreWhy should we care about Language Modeling 70 Language Modeling is a benchmark task that helps us measure our progress on predicting language use Language Modeling is a subcomponent of many NLP tasks especially those involving generating text or estimating the probability of text Predictive typing Speech recognition Handwriting recognition Spelling grammar correction Authorship identification Machine translation Summarization Dialogue etc.', 'But they really are pretty amazing. Essential to these successes is the use of LSTMs a very special kind of recurrent neural network which works for many tasks much much better than the standard version. Almost all exciting results based on recurrent neural networks are achieved with them. It s these LSTMs that this essay will explore. The Problem of Long Term Dependencies One of the appeals of RNNs is the idea that they might be able to connect previous information to the present task such as using previous video frames might inform the understanding of the present frame. If RNNs could do this they d be extremely useful. But can they It depends. Sometimes we only need to look at recent information to perform the present task. For example consider a language model trying to predict the next word based on the previous ones. If we are trying to predict the last word in the clouds are in the sky we don t need any further context it s pretty obvious the next word is going to be sky. In such cases where the gap between the relevant information and the place that it s needed is small RNNs can learn to use the past information. But there are also cases where we need more context. Consider trying to predict the last word in the text I grew up in France I speak fluent French. Recent information suggests that the next word is probably the name of a language but if we want to narrow down which language we need the context of France from further back. It s entirely possible for the gap between the relevant information and the point where it is needed to become very large. Unfortunately as that gap grows RNNs become unable to learn to connect the information. In theory RNNs are absolutely capable of handling such long term dependencies. A human could carefully pick parameters for them to solve toy problems of this form. Sadly in practice RNNs don t seem to be able to learn them. The problem was explored in depth by Hochreiter 1991 German http people.idsia.ch juergen SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf and Bengio et al. 1994 http www dsi.ing.unifi.it paolo ps tnn 94 gradient.pdf who found some pretty fundamental reasons why it might be difficult.']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9107272802407272,1.0,0.9468165641005564,0.0434782608695652
272,What are the methods of generation in conditioned language modeling?,"The methods of generation in conditioned language modeling are Sampling and Argmax. Sampling involves trying to generate a random sentence according to the probability distribution, while Argmax involves trying to generate the sentence with the highest probability. Ancestral Sampling is another method where words are randomly generated one by one.

Source: SEP 775 - Computational Natural Language Processing, lecture04 Conditioned Generation.","['file name: lecture04 Conditioned Generation. Hamidreza Mahyar mahyarh@mcmaster.ca Computational Natural Language Processing Conditioned GenerationLanguage Models Language models are generative models of text s P x The Malfoys said Hermione. Harry was watching him. He looked like Madame Maxime. When she strode up the wrong staircase to visit himself. I m afraid I ve definitely been suspended from power no chance indeed said Snape. He put his head back behind them and read groups as they crossed a corner and fluttered down onto their ink lamp and picked up his spoon. The doorbell rang. It was a lot cleaner down in London. Text Credit Max Deutsch https medium.com deep writingConditioned Language Models Not just generate text generate text according to some specification Input X Structured Data English Document Utterance Image Speech Output Y Text NL Description Japanese Short Description Response Text Transcript Task NL Generation Translation Summarization Response Generation Image Captioning Speech RecognitionFormulation and ModelingCalculating the Probability of a SentenceConditional Language ModelsOne Type of Language Model LSTM LSTM LSTM LSTM predict hate predict this predict movie predict s LSTM Mikolov et al. 2011 s I hate this movie predict ILSTM LSTM LSTM LSTM LSTM s LSTM LSTM LSTM LSTM argmax argmax argmax argmax argmax s One Type of Conditional Language Model I hate this movie kono eiga ga kirai hate this movie Sutskever et al. 2014 Encoder I DecoderHow to Pass Hidden State Initialize decoder w encoder Sutskever et al. 2014 encoder decoder Transform can be different dimensions encoder transform decoder encoder Input at every time step Kalchbrenner Blunsom 2013 decoder decoder decoderMethods of GenerationThe Generation Problem We have a model of P Y X how do we use it to generate a sentence Two methods Sampling Try to generate a random sentence according to the probability distribution. Argmax Try to generate the sentence with the highest probability.Ancestral Sampling Randomly generate words one by one.', 'Empirical evaluation and combination of advanced language modeling tech niques. In Proc. 12th annual conference of the in ternational speech communication association IN TERSPEECH 2011 . Mikolov T. Sutskever I. Deoras A. Le H. S. Kombrink S. and Cernocky J. 2012 . Subword language modeling with neural networks. preprint http www.Ô¨Åt.vutbr.cz imikolov rnnlm char.pdf . Moreira M. and Fiesler E. 1995 . Neural net works with adaptive learning rate and momentum terms. Idiap RR Idiap RR 04 1995 IDIAP Mar tigny Switzerland. Pascanu R. and Jaeger H. 2011 . A neurodynamical model for working memory. Neural Netw. 24 199 207. Rumelhart D. E. Hinton G. E. and Williams R. J. 1986 . Learning representations by back propagating errors. Nature 323 6088 533 536. Strogatz S. 1994 . Nonlinear Dynamics And Chaos With Applications To Physics Biology Chemistry And Engineering Studies in Nonlinearity . Studies in nonlinearity. Perseus Books Group 1 edition. Sutskever I. Martens J. and Hinton G. 2011 . Generating text with recurrent neural networks. In L. Getoor and T. ScheÔ¨Äer editors Proceedings of the 28th International Conference on Machine Learning ICML 11 ICML 11 pages 1017 1024 New York NY USA. ACM. Werbos P. J. 1988 . Generalization of backpropa gation with application to a recurrent gas market model. Neural Networks 1 4 339 356. Analytical analysis of the exploding and vanishing gradients problem xt WrecœÉ xt 1 Winut b 11 Let us consider the term gT k Et xt xt xk xk Œ∏ for the linear version of the parametrization in equation 11 i.e.']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.0000000000000002,0.75,0.938499094017048,0.025
273,What is beam search and how is it used in language generation?,"Beam search is a search algorithm used in natural language generation to find the most likely sequence of words. It works by maintaining a set of partial hypotheses (sequences of words) and expanding them at each step by considering multiple possible next words. The algorithm keeps track of a fixed number of these partial hypotheses, called the ""beam width,"" and selects the most promising ones based on a scoring function. This helps in exploring different possibilities efficiently and finding the best sequence of words according to the model's probabilities.

Source: Lecture on Computational Natural Language Processing","['BART Denoising sequence to sequence pre training for natural language generation translation and comprehension. In Association for Computa tional Linguistics ACL pages 7871 7880. Patrick Lewis Ethan Perez Aleksandara Piktus Fabio Petroni Vladimir Karpukhin Naman Goyal Heinrich K uttler Mike Lewis Wen tau Yih Tim Rockt aschel Sebastian Riedel and Douwe Kiela. 2020b. Retrieval augmented generation for knowledge intensive NLP tasks. In Advances in Neural Information Processing Systems NeurIPS .Yankai Lin Haozhe Ji Zhiyuan Liu and Maosong Sun. 2018. Denoising distantly supervised open domain question answering. In Association for Computa tional Linguistics ACL pages 1736 1745. Sewon Min Danqi Chen Hannaneh Hajishirzi and Luke Zettlemoyer. 2019a. A discrete hard EM ap proach for weakly supervised question answering. In Empirical Methods in Natural Language Process ing EMNLP . Sewon Min Danqi Chen Luke Zettlemoyer and Han naneh Hajishirzi. 2019b. Knowledge guided text re trieval and reading for open domain question answer ing. ArXiv abs 1911.03868. Dan Moldovan Marius Pas ca Sanda Harabagiu and Mihai Surdeanu. 2003. Performance issues and er ror analysis in an open domain question answering system. ACM Transactions on Information Systems TOIS 21 2 133 154. Stephen Mussmann and Stefano Ermon. 2016. Learn ing and inference via maximum inner product search. In International Conference on Machine Learning ICML pages 2587 2596. Yixin Nie Songhe Wang and Mohit Bansal. 2019. Re vealing the importance of semantic retrieval for ma chine reading at scale. In Empirical Methods in Nat ural Language Processing EMNLP . Rodrigo Nogueira and Kyunghyun Cho. 2019. Passage re ranking with BERT.', 'As there are 40 different phrase labels in WSJ each additional tree related lookup tables has 161 entries 40 4 1 corresponding to the IBES segment tags plus the extra O tag. We call level 0 the information associated with the leaves of the original Charniak parse tree. The lookup table for level 0 encodes the corresponding IOBES phrase tags for each words. We obtain levels 1 to 4 by repeatedly trimming the leaves as shown in Figure 6. We labeled O words belonging to the root node S or all words of the sentence if the root itself has been trimmed. Experiments were performed using the LM2 language model using the same network archi tectures see Table 5 and using additional lookup tables of dimension 5 for each parse tree level. Table 12 reports the performance improvements obtained by providing increasing levels of parse 18. In a more recent work Collobert 2011 we propose an extension of this approach for the generation of full syntactic parse trees using a recurrent version of our architecture. 2522NATURAL LANGUAGE PROCESSING ALMOST FROM SCRATCH Approach SRL valid test Benchmark System six parse trees 77.35 77.92 Benchmark System top Charniak parse tree only 74.76 NN SLL LM2 72.29 74.15 NN SLL LM2 Charniak level 0 only 74.44 75.65 NN SLL LM2 Charniak levels 0 1 74.50 75.81 NN SLL LM2 Charniak levels 0 to 2 75.09 76.05 NN SLL LM2 Charniak levels 0 to 3 75.12 75.89 NN SLL LM2 Charniak levels 0 to 4 75.42 76.06 NN SLL LM2 CHUNK 74.72 NN SLL LM2 PT0 75.49 Table 12 Generalization performance on the SRL task of our NN architecture compared with the benchmark system. We show performance of our system fed with different levels of depth of the Charniak parse tree.']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9218209045611296,1.0,0.9372576152611776,0.0
274,Explain the concept of model ensembling in language models.,"Model ensembling in language models refers to the technique of combining multiple individual models to improve overall performance. This can be done by training different models with varying architectures, hyperparameters, or training data, and then combining their predictions in some way, such as averaging or taking a vote. By leveraging the strengths of each individual model, ensembling can help improve the accuracy, robustness, and generalization of the overall language model.

Source: SEP 775 - Computational Natural Language Processing","['For this graph we also include 6 additional extra small models with as few as 100 000 parameters. As observed in KMH 20 language modeling performance follows a power law when making efÔ¨Åcient use of training compute. After extending this trend by two more orders of magnitude we observe only a slight if any departure from the power law. One might worry that these improvements in cross entropy loss come only from modeling spurious details of our training corpus. However we will see in the following sections that improvements in cross entropy loss lead to consistent performance gains across a broad spectrum of natural language tasks. Below we evaluate the 8 models described in Section 2 the 175 billion parameter parameter GPT 3 and 7 smaller models on a wide range of datasets. We group the datasets into 9 categories representing roughly similar tasks. In Section 3.1 we evaluate on traditional language modeling tasks and tasks that are similar to language modeling such as Cloze tasks and sentence paragraph completion tasks. In Section 3.2 we evaluate on closed book question answering tasks tasks which require using the information stored in the model s parameters to answer general knowledge questions. In Section 3.3 we evaluate the model s ability to translate between languages especially one shot and few shot . In Section 3.4 we evaluate the model s performance on Winograd Schema like tasks. In Section 3.5 we evaluate on datasets that involve commonsense reasoning or question answering. In Section 3.6 we evaluate on reading comprehension tasks in Section 3.7 we evaluate on the SuperGLUE benchmark suite and in 3.8 we brieÔ¨Çy explore NLI. Finally in Section 3.9 we invent some additional tasks designed especially to probe in context learning abilities these tasks focus on on the Ô¨Çy reasoning adaptation skills or open ended text synthesis. We evaluate all tasks in the few shot one shot and zero shot settings. 10Figure 3.1 Smooth scaling of performance with compute. Performance measured in terms of cross entropy validation loss follows a power law trend with the amount of compute used for training. The power law behavior observed in KMH 20 continues for an additional two orders of magnitude with only small deviations from the predicted curve.', 'Any kind of knowledge of the test set can cause the perplexity to be artiÔ¨Åcially low. The perplexity of two language models is only comparable if they use identical vocabularies. An intrinsic improvement in perplexity does not guarantee an extrinsic im provement in the performance of a language processing task like speech recognition10 CHAPTER 3 N GRAM LANGUAGE MODELS or machine translation. Nonetheless because perplexity often correlates with such improvements it is commonly used as a quick check on an algorithm. But a model s improvement in perplexity should always be conÔ¨Årmed by an end to end evaluation of a real task before concluding the evaluation of the model. 3.3 Sampling sentences from a language model One important way to visualize what kind of knowledge a language model embodies is to sample from it. Sampling from a distribution means to choose random points sampling according to their likelihood. Thus sampling from a language model which rep resents a distribution over sentences means to generate some sentences choosing each sentence according to its likelihood as deÔ¨Åned by the model. Thus we are more likely to generate sentences that the model thinks have a high probability and less likely to generate sentences that the model thinks have a low probability. This technique of visualizing a language model by sampling was Ô¨Årst suggested very early on by Shannon 1951 and Miller and Selfridge 1950 . It s simplest to visualize how this works for the unigram case. Imagine all the words of the English language covering the probability space between 0 and 1 each word covering an interval proportional to its frequency. Fig. 3.3 shows a visualization using a unigram LM computed from the text of this book. We choose a random value between 0 and 1 Ô¨Ånd that point on the probability line and print the word whose interval includes this chosen value. We continue choosing random numbers and generating words until we randomly generate the sentence Ô¨Ånal token s .']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9582728788513124,0.0,0.9194816764119222,0.0303030303030303
275,How does ensemble distillation differ from parameter averaging?,"Ensemble distillation differs from parameter averaging in that ensemble distillation involves training a model to copy the ensemble by matching the distribution over predicted words, aiming to make the same mistakes as the ensemble to increase accuracy. On the other hand, parameter averaging is a method where models are written out several times near the end of training, and the average of parameters is taken to achieve some effects of ensembling, but it only works for models within the same run without the focus on matching the ensemble's mistakes.

Source: Lecture04_Conditioned_Generation","['An exact method for sampling from P X no further work needed.Greedy Search One by one pick the single highest probability word Not exact real problems Will often generate the easy words first Will prefer multiple common words to one rare wordBeam Search Instead of picking one high probability word maintain several paths Some in reading materials more in a later classModel EnsemblingEnsembling Why Multiple models make somewhat uncorrelated errors Models tend to be more uncertain when they are about to make errors Smooths over idiosyncrasies of the model LSTM1 predict1 I LSTM2 predict2 Combine predictions from multiple models s sLinear Interpolation Take a weighted average of the M model probabilities Second term often set to uniform distribution 1 MLog linear Interpolation Weighted combination of log probabilities normalize Interpolation coefficient often set to uniform distribution 1 MLinear or Log Linear Think of it in logic Linear Logical OR the interpolated model likes any choice that a model gives a high probability use models with models that capture different traits necessary when any model can assign zero probability Log Linear Logical AND interpolated model only likes choices where all models agree use when you want to restrict possible answersParameter Averaging Problem Ensembling means we have to use M models at test time increasing our time memory complexity Parameter averaging is a cheap way to get some good effects of ensembling Basically write out models several times near the end of training and take the average of parametersEnsemble Distillation e.g. Kim et al. 2016 Problem parameter averaging only works for models within the same run Knowledge distillation trains a model to copy the ensemble Specifically it tries to match the description over predicted words Why We want the model to make the same mistakes as an ensemble Shown to increase accuracy notablyStacking What if we have two very different models where prediction of outputs is done in very different ways e.g. a phrase based translation model and a neural MT model Niehues et al. 2017 Stacking uses the output of one system in calculating features for another systemCase Studies in Conditional Language ModelingFrom Images e.g. Karpathy et al. 2015 Input is image features output is text Use standard image encoders e.g.', 'Training Strategy Distillation Objective Design a loss function that effectively combines the traditio nal task specific loss and a distillation loss that measures the discrepancy between the teacher s and student s hidden states. Dataset Selection Utilize task specific datasets like the Stanford Sentiment Treebank SST for sentiment analysis CoNLL 2003 for NER and WMT datasets for machine translation. Training and Distillation Fine tune the student model on the selected dataset while also learning from the teacher model s hidden state outputs to achieve similar perfor mance. McMaster University 4McMaster University SEP 775 Evaluation Metrics Performance Metrics Use accuracy for classification tasks sentiment analysis NER and BLEU scores for generative tasks machine translation to measure the student mo del s effectiveness. Efficiency Metrics Evaluate model size reduction and inference speed improvements in the student model compared to the teacher model. Fidelity Metrics Measure the similarity between the student and teacher model out puts using techniques such as cosine similarity for hidden states. McMaster University 5McMaster University SEP 775 General Evaluation Criteria and Submission Requirements Evaluation Criteria The evaluation of the projects will be based on the following criteria 1. Accuracy and Performance The model s performance on the task measured against a predefined metric e.g. BLEU for generation F1 for classification tasks perplexity for language models . 2. Quality of Code Clarity organization and documentation of the code. The code should be well structured readable and accompanied by comments explaining the func tionality. 3. Data Preprocessing and Handling The effectiveness of data cleaning preprocessing and handling techniques. This includes dealing with missing values data augmentation and the use of effective data loaders. 4. Model Optimization and Training The effectiveness of model optimization techni ques including hyperparameter tuning regularization methods and training procedures. 5. Analysis and Interpretation Depth of analysis on the model s performance including error analysis limitations of the model and interpretation of the results. Insightful observations and potential improvements should be discussed. 6. Project Report The quality of the project report including clarity of writing or ganization and thoroughness in covering the project s scope methodology results and conclusions. The report should also include a clear abstract and introduction to the problem. 7. Innovation and Creativity The uniqueness of the approach and creativity in solving the problem.']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.987278043698859,1.0,0.9678440245463436,0.05
276,What are some common evaluation metrics for conditioned language models?,"Some common evaluation metrics for conditioned language models include BLEU, Embedding based Metrics (such as BertScore, BLEURT, COMET, PRISM), and Perplexity. Additionally, meta-evaluation, which involves running human evaluation and automatic evaluation on the same outputs and calculating correlation, is also used to evaluate conditioned language models. 

Source: SEP 775 - Computational Natural Language Processing, lecture04_Conditioned_Generation","['2015 Input is image features output is text Use standard image encoders e.g. CNN Often pre trained on large databases such as ImageNetCheck out this review paperHow do we EvaluateBasic Evaluation Paradigm Use parallel test set Use system to generate translations Compare target translations w referenceHuman Evaluation Ask a human to do evaluation Final goal but slow expensive and sometimes inconsistentBLEU Works by comparing n gram overlap w reference Pros Easy to use good for measuring system improvement Cons Often doesn t match human eval bad for comparing very different systemsEmbedding based Metrics Recently many metrics based on neural models BertScore Find similarity between BERT embeddings unsupervised Zhang et al. 2020 BLEURT Train BERT to predict human evaluation scores Sellam et al. 2020 COMET Train model to predict human eval also using source sentence Rei et al. 2020 PRISM Model based on training paraphrasing model Thompson and Post 2020Perplexity Calculate the perplexity of the words in the held out set without doing generation Pros Naturally solves multiple reference problem Cons Doesn t consider decoding or actually generating output. May be reasonable for problems with lots of ambiguity.Which One to Use Meta evaluation runs human evaluation and automatic evaluation on the same outputs calculates correlation Examples WMT Metrics Task for MT Mathur et al. 2021 RealSumm for summarization Evaluation is hard especially with good systems Most metrics had no correlation w human eval over best systems at some WMT 2019 tasksQuestions', '. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 6.2 Fairness Bias and Representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36 6.3 Energy Usage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 7 Related Work 39 8 Conclusion 40 A Details of Common Crawl Filtering 43 B Details of Model Training 43 C Details of Test Set Contamination Studies 43 D Total Compute Used to Train Language Models 46 E Human Quality Assessment of Synthetic News Articles 46 F Additional Samples from GPT 3 48 G Details of Task Phrasing and SpeciÔ¨Åcations 50 H Results on All Tasks for All Model Sizes 63 21 Introduction Recent years have featured a trend towards pre trained language representations in NLP systems applied in increasingly Ô¨Çexible and task agnostic ways for downstream transfer.']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.0000000000000002,1.0,0.9182146560413416,0.0
277,What is the significance of stacking in model ensembling?,"Stacking is significant in model ensembling because it allows for combining the outputs of two very different models that predict outputs in different ways. This approach, as described in Niehues et al. 2017, uses the output of one system to calculate features for another system, enabling the models to complement each other's strengths and weaknesses, ultimately improving overall performance. 

Source: Lecture04_Conditioned_Generation","['An exact method for sampling from P X no further work needed.Greedy Search One by one pick the single highest probability word Not exact real problems Will often generate the easy words first Will prefer multiple common words to one rare wordBeam Search Instead of picking one high probability word maintain several paths Some in reading materials more in a later classModel EnsemblingEnsembling Why Multiple models make somewhat uncorrelated errors Models tend to be more uncertain when they are about to make errors Smooths over idiosyncrasies of the model LSTM1 predict1 I LSTM2 predict2 Combine predictions from multiple models s sLinear Interpolation Take a weighted average of the M model probabilities Second term often set to uniform distribution 1 MLog linear Interpolation Weighted combination of log probabilities normalize Interpolation coefficient often set to uniform distribution 1 MLinear or Log Linear Think of it in logic Linear Logical OR the interpolated model likes any choice that a model gives a high probability use models with models that capture different traits necessary when any model can assign zero probability Log Linear Logical AND interpolated model only likes choices where all models agree use when you want to restrict possible answersParameter Averaging Problem Ensembling means we have to use M models at test time increasing our time memory complexity Parameter averaging is a cheap way to get some good effects of ensembling Basically write out models several times near the end of training and take the average of parametersEnsemble Distillation e.g. Kim et al. 2016 Problem parameter averaging only works for models within the same run Knowledge distillation trains a model to copy the ensemble Specifically it tries to match the description over predicted words Why We want the model to make the same mistakes as an ensemble Shown to increase accuracy notablyStacking What if we have two very different models where prediction of outputs is done in very different ways e.g. a phrase based translation model and a neural MT model Niehues et al. 2017 Stacking uses the output of one system in calculating features for another systemCase Studies in Conditional Language ModelingFrom Images e.g. Karpathy et al. 2015 Input is image features output is text Use standard image encoders e.g.', 'One potential explana tion for this result could be that for small scale models learning the 40 tasks used during instruction tuning Ô¨Ålls the entire model capacity causing these models to perform worse on new tasks. Under this potential explanation for the larger scale models instruction tuning Ô¨Ålls up some model capacity but also teaches these models how to follow instructions allowing them to generalize to new tasks with the remaining capacity. 4.3 ROLE OF INSTRUCTIONS 20 30 40 50 60 FT no instruction Eval instruction FT dataset name Eval instruction FT dataset name Eval dataset name FT instruction Eval instruction FLAN 37.3 46.6 47.0 55.2 Zero shot performance 4 task cluster avg. Figure 8 Ablation study result using mod els with instructions removed from Ô¨Ånetun ing FT . In a Ô¨Ånal ablation study we explore the role of in structions during Ô¨Ånetuning as one possibility is that performance gains come entirely from multi task Ô¨Åne tuning and the model could perform just as well without instructions. We hence consider two Ô¨Ånetuning setups without instructions. In a no template setup only inputs and outputs were given to the model e.g. for transla tion the input would be The dog runs. and the output would be Le chien court. . In a dataset name setup each input is prepended with the name of the task and dataset e.g. for translation to French the input would be Translation WMT 14 to French The dog runs. . We compare these two ablations to FLAN s Ô¨Ånetun ing procedure which used natural instructions e.g. Please translate this sentence to French The dog runs. . We perform evaluations for four held out clus ters from Figure 5. For the no template setup we used the FLAN instructions during zero shot inference because if we used no template the model would not know what task to perform . For models Ô¨Ånetuned on dataset name only we report zero shot performance for FLAN instructions as well as using the dataset name.']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9829394607477268,1.0,0.9144975244871092,0.0588235294117647
278,How are images used as input for conditioned language models?,"Images are used as input for conditioned language models by embedding them into a common vector space along with sentences. In this process, a model encodes sentences using a GRU and encodes images using the outputs of a pre-trained VGG ConvNet. The order embedding model represents images and sentences as a 2-level partial ordering and utilizes a specific scoring function to establish relationships between images and language elements. Additionally, layer normalization is applied to the GRU in order to enhance the performance of the model.","['With fewer layers its larger receptive Ô¨Åelds allow the Image Transformer to signiÔ¨Åcantly improve over the state of the art in unconditional probabilis tic image modeling of comparatively complex images from ImageNet as well as super resolution. We further hope to have provided additional evidence that even in the light of generative adversarial networks likelihood based models of images is very much a promising area for further research as is using network architectures such as the Image Transformer in GANs. In future work we would like to explore a broader variety of conditioning information including free form text as previously proposed Mansimov et al. 2015 and tasks combining modalities such as language driven editing of images. Fundamentally we aim to move beyond still images to video Kalchbrenner et al. 2016 and towards applications in model based reinforcement learning. References Ba Jimmy Lei Kiros Jamie Ryan and Hinton Geoffrey E. Layer normalization. arXiv preprint arXiv 1607.06450 2016. Bellemare Marc G. Srinivasan Sriram Ostrovski Georg Schaul Tom Saxton David and Munos R emi. Unifying count based exploration and intrinsic motivation. CoRR abs 1606.01868 2016. URL http arxiv.org abs 1606.01868. Bengio Yoshua and Bengio Samy. Modeling high dimensional discrete data with multi layer neural net works. In Neural Information Processing Systems pp. 400 406. MIT Press 2000. Berthelot David Schumm Tom and Metz Luke. BEGAN boundary equilibrium generative adversarial networks.Image Transformer CoRR abs 1703.10717 2017. URL http arxiv. org abs 1703.10717. Chen Xi Mishra Nikhil Rohaninejad Mostafa and Abbeel Pieter. Pixelsnail An improved autoregres sive generative model. arXiv preprint arXiv 1712.09763 2017. Cheng Jianpeng Dong Li and Lapata Mirella. Long short term memory networks for machine reading. arXiv preprint arXiv 1601.06733 2016.', 'Unless otherwise noted the default initialization of layer normalization is to set the adaptive gains to 1 and the biases to 0 in the experiments. 6.1 Order embeddings of images and language In this experiment we apply layer normalization to the recently proposed order embeddings model of Vendrov et al. 2016 for learning a joint embedding space of images and sentences. We follow the same experimental protocol as Vendrov et al. 2016 and modify their publicly available code to incorporate layer normalization 1 which utilizes Theano Team et al. 2016 . Images and sen tences from the Microsoft COCO dataset Lin et al. 2014 are embedded into a common vector space where a GRU Cho et al. 2014 is used to encode sentences and the outputs of a pre trained VGG ConvNet Simonyan and Zisserman 2015 10 crop are used to encode images. The order embedding model represents images and sentences as a 2 level partial ordering and replaces the cosine similarity scoring function used in Kiros et al. 2014 with an asymmetric one. 1https github.com ivendrov order embedding 60 100 200 300 400 500 600 700 800 training steps thousands 0.4 0.5 0.6 0.7 0.8 0.9 1.0 validation error rate Attentive reader LSTM BN LSTM BN everywhere LN LSTM Figure 2 Validation curves for the attentive reader model. BN results are taken from Cooijmans et al. 2016 . We trained two models the baseline order embedding model as well as the same model with layer normalization applied to the GRU. After every 300 iterations we compute Recall@K R@K values on a held out validation set and save the model whenever R@K improves. The best performing models are then evaluated on 5 separate test sets each containing 1000 images and 5000 captions for which the mean results are reported. Both models use Adam Kingma and Ba 2014 with the same initial hyperparameters and both models are trained using the same architectural choices as used in Vendrov et al. 2016 . We refer the reader to the appendix for a description of how layer normalization is applied to GRU.']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.000000000000002,1.0,0.8938280667692573,0.0
279,What challenges exist in evaluating conditioned language models?,"One challenge in evaluating conditioned language models is that larger models do not necessarily generalize better out of distribution, as they may be overly specific to the training distribution and struggle to generalize well outside of it. Additionally, the performance of fine-tuned models on specific benchmarks, even when seemingly at human level, may exaggerate their actual performance on the underlying task. Furthermore, humans do not require large supervised datasets to learn most language tasks, as they can often learn from brief directives in natural language or a small number of demonstrations. This contrasts with the challenges faced by language models in terms of generalization and performance evaluation.

Source: Language Models_are_Few_Shot_Learners.txt","['. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 6.2 Fairness Bias and Representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36 6.3 Energy Usage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 7 Related Work 39 8 Conclusion 40 A Details of Common Crawl Filtering 43 B Details of Model Training 43 C Details of Test Set Contamination Studies 43 D Total Compute Used to Train Language Models 46 E Human Quality Assessment of Synthetic News Articles 46 F Additional Samples from GPT 3 48 G Details of Task Phrasing and SpeciÔ¨Åcations 50 H Results on All Tasks for All Model Sizes 63 21 Introduction Recent years have featured a trend towards pre trained language representations in NLP systems applied in increasingly Ô¨Çexible and task agnostic ways for downstream transfer.', 'For instance HLW 20 observe that larger models do not necessarily generalize better out of distribution. There is evidence that suggests that the generalization achieved under this paradigm can be poor because the model is overly speciÔ¨Åc to the training distribution and does not generalize well outside it YdC 19 MPL19 . Thus the performance of Ô¨Åne tuned models on speciÔ¨Åc benchmarks even when it is nominally at human level may exaggerate actual performance on the underlying task GSL 18 NK19 . Third humans do not require large supervised datasets to learn most language tasks a brief directive in natural language e.g. please tell me if this sentence describes something happy or something sad or at most a tiny number of demonstrations e.g. here are two examples of people acting brave please give a third example of bravery is often Figure 1.1 Language model meta learning. During unsupervised pre training a language model develops a broad set of skills and pattern recognition abilities. It then uses these abilities at inference time to rapidly adapt to or recognize the desired task. We use the term in context learning to describe the inner loop of this process which occurs within the forward pass upon each sequence. The sequences in this diagram are not intended to be representative of the data a model would see during pre training but are intended to show that there are sometimes repeated sub tasks embedded within a single sequence. 3Figure 1.2 Larger models make increasingly efÔ¨Åcient use of in context information. We show in context learning performance on a simple task requiring the model to remove random symbols from a word both with and without a natural language task description see Sec. 3.9.2 . The steeper in context learning curves for large models demonstrate improved ability to learn a task from contextual information. We see qualitatively similar behavior across a wide range of tasks. sufÔ¨Åcient to enable a human to perform a new task to at least a reasonable degree of competence. Aside from pointing to a conceptual limitation in our current NLP techniques this adaptability has practical advantages it allows humans to seamlessly mix together or switch between many tasks and skills for example performing addition during a lengthy dialogue.']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9853673299566266,1.0,0.8508409913564148,0.0222222222222222
280,What is the instructor's name for this course?,"The instructor's name for the course ""Introduction to Computational Natural Language Processing"" is Hamidreza Mahyar. 

Source: NLP Winter 2024 Course Outline. Course Outline 1. COURSE INFORMATION Session Offered Winter 2024.","['file name: NLP Winter 2024 Course Outline. Course Outline 1. COURSE INFORMATION Session Offered Winter 2024 Course Name Introduction to Computational Natural Language Processing Course Code SEP 775 Date s and Time s of lectures Lectures In person Wednesday 3 30 pm 6 30 pm Program Name System and Technology Calendar Description This graduate course introduces fundamental concepts in computational natural language processing NLP and their applications to teaching students how to deal with textual data in Artificial Intelligence. This course demonstrates how machines can learn different tasks in natural language such as language modeling text generation machine translation and language understanding. In this regard we go over the most promising methods in this literature and the most recent state of the art techniques. Moreover this course explores different real world applications of NLP and helps students get hands on experience in this field. Instructor Hamidreza Mahyar E Mail mahyarh@mcmaster.ca Office Hours upon request 2. COURSE SPECIFICS Course Description This course covers some fundamental concepts in natural language processing NLP and how they apply in real world use cases. Instruction Type Code Type Hours per term C Classroom instruction 39 L Laboratory workshop or fieldwork T Tutorial DE Distance education Total Hours 39 Resources ISBN Textbook Title Edition Author Publisher Natural Language Processing Jacob Eisenstein Speech and Language Processing Dan Jurafsky and James H. Martin Deep Learning Ian Goodfellow Yoshua Bengio and Aaron Courville Prerequisite s Proficiency in Python programming language Some Machine Learning and or Deep Learning knowledge is a plus Corequisite s Antirequisite s Course Specific Policies All work must be shown to get full credit. Specific policy of Projects Assignments submissionsProjects Assignments will be conducted online by using related software. Completed Projects Assignments should be uploaded to the drop box before midnight of the due date. Projects Assignments missed due to legitimate reasons must be completed later mutually agreed with the instructor. Departmental Policies The use of cell phones iPods laptops and other personal electronic devices are prohibited from the classroom during the class time unless the instructor makes an explicit exception. Announcements made in class or placed on Avenue are considered to have been communicated to all students including those individuals that are not in class. Instructor has the right to submit work to software to identify plagiarism.', 'file name: lecture07 Prompting Instruction Finetuning and RLHF . Hamidreza Mahyar mahyarh@mcmaster.ca Computational Natural Language Processing Prompting Instruction Finetuning and RLHFReminders Project proposals will be posted this week. A2 due Saturday 11 59PM We still recommend using Colab for the assignments in case you run into trouble e.g. you have exceeded Colab quota Please contact us 2Larger and larger models 3 https www.economist.com interactive briefing 2022 06 11 huge foundation models are turbo charging ai progress GPT 4 has 1.7 trillion parametersTrained on more and more data tokens seen during training https babylm.github.io 4 GPT 4 has seen 13 trillion tokensRecap What kinds of things does pretraining learn 5 Stanford University is located in California. Trivia I put fork down on the table. syntax The woman walked across the street checking for traffic over shoulder. coreference I went to the ocean to see the fish turtles seals and . lexical semantics topic Overall the value I got from the two hours watching it was the sum total of the popcorn and the drink. The movie was . sentiment Iroh went into the kitchen to make some tea. Standing next to Iroh Zuko pondered his destiny. Zuko left the .']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.915575637400826,1.0,0.8990640787794154,0.1923076923076923
281,What is the email address of the instructor for this course?,"The email address of the instructor for the course Computational Natural Language Processing is mahyarh@mcmaster.ca. 

Source: lecture07_Prompting_Instruction_Finetuning_and_RLHF","['If you have any questions or concerns about such disclosure please discuss this with the course instructor. ONLINE PROCTORING Some courses may use online proctoring software for tests and exams. This software may require students to turn on their video camera present identification monitor and record their computer activities and or lock restrict their browser or other applications software during tests or exams. This software may be required to be installed before the test exam begins. COMMUNICATIONS It is the student s responsibility to Maintain current contact information with the University including address phone numbers and emergency contact information. Use the University provided e mail address or maintain a valid forwarding e mail address. Regularly check the official University communications channels. Official University communications are considered received if sent by postal mail by fax or by e mail to the student s designated primary e mail account via their @mcmaster.ca alias. Accept that forwarded e mails may be lost and that e mail is considered received if sent via the student s @mcmaster.ca alias. Check the McMaster Avenue email and course websites on a regular basis during the term. CONDUCT EXPECTATIONS As a McMaster student you have the right to experience and the responsibility to demonstrate respectful and dignified interactions within all of our living learning and working communities. These expectations are described in the Code of Student Rights Responsibilities the Code . All students share the responsibility of maintaining a positive environment for the academic and personal growth of all McMaster community members whether in person or online. It is essential that students be mindful of their interactions online as the Code remains in effect in virtual learning environments. The Code applies to any interactions that adversely affect disrupt or interfere with reasonable participation in University activities. Student disruptions or behaviours that interfere with university functions on online platforms e.g. use of Avenue 2 Learn WebEx or Zoom for delivery will be taken very seriously and will be investigated. Outcomes may include restriction or removal of the involved students access to these platforms. ACADEMIC ACCOMMODATION OF STUDENTS WITH DISABILITIES Students with disabilities who require academic accommodation must contact Student Accessibility Services SAS at 905 525 9140 ext. 28652 or sas@mcmaster.ca to make arrangements with a Program Coordinator.', 'file name: lecture07 Prompting Instruction Finetuning and RLHF . Hamidreza Mahyar mahyarh@mcmaster.ca Computational Natural Language Processing Prompting Instruction Finetuning and RLHFReminders Project proposals will be posted this week. A2 due Saturday 11 59PM We still recommend using Colab for the assignments in case you run into trouble e.g. you have exceeded Colab quota Please contact us 2Larger and larger models 3 https www.economist.com interactive briefing 2022 06 11 huge foundation models are turbo charging ai progress GPT 4 has 1.7 trillion parametersTrained on more and more data tokens seen during training https babylm.github.io 4 GPT 4 has seen 13 trillion tokensRecap What kinds of things does pretraining learn 5 Stanford University is located in California. Trivia I put fork down on the table. syntax The woman walked across the street checking for traffic over shoulder. coreference I went to the ocean to see the fish turtles seals and . lexical semantics topic Overall the value I got from the two hours watching it was the sum total of the popcorn and the drink. The movie was . sentiment Iroh went into the kitchen to make some tea. Standing next to Iroh Zuko pondered his destiny. Zuko left the .']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.927217552317146,1.0,0.902150513037832,0.0344827586206896
282,When is this course held every week?,"The course ""Introduction to Computational Natural Language Processing"" (SEP 775) is held in person on Wednesdays from 3:30 pm to 6:30 pm.","['Thus a pear would Ô¨Çoat. So the answer is no. Table 26 Few shot exemplars for full chain of thought prompt for Date Understanding. PROMPT FOR DATE UNDERSTANDING Q 2015 is coming in 36 hours. What is the date one week from today in MM DD YYYY A If 2015 is coming in 36 hours then it is coming in 2 days. 2 days before 01 01 2015 is 12 30 2014 so today is 12 30 2014. So one week from today will be 01 05 2015. So the answer is 01 05 2015. Q The Ô¨Årst day of 2019 is a Tuesday and today is the Ô¨Årst Monday of 2019. What is the date today in MM DD YYYY A If the Ô¨Årst day of 2019 was Tuesday then 01 01 2019 was a Tuesday. Today is the Ô¨Årst monday would be six days later. So today is 01 07 2019. So the answer is 01 07 2019. Q The concert was scheduled to be on 06 01 1943 but was delayed by one day to today. What is the date 10 days ago in MM DD YYYY A One day after 06 01 1943 is 06 02 1943 so today is 06 02 1943. 10 days before today is 05 23 1943. So the answer is 05 23 1943. Q It is 4 19 1969 today. What is the date 24 hours later in MM DD YYYY A Today is 04 19 1969. 24 hours later is one day after today which would be 04 20 1969. So the answer is 04 20 1969. Q Jane thought today is 3 11 2002 but today is in fact Mar 12 which is 1 day later.', 'file name: NLP Winter 2024 Course Outline. Course Outline 1. COURSE INFORMATION Session Offered Winter 2024 Course Name Introduction to Computational Natural Language Processing Course Code SEP 775 Date s and Time s of lectures Lectures In person Wednesday 3 30 pm 6 30 pm Program Name System and Technology Calendar Description This graduate course introduces fundamental concepts in computational natural language processing NLP and their applications to teaching students how to deal with textual data in Artificial Intelligence. This course demonstrates how machines can learn different tasks in natural language such as language modeling text generation machine translation and language understanding. In this regard we go over the most promising methods in this literature and the most recent state of the art techniques. Moreover this course explores different real world applications of NLP and helps students get hands on experience in this field. Instructor Hamidreza Mahyar E Mail mahyarh@mcmaster.ca Office Hours upon request 2. COURSE SPECIFICS Course Description This course covers some fundamental concepts in natural language processing NLP and how they apply in real world use cases. Instruction Type Code Type Hours per term C Classroom instruction 39 L Laboratory workshop or fieldwork T Tutorial DE Distance education Total Hours 39 Resources ISBN Textbook Title Edition Author Publisher Natural Language Processing Jacob Eisenstein Speech and Language Processing Dan Jurafsky and James H. Martin Deep Learning Ian Goodfellow Yoshua Bengio and Aaron Courville Prerequisite s Proficiency in Python programming language Some Machine Learning and or Deep Learning knowledge is a plus Corequisite s Antirequisite s Course Specific Policies All work must be shown to get full credit. Specific policy of Projects Assignments submissionsProjects Assignments will be conducted online by using related software. Completed Projects Assignments should be uploaded to the drop box before midnight of the due date. Projects Assignments missed due to legitimate reasons must be completed later mutually agreed with the instructor. Departmental Policies The use of cell phones iPods laptops and other personal electronic devices are prohibited from the classroom during the class time unless the instructor makes an explicit exception. Announcements made in class or placed on Avenue are considered to have been communicated to all students including those individuals that are not in class. Instructor has the right to submit work to software to identify plagiarism.']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.8459878996307642,1.0,0.8737838994181921,0.1081081081081081
283,What is the name of the TA for this course?,The AI Teaching Assistant for the course is referred to as AI TA. It aims to provide an interactive platform for students to ask questions and receive guidance on course materials. It is designed to mirror the support traditionally offered by human teaching assistants.,"['file name: SEP775 FinalProjects 2. McMaster University SEP 775 Final Projects Project 1 AI Teaching Assistant Project Overview The goal of this project is to create an AI Teaching Assistant AI TA . The AI TA aims to provide an interactive platform for students to ask questions and receive guidance on course materials mirroring the support traditionally offered by human teaching assistants. The project emphasizes the application of NLP techniques to create an educational tool that enhances the learning experience through dynamic interaction and content understanding. Project Objectives Automate FAQ Responses Develop the AI TA to autonomously address frequently asked questions related to course logistics assignments and content. Content Understanding and Query Handling Implement NLP strategies enabling the AI TA to interpret and respond to inquiries about the uploaded course material accurately. Dynamic Interaction Design the AI TA to interact with students conversationally providing information and resources pertinent to their queries. Technical Approach NLP Engine Components Language Model Integration Leverage pre trained NLP models e.g. BERT Llama for robust text understanding and generation tapping into their vast linguistic knowledge. Natural Language Understanding NLU Employ models like BERT for NLU tasks to accurately parse student inquiries determining their intent and extracting relevant details for precise responses. Retrieval Augmented Generation RAG Central to the NLP Engine RAG com bines retrieval methods to access information within a structured database of course materials with generative capabilities to formulate coherent and context aware respon ses . This approach ensures the AI TA delivers informative answers directly pulling from course content and synthesizing it into helpful responses. Response Generation Utilize the designed AI TA to generate responses that are not only accurate but also engaging and pedagogically effective. Interaction Interface Simplified Interface Development Construct an intuitive web or command line in terface that allows straightforward question submission and response retrieval ensuring ease of use for student interactions with the AI TA. McMaster University 1McMaster University SEP 775 Project 2 TV Show Script Generation Project Overview For this project you will harness the power of Natural Language Processing NLP and Machine Learning to create a script for a new episode of a TV show you enjoy. The challenge is to design a model that captures the essence style dialogue and storyline of the original series thereby generating a script that feels authentic and could seamlessly fit into the show.', '2015. Teaching ma chines to read and comprehend. In Advances in Neu ral Information Processing Systems NIPS . Daniel Hewlett Alexandre Lacoste Llion Jones Illia Polosukhin Andrew Fandrianto Jay Han Matthew Kelcey and David Berthelot. 2016. Wikireading A novel large scale language understanding task over wikipedia. In Association for Computational Lin guistics ACL . pages 1535 1545. Felix Hill Antoine Bordes Sumit Chopra and Jason Weston. 2016. The Goldilocks Principle Reading children s books with explicit memory representa tions. In International Conference on Learning Rep resentations ICLR . Minyoung Huh Pulkit Agrawal and Alexei A Efros. 2016. What makes ImageNet good for transfer learning arXiv preprint arXiv 1608.08614 . Mohit Iyyer Jordan L Boyd Graber Leonardo Max Batista Claudino Richard Socher and Hal Daum e III. 2014. A neural network for factoid ques tion answering over paragraphs. In Empirical Meth ods in Natural Language Processing EMNLP . pages 633 644. Rudolf Kadlec Ondrej Bajgar and Jan Kleindienst. 2016. From particular to general A preliminary case study of transfer learning in reading compre hension. Machine Intelligence Workshop NIPS . Diederik Kingma and Jimmy Ba. 2014. Adam A method for stochastic optimization. arXiv preprint arXiv 1412.6980 . Kenton Lee Tom Kwiatkowski Ankur Parikh and Di panjan Das. 2016. Learning recurrent span repre sentations for extractive question answering. arXiv preprint arXiv 1611.01436 . Christopher D Manning Mihai Surdeanu John Bauer Jenny Finkel Steven J Bethard and David Mc Closky. 2014. The stanford corenlp natural lan guage processing toolkit. In Association for Com putational Linguistics ACL . pages 55 60.']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.858217201738217,1.0,0.8194825198603224,0.0192307692307692
284,What is Word2vec?,"Word2Vec is a probabilistic model used in natural language processing to generate word embeddings. It is designed to learn distributed representations of words in a continuous vector space based on the context in which words appear. The model aims to capture semantic relationships between words by representing them as vectors in such a way that words with similar meanings are closer together in the vector space. Word2Vec is commonly used for various NLP tasks such as sentiment analysis, named entity recognition, and machine translation. It involves training a neural network to predict the likelihood of a word given its context or vice versa, which results in dense, low-dimensional representations of words that can capture linguistic regularities and patterns.

Source: SEP 775 - Computational Natural Language Processing, lecture01_notes_Introduction.txt","['Skipgram word2vec. As usual we have a finite vocabulary V. Let C O be random variables representing an unknown pair of C V a center word and O V an outside word appearing in the context of the center word . We ll use c o to refer to specific values of the random variables. Let U R V d and V R V d. Note that each word in V is associated with a single row of U and one of V we think of this as resulting from an arbitrary ordering of V. The word2vec model is a probabilistic model specified as follows where uw refers to the row of U corresponding to word w V and likewise for V pU V o c exp u o vc w V exp u wvc 4 This may be familiar to you as the softmax function which takes arbitrary scores here one for each word in the vocabulary resulting from dot products and produces a probability distribution where larger scored things get higher probability. Note that the vector of probabilities over all words given a center word pU V c R V is a lot like a row of our old co occurrence matrix Xc. The story isn t over yet this is just a model. How do we estimate the values of the parameters U V We learn to minimize the cross entropy loss objective with the true distribution P O C min U V Eo c log pU V o c . 5 This equation should be read as minimize with respect to param eters U and V the expectation over values of o and c drawn from the distributions of O and V the negative log probability under the U V model of that value of o given that value of c . There is so much rich detail to get into here. How do we perform the min operation How do we get the random variables O and C Why the negative log of the probability Why is this so much better than co occurrence counting Can you tell why not all distributions over o given c can be represented by this model Should this be good Bad Surprising Obvious For now let s go through a few details about how to implement this in practice.', 'file name: SEP775 Assignment1. McMaster University SEP 775 Assignment 1 In Depth Word Vectors Analysis Total 50 Points Objectives This assignment focuses on a comprehensive understanding of word vector technologies specifically Word2Vec and GloVe. You will explore their applications visualize the results and analyze the semantic and syntactic relationships they capture. 1. Building and Analyzing Word Vectors with Word2Vec 20 Points Task Create word vectors using the Word2Vec model on a selected corpus. Visualization Utilize PCA or t SNE for visualizing these vectors in 2D. Analysis Discuss the word relationships and clusters formed. 2. GloVe Vectors Advanced Analysis 20 Points Task Analyze word relationships using pre trained GloVe vectors. Activity Conduct an analogy task e.g. king man woman with examples. Explanation Explain the results of the analogy tasks. 3. Semantic and Syntactic Word Relationships 10 Points Comparison Evaluate Word2Vec and GloVe for capturing semantic and syntactic relationships. Illustration Use specific word pairs or groups to demonstrate differences in repre sentation by each model. McMaster University 1McMaster University SEP 775 Resources for Word2Vec and GloVe Word2Vec Pre trained Word2Vec embeddings can be found on repositories such as HuggingFace or directly using Gensim. GloVe Pre trained GloVe embeddings are available at the GloVe website or through HuggingFace. Submission Guidelines Submit all code in a ZIP file including Jupyter Notebooks and a detailed PDF report with written explanations and visualizations. Clearly label each part and question in your submissions. Deadline Feb. 7 2024 Rubric and Expectations Code Quality and Functionality 40 Code should be well organized commented and functioning as intended. The use of Python and relevant libraries e.g. Gensim for Word2Vec should demonstrate a good grasp of the tools. Analysis and Interpretation 30 Written explanations should be insightful de monstrating a deep understanding of the word vector models. Analysis of visualizations analogy tasks and model comparisons should be thorough and reflective. Visualization 20 Visualizations should be clear accurately labeled and effectively convey the relationships or patterns discovered in the data. Adherence to Guidelines 10 Submissions should follow the provided guidelines including format labeling and adherence to the deadline. McMaster University 2']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.974819939000597,1.0,0.9219289882303002,0.027027027027027
285,What are the main learning goals of the course?,"The main learning goals of the course include experimenting with different values of K on a development set and running the best value on the test set, using natural language prompts and demonstrations, comparing LM likelihoods for correct completions, normalizing per token likelihood for length, framing tasks for binary classification as multiple choice, using beam search for free form completion tasks, scoring models using F1 similarity score, BLEU, or exact match, and reporting final results on publicly available test sets. Additionally, the course aims to display training curves for different models and cover tasks such as language modeling, closed book question answering, translation, Winograd style tasks, and common sense reasoning. 

Source: Language_Models_are_Few_Shot_Learners.txt","['Larger values of K are usually but not always better so when a separate development and test set are available we experiment with a few values of K on the development set and then run the best value on the test set. For some tasks see Appendix G we also use a natural language prompt in addition to or for K 0 instead of demonstrations. On tasks that involve choosing one correct completion from several options multiple choice we provide K examples of context plus correct completion followed by one example of context only and compare the LM likelihood of each completion. For most tasks we compare the per token likelihood to normalize for length however on a small number of datasets ARC OpenBookQA and RACE we gain additional beneÔ¨Åt as measured on the development set by normalizing by the unconditional probability of each completion by computing P completion context P completion answer context where answer context is the string Answer or A and is used to prompt that the completion should be an answer but is otherwise generic. On tasks that involve binary classiÔ¨Åcation we give the options more semantically meaningful names e.g. True or False rather than 0 or 1 and then treat the task like multiple choice we also sometimes frame the task similar to what is done by RSR 19 see Appendix G for details. On tasks with free form completion we use beam search with the same parameters as RSR 19 a beam width of 4 and a length penalty of Œ± 0.6. We score the model using F1 similarity score BLEU or exact match depending on what is standard for the dataset at hand. Final results are reported on the test set when publicly available for each model size and learning setting zero one and few shot . When the test set is private our model is often too large to Ô¨Åt on the test server so we report results on the development set. We do submit to the test server on a small number of datasets SuperGLUE TriviaQA PiQa where we were able to make submission work and we submit only the 200B few shot results and report development set results for everything else. 3 Results In Figure 3.1 we display training curves for the 8 models described in Section 2.', '. . . . . . . . . . . . . . . . . . . . 10 3 Results 10 3.1 Language Modeling Cloze and Completion Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 3.2 Closed Book Question Answering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 3.3 Translation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 3.4 Winograd Style Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 3.5 Common Sense Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999999999999996,0.8,0.8675619121099238,0.0
286,How is the course grade distributed?,"The course grade is distributed as follows: 4 Assignments (20%), 4 Quizzes (10%), Midterm Test (35%), and Group Project (35%). The percentage distribution of each module may be adjusted, and percentage grades will be converted to letter grades and grade points per the University calendar.","['Instructor has the right to submit work to software to identify plagiarism. 3. SUB TOPIC S Week 1 Overview Human language Word meanings Word2Vec and Optimization basics Week 2 Intro to neural networks Gradients and backpropagation Week 3 Recurrent Neural Networks and Language Models Week 4 Conditioned Generation Week 5 Part 1 Self attention and transformers Part 2 Seq2Seq models Week 6 Midterm Test Week 7 Midterm Recess No class Week 8 Prompting Instruction Finetuning and RLHF Week 9 Question Answering systems Week 10 Multimodal models Week 11 Project presentation and Q A Week 12 Project presentation and Q A Week 13 Project presentation and Q A Classes end April 10th 4. ASSESSMENT OF LEARNING Weight 4 Assignments 20 4 Quizzes 10 Midterm Test 35 Group Project 35 TOTAL 100Note that the percentage distribution of each module may be adjusted. Percentage grades will be converted to letter grades and grade points per the University calendar. 5. LEARNING OUTCOMES By the end of this course the students will know about Introduction to Deep Learning Word representations Language models Pre trained language models Text generation Seq2Seq models Prompting Feedback systems Multimodal models NLP applications e.g. Q A systems 6. COURSE OUTLINE APPROVED ADVISORY STATEMENTS ANTI DISCRIMINATION The Faculty of Engineering is concerned with ensuring an environment that is free of all discrimination. If there is a problem individuals are reminded that they should contact the Associate Director Graduate Studies the Sexual Harassment Officer or the Human Rights Consultant as soon as possible. http www.mcmaster.ca policy General HR Discrimination_Harassment_Sexual_Harassment Prevention Response.pdf ACADEMIC INTEGRITY You are expected to exhibit honesty and use ethical behaviour in all aspects of the learning process. Academic credentials you earn are rooted in principles of honesty and academic integrity. It is your responsibility to understand what constitutes academic dishonesty. Academic dishonesty is to knowingly act or fail to act in a way that results or could result in unearned academic credit or advantage. This behaviour can result in serious consequences e.g.', 'In comparison DPR is able to return Top 1 Top 5 Top 20 Top 100 Gold 44.9 66.8 78.1 85.0 Dist. Sup. 43.9 65.3 77.1 84.4 Table 5 Retrieval accuracy on the development set of Natural Questions trained on passages that match the gold context Gold or the top BM25 passage that con tains the answer Dist. Sup. . Sim Loss Retrieval Accuracy Top 1 Top 5 Top 20 Top 100 DP NLL 44.9 66.8 78.1 85.0 Triplet 41.6 65.0 77.2 84.5 L2 NLL 43.5 64.7 76.1 83.1 Triplet 42.2 66.0 78.1 84.9 Table 6 Retrieval Top k accuracy on the development set of Natural Questions using different similarity and loss functions. the correct answer presumably by matching body of water with semantic neighbors such as sea and channel even though no lexical overlap exists. The second example is one where BM25 does better. The salient phrase Thoros of Myr is critical and DPR is unable to capture it. D Joint Training of Retriever and Reader We Ô¨Åx the passage encoder in our joint training scheme while allowing only the question encoder to receive backpropagation signal from the com bined retriever reader loss function. This allows us to leverage the HNSW based FAISS index for efÔ¨Åcient low latency retrieving without reindexing the passages during model updates. Our loss func tion largely follows ORQA s approach which uses log probabilities of positive passages selected from the retriever model and correct spans and passages selected from the reader model. Since the passage encoder is Ô¨Åxed we could use larger amount of retrieved passages when calculating the retriever loss. SpeciÔ¨Åcally we get top 100 passages for each question in a mini batch and use the method similar to in batch negative training all retrieved passages vectors participate in the loss calculation for all questions in a batch.']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999999999999996,0.3333333333333333,0.9634955018473896,0.0
287,What is the policy for late submissions in this course?,"Students should contact their instructors as soon as possible to make alternative arrangements for classes, assignments, and tests in case of late submissions. It is important to communicate with the instructor regarding any delays in submitting work.","['28652 or sas@mcmaster.ca to make arrangements with a Program Coordinator. For further information consult McMaster University s Academic Accommodation of Students with Disabilities policy. ACADEMIC ACCOMMODATION FOR RELIGIOUS INDIGENOUS OR SPIRITUAL OBSERVANCES RISOStudents requiring academic accommodation based on religious indigenous or spiritual observances should follow the procedures set out in the RISO policy. Students should submit their request to their Faculty Office normally within 10 working days of the beginning of term in which they anticipate a need for accommodation or to the Registrar s Office prior to their examinations. Students should also contact their instructors as soon as possible to make alternative arrangements for classes assignments and tests. http www.mcmaster.ca policy Students AcademicStudies Studentcode.pdf COPYRIGHT AND RECORDING Students are advised that lectures demonstrations performances and any other course material provided by an instructor include copyright protected works. The Copyright Act and copyright law protect every original literary dramatic musical and artistic work including lectures by University instructors The recording of lectures tutorials or other methods of instruction may occur during a course. Recording may be done by either the instructor for the purpose of authorized distribution or by a student for the purpose of personal study. Students should be aware that their voice and or image may be recorded by others during the class. Please speak with the instructor if this is a concern for you. EXTREME CIRCUMSTANCES The University reserves the right to change the dates and deadlines for any or all courses in extreme circumstances e.g. severe weather labour disruptions etc. . Changes will be communicated through regular McMaster communication channels such as McMaster Daily News A2L and or McMaster email.', 'This behaviour can result in serious consequences e.g. the grade of zero on an assignment loss of credit with a notation on the transcript notation reads Grade of F assigned for academic dishonesty and or suspension or expulsion from the university. For information on the various types of academic dishonesty please refer to the Academic Integrity Policy located at https secretariat.mcmaster.ca university policies procedures guidelines The following illustrates only three forms of academic dishonesty plagiarism e.g. the submission of work that is not one s own or for which other credit has been obtained. improper collaboration in group work. copying or using unauthorized aids in tests and examinations. AUTHENTICITY PLAGIARISM DETECTION Some courses may use a web based service Turnitin.com to reveal authenticity and ownership of student submitted work. For courses using such software students will be expected to submit their work electronically either directly to Turnitin.com or via an online learning platform e.g. A2L etc. using plagiarism detection a service supported by Turnitin.com so it can be checked for academic dishonesty. Students who do not wish their work to be submitted through the plagiarism detection software must inform the Instructor before the assignment is due. No penalty will be assigned to a student who does not submit work to the plagiarism detection software. All submitted work is subject to normal verification that standards of academicintegrity have been upheld e.g. on line search other software etc. . For more details about McMaster s use of Turnitin.com please go to www.mcmaster.ca academicintegrity. COURSES WITH AN ON LINE ELEMENT Some courses may use on line elements e.g. e mail Avenue to Learn A2L LearnLink web pages capa Moodle ThinkingCap etc. . Students should be aware that when they access the electronic components of a course using these elements private information such as first and last names user names for the McMaster e mail accounts and program affiliation may become apparent to all other students in the same course. The available information is dependent on the technology used. Continuation in a course that uses on line elements will be deemed consent to this disclosure. If you have any questions or concerns about such disclosure please discuss this with the course instructor.']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9207640314034394,0.5,0.856014383331178,0.0
288,What does the course hope to teach regarding word vectors?,"The course hopes to teach the foundations of effective modern methods for deep learning applied to NLP, including basics and key methods used in NLP in 2024 such as word vectors, feed-forward networks, recurrent networks, attention, encoder-decoder models, transformers, and large pre-trained language models. Additionally, it aims to provide a big picture understanding of human languages and the challenges in understanding and producing them via computers, as well as the ability to build systems in PyTorch for major NLP problems like word meaning, dependency parsing, machine translation, and question answering. Collaboration policies and late day policies are also emphasized.","['file name: lecture01 wordvecs1. Hamidreza Mahyar mahyarh@mcmaster.ca Computational Natural Language Processing Overview of NLP and Word VectorsLecture Plan 2 Lecture 1 Introduction and Word Vectors 1. The course 2. Human language and word meaning 3. Word2vec introduction 4. Word2vec objective function gradients 5. Optimization basics 6. Looking at word vectors Key learning today The astounding result that word meaning can be represented rather well by a high dimensional vector of real numbersCourse logistics in brief 3 Instructor Hamidreza Mahyar Head TA Reza Namazi Time Wednesday 3 30pm 6 30pm We put a lot of other important information on Avenue to Learn. Please read itWhat do we hope to teach A.k.a. learning goals 4 1. The foundations of the effective modern methods for deep learning applied to NLP Basics first then key methods used in NLP in 2024 Word vectors feed forward networks recurrent networks attention encoder decoder models transformers large pre trained language models etc. 2. A big picture understanding of human languages and the difficulties in understanding and producing them via computers 3. An understanding of and ability to build systems in PyTorch for some of the major problems in NLP Word meaning dependency parsing machine translation question answeringCourse work and grading policy 5 4 Assignments 20 4 Quizzes 10 Midterm 35 Final Group Project 1 3 people 34 Late day policy NO free late days 1 off course grade per day late Assignments not accepted more than 3 days late unless given permission in advance Collaboration policy Understand allowed collaboration and how to document it Don t take code off the web acknowledge working with other students write your own assignment solutionsTrained on text data neural machine translation is quite good https kiswahili.tuko.co.keThe SEC said Musk your tweets are a blight. They really could cost you your job if you don t stop all this tweeting at night. Then Musk cried Why The tweets I wrote are not mean I don t use all caps and I m sure that my tweets are clean. But your tweets can move markets and that s why we re sore.', 'Word2vec Overview 21 Word2vec Mikolov et al. 2013 is a framework for learning word vectors Idea We have a large corpus body of text a long list of words Every word in a fixed vocabulary is represented by a vector Go through each position t in the text which has a center word c and context outside words o Use the similarity of the word vectors for c and o to calculate the probability of o given c or vice versa Keep adjusting the word vectors to maximize this probabilityWord2Vec Overview Example windows and process for computing ùëÉùë§ùë° ùëó ùë§ùë° crises banking into turning problems as center word at position t outside context words in window of size 2 outside context words in window of size 2 ùëÉùë§ùë° 1 ùë§ùë° 22 ùëÉùë§ùë° 2 ùë§ùë° ùëÉùë§ùë° 1 ùë§ùë° ùëÉùë§ùë° 2 ùë§ùë°Word2Vec Overview Example windows and process for computing ùëÉùë§ùë° ùëó ùë§ùë° crises banking into turning problems as center word at position t outside context words in window of size 2 outside context words in window of size 2 ùëÉùë§ùë° 2 ùë§ùë° ùëÉùë§ùë° 1 ùë§ùë° 23 ùëÉùë§ùë° 2 ùë§ùë° ùëÉùë§ùë° 1 ùë§ùë°Word2vec objective function For each position ùë° 1 ùëá predict context words within a window of fixed size m given center word ùë§ùë°. Data likelihood The objective function ùêΩùúÉis the average negative log likelihood Minimizing objective function Maximizing predictive accuracy sometimes called a cost or loss function 24Word2vec objective function We want to minimize the objective function Question How to calculate ùëÉùë§ùë° ùëó ùë§ùë° ùúÉ Answer We will use two vectors per word w']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.8935645846756067,0.8,0.8892719399870317,0.0588235294117647
289,How does Word2vec represent word meaning?,"Word2vec represents each word in a fixed vocabulary as a low-dimensional vector, much smaller than the vocabulary size. It learns the value of each word's vector to be predictive based on the distribution of words in a usually short context (2-4 words). The model, specifically the skipgram word2vec algorithm, assigns a vector to each word in the vocabulary and uses a simple function to predict the context words around a given center word. This approach helps capture semantic and syntactic relationships between words in the vector space representation.","['Here s an example where a few relevant windows for co occurrence are labeled Larger notions of co occurrence e.g. large windows or documents lead to more semantic or even topic encoding representations shorter windows lead to more syntax encoding representa tions It s hot and delicious. I poured the tea z center word for 1 my uncle 3. document In brief shorter windows like the one word window above labeled 1 seem to encode syntactic properties. For example nouns tend to appear right next to the or is. Plural nouns don t appear right next to a. Larger windows tend to encode more semantic and at extremes topic like properties. Note how poured or delicious may occur farther from tea but still be relevant. Document level windows for large documents 1000s of words intuitively represent words by what kinds of documents they appear in sports law medicine etc. Another design decision we made was to represent explicit counts of words in V sized vectors. This ends up being a big mistake. We ve already stated that high dimensional vectors tend to be un wieldy in today s neural systems. But another issue is that raw counts of words end up over emphasizing the importance of very common words like the. Taking the log token frequency ends up being much more useful. A very influential paper on word repre sentation taught us much more about what is wrong with the raw co occurrence method by introducing GloVe Pennington et al. 2014 a co occurence based word representation algorithm that works as well as word2vec the method we ll introduce in the next section. However many of the details of word2vec will hold true in methods that we ll proceed to further in the course so we ll focus our time on that. 3.2 Word2vec model and objective The word2vec model represents each word in a fixed vocabulary as a low dimensional much smaller than vocabulary size vector. It learns the value of each word s vector to be predictive via a simple function of the distribution of words in a usually short 2 4 words context. The model we ll describe here is called the skipgram word2vec algo rithm. Skipgram word2vec.', 'Skipgram word2vec. As usual we have a finite vocabulary V. Let C O be random variables representing an unknown pair of C V a center word and O V an outside word appearing in the context of the center word . We ll use c o to refer to specific values of the random variables. Let U R V d and V R V d. Note that each word in V is associated with a single row of U and one of V we think of this as resulting from an arbitrary ordering of V. The word2vec model is a probabilistic model specified as follows where uw refers to the row of U corresponding to word w V and likewise for V pU V o c exp u o vc w V exp u wvc 4 This may be familiar to you as the softmax function which takes arbitrary scores here one for each word in the vocabulary resulting from dot products and produces a probability distribution where larger scored things get higher probability. Note that the vector of probabilities over all words given a center word pU V c R V is a lot like a row of our old co occurrence matrix Xc. The story isn t over yet this is just a model. How do we estimate the values of the parameters U V We learn to minimize the cross entropy loss objective with the true distribution P O C min U V Eo c log pU V o c . 5 This equation should be read as minimize with respect to param eters U and V the expectation over values of o and c drawn from the distributions of O and V the negative log probability under the U V model of that value of o given that value of c . There is so much rich detail to get into here. How do we perform the min operation How do we get the random variables O and C Why the negative log of the probability Why is this so much better than co occurrence counting Can you tell why not all distributions over o given c can be represented by this model Should this be good Bad Surprising Obvious For now let s go through a few details about how to implement this in practice.']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9584550878885092,1.0,0.9099766527085352,0.03125
290,What is the main focus of this lecture on neural networks?,"The main focus of this lecture on neural networks is to introduce single and multilayer neural networks for classification purposes, discussing how they can be trained using backpropagation, and providing practical tips and tricks for training neural networks. Additionally, the lecture covers topics such as neuron units, non-linearities, gradient checks, Xavier parameter initialization, learning rates, and Adagrad, as well as the motivation for using recurrent neural networks as a language model.","['file name: lecture02 notes Backpropagation.  Neural networks. Forward computation. Backward propagation. Neuron Units. Max margin Loss. Gradient checks. Xavier parameter initialization. Learning rates. Adagrad. This set of notes introduces single and multilayer neural networks and how they can be used for classiÔ¨Åcation purposes. We then dis cuss how they can be trained using a distributed gradient descent technique known as backpropagation. We will see how the chain rule can be used to make parameter updates sequentially. After a rigorous mathematical discussion of neural networks we will discuss some practical tips and tricks in training neural networks involving neuron units non linearities gradient checks Xavier parameter ini tialization learning rates Adagrad etc. Lastly we will motivate the use of recurrent neural networks as a language model. 1 Neural Networks Foundations Figure 1 We see here how a non linear decision boundary separates the data very well. This is the prowess of neural networks. Fun Fact Neural networks are biologically in spired classiÔ¨Åers which is why they are often called artiÔ¨Åcial neural networks to distinguish them from the organic kind. However in reality human neural networks are so much more capable and complex from artiÔ¨Åcial neural net works that it is usually better to not draw too many parallels between the two. We established in our previous discussions the need for non linear classiÔ¨Åers since most data are not linearly separable and thus our classiÔ¨Åcation performance on them is limited. Neural networks are a family of classiÔ¨Åers with non linear decision boundary as seen in Figure 1. Now that we know the sort of decision boundaries neural networks create let us see how they manage doing so. 1.1 A Neuron A neuron is a generic computational unit that takes n inputs and produces a single output. What differentiates the outputs of different neurons is their parameters also referred to as their weights . One of the most popular choices for neurons is the sigmoid or binary lo gistic regression unit.', 'One of the most popular choices for neurons is the sigmoid or binary lo gistic regression unit. This unit takes an n dimensional input vector x and produces the scalar activation output a. This neuron is also associated with an n dimensional weight vector w and a bias scalar b. The output of this neuron is then a 1 1 exp wTx b We can also combine the weights and bias term above to equiva Neuron A neuron is the fundamental building block of neural networks. We will see that a neuron can be one of many functions that allows for non linearities to accrue in the network. lently formulate a 1 1 exp wT b x 1 Figure 2 This image captures how in a sigmoid neuron the input vector x is Ô¨Årst scaled summed added to a bias unit and then passed to the squashing sigmoid function. This formulation can be visualized in the manner shown in Fig ure 2. 1.2 A Single Layer of Neurons We extend the idea above to multiple neurons by considering the case where the input x is fed as an input to multiple such neurons as shown in Figure 3. If we refer to the different neurons weights as w 1 w m and the biases as b1 bm we can say the respective activations are a1 am a1 1 1 exp w 1 Tx b1 . . . am 1 1 exp w m Tx bm Let us deÔ¨Åne the following abstractions to keep the notation simple and useful for more complex networks œÉ z 1 1 exp z1 . . . 1 1 exp zm b b1 . . . bm Rm W w 1 T w m T Rm n We can now write the output of scaling and biases as Figure 3 This image captures how multiple sigmoid units are stacked on the right all of which receive the same input x. z Wx b The activations of the sigmoid function can then be written as a 1 . . . a m œÉ z œÉ Wx b So what do these activations really tell us Well one can think of these activations as indicators of the presence of some weighted combination of features. We can then use a combination of these activations to perform classiÔ¨Åcation tasks.']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.0000000000000002,1.0,0.9256558461607548,0.0303030303030303
291,What are the non-linear functions mentioned for neural networks?,"The non-linear functions mentioned for neural networks are sigmoid or binary logistic regression units. These are popular choices for neurons in neural networks. 

Source: lecture02 notes Backpropagation. Neural networks.","['file name: lecture02 notes Backpropagation.  Neural networks. Forward computation. Backward propagation. Neuron Units. Max margin Loss. Gradient checks. Xavier parameter initialization. Learning rates. Adagrad. This set of notes introduces single and multilayer neural networks and how they can be used for classiÔ¨Åcation purposes. We then dis cuss how they can be trained using a distributed gradient descent technique known as backpropagation. We will see how the chain rule can be used to make parameter updates sequentially. After a rigorous mathematical discussion of neural networks we will discuss some practical tips and tricks in training neural networks involving neuron units non linearities gradient checks Xavier parameter ini tialization learning rates Adagrad etc. Lastly we will motivate the use of recurrent neural networks as a language model. 1 Neural Networks Foundations Figure 1 We see here how a non linear decision boundary separates the data very well. This is the prowess of neural networks. Fun Fact Neural networks are biologically in spired classiÔ¨Åers which is why they are often called artiÔ¨Åcial neural networks to distinguish them from the organic kind. However in reality human neural networks are so much more capable and complex from artiÔ¨Åcial neural net works that it is usually better to not draw too many parallels between the two. We established in our previous discussions the need for non linear classiÔ¨Åers since most data are not linearly separable and thus our classiÔ¨Åcation performance on them is limited. Neural networks are a family of classiÔ¨Åers with non linear decision boundary as seen in Figure 1. Now that we know the sort of decision boundaries neural networks create let us see how they manage doing so. 1.1 A Neuron A neuron is a generic computational unit that takes n inputs and produces a single output. What differentiates the outputs of different neurons is their parameters also referred to as their weights . One of the most popular choices for neurons is the sigmoid or binary lo gistic regression unit.', 'f on previous slide Why they re needed Neural networks do function approximation e.g. regression or classification Without non linearities deep neural networks can t do anything more than a linear transform Extra layers could just be compiled down into a single linear transform W1 W2 x Wx But with more layers that include non linearities they can approximate any complex function 5Training with cross entropy loss you use this in PyTorch Until now our objective was stated as to maximize the probability of the correct class y or equivalently we can minimize the negative log probability of that class Now restated in terms of cross entropy a concept from information theory Let the true probability distribution be p let our computed model probability be q The cross entropy is Assuming a ground truth or true or gold or target probability distribution that is 1 at the right class and 0 everywhere else p 0 0 1 0 0 then Because of one hot p the only term left is the negative log probability of the true class yi log ùëù ùë¶i ùë•i Cross entropy can be used in other ways with a more interesting p but for now just know that you ll want to use it as the loss in PyTorch 6Remember Stochastic Gradient Descent Update equation i.e. for each parameter In deep learning ùúÉincludes the data representation e.g. word vectors too How can we compute ùúÉùêΩ ùúÉ 1. By hand 2. Algorithmically the backpropagation algorithm ùõº step size or learning rate 7Lecture Plan 8 Gradients 1. Introduction 2. Matrix calculus 3.']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.961103418887181,0.6666666666666666,0.8998827837849427,0.04
292,What is the purpose of non-linearities in neural networks?,"The purpose of non-linearities in neural networks is to allow for the creation of non-linear decision boundaries, which are necessary for effectively classifying data that is not linearly separable. Non-linearities enable neural networks to capture complex patterns and relationships in the data that linear models cannot handle effectively.","['file name: lecture02 notes Backpropagation.  Neural networks. Forward computation. Backward propagation. Neuron Units. Max margin Loss. Gradient checks. Xavier parameter initialization. Learning rates. Adagrad. This set of notes introduces single and multilayer neural networks and how they can be used for classiÔ¨Åcation purposes. We then dis cuss how they can be trained using a distributed gradient descent technique known as backpropagation. We will see how the chain rule can be used to make parameter updates sequentially. After a rigorous mathematical discussion of neural networks we will discuss some practical tips and tricks in training neural networks involving neuron units non linearities gradient checks Xavier parameter ini tialization learning rates Adagrad etc. Lastly we will motivate the use of recurrent neural networks as a language model. 1 Neural Networks Foundations Figure 1 We see here how a non linear decision boundary separates the data very well. This is the prowess of neural networks. Fun Fact Neural networks are biologically in spired classiÔ¨Åers which is why they are often called artiÔ¨Åcial neural networks to distinguish them from the organic kind. However in reality human neural networks are so much more capable and complex from artiÔ¨Åcial neural net works that it is usually better to not draw too many parallels between the two. We established in our previous discussions the need for non linear classiÔ¨Åers since most data are not linearly separable and thus our classiÔ¨Åcation performance on them is limited. Neural networks are a family of classiÔ¨Åers with non linear decision boundary as seen in Figure 1. Now that we know the sort of decision boundaries neural networks create let us see how they manage doing so. 1.1 A Neuron A neuron is a generic computational unit that takes n inputs and produces a single output. What differentiates the outputs of different neurons is their parameters also referred to as their weights . One of the most popular choices for neurons is the sigmoid or binary lo gistic regression unit.', 'In order to modify the previously discussed Softmax model to use windows of words for classiÔ¨Åcation we would simply substitute x i with x i window in the following manner x i window x i 2 x i 1 x i x i 1 x i 2 As a result when we evaluate the gradient of the loss with respect to the words we will receive gradients for the word vectors Œ¥window x i 2 x i 1 x i x i 1 x i 2 The gradient will of course need to be distributed to update the corresponding word vectors in implementation. 3.5 Non linear ClassiÔ¨Åers Figure 9 Here we see that many exam ples are wrongly classiÔ¨Åed even though the best linear decision boundary is chosen. This is due linear decision boundaries have limited model capacity for this dataset. We now introduce the need for non linear classiÔ¨Åcation models such as neural networks. We see in Figure 9 that a linear classiÔ¨Åer mis classiÔ¨Åes many datapoints. Using a non linear decision boundary as shown in Figure 10 we manage to classify all training points accu rately. Although oversimpliÔ¨Åed this is a classic case demonstrating the need for non linear decision boundaries. In the next set of notes we study neural networks as a class of non linear models that have performed particularly well in deep learning applications. Figure 10 Here we see that the non linear decision boundary allows for much better classiÔ¨Åcation of datapoints.']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999976291464472,1.0,0.9059389987023324,0.0294117647058823
293,What is cross entropy loss used for in PyTorch?,"Cross entropy loss is used in PyTorch to minimize the negative log probability of the correct class or target class in a classification task. It is a concept from information theory where the cross entropy is calculated between the true probability distribution and the model's predicted probability distribution. By minimizing the cross entropy loss, the model aims to improve its accuracy in predicting the correct class. 

Source: lecture02_Neural_Nets","['f on previous slide Why they re needed Neural networks do function approximation e.g. regression or classification Without non linearities deep neural networks can t do anything more than a linear transform Extra layers could just be compiled down into a single linear transform W1 W2 x Wx But with more layers that include non linearities they can approximate any complex function 5Training with cross entropy loss you use this in PyTorch Until now our objective was stated as to maximize the probability of the correct class y or equivalently we can minimize the negative log probability of that class Now restated in terms of cross entropy a concept from information theory Let the true probability distribution be p let our computed model probability be q The cross entropy is Assuming a ground truth or true or gold or target probability distribution that is 1 at the right class and 0 everywhere else p 0 0 1 0 0 then Because of one hot p the only term left is the negative log probability of the true class yi log ùëù ùë¶i ùë•i Cross entropy can be used in other ways with a more interesting p but for now just know that you ll want to use it as the loss in PyTorch 6Remember Stochastic Gradient Descent Update equation i.e. for each parameter In deep learning ùúÉincludes the data representation e.g. word vectors too How can we compute ùúÉùêΩ ùúÉ 1. By hand 2. Algorithmically the backpropagation algorithm ùõº step size or learning rate 7Lecture Plan 8 Gradients 1. Introduction 2. Matrix calculus 3.', 'The notation in this image is slightly different here the equivalent of W hh is Wh W hx is We and W S is U. E converts word inputs x t to word embeddings e t . The Ô¨Ånal softmax over the vocabulary shows us the probability of various options for token x 5 condi tioned on all previous tokens. The input could be much longer than 4 5 tokens. 2.1 RNN Loss and Perplexity The loss function used in RNNs is often the cross entropy error in troduced in earlier notes. Equation 7 shows this function as the sum over the entire vocabulary at time step t. J t Œ∏ V j 1 yt j log ÀÜ yt j 7 The cross entropy error over a corpus of size T is J 1 T T t 1 J t Œ∏ 1 T T t 1 V j 1 yt j log ÀÜ yt j 8 Equation 9 is called the perplexity relationship it is basically 2 to the power of the negative log probability of the cross entropy error function shown in Equation 8. Perplexity is a measure of confusion where lower values imply more conÔ¨Ådence in predicting the next word in the sequence compared to the ground truth outcome . Perplexity 2J 9 2.2 Advantages Disadvantages and Applications of RNNs RNNs have several advantages 1. They can process input sequences of any length 2. The model size does not increase for longer input sequence lengths 3. Computation for step t can in theory use information from many steps back. 4. The same weights are applied to every timestep of the input so there is symmetry in how inputs are processed However they also have some disadvantages 1. Computation is slow because it is sequential it cannot be paral lelized 2. In practice it is difÔ¨Åcult to access information from many steps back due to problems like vanishing and exploding gradients which we discuss in the following subsection The amount of memory required to run a layer of RNN is pro portional to the number of words in the corpus.']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999999999999988,1.0,0.9661503638298036,0.1
294,What is the significance of the backpropagation algorithm?,"The significance of the backpropagation algorithm lies in its ability to efficiently compute gradients in neural networks by recursively applying the chain rule along the computation graph. This allows for the calculation of how changing the parameters in the network affects the final output, enabling the model to learn and improve its performance over time. Understanding backpropagation is crucial for debugging and enhancing models, as it provides insights into why certain adjustments are made during the training process. Additionally, it helps in addressing issues like exploding and vanishing gradients, which can impact the learning process of neural networks. 
Source: SEP 775 - Computational Natural Language Processing, lecture02_Neural_Nets.txt","['Thus W 1 W and W 2 UT using notation from Section 1.3. Let us begin Suppose the cost J 1 sc s is positive and we want to perform the update of parameter W 1 14 in Figure 5 and Figure 6 we must realize that W 1 14 only contributes to z 2 1 and thus a 2 1 . This fact is crucial to understanding backpropagation backpropagated gradients are only affected by values they contribute to. a 2 1 is consequently used in the forward computation of score by multiplication with W 2 1 . We can see from the max margin loss that J s J sc 1 Therefore we will work with s W 1 ij here for simplicity. Thus s W 1 ij W 2 a 2 W 1 ij W 2 i a 2 i W 1 ij W 2 i a 2 i W 1 ij W 2 i a 2 i W 1 ij W 2 i a 2 i z 2 i z 2 i W 1 ij W 2 i f z 2 i z 2 i z 2 i W 1 ij W 2 i f z 2 i z 2 i W 1 ij W 2 i f z 2 i W 1 ij b 1 i a 1 1 W 1 i1 a 1 2 W 1 i2 a 1 3 W 1 i3 a 1 4 W 1 i4 W 2 i f z 2 i W 1 ij b 1 i k a 1 k W 1 ik W 2 i f z 2 i a 1 j Œ¥ 2 i a 1 j We see above that the gradient reduces to the product Œ¥ 2 i a 1 j where Œ¥ 2 i is essentially the error propagating backwards from the i th neu ron in layer 2. a 1 j is an input fed to i th neuron in layer 2 when scaled by Wij.', 'do backpropagation for you but mainly leave layer node writer to hand calculate the local derivative 74Backprop Implementations 75Implementation forward backward API 76Implementation forward backward API 77Manual Gradient checking Numeric Gradient For small h 1e 4 Easy to implement correctly But approximate and very slow You have to recompute f for every parameter of our model Useful for checking your implementation In the old days we hand wrote everything doing this everywhere was the key test Now much less needed you can use it to check layers are correctly implemented 78Summary We ve mastered the core technology of neural nets Backpropagation recursively and hence efficiently apply the chain rule along computation graph downstream gradient upstream gradient x local gradient Forward pass compute results of operations and save intermediate values Backward pass apply chain rule to compute gradients 79Why learn all these details about gradients 80 Modern deep learning frameworks compute gradients for you But why take a class on compilers or systems when they are implemented for you Understanding what is going on under the hood is useful Backpropagation doesn t always work perfectly out of the box Understanding why is crucial for debugging and improving models See Karpathy article https medium.com @karpathy yes you should understand backprop e2f06eab496b Example in future lecture exploding and vanishing gradients']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.000000000000001,0.8571428571428571,0.9401246323692252,0.125
295,What does the Jacobian matrix represent in the context of neural networks?,"In the context of neural networks, the Jacobian matrix represents a matrix of partial derivatives. Specifically, when dealing with a function with multiple outputs and multiple inputs, the Jacobian matrix is an m x n matrix that contains the partial derivatives of the outputs with respect to the inputs. It helps in understanding how small changes in the inputs affect the outputs of the function.","['The vanishing gradients problem refers to the opposite behaviour when long term components go exponentially fast to norm 0 making it impossible for the model to learn correlation between temporally dis tant events. 2.1. The mechanics To understand this phenomenon we need to look at the form of each temporal component and in particular at the matrix factors xt xk see equation 5 that take the form of a product of t k Jacobian matrices. In the same way a product of t k real numbers can shrink to zero or explode to inÔ¨Ånity so does this product of matrices along some direction v . In what follows we will try to formalize these intu itions extending a similar derivation done in Bengio et al. 1994 where only a single hidden unit case was considered . If we consider a linear version of the model i.e. set œÉ to the identity function in equation 11 we can use the power iteration method to formally analyze this prod uct of Jacobian matrices and obtain tight conditions for when the gradients explode or vanish see the sup plementary materials for a detailed derivation of these conditions . It is suÔ¨Écient for the largest eigenvalue Œª1 of the recurrent weight matrix to be smaller than 1 for long term components to vanish as t and necessary for it to be larger than 1 for gradients to explode. We can generalize these results for nonlinear functions œÉ where the absolute values of œÉ x is bounded say by a value Œ≥ R and therefore diag œÉ xk Œ≥. We Ô¨Årst prove that it is suÔ¨Écient for Œª1 1 Œ≥ where Œª1 is the absolute value of the largest eigenvalue of the recurrent weight matrix Wrec for the vanishing gradient problem to occur. Note that we assume the parametrization given by equation 11 . The Jacobian matrix xk 1 xk is given by WT recdiag œÉ xk . The 2 norm of this Jacobian is bounded by the product ofOn the diÔ¨Éculty of training Recurrent Neural Networks the norms of the two matrices see equation 6 .', 'Introduction 2. Matrix calculus 3. BackpropagationComputing Gradients by Hand 9 Matrix calculus Fully vectorized gradients Multivariable calculus is just like single variable calculus if you use matrices Much faster and more useful than non vectorized gradients But doing a non vectorized gradient can be good for intuition recall the first lecture for an exampleGradients Given a function with 1 output and 1 input ùëìùë• ùë•3 It s gradient slope is its derivative ùëëf 3ùë•2 10 ùëëùë• How much will the output change if we change the input a bit At x 1 it changes about 3 times as much 1.013 1.03 At x 4 it changes about 48 times as much 4.013 64.48Gradients Given a function with 1 output and n inputs Its gradient is a vector of partial derivatives with respect to each input 11Jacobian Matrix Generalization of the Gradient Given a function with m outputs and n inputs It s Jacobian is an m x n matrix of partial derivatives 12Chain Rule For composition of one variable functions multiply derivatives For multiple variables functions multiply Jacobians 13Example Jacobian Elementwise activation Function 14Example Jacobian Elementwise activation Function Function has n outputs and n inputs n by n Jacobian 15Example Jacobian Elementwise activation Function 16Example Jacobian Elementwise activation Function 17Example Jacobian Elementwise activation Function 18Other Jacobians Compute these at home for practice Check your answers with the lecture notes 19Other Jacobians Compute these at home for practice Check your answers with the lecture notes 20Other Jacobians Compute these at home for practice Check your answers with the lecture notes Fine print This is the correct Jacobian. Later we discuss the shape convention using it the answer would be h. 21Other Jacobians Compute these at home for practice 22Back to our Neural Net x xmuseums xin xParis xare xamazing 23Back to our Neural Net Let s find Really we care about the gradient of the loss Jt but we will compute the gradient of the score for simplicity x xmuseums xin xParis xare xamazing 241.']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999999999999988,1.0,0.953200830322068,0.0526315789473684
296,How is the derivative with respect to a matrix described?,"The derivative with respect to a matrix is described using the Jacobian matrix, which is an m x n matrix of partial derivatives when dealing with a function with m outputs and n inputs. This matrix helps in organizing the gradients in a structured way for easier computation and optimization during processes like gradient descent.","['So z x ij zi xj xj f xi f xi if i j 0 if otherwise So we see that the Jacobian z x is a diagonal matrix where the entry at i i is the derivative of f applied to xi. We can write this as z x diag f x . Since multiplication by a diagonal matrix is the same as doing elementwise multiplication by the diagonal we could also write f x when applying the chain rule. 5 Matrix times column vector with respect to the matrix z W x Œ¥ J z what is J W J z z W Œ¥ z W This is a bit more complicated than the other identities. The reason for in cluding J z in the above problem formulation will become clear in a moment. First suppose we have a loss function J a scalar and are computing its gradient with respect to a matrix W Rn m. Then we could think of J as a function of W taking nm inputs the entries of W to a single output J . This means the Jacobian J W would be a 1 nm vector. But in practice this is not a very useful way of arranging the gradient. It would be much nicer if the derivatives were in a n m matrix like this J W J W11 . . . J W1m . . . ... . . . J Wn1 . . . J Wnm Since this matrix has the same shape as W we could just subtract it times the learning rate from W when doing gradient descent. So in a slight abuse of notation let s Ô¨Ånd this matrix as J W instead. This way of arranging the gradients becomes complicated when computing z W . Unlike J z is a vector. So if we are trying to rearrange the gradients like with J W z W would be an n m n tensor Luckily we can avoid the issue by taking the gradient with respect to a single weight Wij instead. 3z Wij is just a vector which is much easier to deal with. We have zk m X l 1 Wklxl zk Wij m X l 1 xl Wij Wkl Note that Wij Wkl 1 if i k and j l and 0 if otherwise. So if k i everything in the sum is zero and the gradient is zero.', 'Introduction 2. Matrix calculus 3. BackpropagationComputing Gradients by Hand 9 Matrix calculus Fully vectorized gradients Multivariable calculus is just like single variable calculus if you use matrices Much faster and more useful than non vectorized gradients But doing a non vectorized gradient can be good for intuition recall the first lecture for an exampleGradients Given a function with 1 output and 1 input ùëìùë• ùë•3 It s gradient slope is its derivative ùëëf 3ùë•2 10 ùëëùë• How much will the output change if we change the input a bit At x 1 it changes about 3 times as much 1.013 1.03 At x 4 it changes about 48 times as much 4.013 64.48Gradients Given a function with 1 output and n inputs Its gradient is a vector of partial derivatives with respect to each input 11Jacobian Matrix Generalization of the Gradient Given a function with m outputs and n inputs It s Jacobian is an m x n matrix of partial derivatives 12Chain Rule For composition of one variable functions multiply derivatives For multiple variables functions multiply Jacobians 13Example Jacobian Elementwise activation Function 14Example Jacobian Elementwise activation Function Function has n outputs and n inputs n by n Jacobian 15Example Jacobian Elementwise activation Function 16Example Jacobian Elementwise activation Function 17Example Jacobian Elementwise activation Function 18Other Jacobians Compute these at home for practice Check your answers with the lecture notes 19Other Jacobians Compute these at home for practice Check your answers with the lecture notes 20Other Jacobians Compute these at home for practice Check your answers with the lecture notes Fine print This is the correct Jacobian. Later we discuss the shape convention using it the answer would be h. 21Other Jacobians Compute these at home for practice 22Back to our Neural Net x xmuseums xin xParis xare xamazing 23Back to our Neural Net Let s find Really we care about the gradient of the loss Jt but we will compute the gradient of the score for simplicity x xmuseums xin xParis xare xamazing 241.']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999999999999996,1.0,0.8989139531753821,0.0454545454545454
297,What role does automatic differentiation play in modern deep learning frameworks?,"Automatic differentiation plays a crucial role in modern deep learning frameworks as it allows for efficient computation of gradients needed for training neural networks. By automatically calculating the derivatives of the loss function with respect to the model's parameters, automatic differentiation simplifies the implementation of backpropagation, which is essential for updating the weights of neural networks during the training process. This feature enables deep learning practitioners to focus on designing and experimenting with complex neural network architectures without having to manually derive and implement the gradient calculations.","['The parameters in the neural network are learnt using gradient based optimization algorithms with the gradients being computed by back propagation. One of the challenges of deep learning is that the gradients with respect to the weights in one layer are highly dependent on the outputs of the neurons in the previous layer especially if these outputs change in a highly correlated way. Batch normalization Ioffe and Szegedy 2015 was proposed to reduce such undesirable covariate shift . The method normalizes the summed inputs to each hidden unit over the training cases. SpeciÔ¨Åcally for the ith summed input in the lth layer the batch normalization method rescales the summed inputs according to their variances under the distribution of the data al i gl i œÉl i al i ¬µl i ¬µl i E x P x al i œÉl i r E x P x h al i ¬µl i 2i 2 where al i is normalized summed inputs to the ith hidden unit in the lth layer and gi is a gain parame ter scaling the normalized activation before the non linear activation function. Note the expectation is under the whole training data distribution. It is typically impractical to compute the expectations in Eq. 2 exactly since it would require forward passes through the whole training dataset with the current set of weights. Instead ¬µ and œÉ are estimated using the empirical samples from the current mini batch. This puts constraints on the size of a mini batch and it is hard to apply to recurrent neural networks. 3 Layer normalization We now consider the layer normalization method which is designed to overcome the drawbacks of batch normalization. Notice that changes in the output of one layer will tend to cause highly correlated changes in the summed inputs to the next layer especially with ReLU units whose outputs can change by a lot. This suggests the covariate shift problem can be reduced by Ô¨Åxing the mean and the variance of the summed inputs within each layer. We thus compute the layer normalization statistics over all the hidden units in the same layer as follows ¬µl 1 H H X i 1 al i œÉl v u u t 1 H H X i 1 al i ¬µl 2 3 where H denotes the number of hidden units in a layer. The difference between Eq. 2 and Eq.', 'The regularizer we propose below prefers solutions for which the error signal preserves norm as it travels back in time ‚Ñ¶ X k ‚Ñ¶k X k E xk 1 xk 1 xk E xk 1 1 2 9 In order to be computationally eÔ¨Écient we only use the immediate partial derivative of ‚Ñ¶with respect to Wrec we consider that xk and E xk 1 as being constant with respect to Wrec when computing the derivative of ‚Ñ¶k as depicted in equation 10 . Note we use the parametrization of equation 11 . This can be done ef Ô¨Åciently because we get the values of E xk from BPTT. We use Theano to compute these gradients Bergstra et al. 2010 Bastien et al. 2012 . ‚Ñ¶ Wrec P k ‚Ñ¶k Wrec P k E xk 1 WT recdiag œÉ xk E xk 1 1 2 Wrec 10 Note that our regularization term only forces the Ja cobian matrices xk 1 xk to preserve norm in the relevant direction of the error E xk 1 not for any direction i.e. we do not enforce that all eigenvalues are close to 1 . The second observation is that we are using a soft con straint therefore we are not ensured the norm of the error signal is preserved. If it happens that these Jaco bian matrices are such that the norm explodes as t k increases then this could lead to the exploding gradi ents problem and we need to deal with it for example as described in section 3.2. This can be seen from the dynamical systems perspective as well preventing vanishing gradients implies that we are pushing the model such that it is further away from the attrac tor such that it does not converge to it case in which the gradients vanish and closer to boundaries between basins of attractions making it more probable for the gradients to explode. 4. Experiments and Results 4.1.']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.0000000000000002,0.75,0.9266477537138744,0.0384615384615384
298,What is the purpose of manual gradient checking?,"The purpose of manual gradient checking is to compare analytical gradients with numerical gradients to ensure they are close, providing a way to verify the correctness of the implementation. It involves evaluating gradients using forward passes and perturbing parameters to compute gradients in both directions for precision and stability. However, manual gradient checking is approximate and slow, requiring recomputation of the function for every parameter, making it useful primarily for verifying the implementation rather than for regular use in large-scale neural networks.","['do backpropagation for you but mainly leave layer node writer to hand calculate the local derivative 74Backprop Implementations 75Implementation forward backward API 76Implementation forward backward API 77Manual Gradient checking Numeric Gradient For small h 1e 4 Easy to implement correctly But approximate and very slow You have to recompute f for every parameter of our model Useful for checking your implementation In the old days we hand wrote everything doing this everywhere was the key test Now much less needed you can use it to check layers are correctly implemented 78Summary We ve mastered the core technology of neural nets Backpropagation recursively and hence efficiently apply the chain rule along computation graph downstream gradient upstream gradient x local gradient Forward pass compute results of operations and save intermediate values Backward pass apply chain rule to compute gradients 79Why learn all these details about gradients 80 Modern deep learning frameworks compute gradients for you But why take a class on compilers or systems when they are implemented for you Understanding what is going on under the hood is useful Backpropagation doesn t always work perfectly out of the box Understanding why is crucial for debugging and improving models See Karpathy article https medium.com @karpathy yes you should understand backprop e2f06eab496b Example in future lecture exploding and vanishing gradients', 'We note that this deÔ¨Ånition of the numerical gradient follows very naturally from the deÔ¨Ånition of the derivative where in the scalar case f x f x œµ f x œµ Gradient checks are a great way to compare analytical and numerical gradients. Analytical gradients should be close and numerical gradients can be calculated using f Œ∏ J Œ∏ i J Œ∏ i 2œµ J Œ∏ i and J Œ∏ i can be evalu ated using two forward passes. An implementation of this can be seen in Snippet 2.1. Of course there is a slight difference the deÔ¨Ånition above only perturbs x in the positive direction to compute the gradient. While it would have been perfectly acceptable to deÔ¨Åne the numerical gradi ent in this way in practice it is often more precise and stable to use the centered difference formula where we perturb a parameter in both directions. The intuition is that to get a better approximation of the derivative slope around a point we need to examine the func tion f s behavior both to the left and right of that point. It can also be shown using Taylor s theorem that the centered difference formula has an error proportional to œµ2 which is quite small whereas the derivative deÔ¨Ånition is more error prone. Now a natural question you might ask is if this method is so pre cise why do we not use it to compute all of our network gradients instead of applying back propagation The simple answer as hinted earlier is inefÔ¨Åciency recall that every time we want to compute the gradient with respect to an element we need to make two forward passes through the network which will be computationally expen sive. Furthermore many large scale neural networks can contain millions of parameters and computing two passes per parameter is clearly not optimal. And since in optimization techniques such as SGD we must compute the gradients once per iteration for several thousands of iterations it is obvious that this method quickly grows intractable. This inefÔ¨Åciency is why we only use gradient check to verify the correctness of our analytic gradients which are much quicker to compute.']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999999999999996,1.0,0.9361198492922688,0.0833333333333333
299,Why is understanding gradients important despite automatic differentiation?,"Understanding gradients is important despite automatic differentiation because it allows us to verify the correctness of our analytic gradients through methods like gradient checks. While automatic differentiation is efficient for computing gradients in neural networks, having an understanding of gradients helps in ensuring the accuracy of the computations and can provide insights into the behavior of the network during training. Additionally, understanding gradients is crucial for debugging and optimizing neural network models.","['We note that this deÔ¨Ånition of the numerical gradient follows very naturally from the deÔ¨Ånition of the derivative where in the scalar case f x f x œµ f x œµ Gradient checks are a great way to compare analytical and numerical gradients. Analytical gradients should be close and numerical gradients can be calculated using f Œ∏ J Œ∏ i J Œ∏ i 2œµ J Œ∏ i and J Œ∏ i can be evalu ated using two forward passes. An implementation of this can be seen in Snippet 2.1. Of course there is a slight difference the deÔ¨Ånition above only perturbs x in the positive direction to compute the gradient. While it would have been perfectly acceptable to deÔ¨Åne the numerical gradi ent in this way in practice it is often more precise and stable to use the centered difference formula where we perturb a parameter in both directions. The intuition is that to get a better approximation of the derivative slope around a point we need to examine the func tion f s behavior both to the left and right of that point. It can also be shown using Taylor s theorem that the centered difference formula has an error proportional to œµ2 which is quite small whereas the derivative deÔ¨Ånition is more error prone. Now a natural question you might ask is if this method is so pre cise why do we not use it to compute all of our network gradients instead of applying back propagation The simple answer as hinted earlier is inefÔ¨Åciency recall that every time we want to compute the gradient with respect to an element we need to make two forward passes through the network which will be computationally expen sive. Furthermore many large scale neural networks can contain millions of parameters and computing two passes per parameter is clearly not optimal. And since in optimization techniques such as SGD we must compute the gradients once per iteration for several thousands of iterations it is obvious that this method quickly grows intractable. This inefÔ¨Åciency is why we only use gradient check to verify the correctness of our analytic gradients which are much quicker to compute.', 'Introduction 2. Matrix calculus 3. BackpropagationComputing Gradients by Hand 9 Matrix calculus Fully vectorized gradients Multivariable calculus is just like single variable calculus if you use matrices Much faster and more useful than non vectorized gradients But doing a non vectorized gradient can be good for intuition recall the first lecture for an exampleGradients Given a function with 1 output and 1 input ùëìùë• ùë•3 It s gradient slope is its derivative ùëëf 3ùë•2 10 ùëëùë• How much will the output change if we change the input a bit At x 1 it changes about 3 times as much 1.013 1.03 At x 4 it changes about 48 times as much 4.013 64.48Gradients Given a function with 1 output and n inputs Its gradient is a vector of partial derivatives with respect to each input 11Jacobian Matrix Generalization of the Gradient Given a function with m outputs and n inputs It s Jacobian is an m x n matrix of partial derivatives 12Chain Rule For composition of one variable functions multiply derivatives For multiple variables functions multiply Jacobians 13Example Jacobian Elementwise activation Function 14Example Jacobian Elementwise activation Function Function has n outputs and n inputs n by n Jacobian 15Example Jacobian Elementwise activation Function 16Example Jacobian Elementwise activation Function 17Example Jacobian Elementwise activation Function 18Other Jacobians Compute these at home for practice Check your answers with the lecture notes 19Other Jacobians Compute these at home for practice Check your answers with the lecture notes 20Other Jacobians Compute these at home for practice Check your answers with the lecture notes Fine print This is the correct Jacobian. Later we discuss the shape convention using it the answer would be h. 21Other Jacobians Compute these at home for practice 22Back to our Neural Net x xmuseums xin xParis xare xamazing 23Back to our Neural Net Let s find Really we care about the gradient of the loss Jt but we will compute the gradient of the score for simplicity x xmuseums xin xParis xare xamazing 241.']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9691724285001032,1.0,0.9205391140680484,0.0625
300,What is the core concept introduced in this class that leads to GPT-3 and ChatGPT?,"The core concept introduced in this class that leads to GPT-3 and ChatGPT is in-context learning, which involves training models to perform well on tasks with minimal or no task-specific training data by leveraging their pre-trained knowledge and capabilities. This concept allows models like GPT-3 and ChatGPT to excel in few-shot and zero-shot learning scenarios by understanding and adapting to new tasks based on their existing understanding of language and patterns. 

Source: Language_Models_are_Few_Shot_Learners.txt","['On the Easy version of the dataset questions which either of the mentioned baseline approaches answered correctly GPT 3 achieves 68.8 71.2 and 70.1 which slightly exceeds a Ô¨Åne tuned RoBERTa baseline from KKS 20 . However both of these results are still much worse than the overall SOTAs achieved by the UniÔ¨ÅedQA which exceeds GPT 3 s few shot results by 27 on the challenge set and 22 on the easy set. On OpenBookQA MCKS18 GPT 3 improves signiÔ¨Åcantly from zero to few shot settings but is still over 20 points short of the overall SOTA. GPT 3 s few shot performance is similar to a Ô¨Åne tuned BERT Large baseline on the leaderboard. Overall in context learning with GPT 3 shows mixed results on commonsense reasoning tasks with only small and inconsistent gains observed in the one and few shot learning settings for both PIQA and ARC but a signiÔ¨Åcant improvement is observed on OpenBookQA. GPT 3 sets SOTA on the new PIQA dataset in all evaluation settings. 3.6 Reading Comprehension Next we evaluate GPT 3 on the task of reading comprehension. We use a suite of 5 datasets including abstractive multiple choice and span based answer formats in both dialog and single question settings. We observe a wide spread in GPT 3 s performance across these datasets suggestive of varying capability with different answer formats. In general we observe GPT 3 is on par with initial baselines and early results trained using contextual representations on each respective dataset. GPT 3 performs best within 3 points of the human baseline on CoQA RCM19 a free form conversational dataset and performs worst 13 F1 below an ELMo baseline on QuAC CHI 18 a dataset which requires modeling structured dialog acts and answer span selections of teacher student interactions.', 'Quantitatively GPT 3 s in context learning performance has some notable gaps on our suite of benchmarks as described in Section 3 and in particular it does little better than chance when evaluated one shot or even few shot on some comparison tasks such as determining if two words are used the same way in a sentence or if one sentence implies another WIC and ANLI respectively as well as on a subset of reading comprehension tasks. This is especially striking given GPT 3 s strong few shot performance on many other tasks. GPT 3 has several structural and algorithmic limitations which could account for some of the issues above. We focused on exploring in context learning behavior in autoregressive language models because it is straightforward to both sample and compute likelihoods with this model class. As a result our experiments do not include any bidirectional architectures or other training objectives such as denoising. This is a noticeable difference from much of the recent literature which has documented improved Ô¨Åne tuning performance when using these approaches over standard language models RSR 19 . Thus our design decision comes at the cost of potentially worse performance on tasks which empirically beneÔ¨Åt from bidirectionality. This may include Ô¨Åll in the blank tasks tasks that involve looking back and comparing two pieces of content or tasks that require re reading or carefully considering a long passage and then generating a very short answer. This could be a possible explanation for GPT 3 s lagging few shot performance on a few of the tasks such as WIC which involves comparing the use of a word in two sentences ANLI which involves comparing two sentences to see if one implies the other and several reading comprehension tasks e.g. QuAC and RACE . We also conjecture based on past literature that a large bidirectional model would be stronger at Ô¨Åne tuning than GPT 3. Making a bidirectional model at the scale of GPT 3 and or trying to make bidirectional models work with few or zero shot learning is a promising direction for future research and could help achieve the best of both worlds .']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999999999999996,0.6,0.8795257578906883,0.0454545454545454
301,What is the purpose of Dropout as mentioned in the document?,"The purpose of Dropout, as mentioned in the document, is to act as a powerful technique for regularization in neural networks. During training, Dropout randomly drops a subset of neurons with a certain probability during each forward and backward pass, or keeps each neuron alive with a probability p. This process helps prevent overfitting, allows the network to learn more meaningful information from the data, and generally leads to higher performance on the given task. By training exponentially many smaller networks at once and averaging their predictions, Dropout effectively helps improve the overall performance of the neural network.","['Imposing such a con straint can also be interpreted as the prior Bayesian belief that the optimal weights are close to zero how close depends on the value of Œª. Choosing the right value of Œª is critical and must be chosen via hyperparameter tuning. Too high a value of Œª causes most of the weights to be set too close to 0 and the model does not learn anything meaningful from the training data often obtaining poor ac curacy on training validation and testing sets. Too low a value and we fall into the domain of overÔ¨Åtting once again. It must be noted that the bias terms are not regularized and do not contribute to the cost term above try thinking about why this is the case There are indeed other types of regularization that are sometimes used such as L1 regularization which sums over the absolute values rather than squares of parameter elements however this is less commonly applied in practice since it leads to sparsity of parameter weights. In the next section we discuss dropout which effectively acts as another form of regularization by randomly dropping i.e. setting to zero neurons in the forward pass. 2.3 Dropout Dropout is a powerful technique for regularization Ô¨Årst introduced by Srivastava et al. in Dropout A Simple Way to Prevent Neural Net works from OverÔ¨Åtting. The idea is simple yet effective during train ing we will randomly drop with some probability 1 p a subset of neurons during each forward backward pass or equivalently we will keep alive each neuron with a probability p . Then during testing we will use the full network to compute our predictions. The result is that the network typically learns more meaningful informa tion from the data is less likely to overÔ¨Åt and usually obtains higher performance overall on the task at hand. One intuitive reason why this technique should be so effective is that what dropout is doing is essentially doing is training exponentially many smaller networks at once and averaging over their predictions. Dropout applied to an artiÔ¨Åcial neural network. Image credits to Srivastava et al. In practice the way we introduce dropout is that we take the out put h of each layer of neurons and keep each neuron with prob ability p and else set it to 0. Then during back propagation we only pass gradients through neurons that were kept alive during the forward pass.', '0.73 I ve become pathetic pining after a guy my ex. Would like to reach state of less pathetic. If more info is necessary please let me know. 0.69 I have body issues body acne scarring and weight issues that prevent me from having a normal life without shame and prevent me from having a better sex life with my BF. 1.0 Do you take someone back after they ve turned you down off even if you can t see them in person or are they just not worth the risk 0.52 Table 19 Qualitative examples showing the change in reward of the reward model on human generated edits to TL DR summaries that make the summaries better. Examples are randomly selected from the set where the edit distance was less than 5 and the magnitude of change in reward was greater than 0.5. Text in strike through was removed from the original summary in the edit and text in bold was added. The reward model is sensitive to small but semantically meaningful changes in the summary although it makes errors on occasion. TL DR 1.3B sup. T 0.7 researcher labeler labeler ensem ble length copying ROUGE 1.3B sup logprob 1.3B RM 6.7B sup logprob 6.7B RM researcher 73.4 4.1 77.7 2.1 84.4 3.3 55.5 4.3 62.3 4.1 59.1 4.2 61.8 4.8 72.2 4.5 62.8 4.7 78.0 3.9 labeler 77.7 2.1 68.6 1.7 74.4 2.0 54.4 1.3 58.0 1.2 57.7 1.3 58.7 2.0 65.8 2.0 61.9 2.1 70.8 1.8 labeler ensemble 84.4 3.3 74.4 2.0 60.6 4.0 62.7 3.8 59.']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9300597327790432,1.0,0.9190422593714438,0.04
302,What are some approaches to address the vanishing gradient problem in RNNs?,"Some approaches to address the vanishing gradient problem in RNNs include initializing weight matrices with an identity matrix instead of random values and using Rectified Linear Units (ReLU) instead of the sigmoid function to allow gradients to flow through neurons without getting attenuated. Additionally, using deep bidirectional RNNs can help by considering future words in predictions and maintaining two hidden layers for left-to-right and right-to-left propagation. These techniques can help mitigate the vanishing gradient issue in RNNs. 

Source: lecture03_notes_LM_RNN.txt","['av steps. 3. Dealing wi vanishing g 3.1. Previous so Using an L1 or L2 help with explodin ters initialized wit Wrec is probably that the gradient tion found in secti ensure that durin exceeds 1. This a ple regime with a where any informa Figure 7 Gradient explosion clipping visualization To solve the problem of vanishing gradients we introduce two tech niques. The Ô¨Årst technique is that instead of initializing W hh ran domly start off from an identity matrix initialization. The second technique is to use the RectiÔ¨Åed Linear Units ReLU in stead of the sigmoid function. The derivative for the ReLU is either 0 or 1. This way gradients would Ô¨Çow through the neurons whose deriva tive is 1 without getting attenuated while propagating back through time steps. 2.5 Deep Bidirectional RNNs So far we have focused on RNNs that condition on past words to predict the next word in the sequence. It is possible to make predic tions based on future words by having the RNN model read through the corpus backwards. Irsoy et al. shows a bi directional deep neu ral network at each time step t this network maintains two hidden layers one for the left to right propagation and another for the right to left propagation. To maintain two hidden layers at any time this network consumes twice as much memory space for its weight and bias parameters. The Ô¨Ånal classiÔ¨Åcation result ÀÜ yt is generated through combining the score results produced by both RNN hidden layers. Fig ure 8 shows the bi directional network architecture and Equations 17 and 18 show the mathematical formulation behind setting up the bi directional RNN hidden layer. The only difference between these two relationships is in the direction of recursing through the corpus. Equa tion 19 shows the classiÔ¨Åcation relationship used for predicting the next word via summarizing past and future word representations. Bidirectionality h t f W xt V h t f W xt V yt g U h t h t y h x now represents summarizes the past and around a single token.', 'Problems with RNNs Vanishing and Exploding Gradients 48Vanishing gradient intuition 49Vanishing gradient intuition chain rule 50Vanishing gradient intuition chain rule 51Vanishing gradient intuition chain rule 52Vanishing gradient intuition What happens if these are small Vanishing gradient problem When these are small the gradient signal gets smaller and smaller as it backpropagates further 53Vanishing gradient proof sketch linear case Recall What if were the identity function 53 Source On the difficulty of training recurrent neural networks Pascanu et al 2013. http proceedings.mlr.press v28 pascanu13.pdf and supplemental materials at http proceedings.mlr.press v28 pascanu13 supp.pdf If Wh is small then this term gets exponentially problematic as becomes large chain rule Consider the gradient of the loss on step with respect to the hidden state on some previous step . Let chain rule value of ONLY READ IF INTERESTEDVanishing gradient proof sketch linear case What s wrong with Consider if the eigenvalues of are all less than 1 We can write as a basis What about nonlinear activations i.e. what we use Pretty much the same thing except the proof requires for some dependent on dimensionality and Source On the difficulty of training recurrent neural networks Pascanu et al 2013. http proceedings.mlr.press v28 pascanu13.pdf and supplemental materials at http proceedings.mlr.press v28 pascanu13 supp.pdf 54 eigenvectors using the eigenvectors of Approaches 0 as grows so gradient vanishes sufficient but not necessary ONLY READ IF INTERESTEDWhy is vanishing gradient a problem Gradient signal from far away is lost because it s much smaller than gradient signal from close by. So model weights are updated only with respect to near effects not long term effects. 56Effect of vanishing gradient on RNN LM 57 LM task When she tried to print her tickets she found that the printer was out of toner. She went to the stationery store to buy more toner. It was very overpriced. After installing the toner into the printer she finally printed her To learn from this training example the RNN LM needs to model the dependency between tickets on the 7th step and the target word tickets at the end.']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.0000000000000009,1.0,0.8183902110084602,0.037037037037037
303,"What is perplexity, and how is it used in evaluating language models?","Perplexity is a measure used to evaluate the performance of language models. It indicates how well a probability distribution or language model predicts a sample. A lower perplexity value suggests that the model is more accurate in predicting the sample. It is important to note that perplexity should be compared between language models with identical vocabularies to ensure a fair comparison. While a decrease in perplexity may indicate an improvement in the language model, it does not guarantee better performance in tasks like speech recognition or machine translation. Therefore, it is recommended to confirm any improvement in perplexity with an end-to-end evaluation of a real task before drawing conclusions about the model's effectiveness. Sampling sentences from a language model is one way to visualize the knowledge it embodies, as it generates sentences based on their likelihood according to the model. This technique can help understand what kind of sentences the model is more likely to produce based on its learned probabilities. 
Source: Lecture notes - N Gram Language Models","['Any kind of knowledge of the test set can cause the perplexity to be artiÔ¨Åcially low. The perplexity of two language models is only comparable if they use identical vocabularies. An intrinsic improvement in perplexity does not guarantee an extrinsic im provement in the performance of a language processing task like speech recognition10 CHAPTER 3 N GRAM LANGUAGE MODELS or machine translation. Nonetheless because perplexity often correlates with such improvements it is commonly used as a quick check on an algorithm. But a model s improvement in perplexity should always be conÔ¨Årmed by an end to end evaluation of a real task before concluding the evaluation of the model. 3.3 Sampling sentences from a language model One important way to visualize what kind of knowledge a language model embodies is to sample from it. Sampling from a distribution means to choose random points sampling according to their likelihood. Thus sampling from a language model which rep resents a distribution over sentences means to generate some sentences choosing each sentence according to its likelihood as deÔ¨Åned by the model. Thus we are more likely to generate sentences that the model thinks have a high probability and less likely to generate sentences that the model thinks have a low probability. This technique of visualizing a language model by sampling was Ô¨Årst suggested very early on by Shannon 1951 and Miller and Selfridge 1950 . It s simplest to visualize how this works for the unigram case. Imagine all the words of the English language covering the probability space between 0 and 1 each word covering an interval proportional to its frequency. Fig. 3.3 shows a visualization using a unigram LM computed from the text of this book. We choose a random value between 0 and 1 Ô¨Ånd that point on the probability line and print the word whose interval includes this chosen value. We continue choosing random numbers and generating words until we randomly generate the sentence Ô¨Ånal token s .', 'Now we see the following test set 0 0 0 0 0 3 0 0 0 0. We should expect the perplexity of this test set to be lower since most of the time the next number will be zero which is very predictable i.e. has a high probability. Thus although the branching factor is still 10 the perplexity or weighted branching factor is smaller. We leave this exact calculation as exercise 3.12. We see in Section 3.8 that perplexity is also closely related to the information theoretic notion of entropy. We mentioned above that perplexity is a function of both the text and the lan guage model given a text W different language models will have different perplex ities. Because of this perplexity can be used to compare different n gram models. Let s look at an example in which we trained unigram bigram and trigram gram mars on 38 million words including start of sentence tokens from the Wall Street Journal using a 19 979 word vocabulary. We then computed the perplexity of each of these models on a test set of 1.5 million words using Eq. 3.16 for unigrams Eq. 3.17 for bigrams and the corresponding equation for trigrams. The table below shows the perplexity of a 1.5 million word WSJ test set according to each of these grammars. Unigram Bigram Trigram Perplexity 962 170 109 As we see above the more information the n gram gives us about the word sequence the higher the probability the n gram will assign to the string. A trigram model is less surprised than a unigram model because it has a better idea of what words might come next and so it assigns them a higher probability. And the higher the probability the lower the perplexity since as Eq. 3.15 showed perplexity is related inversely to the likelihood of the test sequence according to the model . So a lower perplexity can tell us that a language model is a better predictor of the words in the test set. Note that in computing perplexities the n gram model P must be constructed without any knowledge of the test set or any prior knowledge of the vocabulary of the test set.']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.99780755788776,1.0,0.9301598161754112,0.0303030303030303
304,How does LSTM solve the vanishing gradient problem?,"LSTM (Long Short-Term Memory) units introduce a special set of units with a recurrent connection to itself fixed to 1. The flow of information into and from the unit is controlled by input and output gates, and their behavior is learned. This architecture allows LSTM to address the vanishing gradient problem by enabling the model to learn long-term dependencies more effectively compared to traditional RNNs. Additionally, LSTM's gating mechanisms help in preserving the gradient flow during backpropagation, which mitigates the vanishing gradient issue commonly encountered in training RNNs. 

Source: Lecture slides or materials on Recurrent Neural Networks in the SEP 775 - Computational Natural Language Processing course.","['SpeciÔ¨Åcally it introduces a special set of units called LSTM units which are linear and have a recurrent connection to itself which is Ô¨Åxed to 1. The Ô¨Çow of information into the unit and from the unit is guarded by an input and output gates their behaviour is learned . There are several variations of this basic structure. This solution does not address explicitly the exploding gradients problem. Sutskever et al. 2011 use the Hessian Free opti mizer in conjunction with structural damping a spe ciÔ¨Åc damping strategy of the Hessian. This approach seems to deal very well with the vanishing gradient though more detailed analysis is still missing. Pre sumably this method works because in high dimen sional spaces there is a high probability for long term components to be orthogonal to short term ones. This would allow the Hessian to rescale these components independently. In practice one can not guarantee that this property holds. As discussed in section 2.3 this method is able to deal with the exploding gradient as well. Structural damping is an enhancement that forces the change in the state to be small when the pa rameter changes by some small value Œ∏. This asks for the Jacobian matrices xt Œ∏ to have small norm hence further helping with the exploding gradients problem. The fact that it helps when training recurrent neural models on long sequences suggests that while the cur vature might explode at the same time with the gradi ent it might not grow at the same rate and hence not be suÔ¨Écient to deal with the exploding gradient. Echo State Networks LukoÀá seviÀá cius and Jaeger 2009 avoid the exploding and vanishing gradients problem by not learning the recurrent and input weights. They are sampled from hand crafted distributions. Because usually the largest eigenvalue of the recurrent weight is by construction smaller than 1 information fed in to the model has to die out exponentially fast. This means that these models can not easily deal with long term dependencies even though the reason is slightly diÔ¨Äerent from the vanishing gradients problem. An extension to the classical model is represented by leaky integration units Jaeger et al.', 'The vanishing gradients problem refers to the opposite behaviour when long term components go exponentially fast to norm 0 making it impossible for the model to learn correlation between temporally dis tant events. 2.1. The mechanics To understand this phenomenon we need to look at the form of each temporal component and in particular at the matrix factors xt xk see equation 5 that take the form of a product of t k Jacobian matrices. In the same way a product of t k real numbers can shrink to zero or explode to inÔ¨Ånity so does this product of matrices along some direction v . In what follows we will try to formalize these intu itions extending a similar derivation done in Bengio et al. 1994 where only a single hidden unit case was considered . If we consider a linear version of the model i.e. set œÉ to the identity function in equation 11 we can use the power iteration method to formally analyze this prod uct of Jacobian matrices and obtain tight conditions for when the gradients explode or vanish see the sup plementary materials for a detailed derivation of these conditions . It is suÔ¨Écient for the largest eigenvalue Œª1 of the recurrent weight matrix to be smaller than 1 for long term components to vanish as t and necessary for it to be larger than 1 for gradients to explode. We can generalize these results for nonlinear functions œÉ where the absolute values of œÉ x is bounded say by a value Œ≥ R and therefore diag œÉ xk Œ≥. We Ô¨Årst prove that it is suÔ¨Écient for Œª1 1 Œ≥ where Œª1 is the absolute value of the largest eigenvalue of the recurrent weight matrix Wrec for the vanishing gradient problem to occur. Note that we assume the parametrization given by equation 11 . The Jacobian matrix xk 1 xk is given by WT recdiag œÉ xk . The 2 norm of this Jacobian is bounded by the product ofOn the diÔ¨Éculty of training Recurrent Neural Networks the norms of the two matrices see equation 6 .']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.8686633243404812,1.0,0.9186661280750156,0.0333333333333333
305,"What is the impact of bidirectional RNNs, and where are they applicable?","Bidirectional RNNs have the advantage of being able to access information from both past and future contexts simultaneously, which can be beneficial for tasks where context from both directions is important, such as machine translation, speech recognition, and sentiment analysis. They are applicable in tasks where understanding the entire sequence is crucial for making accurate predictions or classifications. Additionally, bidirectional RNNs are useful in tasks where context from both directions can provide a more comprehensive understanding of the data, leading to improved performance in tasks like named entity recognition and part-of-speech tagging.","['The notation in this image is slightly different here the equivalent of W hh is Wh W hx is We and W S is U. E converts word inputs x t to word embeddings e t . The Ô¨Ånal softmax over the vocabulary shows us the probability of various options for token x 5 condi tioned on all previous tokens. The input could be much longer than 4 5 tokens. 2.1 RNN Loss and Perplexity The loss function used in RNNs is often the cross entropy error in troduced in earlier notes. Equation 7 shows this function as the sum over the entire vocabulary at time step t. J t Œ∏ V j 1 yt j log ÀÜ yt j 7 The cross entropy error over a corpus of size T is J 1 T T t 1 J t Œ∏ 1 T T t 1 V j 1 yt j log ÀÜ yt j 8 Equation 9 is called the perplexity relationship it is basically 2 to the power of the negative log probability of the cross entropy error function shown in Equation 8. Perplexity is a measure of confusion where lower values imply more conÔ¨Ådence in predicting the next word in the sequence compared to the ground truth outcome . Perplexity 2J 9 2.2 Advantages Disadvantages and Applications of RNNs RNNs have several advantages 1. They can process input sequences of any length 2. The model size does not increase for longer input sequence lengths 3. Computation for step t can in theory use information from many steps back. 4. The same weights are applied to every timestep of the input so there is symmetry in how inputs are processed However they also have some disadvantages 1. Computation is slow because it is sequential it cannot be paral lelized 2. In practice it is difÔ¨Åcult to access information from many steps back due to problems like vanishing and exploding gradients which we discuss in the following subsection The amount of memory required to run a layer of RNN is pro portional to the number of words in the corpus.', 'h h h Figure 8 A bi directional RNN model h t f W xt V h t 1 b 17 h t f W xt V h t 1 b 18 ÀÜ yt g Uht c g U h t h t c 19 RNNs can also be multi layered. Figure 9 shows a multi layer bi directional RNN where each lower layer feeds the next layer. As shown in this Ô¨Ågure in this network architecture at time step t each interme diate neuron receives one set of parameters from the previous time step in the same RNN layer and two sets of parameters from the previous RNN hidden layer one input comes from the left to right RNN and the other from the right to left RNN. To construct a Deep RNN with L layers the above relationships are modiÔ¨Åed to the relationships in Equations 20 and 21 where the input to each intermediate neuron at level i is the output of the RNN at layer i 1 at the same time step t. The output ÀÜ y at each time step is the result of propagating input parameters through all hidden layers Equation 22 . Going Deep h i t f W i ht i 1 h i t f W i ht i 1 yt g U h t L h y h 3 x Each memory layer passes an intermediate seq representation to the next. h 2 h 1 Figure 9 A deep bi directional RNN with three RNN layers. h i t f W i h i 1 t V i h i t 1 b i 20 h i t f W i h i 1 t V i h i t 1 b i 21 ÀÜ yt g Uht c g U h L t h L t c 22 2.6 Application RNN Translation Model Traditional translation models are quite complex they consist of nu merous machine learning algorithms applied to different stages of the language translation pipeline. In this section we discuss the potential for adopting RNNs as a replacement to traditional translation mod ules. Consider the RNN example model shown in Figure 10 here the German phrase Echt dicke Kiste is translated to Awesome sauce.']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.932795256548958,0.8,0.946451631923856,0.0416666666666666
306,Describe the concept of multi-layer or stacked RNNs and their benefit.,"Multi-layer or stacked RNNs involve applying multiple RNNs on top of each other, allowing the network to compute more complex representations. The lower RNN layers are responsible for computing lower-level features, while the higher RNN layers compute higher-level features. Stacked RNNs are also known as multi-layer RNNs and have been found to work better than single-layer RNNs in various tasks such as Neural Machine Translation. Typically, high-performing RNNs have multiple layers, but they are not as deep as convolutional or feedforward networks. Skip connections or dense connections are often used to train deeper RNNs effectively. Transformer-based networks like BERT are usually deeper, with around 12 or 24 layers, and they incorporate skipping connections extensively.

Source: Lecture03_RNNs_and_LLMs.txt","['You will learn more about transformers including BERT in a couple of weeksMulti layer RNNs RNNs are already deep on one dimension they unroll over many timesteps We can also make them deep in another dimension by applying multiple RNNs this is a multi layer RNN. This allows the network to compute more complex representations The lower RNNs should compute lower level features and the higher RNNs should compute higher level features. Multi layer RNNs are also called stacked RNNs. 80Multi layer RNNs was terribly exciting the movie RNN layer 1 RNN layer 2 81 RNN layer 3 The hidden states from RNN layer i are the inputs to RNN layer i 1Multi layer RNNs in practice 39 Multi layer or stacked RNNs allow a network to compute more complex representations they work better than just have one layer of high dimensional encodings The lower RNNs should compute lower level features and the higher RNNs should compute higher level features. High performing RNNs are usually multi layer but aren t as deep as convolutional or feed forward networks For example In a 2017 paper Britz et al. find that for Neural Machine Translation 2 to 4 layers is best for the encoder RNN and 4 layers is best for the decoder RNN Often 2 layers is a lot better than 1 and 3 might be a little better than 2 Usually skip connections dense connections are needed to train deeper RNNs e.g. 8 layers Transformer based networks e.g. BERT are usually deeper like 12 or 24 layers. You will learn about Transformers later they have a lot of skipping like connections Massive Exploration of Neural Machine Translation Architecutres Britz et al 2017. https arxiv.org pdf 1703.03906.pdfTerminology By the end of the course You will understand phrases like stacked bidirectional LSTMs with residual connections and self attention The RNN described in this lecture simple vanilla Elman RNN You learned about other RNN flavors like LSTM and GRU and multi layer RNNs 83', 'h h h Figure 8 A bi directional RNN model h t f W xt V h t 1 b 17 h t f W xt V h t 1 b 18 ÀÜ yt g Uht c g U h t h t c 19 RNNs can also be multi layered. Figure 9 shows a multi layer bi directional RNN where each lower layer feeds the next layer. As shown in this Ô¨Ågure in this network architecture at time step t each interme diate neuron receives one set of parameters from the previous time step in the same RNN layer and two sets of parameters from the previous RNN hidden layer one input comes from the left to right RNN and the other from the right to left RNN. To construct a Deep RNN with L layers the above relationships are modiÔ¨Åed to the relationships in Equations 20 and 21 where the input to each intermediate neuron at level i is the output of the RNN at layer i 1 at the same time step t. The output ÀÜ y at each time step is the result of propagating input parameters through all hidden layers Equation 22 . Going Deep h i t f W i ht i 1 h i t f W i ht i 1 yt g U h t L h y h 3 x Each memory layer passes an intermediate seq representation to the next. h 2 h 1 Figure 9 A deep bi directional RNN with three RNN layers. h i t f W i h i 1 t V i h i t 1 b i 20 h i t f W i h i 1 t V i h i t 1 b i 21 ÀÜ yt g Uht c g U h L t h L t c 22 2.6 Application RNN Translation Model Traditional translation models are quite complex they consist of nu merous machine learning algorithms applied to different stages of the language translation pipeline. In this section we discuss the potential for adopting RNNs as a replacement to traditional translation mod ules. Consider the RNN example model shown in Figure 10 here the German phrase Echt dicke Kiste is translated to Awesome sauce.']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9441586103598368,1.0,0.9441371811410124,0.0588235294117647
307,How does Dropout work during training and testing?,"During training, Dropout works by randomly dropping a subset of neurons with a certain probability (1-p) during each forward and backward pass. This means that some neurons are set to zero while others are kept alive with a probability p. During testing, all weights are multiplied by 1/p, and no dropout is applied. This technique helps prevent overfitting, as it effectively trains multiple smaller networks simultaneously and averages their predictions. 
Source: SEP775_NLP - lecture02_notes_Backpropagation.txt","['Imposing such a con straint can also be interpreted as the prior Bayesian belief that the optimal weights are close to zero how close depends on the value of Œª. Choosing the right value of Œª is critical and must be chosen via hyperparameter tuning. Too high a value of Œª causes most of the weights to be set too close to 0 and the model does not learn anything meaningful from the training data often obtaining poor ac curacy on training validation and testing sets. Too low a value and we fall into the domain of overÔ¨Åtting once again. It must be noted that the bias terms are not regularized and do not contribute to the cost term above try thinking about why this is the case There are indeed other types of regularization that are sometimes used such as L1 regularization which sums over the absolute values rather than squares of parameter elements however this is less commonly applied in practice since it leads to sparsity of parameter weights. In the next section we discuss dropout which effectively acts as another form of regularization by randomly dropping i.e. setting to zero neurons in the forward pass. 2.3 Dropout Dropout is a powerful technique for regularization Ô¨Årst introduced by Srivastava et al. in Dropout A Simple Way to Prevent Neural Net works from OverÔ¨Åtting. The idea is simple yet effective during train ing we will randomly drop with some probability 1 p a subset of neurons during each forward backward pass or equivalently we will keep alive each neuron with a probability p . Then during testing we will use the full network to compute our predictions. The result is that the network typically learns more meaningful informa tion from the data is less likely to overÔ¨Åt and usually obtains higher performance overall on the task at hand. One intuitive reason why this technique should be so effective is that what dropout is doing is essentially doing is training exponentially many smaller networks at once and averaging over their predictions. Dropout applied to an artiÔ¨Åcial neural network. Image credits to Srivastava et al. In practice the way we introduce dropout is that we take the out put h of each layer of neurons and keep each neuron with prob ability p and else set it to 0. Then during back propagation we only pass gradients through neurons that were kept alive during the forward pass.', 'like an ensemble model Nowadays usually thought of as strong feature dependent regularizer Wager Wang Liang 2013DropoutDropout Srivastava Hinton Krizhevsky Sutskever Salakhutdinov 2012 JMLR 2014 During training For each data point each time Randomly set input to 0 with probability ùëù dropout ratio often p 0.5 except p 0.15 for input layer via dropout mask During testing Multiply all weights by 1 ùëù No other dropout ùë•2 ùë•3 0 ùë§1 ùë§2 ùë§3 ùë§4 ùëè 1 ùë¶ ùë§1ùë•1 ùë§2ùë•2 ùë§3ùë•3 ùë• ùë• 1 1 Train 1 Train 2 Test ùë•3 0 1 ùë¶ ùë§1ùë•1 ùë§3ùë•3 ùëè ùë§1 ùë§2 ùë§3 ùë§4 ùëè ùë•1 0 ùë•2 ùë•3 ùë•4 ùë§1 ùë§2 ùë§3 ùë§4 ùëè 1 ùë¶ 1 ùëù ùë§1ùë•1 ùë§2ùë•2 ùë§3ùë•3 ùë§4ùë•4 6Vectorization E.g. looping over word vectors versus concatenating them all into one large matrix and then multiplying the softmax weights with that matrix for loop 1000 loops best of 3 639 ¬µs per loop Using single a C x N matrix 10000 loops best of 3 53.8 ¬µs per loop Matrices are awesome Always try to use vectors and matrices rather than for loops The speed gain goes from 1 to 2 orders of magnitude with GPUs 7Parameter Initialization You normally must initialize weights to small random values i.e. not zero matrices To avoid symmetries that prevent learning specialization Initialize hidden layer biases to 0 and output or reconstruction biases to optimal value if weights were 0 e.g.']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9442827756529502,1.0,0.9296841679729628,0.375
308,What is the role of parameter initialization in neural networks?,"Parameter initialization in neural networks plays a crucial role in achieving superlative performance. By initializing the weights to small random numbers around 0, we can help the network converge faster and achieve lower error rates, especially when using activation functions like sigmoid and tanh. This initialization strategy aims to maintain activation variances and backpropagated gradient variances across layers, which helps prevent issues like vanishing gradients during training. Without proper initialization, gradient variances, which are indicative of information flow, tend to decrease across layers during backpropagation, hindering the network's learning process.","['This is useful since input features are often measured in different units but we often want to initially consider all features as equally important. The way we accomplish this is by simply dividing the features by their respective standard deviation calculated across the training set. Whitening Not as commonly used as mean subtraction normalization whiten ing essentially converts the data to a have an identity covariance matrix that is features become uncorrelated and have a variance of 1. This is done by Ô¨Årst mean subtracting the data as usual to get X . We can then take the Singular Value Decomposition SVD of X to get matrices U S V. We then compute UX to project X into the basis deÔ¨Åned by the columns of U. We Ô¨Ånally divide each dimension of the result by the corresponding singular value in S to scale our data appropriately if a singular value is zero we can just divide by a small number instead . 2.6 Parameter Initialization A key step towards achieving superlative performance with a neu ral network is initializing the parameters in a reasonable way. A good starting strategy is to initialize the weights to small random numbers normally distributed around 0 and in practice this often words acceptably well. However in Understanding the dif ficulty of training deep feedforward neural networks 2010 Xavier et al study the effect of different weight and bias initialization schemes on training dynamics. The empirical Ô¨Åndings suggest that for sigmoid and tanh activation units faster convergence and lower error rates are achieved when the weights of a matrix W Rn l 1 n l are initialized randomly with a uniform distribution as follows W U s 6 n l n l 1 s 6 n l n l 1 Where n l is the number of input units to W fan in and n l 1 is the number of output units from W fan out . In this parameter initialization scheme bias units are initialized to 0. This approach attempts to maintain activation variances as well as backpropagated gradient variances across layers. Without such initialization the gradient variances which are a proxy for information generally decrease with backpropagation across layers. 2.7 Learning Strategies The rate magnitude of model parameter updates during training can be controlled using the learning rate.', 'file name: lecture02 notes Backpropagation.  Neural networks. Forward computation. Backward propagation. Neuron Units. Max margin Loss. Gradient checks. Xavier parameter initialization. Learning rates. Adagrad. This set of notes introduces single and multilayer neural networks and how they can be used for classiÔ¨Åcation purposes. We then dis cuss how they can be trained using a distributed gradient descent technique known as backpropagation. We will see how the chain rule can be used to make parameter updates sequentially. After a rigorous mathematical discussion of neural networks we will discuss some practical tips and tricks in training neural networks involving neuron units non linearities gradient checks Xavier parameter ini tialization learning rates Adagrad etc. Lastly we will motivate the use of recurrent neural networks as a language model. 1 Neural Networks Foundations Figure 1 We see here how a non linear decision boundary separates the data very well. This is the prowess of neural networks. Fun Fact Neural networks are biologically in spired classiÔ¨Åers which is why they are often called artiÔ¨Åcial neural networks to distinguish them from the organic kind. However in reality human neural networks are so much more capable and complex from artiÔ¨Åcial neural net works that it is usually better to not draw too many parallels between the two. We established in our previous discussions the need for non linear classiÔ¨Åers since most data are not linearly separable and thus our classiÔ¨Åcation performance on them is limited. Neural networks are a family of classiÔ¨Åers with non linear decision boundary as seen in Figure 1. Now that we know the sort of decision boundaries neural networks create let us see how they manage doing so. 1.1 A Neuron A neuron is a generic computational unit that takes n inputs and produces a single output. What differentiates the outputs of different neurons is their parameters also referred to as their weights . One of the most popular choices for neurons is the sigmoid or binary lo gistic regression unit.']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9872076333770003,1.0,0.9103122805237664,0.0285714285714285
309,"What are the differences between Vanilla RNNs, LSTMs, and GRUs?","Vanilla RNNs suffer from the vanishing gradient problem, making it hard to capture long-term dependencies. LSTMs address this by using a more complex structure with forget, input, and output gates, allowing for better long-term memory retention. GRUs, a variation of LSTMs, combine the forget and input gates into a single update gate, simplifying the model while still capturing long-term dependencies effectively. Additionally, GRUs merge the cell state and hidden state, making them easier to train compared to traditional RNNs.","['We only forget when we re going to input something in its place. We only input new values to the state when we forget something older. A slightly more dramatic variation on the LSTM is the Gated Recurrent Unit or GRU introduced by Cho et al. 2014 http arxiv.org pdf 1406.1078v3.pdf . It combines the forget and input gates into a single update gate. It also merges the cell state and hidden state and makes some other changes. The resulting model is simpler than standard LSTM models and has been growing increasingly popular. These are only a few of the most notable LSTM variants. There are lots of others like Depth Gated RNNs by Yao et al. 2015 http arxiv.org pdf 1508.03790v2.pdf . There s also some completely different approach to tackling long term dependencies like Clockwork RNNs by Koutnik et al. 2014 http arxiv.org pdf 1402.3511v1.pdf . Which of these variants is best Do the differences matter Greff et al. 2015 http arxiv.org pdf 1503.04069.pdf do a nice comparison of popular variants finding that they re all about the same. Jozefowicz et al. 2015 http jmlr.org proceedings papers v37 jozefowicz15.pdf tested more than ten thousand RNN architectures finding some that worked better than LSTMs on certain tasks. Conclusion Earlier I mentioned the remarkable results people are achieving with RNNs. Essentially all of these are achieved using LSTMs. They really work a lot better for most tasks Written down as a set of equations LSTMs look pretty intimidating. Hopefully walking through them step by step in this essay has made them a bit more approachable.1 24 24 3 23 PM Understanding LSTM Networks colah s blog 7 7 Attention and Augmented Recurrent Neural Networks On Distill Conv Nets A Modular Perspective Neural Networks Manifolds and Topology Deep Learning NLP and Representations LSTMs were a big step in what we can accomplish with RNNs.', 'Extension V given a word sequence A B C in German whose transla tion is X Y in English instead of training the RNN using A B C X Y train it using C B A X Y. The intutition behind this technique is that A is more likely to be translated to X. Thus given the vanishing gradient problem discussed earlier reversing the order of the input words can help reduce the error rate in generating the output phrase. 3 Gated Recurrent Units Beyond the extensions discussed so far RNNs have been found to per form better with the use of more complex units for activation. So far we have discussed methods that transition from hidden state ht 1 to ht using an afÔ¨Åne transformation and a point wise nonlinearity. Here we discuss the use of a gated activation function thereby modifying the RNN architecture. What motivates this Well although RNNs can theoretically capture long term dependencies they are very hard to actually train to do this. Gated recurrent units are designed in a manner to have more persistent memory thereby making it easier for RNNs to capture long term dependencies. Let us see mathematically how a GRU uses ht 1 and xt to generate the next hidden state ht. We will then dive into the intuition of this architecture. zt œÉ W z xt U z ht 1 Update gate rt œÉ W r xt U r ht 1 Reset gate ht tanh rt Uht 1 Wxt New memory ht 1 zt ht zt ht 1 Hidden state The above equations can be thought of a GRU s four fundamental op erational stages and they have intuitive interpretations that make this model much more intellectually satisfying see Figure 12 1. New memory generation A new memory ht is the consolidation of a new input word xt with the past hidden state ht 1. Anthropomor phically this stage is the one who knows the recipe of combining a newly observed word with the past hidden state ht 1 to summarize this new word in light of the contextual past as the vector ht. 2. Reset Gate The reset signal rt is responsible for determining how important ht 1 is to the summarization ht.']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.8650579425758798,1.0,0.9519946251966552,0.037037037037037
310,What is the difference between traditional language models and conditioned language models?,"Traditional language models are generative models of text, while conditioned language models not only generate text but do so according to some specification or input data. Conditioned language models take into account structured data, such as English documents, utterances, images, speech, and generate text based on this input. They are designed to produce text that is conditioned on specific information or context provided to the model.

Source: lecture04 Conditioned Generation","['file name: lecture04 Conditioned Generation. Hamidreza Mahyar mahyarh@mcmaster.ca Computational Natural Language Processing Conditioned GenerationLanguage Models Language models are generative models of text s P x The Malfoys said Hermione. Harry was watching him. He looked like Madame Maxime. When she strode up the wrong staircase to visit himself. I m afraid I ve definitely been suspended from power no chance indeed said Snape. He put his head back behind them and read groups as they crossed a corner and fluttered down onto their ink lamp and picked up his spoon. The doorbell rang. It was a lot cleaner down in London. Text Credit Max Deutsch https medium.com deep writingConditioned Language Models Not just generate text generate text according to some specification Input X Structured Data English Document Utterance Image Speech Output Y Text NL Description Japanese Short Description Response Text Transcript Task NL Generation Translation Summarization Response Generation Image Captioning Speech RecognitionFormulation and ModelingCalculating the Probability of a SentenceConditional Language ModelsOne Type of Language Model LSTM LSTM LSTM LSTM predict hate predict this predict movie predict s LSTM Mikolov et al. 2011 s I hate this movie predict ILSTM LSTM LSTM LSTM LSTM s LSTM LSTM LSTM LSTM argmax argmax argmax argmax argmax s One Type of Conditional Language Model I hate this movie kono eiga ga kirai hate this movie Sutskever et al. 2014 Encoder I DecoderHow to Pass Hidden State Initialize decoder w encoder Sutskever et al. 2014 encoder decoder Transform can be different dimensions encoder transform decoder encoder Input at every time step Kalchbrenner Blunsom 2013 decoder decoder decoderMethods of GenerationThe Generation Problem We have a model of P Y X how do we use it to generate a sentence Two methods Sampling Try to generate a random sentence according to the probability distribution. Argmax Try to generate the sentence with the highest probability.Ancestral Sampling Randomly generate words one by one.', 'For instance HLW 20 observe that larger models do not necessarily generalize better out of distribution. There is evidence that suggests that the generalization achieved under this paradigm can be poor because the model is overly speciÔ¨Åc to the training distribution and does not generalize well outside it YdC 19 MPL19 . Thus the performance of Ô¨Åne tuned models on speciÔ¨Åc benchmarks even when it is nominally at human level may exaggerate actual performance on the underlying task GSL 18 NK19 . Third humans do not require large supervised datasets to learn most language tasks a brief directive in natural language e.g. please tell me if this sentence describes something happy or something sad or at most a tiny number of demonstrations e.g. here are two examples of people acting brave please give a third example of bravery is often Figure 1.1 Language model meta learning. During unsupervised pre training a language model develops a broad set of skills and pattern recognition abilities. It then uses these abilities at inference time to rapidly adapt to or recognize the desired task. We use the term in context learning to describe the inner loop of this process which occurs within the forward pass upon each sequence. The sequences in this diagram are not intended to be representative of the data a model would see during pre training but are intended to show that there are sometimes repeated sub tasks embedded within a single sequence. 3Figure 1.2 Larger models make increasingly efÔ¨Åcient use of in context information. We show in context learning performance on a simple task requiring the model to remove random symbols from a word both with and without a natural language task description see Sec. 3.9.2 . The steeper in context learning curves for large models demonstrate improved ability to learn a task from contextual information. We see qualitatively similar behavior across a wide range of tasks. sufÔ¨Åcient to enable a human to perform a new task to at least a reasonable degree of competence. Aside from pointing to a conceptual limitation in our current NLP techniques this adaptability has practical advantages it allows humans to seamlessly mix together or switch between many tasks and skills for example performing addition during a lengthy dialogue.']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.0000000000000002,1.0,0.9610972185623312,0.0
311,How can LSTM networks be used in language modeling?,"LSTM networks can be used in language modeling by predicting the next word in a sequence of words. They are particularly effective for tasks like handwriting recognition, speech recognition, machine translation, parsing, image captioning, and generating text due to their ability to handle long-term dependencies in the data. LSTMs have shown real-world success in achieving state-of-the-art results in various NLP tasks, making them a dominant approach in the field. Additionally, LSTMs have been used to build language models that predict the next word in a sentence, making them a valuable tool for tasks involving generating text or estimating the probability of text.","['are hard to train Another solution lots of new deep feedforward convolutional architectures add more direct connections thus allowing the gradient to flow Deep Residual Learning for Image Recognition He et al 2015. https arxiv.org pdf 1512.03385.pdfLSTMs real world success 68 In 2013 2015 LSTMs started achieving state of the art results Successful tasks include handwriting recognition speech recognition machine translation parsing and image captioning as well as language models LSTMs became the dominant approach for most NLP tasks Now 2019 2023 Transformers have become dominant for all tasks For example in WMT a Machine Translation conference competition In WMT 2014 there were 0 neural machine translation systems In WMT 2016 the summary report contains RNN 44 times and these systems won In WMT 2019 RNN 7 times Transformer 105 times Source Findings of the 2016 Conference on Machine Translation WMT16 Bojar et al. 2016 http www.statmt.org wmt16 pdf W16 2301.pdf Source Findings of the 2018 Conference on Machine Translation WMT18 Bojar et al. 2018 http www.statmt.org wmt18 pdf WMT028.pdf Source Findings of the 2019 Conference on Machine Translation WMT19 Barrault et al. 2019 http www.statmt.org wmt18 pdf WMT028.pdf5. Recap 69 Language Model A system that predicts the next word Recurrent Neural Network A family of neural networks that Take sequential input of any length Apply the same weights on each step Can optionally produce output on each step Recurrent Neural Network Language Model We ve shown that RNNs are a great way to build a LM despite some problems RNNs are also useful for much moreWhy should we care about Language Modeling 70 Language Modeling is a benchmark task that helps us measure our progress on predicting language use Language Modeling is a subcomponent of many NLP tasks especially those involving generating text or estimating the probability of text Predictive typing Speech recognition Handwriting recognition Spelling grammar correction Authorship identification Machine translation Summarization Dialogue etc.', 'But they really are pretty amazing. Essential to these successes is the use of LSTMs a very special kind of recurrent neural network which works for many tasks much much better than the standard version. Almost all exciting results based on recurrent neural networks are achieved with them. It s these LSTMs that this essay will explore. The Problem of Long Term Dependencies One of the appeals of RNNs is the idea that they might be able to connect previous information to the present task such as using previous video frames might inform the understanding of the present frame. If RNNs could do this they d be extremely useful. But can they It depends. Sometimes we only need to look at recent information to perform the present task. For example consider a language model trying to predict the next word based on the previous ones. If we are trying to predict the last word in the clouds are in the sky we don t need any further context it s pretty obvious the next word is going to be sky. In such cases where the gap between the relevant information and the place that it s needed is small RNNs can learn to use the past information. But there are also cases where we need more context. Consider trying to predict the last word in the text I grew up in France I speak fluent French. Recent information suggests that the next word is probably the name of a language but if we want to narrow down which language we need the context of France from further back. It s entirely possible for the gap between the relevant information and the point where it is needed to become very large. Unfortunately as that gap grows RNNs become unable to learn to connect the information. In theory RNNs are absolutely capable of handling such long term dependencies. A human could carefully pick parameters for them to solve toy problems of this form. Sadly in practice RNNs don t seem to be able to learn them. The problem was explored in depth by Hochreiter 1991 German http people.idsia.ch juergen SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf and Bengio et al. 1994 http www dsi.ing.unifi.it paolo ps tnn 94 gradient.pdf who found some pretty fundamental reasons why it might be difficult.']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.930439267985022,1.0,0.9521114221923718,0.0434782608695652
312,What are the methods of generation in conditioned language modeling?,"The methods of generation in conditioned language modeling are Sampling and Argmax. Sampling involves trying to generate a random sentence according to the probability distribution, while Argmax involves trying to generate the sentence with the highest probability. Ancestral Sampling is another method where words are randomly generated one by one. 

Source: Lecture04 Conditioned Generation","['file name: lecture04 Conditioned Generation. Hamidreza Mahyar mahyarh@mcmaster.ca Computational Natural Language Processing Conditioned GenerationLanguage Models Language models are generative models of text s P x The Malfoys said Hermione. Harry was watching him. He looked like Madame Maxime. When she strode up the wrong staircase to visit himself. I m afraid I ve definitely been suspended from power no chance indeed said Snape. He put his head back behind them and read groups as they crossed a corner and fluttered down onto their ink lamp and picked up his spoon. The doorbell rang. It was a lot cleaner down in London. Text Credit Max Deutsch https medium.com deep writingConditioned Language Models Not just generate text generate text according to some specification Input X Structured Data English Document Utterance Image Speech Output Y Text NL Description Japanese Short Description Response Text Transcript Task NL Generation Translation Summarization Response Generation Image Captioning Speech RecognitionFormulation and ModelingCalculating the Probability of a SentenceConditional Language ModelsOne Type of Language Model LSTM LSTM LSTM LSTM predict hate predict this predict movie predict s LSTM Mikolov et al. 2011 s I hate this movie predict ILSTM LSTM LSTM LSTM LSTM s LSTM LSTM LSTM LSTM argmax argmax argmax argmax argmax s One Type of Conditional Language Model I hate this movie kono eiga ga kirai hate this movie Sutskever et al. 2014 Encoder I DecoderHow to Pass Hidden State Initialize decoder w encoder Sutskever et al. 2014 encoder decoder Transform can be different dimensions encoder transform decoder encoder Input at every time step Kalchbrenner Blunsom 2013 decoder decoder decoderMethods of GenerationThe Generation Problem We have a model of P Y X how do we use it to generate a sentence Two methods Sampling Try to generate a random sentence according to the probability distribution. Argmax Try to generate the sentence with the highest probability.Ancestral Sampling Randomly generate words one by one.', 'Empirical evaluation and combination of advanced language modeling tech niques. In Proc. 12th annual conference of the in ternational speech communication association IN TERSPEECH 2011 . Mikolov T. Sutskever I. Deoras A. Le H. S. Kombrink S. and Cernocky J. 2012 . Subword language modeling with neural networks. preprint http www.Ô¨Åt.vutbr.cz imikolov rnnlm char.pdf . Moreira M. and Fiesler E. 1995 . Neural net works with adaptive learning rate and momentum terms. Idiap RR Idiap RR 04 1995 IDIAP Mar tigny Switzerland. Pascanu R. and Jaeger H. 2011 . A neurodynamical model for working memory. Neural Netw. 24 199 207. Rumelhart D. E. Hinton G. E. and Williams R. J. 1986 . Learning representations by back propagating errors. Nature 323 6088 533 536. Strogatz S. 1994 . Nonlinear Dynamics And Chaos With Applications To Physics Biology Chemistry And Engineering Studies in Nonlinearity . Studies in nonlinearity. Perseus Books Group 1 edition. Sutskever I. Martens J. and Hinton G. 2011 . Generating text with recurrent neural networks. In L. Getoor and T. ScheÔ¨Äer editors Proceedings of the 28th International Conference on Machine Learning ICML 11 ICML 11 pages 1017 1024 New York NY USA. ACM. Werbos P. J. 1988 . Generalization of backpropa gation with application to a recurrent gas market model. Neural Networks 1 4 339 356. Analytical analysis of the exploding and vanishing gradients problem xt WrecœÉ xt 1 Winut b 11 Let us consider the term gT k Et xt xt xk xk Œ∏ for the linear version of the parametrization in equation 11 i.e.']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.0000000000000002,0.75,0.93811128284991,0.025
313,What is beam search and how is it used in language generation?,"Beam search is a search algorithm used in natural language generation to find the most likely sequence of words in a probabilistic model. It works by maintaining a set of partial hypotheses (sequences of words) and expanding them based on the probabilities assigned to the next possible words. The beam search algorithm keeps track of a fixed number of top hypotheses (called the beam width) at each step and prunes the rest, allowing for more efficient exploration of the search space while still considering multiple possible sequences. This technique is commonly used in tasks such as machine translation, text summarization, and speech recognition to generate fluent and coherent output sequences.

Source: Lecture on Natural Language Generation in SEP 775 - Computational Natural Language Processing.","['BART Denoising sequence to sequence pre training for natural language generation translation and comprehension. In Association for Computa tional Linguistics ACL pages 7871 7880. Patrick Lewis Ethan Perez Aleksandara Piktus Fabio Petroni Vladimir Karpukhin Naman Goyal Heinrich K uttler Mike Lewis Wen tau Yih Tim Rockt aschel Sebastian Riedel and Douwe Kiela. 2020b. Retrieval augmented generation for knowledge intensive NLP tasks. In Advances in Neural Information Processing Systems NeurIPS .Yankai Lin Haozhe Ji Zhiyuan Liu and Maosong Sun. 2018. Denoising distantly supervised open domain question answering. In Association for Computa tional Linguistics ACL pages 1736 1745. Sewon Min Danqi Chen Hannaneh Hajishirzi and Luke Zettlemoyer. 2019a. A discrete hard EM ap proach for weakly supervised question answering. In Empirical Methods in Natural Language Process ing EMNLP . Sewon Min Danqi Chen Luke Zettlemoyer and Han naneh Hajishirzi. 2019b. Knowledge guided text re trieval and reading for open domain question answer ing. ArXiv abs 1911.03868. Dan Moldovan Marius Pas ca Sanda Harabagiu and Mihai Surdeanu. 2003. Performance issues and er ror analysis in an open domain question answering system. ACM Transactions on Information Systems TOIS 21 2 133 154. Stephen Mussmann and Stefano Ermon. 2016. Learn ing and inference via maximum inner product search. In International Conference on Machine Learning ICML pages 2587 2596. Yixin Nie Songhe Wang and Mohit Bansal. 2019. Re vealing the importance of semantic retrieval for ma chine reading at scale. In Empirical Methods in Nat ural Language Processing EMNLP . Rodrigo Nogueira and Kyunghyun Cho. 2019. Passage re ranking with BERT.', 'As there are 40 different phrase labels in WSJ each additional tree related lookup tables has 161 entries 40 4 1 corresponding to the IBES segment tags plus the extra O tag. We call level 0 the information associated with the leaves of the original Charniak parse tree. The lookup table for level 0 encodes the corresponding IOBES phrase tags for each words. We obtain levels 1 to 4 by repeatedly trimming the leaves as shown in Figure 6. We labeled O words belonging to the root node S or all words of the sentence if the root itself has been trimmed. Experiments were performed using the LM2 language model using the same network archi tectures see Table 5 and using additional lookup tables of dimension 5 for each parse tree level. Table 12 reports the performance improvements obtained by providing increasing levels of parse 18. In a more recent work Collobert 2011 we propose an extension of this approach for the generation of full syntactic parse trees using a recurrent version of our architecture. 2522NATURAL LANGUAGE PROCESSING ALMOST FROM SCRATCH Approach SRL valid test Benchmark System six parse trees 77.35 77.92 Benchmark System top Charniak parse tree only 74.76 NN SLL LM2 72.29 74.15 NN SLL LM2 Charniak level 0 only 74.44 75.65 NN SLL LM2 Charniak levels 0 1 74.50 75.81 NN SLL LM2 Charniak levels 0 to 2 75.09 76.05 NN SLL LM2 Charniak levels 0 to 3 75.12 75.89 NN SLL LM2 Charniak levels 0 to 4 75.42 76.06 NN SLL LM2 CHUNK 74.72 NN SLL LM2 PT0 75.49 Table 12 Generalization performance on the SRL task of our NN architecture compared with the benchmark system. We show performance of our system fed with different levels of depth of the Charniak parse tree.']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9218209045611296,1.0,0.9400337144662748,0.0
314,Explain the concept of model ensembling in language models.,"Model ensembling in language models refers to the technique of combining multiple individual models to improve overall performance. By aggregating predictions from different models, ensembling can help reduce overfitting, increase robustness, and enhance the overall accuracy of the language model. This approach is commonly used in natural language processing tasks to leverage the strengths of different models and improve the overall quality of predictions.","['For this graph we also include 6 additional extra small models with as few as 100 000 parameters. As observed in KMH 20 language modeling performance follows a power law when making efÔ¨Åcient use of training compute. After extending this trend by two more orders of magnitude we observe only a slight if any departure from the power law. One might worry that these improvements in cross entropy loss come only from modeling spurious details of our training corpus. However we will see in the following sections that improvements in cross entropy loss lead to consistent performance gains across a broad spectrum of natural language tasks. Below we evaluate the 8 models described in Section 2 the 175 billion parameter parameter GPT 3 and 7 smaller models on a wide range of datasets. We group the datasets into 9 categories representing roughly similar tasks. In Section 3.1 we evaluate on traditional language modeling tasks and tasks that are similar to language modeling such as Cloze tasks and sentence paragraph completion tasks. In Section 3.2 we evaluate on closed book question answering tasks tasks which require using the information stored in the model s parameters to answer general knowledge questions. In Section 3.3 we evaluate the model s ability to translate between languages especially one shot and few shot . In Section 3.4 we evaluate the model s performance on Winograd Schema like tasks. In Section 3.5 we evaluate on datasets that involve commonsense reasoning or question answering. In Section 3.6 we evaluate on reading comprehension tasks in Section 3.7 we evaluate on the SuperGLUE benchmark suite and in 3.8 we brieÔ¨Çy explore NLI. Finally in Section 3.9 we invent some additional tasks designed especially to probe in context learning abilities these tasks focus on on the Ô¨Çy reasoning adaptation skills or open ended text synthesis. We evaluate all tasks in the few shot one shot and zero shot settings. 10Figure 3.1 Smooth scaling of performance with compute. Performance measured in terms of cross entropy validation loss follows a power law trend with the amount of compute used for training. The power law behavior observed in KMH 20 continues for an additional two orders of magnitude with only small deviations from the predicted curve.', 'Any kind of knowledge of the test set can cause the perplexity to be artiÔ¨Åcially low. The perplexity of two language models is only comparable if they use identical vocabularies. An intrinsic improvement in perplexity does not guarantee an extrinsic im provement in the performance of a language processing task like speech recognition10 CHAPTER 3 N GRAM LANGUAGE MODELS or machine translation. Nonetheless because perplexity often correlates with such improvements it is commonly used as a quick check on an algorithm. But a model s improvement in perplexity should always be conÔ¨Årmed by an end to end evaluation of a real task before concluding the evaluation of the model. 3.3 Sampling sentences from a language model One important way to visualize what kind of knowledge a language model embodies is to sample from it. Sampling from a distribution means to choose random points sampling according to their likelihood. Thus sampling from a language model which rep resents a distribution over sentences means to generate some sentences choosing each sentence according to its likelihood as deÔ¨Åned by the model. Thus we are more likely to generate sentences that the model thinks have a high probability and less likely to generate sentences that the model thinks have a low probability. This technique of visualizing a language model by sampling was Ô¨Årst suggested very early on by Shannon 1951 and Miller and Selfridge 1950 . It s simplest to visualize how this works for the unigram case. Imagine all the words of the English language covering the probability space between 0 and 1 each word covering an interval proportional to its frequency. Fig. 3.3 shows a visualization using a unigram LM computed from the text of this book. We choose a random value between 0 and 1 Ô¨Ånd that point on the probability line and print the word whose interval includes this chosen value. We continue choosing random numbers and generating words until we randomly generate the sentence Ô¨Ånal token s .']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9582728788513124,0.0,0.9236329100585589,0.0303030303030303
315,How does ensemble distillation differ from parameter averaging?,"Ensemble distillation differs from parameter averaging in that while parameter averaging involves using multiple models at test time by taking the average of parameters, ensemble distillation trains a model to mimic the ensemble by matching the distribution over predicted words. This way, ensemble distillation aims to make the model make the same mistakes as an ensemble, ultimately increasing accuracy notably. 

Source: SEP 775_FinalProjects_2.txt","['An exact method for sampling from P X no further work needed.Greedy Search One by one pick the single highest probability word Not exact real problems Will often generate the easy words first Will prefer multiple common words to one rare wordBeam Search Instead of picking one high probability word maintain several paths Some in reading materials more in a later classModel EnsemblingEnsembling Why Multiple models make somewhat uncorrelated errors Models tend to be more uncertain when they are about to make errors Smooths over idiosyncrasies of the model LSTM1 predict1 I LSTM2 predict2 Combine predictions from multiple models s sLinear Interpolation Take a weighted average of the M model probabilities Second term often set to uniform distribution 1 MLog linear Interpolation Weighted combination of log probabilities normalize Interpolation coefficient often set to uniform distribution 1 MLinear or Log Linear Think of it in logic Linear Logical OR the interpolated model likes any choice that a model gives a high probability use models with models that capture different traits necessary when any model can assign zero probability Log Linear Logical AND interpolated model only likes choices where all models agree use when you want to restrict possible answersParameter Averaging Problem Ensembling means we have to use M models at test time increasing our time memory complexity Parameter averaging is a cheap way to get some good effects of ensembling Basically write out models several times near the end of training and take the average of parametersEnsemble Distillation e.g. Kim et al. 2016 Problem parameter averaging only works for models within the same run Knowledge distillation trains a model to copy the ensemble Specifically it tries to match the description over predicted words Why We want the model to make the same mistakes as an ensemble Shown to increase accuracy notablyStacking What if we have two very different models where prediction of outputs is done in very different ways e.g. a phrase based translation model and a neural MT model Niehues et al. 2017 Stacking uses the output of one system in calculating features for another systemCase Studies in Conditional Language ModelingFrom Images e.g. Karpathy et al. 2015 Input is image features output is text Use standard image encoders e.g.', 'Training Strategy Distillation Objective Design a loss function that effectively combines the traditio nal task specific loss and a distillation loss that measures the discrepancy between the teacher s and student s hidden states. Dataset Selection Utilize task specific datasets like the Stanford Sentiment Treebank SST for sentiment analysis CoNLL 2003 for NER and WMT datasets for machine translation. Training and Distillation Fine tune the student model on the selected dataset while also learning from the teacher model s hidden state outputs to achieve similar perfor mance. McMaster University 4McMaster University SEP 775 Evaluation Metrics Performance Metrics Use accuracy for classification tasks sentiment analysis NER and BLEU scores for generative tasks machine translation to measure the student mo del s effectiveness. Efficiency Metrics Evaluate model size reduction and inference speed improvements in the student model compared to the teacher model. Fidelity Metrics Measure the similarity between the student and teacher model out puts using techniques such as cosine similarity for hidden states. McMaster University 5McMaster University SEP 775 General Evaluation Criteria and Submission Requirements Evaluation Criteria The evaluation of the projects will be based on the following criteria 1. Accuracy and Performance The model s performance on the task measured against a predefined metric e.g. BLEU for generation F1 for classification tasks perplexity for language models . 2. Quality of Code Clarity organization and documentation of the code. The code should be well structured readable and accompanied by comments explaining the func tionality. 3. Data Preprocessing and Handling The effectiveness of data cleaning preprocessing and handling techniques. This includes dealing with missing values data augmentation and the use of effective data loaders. 4. Model Optimization and Training The effectiveness of model optimization techni ques including hyperparameter tuning regularization methods and training procedures. 5. Analysis and Interpretation Depth of analysis on the model s performance including error analysis limitations of the model and interpretation of the results. Insightful observations and potential improvements should be discussed. 6. Project Report The quality of the project report including clarity of writing or ganization and thoroughness in covering the project s scope methodology results and conclusions. The report should also include a clear abstract and introduction to the problem. 7. Innovation and Creativity The uniqueness of the approach and creativity in solving the problem.']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9915186957992388,0.8,0.9611407454448588,0.05
316,What are some common evaluation metrics for conditioned language models?,"Some common evaluation metrics for conditioned language models include BLEU, embedding-based metrics like BertScore, BLEURT, COMET, PRISM, and Perplexity. These metrics are used to assess the performance of language models in generating text based on certain conditions or inputs. 

Source: SEP 775 - Computational Natural Language Processing, lecture04_Conditioned_Generation","['2015 Input is image features output is text Use standard image encoders e.g. CNN Often pre trained on large databases such as ImageNetCheck out this review paperHow do we EvaluateBasic Evaluation Paradigm Use parallel test set Use system to generate translations Compare target translations w referenceHuman Evaluation Ask a human to do evaluation Final goal but slow expensive and sometimes inconsistentBLEU Works by comparing n gram overlap w reference Pros Easy to use good for measuring system improvement Cons Often doesn t match human eval bad for comparing very different systemsEmbedding based Metrics Recently many metrics based on neural models BertScore Find similarity between BERT embeddings unsupervised Zhang et al. 2020 BLEURT Train BERT to predict human evaluation scores Sellam et al. 2020 COMET Train model to predict human eval also using source sentence Rei et al. 2020 PRISM Model based on training paraphrasing model Thompson and Post 2020Perplexity Calculate the perplexity of the words in the held out set without doing generation Pros Naturally solves multiple reference problem Cons Doesn t consider decoding or actually generating output. May be reasonable for problems with lots of ambiguity.Which One to Use Meta evaluation runs human evaluation and automatic evaluation on the same outputs calculates correlation Examples WMT Metrics Task for MT Mathur et al. 2021 RealSumm for summarization Evaluation is hard especially with good systems Most metrics had no correlation w human eval over best systems at some WMT 2019 tasksQuestions', '. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 6.2 Fairness Bias and Representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36 6.3 Energy Usage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 7 Related Work 39 8 Conclusion 40 A Details of Common Crawl Filtering 43 B Details of Model Training 43 C Details of Test Set Contamination Studies 43 D Total Compute Used to Train Language Models 46 E Human Quality Assessment of Synthetic News Articles 46 F Additional Samples from GPT 3 48 G Details of Task Phrasing and SpeciÔ¨Åcations 50 H Results on All Tasks for All Model Sizes 63 21 Introduction Recent years have featured a trend towards pre trained language representations in NLP systems applied in increasingly Ô¨Çexible and task agnostic ways for downstream transfer.']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.0000000000000002,0.7142857142857143,0.9191628995739662,0.0
317,What is the significance of stacking in model ensembling?,"Stacking is significant in model ensembling when we have two very different models that predict outputs in different ways. Stacking involves using the output of one system to calculate features for another system, allowing for improved performance by leveraging the strengths of each model. This approach can be beneficial when combining models with diverse characteristics to enhance overall predictive accuracy and robustness. 

Source: Lecture04_Conditioned_Generation","['An exact method for sampling from P X no further work needed.Greedy Search One by one pick the single highest probability word Not exact real problems Will often generate the easy words first Will prefer multiple common words to one rare wordBeam Search Instead of picking one high probability word maintain several paths Some in reading materials more in a later classModel EnsemblingEnsembling Why Multiple models make somewhat uncorrelated errors Models tend to be more uncertain when they are about to make errors Smooths over idiosyncrasies of the model LSTM1 predict1 I LSTM2 predict2 Combine predictions from multiple models s sLinear Interpolation Take a weighted average of the M model probabilities Second term often set to uniform distribution 1 MLog linear Interpolation Weighted combination of log probabilities normalize Interpolation coefficient often set to uniform distribution 1 MLinear or Log Linear Think of it in logic Linear Logical OR the interpolated model likes any choice that a model gives a high probability use models with models that capture different traits necessary when any model can assign zero probability Log Linear Logical AND interpolated model only likes choices where all models agree use when you want to restrict possible answersParameter Averaging Problem Ensembling means we have to use M models at test time increasing our time memory complexity Parameter averaging is a cheap way to get some good effects of ensembling Basically write out models several times near the end of training and take the average of parametersEnsemble Distillation e.g. Kim et al. 2016 Problem parameter averaging only works for models within the same run Knowledge distillation trains a model to copy the ensemble Specifically it tries to match the description over predicted words Why We want the model to make the same mistakes as an ensemble Shown to increase accuracy notablyStacking What if we have two very different models where prediction of outputs is done in very different ways e.g. a phrase based translation model and a neural MT model Niehues et al. 2017 Stacking uses the output of one system in calculating features for another systemCase Studies in Conditional Language ModelingFrom Images e.g. Karpathy et al. 2015 Input is image features output is text Use standard image encoders e.g.', 'One potential explana tion for this result could be that for small scale models learning the 40 tasks used during instruction tuning Ô¨Ålls the entire model capacity causing these models to perform worse on new tasks. Under this potential explanation for the larger scale models instruction tuning Ô¨Ålls up some model capacity but also teaches these models how to follow instructions allowing them to generalize to new tasks with the remaining capacity. 4.3 ROLE OF INSTRUCTIONS 20 30 40 50 60 FT no instruction Eval instruction FT dataset name Eval instruction FT dataset name Eval dataset name FT instruction Eval instruction FLAN 37.3 46.6 47.0 55.2 Zero shot performance 4 task cluster avg. Figure 8 Ablation study result using mod els with instructions removed from Ô¨Ånetun ing FT . In a Ô¨Ånal ablation study we explore the role of in structions during Ô¨Ånetuning as one possibility is that performance gains come entirely from multi task Ô¨Åne tuning and the model could perform just as well without instructions. We hence consider two Ô¨Ånetuning setups without instructions. In a no template setup only inputs and outputs were given to the model e.g. for transla tion the input would be The dog runs. and the output would be Le chien court. . In a dataset name setup each input is prepended with the name of the task and dataset e.g. for translation to French the input would be Translation WMT 14 to French The dog runs. . We compare these two ablations to FLAN s Ô¨Ånetun ing procedure which used natural instructions e.g. Please translate this sentence to French The dog runs. . We perform evaluations for four held out clus ters from Figure 5. For the no template setup we used the FLAN instructions during zero shot inference because if we used no template the model would not know what task to perform . For models Ô¨Ånetuned on dataset name only we report zero shot performance for FLAN instructions as well as using the dataset name.']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9609509521143909,1.0,0.9264920906708872,0.0588235294117647
318,How are images used as input for conditioned language models?,"Images are used as input for conditioned language models by embedding them into a common vector space along with sentences. In this process, a GRU is used to encode sentences, and the outputs of a pre-trained VGG ConvNet are used to encode images. The order embedding model represents images and sentences as a 2-level partial ordering and replaces the cosine similarity scoring function with an asymmetric one. This approach allows for joint embedding of images and language in a conditioned language model.

Source: Layer_Normalization.txt","['With fewer layers its larger receptive Ô¨Åelds allow the Image Transformer to signiÔ¨Åcantly improve over the state of the art in unconditional probabilis tic image modeling of comparatively complex images from ImageNet as well as super resolution. We further hope to have provided additional evidence that even in the light of generative adversarial networks likelihood based models of images is very much a promising area for further research as is using network architectures such as the Image Transformer in GANs. In future work we would like to explore a broader variety of conditioning information including free form text as previously proposed Mansimov et al. 2015 and tasks combining modalities such as language driven editing of images. Fundamentally we aim to move beyond still images to video Kalchbrenner et al. 2016 and towards applications in model based reinforcement learning. References Ba Jimmy Lei Kiros Jamie Ryan and Hinton Geoffrey E. Layer normalization. arXiv preprint arXiv 1607.06450 2016. Bellemare Marc G. Srinivasan Sriram Ostrovski Georg Schaul Tom Saxton David and Munos R emi. Unifying count based exploration and intrinsic motivation. CoRR abs 1606.01868 2016. URL http arxiv.org abs 1606.01868. Bengio Yoshua and Bengio Samy. Modeling high dimensional discrete data with multi layer neural net works. In Neural Information Processing Systems pp. 400 406. MIT Press 2000. Berthelot David Schumm Tom and Metz Luke. BEGAN boundary equilibrium generative adversarial networks.Image Transformer CoRR abs 1703.10717 2017. URL http arxiv. org abs 1703.10717. Chen Xi Mishra Nikhil Rohaninejad Mostafa and Abbeel Pieter. Pixelsnail An improved autoregres sive generative model. arXiv preprint arXiv 1712.09763 2017. Cheng Jianpeng Dong Li and Lapata Mirella. Long short term memory networks for machine reading. arXiv preprint arXiv 1601.06733 2016.', 'Unless otherwise noted the default initialization of layer normalization is to set the adaptive gains to 1 and the biases to 0 in the experiments. 6.1 Order embeddings of images and language In this experiment we apply layer normalization to the recently proposed order embeddings model of Vendrov et al. 2016 for learning a joint embedding space of images and sentences. We follow the same experimental protocol as Vendrov et al. 2016 and modify their publicly available code to incorporate layer normalization 1 which utilizes Theano Team et al. 2016 . Images and sen tences from the Microsoft COCO dataset Lin et al. 2014 are embedded into a common vector space where a GRU Cho et al. 2014 is used to encode sentences and the outputs of a pre trained VGG ConvNet Simonyan and Zisserman 2015 10 crop are used to encode images. The order embedding model represents images and sentences as a 2 level partial ordering and replaces the cosine similarity scoring function used in Kiros et al. 2014 with an asymmetric one. 1https github.com ivendrov order embedding 60 100 200 300 400 500 600 700 800 training steps thousands 0.4 0.5 0.6 0.7 0.8 0.9 1.0 validation error rate Attentive reader LSTM BN LSTM BN everywhere LN LSTM Figure 2 Validation curves for the attentive reader model. BN results are taken from Cooijmans et al. 2016 . We trained two models the baseline order embedding model as well as the same model with layer normalization applied to the GRU. After every 300 iterations we compute Recall@K R@K values on a held out validation set and save the model whenever R@K improves. The best performing models are then evaluated on 5 separate test sets each containing 1000 images and 5000 captions for which the mean results are reported. Both models use Adam Kingma and Ba 2014 with the same initial hyperparameters and both models are trained using the same architectural choices as used in Vendrov et al. 2016 . We refer the reader to the appendix for a description of how layer normalization is applied to GRU.']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.000000000000002,1.0,0.8638797479551088,0.0
319,What challenges exist in evaluating conditioned language models?,"One challenge in evaluating conditioned language models is that larger models may not necessarily generalize better out of distribution, as they can be overly specific to the training distribution and struggle to generalize well outside of it. Additionally, fine-tuned models may perform well on specific benchmarks but may exaggerate their actual performance on underlying tasks. Another challenge is that humans do not require large supervised datasets to learn most language tasks, unlike models that may need extensive training data. This discrepancy in learning requirements poses a challenge in evaluating the true capabilities of conditioned language models. 

Source: Language_Models_are_Few_Shot_Learners.txt","['. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 6.2 Fairness Bias and Representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36 6.3 Energy Usage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 7 Related Work 39 8 Conclusion 40 A Details of Common Crawl Filtering 43 B Details of Model Training 43 C Details of Test Set Contamination Studies 43 D Total Compute Used to Train Language Models 46 E Human Quality Assessment of Synthetic News Articles 46 F Additional Samples from GPT 3 48 G Details of Task Phrasing and SpeciÔ¨Åcations 50 H Results on All Tasks for All Model Sizes 63 21 Introduction Recent years have featured a trend towards pre trained language representations in NLP systems applied in increasingly Ô¨Çexible and task agnostic ways for downstream transfer.', 'For instance HLW 20 observe that larger models do not necessarily generalize better out of distribution. There is evidence that suggests that the generalization achieved under this paradigm can be poor because the model is overly speciÔ¨Åc to the training distribution and does not generalize well outside it YdC 19 MPL19 . Thus the performance of Ô¨Åne tuned models on speciÔ¨Åc benchmarks even when it is nominally at human level may exaggerate actual performance on the underlying task GSL 18 NK19 . Third humans do not require large supervised datasets to learn most language tasks a brief directive in natural language e.g. please tell me if this sentence describes something happy or something sad or at most a tiny number of demonstrations e.g. here are two examples of people acting brave please give a third example of bravery is often Figure 1.1 Language model meta learning. During unsupervised pre training a language model develops a broad set of skills and pattern recognition abilities. It then uses these abilities at inference time to rapidly adapt to or recognize the desired task. We use the term in context learning to describe the inner loop of this process which occurs within the forward pass upon each sequence. The sequences in this diagram are not intended to be representative of the data a model would see during pre training but are intended to show that there are sometimes repeated sub tasks embedded within a single sequence. 3Figure 1.2 Larger models make increasingly efÔ¨Åcient use of in context information. We show in context learning performance on a simple task requiring the model to remove random symbols from a word both with and without a natural language task description see Sec. 3.9.2 . The steeper in context learning curves for large models demonstrate improved ability to learn a task from contextual information. We see qualitatively similar behavior across a wide range of tasks. sufÔ¨Åcient to enable a human to perform a new task to at least a reasonable degree of competence. Aside from pointing to a conceptual limitation in our current NLP techniques this adaptability has practical advantages it allows humans to seamlessly mix together or switch between many tasks and skills for example performing addition during a lengthy dialogue.']",BAAI/bge-small-en-v1.5,512,20,2,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9932392273138442,1.0,0.8466262148510943,0.0222222222222222
320,What is the instructor's name for this course?,"The instructor's name for the course ""Introduction to Computational Natural Language Processing"" is Hamidreza Mahyar. 
Source: NLP_Winter_2024_Course_Outline.txt","['COURSE SPECIFICS Course Description This course covers some fundamental concepts in natural language processing NLP and how they apply in real world use cases. Instruction Type Code Type Hours per term C Classroom instruction 39 L Laboratory workshop or fieldwork T Tutorial DE Distance education Total Hours 39 Resources ISBN Textbook Title Edition Author Publisher Natural Language Processing Jacob Eisenstein Speech and Language Processing Dan Jurafsky and James H. Martin Deep Learning Ian Goodfellow Yoshua Bengio and Aaron Courville Prerequisite s Proficiency in Python programming language Some Machine Learning and or Deep Learning knowledge is a plus Corequisite s Antirequisite s Course Specific Policies All work must be shown to get full credit. Specific policy of Projects Assignments submissionsProjects Assignments will be conducted online by using related software. Completed Projects Assignments should be uploaded to the drop box before midnight of the due date. Projects Assignments missed due to legitimate reasons must be completed later mutually agreed with the instructor.', 'file name: NLP Winter 2024 Course Outline. Course Outline 1. COURSE INFORMATION Session Offered Winter 2024 Course Name Introduction to Computational Natural Language Processing Course Code SEP 775 Date s and Time s of lectures Lectures In person Wednesday 3 30 pm 6 30 pm Program Name System and Technology Calendar Description This graduate course introduces fundamental concepts in computational natural language processing NLP and their applications to teaching students how to deal with textual data in Artificial Intelligence. This course demonstrates how machines can learn different tasks in natural language such as language modeling text generation machine translation and language understanding. In this regard we go over the most promising methods in this literature and the most recent state of the art techniques. Moreover this course explores different real world applications of NLP and helps students get hands on experience in this field. Instructor Hamidreza Mahyar E Mail mahyarh@mcmaster.ca Office Hours upon request 2.', 'file name: lecture01 wordvecs1. Hamidreza Mahyar mahyarh@mcmaster.ca Computational Natural Language Processing Overview of NLP and Word VectorsLecture Plan 2 Lecture 1 Introduction and Word Vectors 1. The course 2. Human language and word meaning 3. Word2vec introduction 4. Word2vec objective function gradients 5. Optimization basics 6. Looking at word vectors Key learning today The astounding result that word meaning can be represented rather well by a high dimensional vector of real numbersCourse logistics in brief 3 Instructor Hamidreza Mahyar Head TA Reza Namazi Time Wednesday 3 30pm 6 30pm We put a lot of other important information on Avenue to Learn. Please read itWhat do we hope to teach A.k.a. learning goals 4 1.', 'Supervising strong learners by amplifying weak experts. arXiv preprint arXiv 1810.08575 2018. 10 P. F. Christiano J. Leike T. Brown M. Martic S. Legg and D. Amodei. Deep reinforcement learning from human preferences. In Advances in Neural Information Processing Systems pages 4299 4307 2017. 11 P. Covington J. Adams and E. Sargin. Deep neural networks for youtube recommendations. In Proceedings of the 10th ACM conference on recommender systems pages 191 198 2016. 12 A. M. Dai and Q. V. Le. Semi supervised sequence learning. In Advances in neural information processing systems pages 3079 3087 2015. 13 J. Dodge G. Ilharco R. Schwartz A. Farhadi H. Hajishirzi and N. Smith. Fine tuning pretrained language models Weight initializations data orders and early stopping.']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9156338799161428,1.0,0.9098366395340948,0.037037037037037
321,What is the email address of the instructor for this course?,"The email address of the instructor for the course ""Computational Natural Language Processing"" is mahyarh@mcmaster.ca.

Source: lecture07_Prompting_Instruction_Finetuning_and_RLHF","['COURSE SPECIFICS Course Description This course covers some fundamental concepts in natural language processing NLP and how they apply in real world use cases. Instruction Type Code Type Hours per term C Classroom instruction 39 L Laboratory workshop or fieldwork T Tutorial DE Distance education Total Hours 39 Resources ISBN Textbook Title Edition Author Publisher Natural Language Processing Jacob Eisenstein Speech and Language Processing Dan Jurafsky and James H. Martin Deep Learning Ian Goodfellow Yoshua Bengio and Aaron Courville Prerequisite s Proficiency in Python programming language Some Machine Learning and or Deep Learning knowledge is a plus Corequisite s Antirequisite s Course Specific Policies All work must be shown to get full credit. Specific policy of Projects Assignments submissionsProjects Assignments will be conducted online by using related software. Completed Projects Assignments should be uploaded to the drop box before midnight of the due date. Projects Assignments missed due to legitimate reasons must be completed later mutually agreed with the instructor.', 'file name: lecture07 Prompting Instruction Finetuning and RLHF . Hamidreza Mahyar mahyarh@mcmaster.ca Computational Natural Language Processing Prompting Instruction Finetuning and RLHFReminders Project proposals will be posted this week. A2 due Saturday 11 59PM We still recommend using Colab for the assignments in case you run into trouble e.g. you have exceeded Colab quota Please contact us 2Larger and larger models 3 https www.economist.com interactive briefing 2022 06 11 huge foundation models are turbo charging ai progress GPT 4 has 1.7 trillion parametersTrained on more and more data tokens seen during training https babylm.github.io 4 GPT 4 has seen 13 trillion tokensRecap What kinds of things does pretraining learn 5 Stanford University is located in California. Trivia I put fork down on the table. syntax The woman walked across the street checking for traffic over shoulder.', '. Students should be aware that when they access the electronic components of a course using these elements private information such as first and last names user names for the McMaster e mail accounts and program affiliation may become apparent to all other students in the same course. The available information is dependent on the technology used. Continuation in a course that uses on line elements will be deemed consent to this disclosure. If you have any questions or concerns about such disclosure please discuss this with the course instructor. ONLINE PROCTORING Some courses may use online proctoring software for tests and exams. This software may require students to turn on their video camera present identification monitor and record their computer activities and or lock restrict their browser or other applications software during tests or exams. This software may be required to be installed before the test exam begins. COMMUNICATIONS It is the student s responsibility to Maintain current contact information with the University including address phone numbers and emergency contact information. Use the University provided e mail address or maintain a valid forwarding e mail address.', 'In this paper we address the question can we train a better dense embedding model using only pairs of questions and passages or answers with out additional pretraining By leveraging the now standard BERT pretrained model Devlin et al. 2019 and a dual encoder architecture Bromley et al. 1994 we focus on developing the right training scheme using a relatively small number of question and passage pairs. Through a series of careful ablation studies our Ô¨Ånal solution is surprisingly simple the embedding is optimized for maximizing inner products of the question and relevant passage vectors with an objective compar ing all pairs of questions and passages in a batch. Our Dense Passage Retriever DPR is exception ally strong. It not only outperforms BM25 by a large margin 65.2 vs. 42.9 in Top 5 accuracy but also results in a substantial improvement on the end to end QA accuracy compared to ORQA 41.5 vs. 33.3 in the open Natural Questions setting Lee et al.']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.920081643441838,1.0,0.9008073890656204,0.0
322,When is this course held every week?,"The course is held every week on Week 1 to Week 13, with specific topics and activities outlined for each week. The classes end on April 10th.","['EXTREME CIRCUMSTANCES The University reserves the right to change the dates and deadlines for any or all courses in extreme circumstances e.g. severe weather labour disruptions etc. . Changes will be communicated through regular McMaster communication channels such as McMaster Daily News A2L and or McMaster email.', '3. SUB TOPIC S Week 1 Overview Human language Word meanings Word2Vec and Optimization basics Week 2 Intro to neural networks Gradients and backpropagation Week 3 Recurrent Neural Networks and Language Models Week 4 Conditioned Generation Week 5 Part 1 Self attention and transformers Part 2 Seq2Seq models Week 6 Midterm Test Week 7 Midterm Recess No class Week 8 Prompting Instruction Finetuning and RLHF Week 9 Question Answering systems Week 10 Multimodal models Week 11 Project presentation and Q A Week 12 Project presentation and Q A Week 13 Project presentation and Q A Classes end April 10th 4. ASSESSMENT OF LEARNING Weight 4 Assignments 20 4 Quizzes 10 Midterm Test 35 Group Project 35 TOTAL 100Note that the percentage distribution of each module may be adjusted. Percentage grades will be converted to letter grades and grade points per the University calendar. 5.', 'So the answer is 01 01 1900. QUESTION Jane scheduled 3 apointments with 5 poeple for tomorrow Tue 7 9 1972 . What is the date one week ago from today in MM DD YYYY MODEL ANSWER INCORRECT Tomorrow is 7 9 1972. One week ago from today is 7 2 1972. So the answer is 7 2 1972. Ground truth is 07 01 1972. QUESTION Today is the palindrome day of 2020 because the MMDDYYYY format of the date is the same backwards as forwards. What is the date tomorrow in MM DD YYYY MODEL ANSWER INCORRECT Today is 02 29 2020. Tomorrow will be 03 01 2020. So the answer is 03 01 2020. Ground truth is 02 03 2020 .', 'What is the date one week from today in MM DD YYYY A If 2015 is coming in 36 hours then it is coming in 2 days. 2 days before 01 01 2015 is 12 30 2014 so today is 12 30 2014. So one week from today will be 01 05 2015. So the answer is 01 05 2015. Q The Ô¨Årst day of 2019 is a Tuesday and today is the Ô¨Årst Monday of 2019. What is the date today in MM DD YYYY A If the Ô¨Årst day of 2019 was Tuesday then 01 01 2019 was a Tuesday. Today is the Ô¨Årst monday would be six days later. So today is 01 07 2019. So the answer is 01 07 2019.']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.8115038661564274,1.0,0.8735347256207815,0.037037037037037
323,What is the name of the TA for this course?,"The name of the TA for this course is AI Teaching Assistant (AI TA). It is designed to provide an interactive platform for students to ask questions and receive guidance on course materials. The AI TA aims to automate FAQ responses, handle content understanding and query handling using NLP strategies, and engage in dynamic interactions with students.","['file name: SEP775 FinalProjects 2. McMaster University SEP 775 Final Projects Project 1 AI Teaching Assistant Project Overview The goal of this project is to create an AI Teaching Assistant AI TA . The AI TA aims to provide an interactive platform for students to ask questions and receive guidance on course materials mirroring the support traditionally offered by human teaching assistants. The project emphasizes the application of NLP techniques to create an educational tool that enhances the learning experience through dynamic interaction and content understanding. Project Objectives Automate FAQ Responses Develop the AI TA to autonomously address frequently asked questions related to course logistics assignments and content. Content Understanding and Query Handling Implement NLP strategies enabling the AI TA to interpret and respond to inquiries about the uploaded course material accurately. Dynamic Interaction Design the AI TA to interact with students conversationally providing information and resources pertinent to their queries. Technical Approach NLP Engine Components Language Model Integration Leverage pre trained NLP models e.g. BERT Llama for robust text understanding and generation tapping into their vast linguistic knowledge.', 'Natural Language Understanding NLU Employ models like BERT for NLU tasks to accurately parse student inquiries determining their intent and extracting relevant details for precise responses. Retrieval Augmented Generation RAG Central to the NLP Engine RAG com bines retrieval methods to access information within a structured database of course materials with generative capabilities to formulate coherent and context aware respon ses . This approach ensures the AI TA delivers informative answers directly pulling from course content and synthesizing it into helpful responses. Response Generation Utilize the designed AI TA to generate responses that are not only accurate but also engaging and pedagogically effective. Interaction Interface Simplified Interface Development Construct an intuitive web or command line in terface that allows straightforward question submission and response retrieval ensuring ease of use for student interactions with the AI TA. McMaster University 1McMaster University SEP 775 Project 2 TV Show Script Generation Project Overview For this project you will harness the power of Natural Language Processing NLP and Machine Learning to create a script for a new episode of a TV show you enjoy.', 'Transactions of the Association of Compu tational Linguistics TACL . Kenton Lee Ming Wei Chang and Kristina Toutanova. 2019. Latent retrieval for weakly supervised open domain question answering. In Association for Com putational Linguistics ACL pages 6086 6096. Mike Lewis Yinhan Liu Naman Goyal Mar jan Ghazvininejad Abdelrahman Mohamed Omer Levy Veselin Stoyanov and Luke Zettlemoyer. 2020a. BART Denoising sequence to sequence pre training for natural language generation translation and comprehension. In Association for Computa tional Linguistics ACL pages 7871 7880. Patrick Lewis Ethan Perez Aleksandara Piktus Fabio Petroni Vladimir Karpukhin Naman Goyal Heinrich K uttler Mike Lewis Wen tau Yih Tim Rockt aschel Sebastian Riedel and Douwe Kiela. 2020b.', 'URL https arxiv.org abs 1806.08730. John McCarthy. Programs with common sense. RLE and MIT computation center 1960. URL http jmc.stanford.edu articles mcc59 mcc59.pdf. Todor Mihaylov Peter Clark Tushar Khot and Ashish Sabharwal. Can a suit of armor conduct electricity A new dataset for open book question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing pp. 2381 2391 2018. URL https aclanthology.org D18 1260. Sewon Min Mike Lewis Luke Zettlemoyer and Hannaneh Hajishirzi. Metaicl Learning to learn in context. arXiv preprint arXiv 2110.15943 2021. URL https arxiv.org abs 2110. 15943.']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.0000000000000002,,0.8286014815979713,0.0256410256410256
324,What is Word2vec?,"Word2vec is a framework for learning word vectors where each word in a fixed vocabulary is represented by a vector. It involves calculating the probability of outside words given a center word or vice versa based on the similarity of their word vectors, and adjusting the word vectors to maximize this probability. The model represents each word in a fixed vocabulary as a low-dimensional vector and learns the value of each word's vector to be predictive based on the distribution of words in a context. The skipgram word2vec algorithm is commonly used, and the model is specified as a probabilistic model with parameters that are estimated by minimizing the cross-entropy loss objective with the true distribution. Word2vec embeddings can be pre-trained and used for various NLP tasks.

Source: Lecture01_wordvecs1.txt, lecture01_notes_Introduction.txt, SEP775_Assignment1.txt","['107 0.109 0.542 0.349 0.271 monetary 0.413 0.582 0.007 0.247 0.216 0.718 0.147 0.051 19Word meaning as a neural word vector visualization 0.286 0.792 0.177 0.107 0.109 0.542 0.349 0.271 0.487 expect 203. Word2vec Overview 21 Word2vec Mikolov et al. 2013 is a framework for learning word vectors Idea We have a large corpus body of text a long list of words Every word in a fixed vocabulary is represented by a vector Go through each position t in the text which has a center word c and context outside words o Use the similarity of the word vectors for c and o to calculate the probability of o given c or vice versa Keep adjusting the word vectors to maximize this probabilityWord2Vec Overview Example windows and process for computing ùëÉùë§ùë°', 'However many of the details of word2vec will hold true in methods that we ll proceed to further in the course so we ll focus our time on that. 3.2 Word2vec model and objective The word2vec model represents each word in a fixed vocabulary as a low dimensional much smaller than vocabulary size vector. It learns the value of each word s vector to be predictive via a simple function of the distribution of words in a usually short 2 4 words context. The model we ll describe here is called the skipgram word2vec algo rithm. Skipgram word2vec. As usual we have a finite vocabulary V. Let C O be random variables representing an unknown pair of C V a center word and O V an outside word appearing in the context of the center word . We ll use c o to refer to specific values of the random variables.', 'Let U R V d and V R V d. Note that each word in V is associated with a single row of U and one of V we think of this as resulting from an arbitrary ordering of V. The word2vec model is a probabilistic model specified as follows where uw refers to the row of U corresponding to word w V and likewise for V pU V o c exp u o vc w V exp u wvc 4 This may be familiar to you as the softmax function which takes arbitrary scores here one for each word in the vocabulary resulting from dot products and produces a probability distribution where larger scored things get higher probability. Note that the vector of probabilities over all words given a center word pU V c R V is a lot like a row of our old co occurrence matrix Xc. The story isn t over yet this is just a model. How do we estimate the values of the parameters U V We learn to minimize the cross entropy loss objective with the true distribution P O C min U V Eo c log pU V o c .', 'Illustration Use specific word pairs or groups to demonstrate differences in repre sentation by each model. McMaster University 1McMaster University SEP 775 Resources for Word2Vec and GloVe Word2Vec Pre trained Word2Vec embeddings can be found on repositories such as HuggingFace or directly using Gensim. GloVe Pre trained GloVe embeddings are available at the GloVe website or through HuggingFace. Submission Guidelines Submit all code in a ZIP file including Jupyter Notebooks and a detailed PDF report with written explanations and visualizations. Clearly label each part and question in your submissions. Deadline Feb. 7 2024 Rubric and Expectations Code Quality and Functionality 40 Code should be well organized commented and functioning as intended. The use of Python and relevant libraries e.g. Gensim for Word2Vec should demonstrate a good grasp of the tools. Analysis and Interpretation 30 Written explanations should be insightful de monstrating a deep understanding of the word vector models.']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999999999999994,1.0,0.9477257151387884,0.0416666666666666
325,What are the main learning goals of the course?,"The main learning goals of the course include:
1. Understanding the foundations of modern methods for deep learning applied to NLP, such as word vectors, feed forward networks, recurrent networks, attention, encoder-decoder models, transformers, and large pre-trained language models.
2. Developing a big picture understanding of human languages and the challenges in understanding and generating them using computers.
3. Exploring question answering techniques, including reading comprehension and open domain textual question answering.

Source: lecture01_wordvecs1.txt","['COURSE SPECIFICS Course Description This course covers some fundamental concepts in natural language processing NLP and how they apply in real world use cases. Instruction Type Code Type Hours per term C Classroom instruction 39 L Laboratory workshop or fieldwork T Tutorial DE Distance education Total Hours 39 Resources ISBN Textbook Title Edition Author Publisher Natural Language Processing Jacob Eisenstein Speech and Language Processing Dan Jurafsky and James H. Martin Deep Learning Ian Goodfellow Yoshua Bengio and Aaron Courville Prerequisite s Proficiency in Python programming language Some Machine Learning and or Deep Learning knowledge is a plus Corequisite s Antirequisite s Course Specific Policies All work must be shown to get full credit. Specific policy of Projects Assignments submissionsProjects Assignments will be conducted online by using related software. Completed Projects Assignments should be uploaded to the drop box before midnight of the due date. Projects Assignments missed due to legitimate reasons must be completed later mutually agreed with the instructor.', 'learning goals 4 1. The foundations of the effective modern methods for deep learning applied to NLP Basics first then key methods used in NLP in 2024 Word vectors feed forward networks recurrent networks attention encoder decoder models transformers large pre trained language models etc. 2. A big picture understanding of human languages and the difficulties in understanding and producing them via computers 3.', 'file name: lecture08 Question Answering. Hamidreza Mahyar mahyarh@mcmaster.ca Computational Natural Language Processing Question Answering2 Lecture plan 1. What is question answering 2. Reading comprehension How to answer questions over a single passage of text 3. Open domain textual question answering How to answer questions over a large collection of documents Assignment 3 is out Start working on your final project1. What is question answering Answer A Question Q The goal of question answering is to build systems that automatically answer questions posed by humans in a natural language The earliest QA systems dated back to 1960s Simmons et al.', '7. Critical Discussion Although we believe that this contribution represents a step towards the NLP from scratch objec tive we are keenly aware that both our goal and our means can be criticized. The main criticism of our goal can be summarized as follows. Over the years the NLP com munity has developed a considerable expertise in engineering effective NLP features. Why should they forget this painfully acquired expertise and instead painfully acquire the skills required to train large neural networks As mentioned in our introduction we observe that no single NLP task really covers the goals of NLP. Therefore we believe that task speciÔ¨Åc engineering i.e. that does not gen eralize to other tasks is not desirable. But we also recognize how much our neural networks owe to previous NLP task speciÔ¨Åc research. The main criticism of our means is easier to address.']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999828094652824,1.0,0.9278272685465844,0.0625
326,How is the course grade distributed?,"The course grade is distributed as follows:
- Assignments: 20%
- Quizzes: 10%
- Midterm Test: 35%
- Group Project: 35%
- Total: 100%

Source: NLP_Winter_2024_Course_Outline","['He Ô¨Årst deÔ¨Ånes a ranking criterion Harris 1968 Section 4.1 Starting for convenience with very short sentence forms say ABC we choose a particular word choice for all the classes say BqCq except one in this case A for every pair of members Ai A j of that word class we ask how the sentence formed with one of the members that is AiBqCq compares as to acceptability with the sentence formed with the other member that is A jBqCq. These gradings are then used to compare sentence forms It now turns out that given the graded n tuples of words for a particular sentence form we can Ô¨Ånd other sentences forms of the same word classes in which the same n tuples of words produce the same grading of sentences. This is an indication that these two sentence forms exploit common words with the same syntac tic function and possibly the same meaning. This observation forms the empirical basis for the construction of operator grammars that describe real world natural languages such as English.', '3. SUB TOPIC S Week 1 Overview Human language Word meanings Word2Vec and Optimization basics Week 2 Intro to neural networks Gradients and backpropagation Week 3 Recurrent Neural Networks and Language Models Week 4 Conditioned Generation Week 5 Part 1 Self attention and transformers Part 2 Seq2Seq models Week 6 Midterm Test Week 7 Midterm Recess No class Week 8 Prompting Instruction Finetuning and RLHF Week 9 Question Answering systems Week 10 Multimodal models Week 11 Project presentation and Q A Week 12 Project presentation and Q A Week 13 Project presentation and Q A Classes end April 10th 4. ASSESSMENT OF LEARNING Weight 4 Assignments 20 4 Quizzes 10 Midterm Test 35 Group Project 35 TOTAL 100Note that the percentage distribution of each module may be adjusted. Percentage grades will be converted to letter grades and grade points per the University calendar. 5.', '4 73.2 79.8 63.2 85.4 85.0 81.4 89.1 77.2 BM25 DPR 76.6 79.8 71.0 85.2 71.5 83.8 84.5 80.5 92.7 81.3 Multi DPR 79.4 78.8 75.0 89.1 51.6 86.0 84.7 82.9 93.9 67.6 BM25 DPR 78.0 79.9 74.7 88.5 66.2 83.9 84.4 82.3 94.1 78.6 Table 2 Top 20 Top 100 retrieval accuracy on test sets measured as the percentage of top 20 100 retrieved passages that contain the answer.', 'We used this classiÔ¨Åer to score Common Crawl documents. We kept each document in our dataset iff np.random.pareto Œ± 1 document_score We chose Œ± 9 in order to take mostly documents the classiÔ¨Åer scored highly but still include some documents that were out of distribution. Œ± was chosen to match the distribution of scores from our classiÔ¨Åer on WebText. We found this re weighting increased quality as measured by loss on a range of out of distribution generative text samples. 2. To further improve model quality and prevent overÔ¨Åtting which becomes increasingly important as model capacity increases we fuzzily deduplicated documents i.e. removed documents with high overlap with other documents within each dataset using Spark s MinHashLSH implementation with 10 hashes using the same features as were used for classiÔ¨Åcation above. We also fuzzily removed WebText from Common Crawl. Overall this decreased dataset size by an average of 10 .']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9898862143010956,1.0,0.9407625749579722,0.0
327,What is the policy for late submissions in this course?,"The policy for late submissions in this course is that students should submit their request for accommodation to their Faculty Office within 10 working days of the beginning of the term or to the Registrar's Office prior to their examinations. They should also contact their instructors as soon as possible to make alternative arrangements for classes, assignments, and tests. Submissions failing to meet requirements may be penalized.","['Students should submit their request to their Faculty Office normally within 10 working days of the beginning of term in which they anticipate a need for accommodation or to the Registrar s Office prior to their examinations. Students should also contact their instructors as soon as possible to make alternative arrangements for classes assignments and tests. http www.mcmaster.ca policy Students AcademicStudies Studentcode.pdf COPYRIGHT AND RECORDING Students are advised that lectures demonstrations performances and any other course material provided by an instructor include copyright protected works. The Copyright Act and copyright law protect every original literary dramatic musical and artistic work including lectures by University instructors The recording of lectures tutorials or other methods of instruction may occur during a course. Recording may be done by either the instructor for the purpose of authorized distribution or by a student for the purpose of personal study. Students should be aware that their voice and or image may be recorded by others during the class. Please speak with the instructor if this is a concern for you.', 'Submissions failing to meet these requirements may be penalized. Please ensure that all submitted materials are well organized and clearly labeled for easy navigation. McMaster University 7', '. Students should be aware that when they access the electronic components of a course using these elements private information such as first and last names user names for the McMaster e mail accounts and program affiliation may become apparent to all other students in the same course. The available information is dependent on the technology used. Continuation in a course that uses on line elements will be deemed consent to this disclosure. If you have any questions or concerns about such disclosure please discuss this with the course instructor. ONLINE PROCTORING Some courses may use online proctoring software for tests and exams. This software may require students to turn on their video camera present identification monitor and record their computer activities and or lock restrict their browser or other applications software during tests or exams. This software may be required to be installed before the test exam begins. COMMUNICATIONS It is the student s responsibility to Maintain current contact information with the University including address phone numbers and emergency contact information. Use the University provided e mail address or maintain a valid forwarding e mail address.', 'EXTREME CIRCUMSTANCES The University reserves the right to change the dates and deadlines for any or all courses in extreme circumstances e.g. severe weather labour disruptions etc. . Changes will be communicated through regular McMaster communication channels such as McMaster Daily News A2L and or McMaster email.']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999999999999996,1.0,0.8715632330806287,0.217391304347826
328,What does the course hope to teach regarding word vectors?,The course hopes to teach that word meaning can be represented rather well by a high dimensional vector of real numbers.,"['file name: lecture01 wordvecs1. Hamidreza Mahyar mahyarh@mcmaster.ca Computational Natural Language Processing Overview of NLP and Word VectorsLecture Plan 2 Lecture 1 Introduction and Word Vectors 1. The course 2. Human language and word meaning 3. Word2vec introduction 4. Word2vec objective function gradients 5. Optimization basics 6. Looking at word vectors Key learning today The astounding result that word meaning can be represented rather well by a high dimensional vector of real numbersCourse logistics in brief 3 Instructor Hamidreza Mahyar Head TA Reza Namazi Time Wednesday 3 30pm 6 30pm We put a lot of other important information on Avenue to Learn. Please read itWhat do we hope to teach A.k.a. learning goals 4 1.', 'in 2006 we would like a classiÔ¨Åed output Jim Person bought 300 shares of Acme Corp. Organization in 2006 Time. Figure 5 We can classify word vectors using simple linear decision boundaries such as the one shown here 2 D word vectors using techniques such as logistic regression and SVMs For such problems we typically begin with a training set of the form x i y i N 1 where x i is a d dimensional word vector generated by some word embedding technique and y i is a C dimensional one hot vector which indicates the labels we wish to eventually predict sentiments other words named entities buy sell decisions etc. . In typical machine learning tasks we usually hold input data and target labels Ô¨Åxed and train weights using optimization techniques such as gradient descent L BFGS Newton s method etc. . In NLP applications however we introduce the idea of retraining the input word vectors when we train for extrinsic tasks. Let us discuss when and why we should consider doing this.', 'Then we search in the vector space for the word closest to X measured by cosine distance and use it as the answer to the question we discard the input question words during this search . When the word vectors are well trained it is possible to Ô¨Ånd the correct answer word smallest using this method. Finally we found that when we train high dimensional word vectors on a large amount of data the resulting vectors can be used to answer very subtle semantic relationships between words such as a city and the country it belongs to e.g. France is to Paris as Germany is to Berlin. Word vectors with such semantic relationships could be used to improve many existing NLP applications such as machine translation information retrieval and question answering systems and may enable other future applications yet to be invented. 5Table 1 Examples of Ô¨Åve types of semantic and nine types of syntactic questions in the Semantic Syntactic Word Relationship test set.', 'There followed a great deal of exploration of methods for obtaining distributional word vectors. Some interesting ideas worth noting include When we wish to apply neural networks to problems in NLP see Figure 4 it s useful to Ô¨Årst map each input word token to its vector and then feed the word vectors into the neural network model which performs a task like translation. The vectors can be Ô¨Åxed in advance or pretrained from a corpus using methods like those above often executed by someone else or they can be treated as parameters of the neural network model and adapted to the task speciÔ¨Åcally e.g. Collobert et al. 2011 . Finetuning refers to initializing the word vectors by pretraining then adapting them through task speciÔ¨Åc learning algorithms. The word vectors can also be initialized to random values then estimated solely through task learning which we might call learning from scratch. 6 We can use expert built data structures like WordNet as additional input to creating word vectors.']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9032612605709416,1.0,0.964419553866905,0.0344827586206896
329,How does Word2vec represent word meaning?,"Word2vec represents word meaning by assigning each word in a fixed vocabulary a vector. These vectors are learned based on the similarity of word vectors for a center word and its context words. The model adjusts the word vectors to maximize the probability of predicting context words given the center word or vice versa. This allows Word2vec to capture semantic relationships between words in a high-dimensional space.

Source: Lecture01_wordvecs1.txt","['107 0.109 0.542 0.349 0.271 monetary 0.413 0.582 0.007 0.247 0.216 0.718 0.147 0.051 19Word meaning as a neural word vector visualization 0.286 0.792 0.177 0.107 0.109 0.542 0.349 0.271 0.487 expect 203. Word2vec Overview 21 Word2vec Mikolov et al. 2013 is a framework for learning word vectors Idea We have a large corpus body of text a long list of words Every word in a fixed vocabulary is represented by a vector Go through each position t in the text which has a center word c and context outside words o Use the similarity of the word vectors for c and o to calculate the probability of o given c or vice versa Keep adjusting the word vectors to maximize this probabilityWord2Vec Overview Example windows and process for computing ùëÉùë§ùë°', 'However many of the details of word2vec will hold true in methods that we ll proceed to further in the course so we ll focus our time on that. 3.2 Word2vec model and objective The word2vec model represents each word in a fixed vocabulary as a low dimensional much smaller than vocabulary size vector. It learns the value of each word s vector to be predictive via a simple function of the distribution of words in a usually short 2 4 words context. The model we ll describe here is called the skipgram word2vec algo rithm. Skipgram word2vec. As usual we have a finite vocabulary V. Let C O be random variables representing an unknown pair of C V a center word and O V an outside word appearing in the context of the center word . We ll use c o to refer to specific values of the random variables.', 'Where static embeddings represent the meaning of word types vocabulary entries contex tual embeddings represent the meaning of word instances instances of a particular10 CHAPTER 11 FINE TUNING AND MASKED LANGUAGE MODELS Input Embeddings Transformer Blocks CLS So long and thanks for all z1 zCLS z2 z3 z4 z5 z6 Figure 11.6 The output of a BERT style model is a contextual embedding vector zi for each input token xi. word type in a particular context. Thus where word2vec had a single vector for each word type contextual embeddings provide a single vector for each instance of that word type in its sentential context. Contextual embeddings can thus be used for tasks like measuring the semantic similarity of two words in context and are useful in linguistic tasks that require models of word meaning. 11.3.1 Contextual Embeddings and Word Sense Words are ambiguous the same word can be used to mean different things.', 'The distributional hypothsis the meaning of a word can be derived from the distribution of contexts in which it appears. That s the high level. But as always the details matter. What does it mean for a word to be near another word Right next to it Two away In the same document How does one represent this encoding and learn it Let s go through some options. 3.1 Co occurrence matrices and document contexts If you were asked to code up the idea represent a word by the distribution of words it appears near you might immediately have the following idea 1. Determine a vocabulary V. 2. Make a matrix of size V V of zeros. 3. Walk through a sequence of documents. For each document for each word w in the document add all the counts of the other words w in the document to the row corresponding to w at the column corresponding to w . 4. Normalize the rows by the sum.']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999999999999986,1.0,0.9241951891417352,0.0416666666666666
330,What is the main focus of this lecture on neural networks?,"The main focus of this lecture on neural networks is on introducing single and multilayer neural networks for classification purposes, discussing how they can be trained using backpropagation, explaining the use of the chain rule for parameter updates, and providing practical tips and tricks for training neural networks involving various aspects such as neuron units, non-linearities, gradient checks, Xavier parameter initialization, learning rates, and Adagrad. Additionally, the lecture motivates the use of recurrent neural networks as a language model.","['file name: lecture02 notes Backpropagation.  Neural networks. Forward computation. Backward propagation. Neuron Units. Max margin Loss. Gradient checks. Xavier parameter initialization. Learning rates. Adagrad. This set of notes introduces single and multilayer neural networks and how they can be used for classiÔ¨Åcation purposes. We then dis cuss how they can be trained using a distributed gradient descent technique known as backpropagation. We will see how the chain rule can be used to make parameter updates sequentially. After a rigorous mathematical discussion of neural networks we will discuss some practical tips and tricks in training neural networks involving neuron units non linearities gradient checks Xavier parameter ini tialization learning rates Adagrad etc. Lastly we will motivate the use of recurrent neural networks as a language model. 1 Neural Networks Foundations Figure 1 We see here how a non linear decision boundary separates the data very well. This is the prowess of neural networks.', 'A few examples of these include hidden Markov models Baum and Petrie 1966 conditional random fields Lafferty et al. 2001 recurrent neural networks Rumelhart et al. 1985 convolutional neural networks LeCun et al. 1989 and support vector machines Cortes and Vapnik 1995 . In this section we ll discuss a bit about the neural modeling ap proaches we ve discussed in Cs 224n so far and how their limitations and changes in the world inspired the modern as of 2023 zeitgeist of self attention and Transformer based architectures. 1.1 Notation and basics Let w1 n be a sequence where each wi V a finite vocabulary. We ll also overload w1 n to be a matrix of one hot vectors w1 n Rn V .', 'file name: lecture02 Neural Nets. Hamidreza Mahyar mahyarh@mcmaster.ca Computational Natural Language Processing Neural Networks Gradients and BackpropagationBinary classification for center word being location We do supervised training and want high score if it s a location ùêΩùë°ùúÉ ùúéùë† 1 1 ùëí ùë† x xmuseums xin xParis xare xamazing predicted model probability of class f Some element wise non linear function e.g.', 'Neural GPUs learn algorithms. In International Conference on Learning Representations ICLR 2016. 18 Nal Kalchbrenner Lasse Espeholt Karen Simonyan Aaron van den Oord Alex Graves and Ko ray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv 1610.10099v2 2017. 19 Yoon Kim Carl Denton Luong Hoang and Alexander M. Rush. Structured attention networks. In International Conference on Learning Representations 2017. 20 Diederik Kingma and Jimmy Ba. Adam A method for stochastic optimization. In ICLR 2015. 21 Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint arXiv 1703.10722 2017. 22 Zhouhan Lin Minwei Feng Cicero Nogueira dos Santos Mo Yu Bing Xiang Bowen Zhou and Yoshua Bengio.']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9995545648584424,1.0,0.922786756324076,0.0263157894736842
331,What are the non-linear functions mentioned for neural networks?,"The non-linear functions mentioned for neural networks are logistic sigmoid, tanh, hard tanh, Swish, Rectified Linear Unit (ReLU), Leaky ReLU, Parametric ReLU, GELU. 

Source: lecture02_Neural_Nets.txt","['Non linearities old and new logistic sigmoid tanh hard tanh 1 0 1 1 Swish arXiv 1710.05941 swish ùë• ùë•. logistic ùë• Rectified Linear Unit ReLU ReLU ùëß max ùëß 0 Leaky ReLU Parametric ReLU 0 0 0 GELU arXiv 1606.08415 GELU ùë• ùë•. ùëÉùëã ùë• ùëã ùëÅ 0 1 ùë•. logistic 1.702ùë•Non linearities i.e. f on previous slide Why they re needed Neural networks do function approximation e.g. regression or classification Without non linearities deep neural networks can t do anything more than a linear transform Extra layers could just be compiled down into a single linear transform W1 W2 x Wx But with more layers that include non linearities they can approximate any complex function 5Training with cross entropy loss you use this in', '3.5 Non linear ClassiÔ¨Åers Figure 9 Here we see that many exam ples are wrongly classiÔ¨Åed even though the best linear decision boundary is chosen. This is due linear decision boundaries have limited model capacity for this dataset. We now introduce the need for non linear classiÔ¨Åcation models such as neural networks. We see in Figure 9 that a linear classiÔ¨Åer mis classiÔ¨Åes many datapoints. Using a non linear decision boundary as shown in Figure 10 we manage to classify all training points accu rately. Although oversimpliÔ¨Åed this is a classic case demonstrating the need for non linear decision boundaries. In the next set of notes we study neural networks as a class of non linear models that have performed particularly well in deep learning applications. Figure 10 Here we see that the non linear decision boundary allows for much better classiÔ¨Åcation of datapoints.', 'file name: lecture02 notes Backpropagation.  Neural networks. Forward computation. Backward propagation. Neuron Units. Max margin Loss. Gradient checks. Xavier parameter initialization. Learning rates. Adagrad. This set of notes introduces single and multilayer neural networks and how they can be used for classiÔ¨Åcation purposes. We then dis cuss how they can be trained using a distributed gradient descent technique known as backpropagation. We will see how the chain rule can be used to make parameter updates sequentially. After a rigorous mathematical discussion of neural networks we will discuss some practical tips and tricks in training neural networks involving neuron units non linearities gradient checks Xavier parameter ini tialization learning rates Adagrad etc. Lastly we will motivate the use of recurrent neural networks as a language model. 1 Neural Networks Foundations Figure 1 We see here how a non linear decision boundary separates the data very well. This is the prowess of neural networks.', '2 Background A feed forward neural network is a non linear mapping from a input pattern x to an output vector y. Consider the lth hidden layer in a deep feed forward neural network and let al be the vector representation of the summed inputs to the neurons in that layer. The summed inputs are computed through a linear projection with the weight matrix W l and the bottom up inputs hl given as follows al i wl i hl hl 1 i f al i bl i 1 where f is an element wise non linear function and wl i is the incoming weights to the ith hidden units and bl i is the scalar bias parameter. The parameters in the neural network are learnt using gradient based optimization algorithms with the gradients being computed by back propagation. One of the challenges of deep learning is that the gradients with respect to the weights in one layer are highly dependent on the outputs of the neurons in the previous layer especially if these outputs change in a highly correlated way. Batch normalization Ioffe and Szegedy 2015 was proposed to reduce such undesirable covariate shift .']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.990775318267588,0.875,0.9625527494985338,0.0285714285714285
332,What is the purpose of non-linearities in neural networks?,"The purpose of non-linearities in neural networks is to enable the network to approximate any complex function by introducing non-linear decision boundaries, allowing for better classification of data points and preventing the network from being limited to just linear transformations. Non-linearities are essential for deep neural networks to perform functions like regression or classification effectively. Without non-linearities, deep neural networks would essentially reduce to a single linear transform, limiting their capacity to learn complex patterns and relationships in the data.","['3.5 Non linear ClassiÔ¨Åers Figure 9 Here we see that many exam ples are wrongly classiÔ¨Åed even though the best linear decision boundary is chosen. This is due linear decision boundaries have limited model capacity for this dataset. We now introduce the need for non linear classiÔ¨Åcation models such as neural networks. We see in Figure 9 that a linear classiÔ¨Åer mis classiÔ¨Åes many datapoints. Using a non linear decision boundary as shown in Figure 10 we manage to classify all training points accu rately. Although oversimpliÔ¨Åed this is a classic case demonstrating the need for non linear decision boundaries. In the next set of notes we study neural networks as a class of non linear models that have performed particularly well in deep learning applications. Figure 10 Here we see that the non linear decision boundary allows for much better classiÔ¨Åcation of datapoints.', 'Non linearities old and new logistic sigmoid tanh hard tanh 1 0 1 1 Swish arXiv 1710.05941 swish ùë• ùë•. logistic ùë• Rectified Linear Unit ReLU ReLU ùëß max ùëß 0 Leaky ReLU Parametric ReLU 0 0 0 GELU arXiv 1606.08415 GELU ùë• ùë•. ùëÉùëã ùë• ùëã ùëÅ 0 1 ùë•. logistic 1.702ùë•Non linearities i.e. f on previous slide Why they re needed Neural networks do function approximation e.g. regression or classification Without non linearities deep neural networks can t do anything more than a linear transform Extra layers could just be compiled down into a single linear transform W1 W2 x Wx But with more layers that include non linearities they can approximate any complex function 5Training with cross entropy loss you use this in', '2 Background A feed forward neural network is a non linear mapping from a input pattern x to an output vector y. Consider the lth hidden layer in a deep feed forward neural network and let al be the vector representation of the summed inputs to the neurons in that layer. The summed inputs are computed through a linear projection with the weight matrix W l and the bottom up inputs hl given as follows al i wl i hl hl 1 i f al i bl i 1 where f is an element wise non linear function and wl i is the incoming weights to the ith hidden units and bl i is the scalar bias parameter. The parameters in the neural network are learnt using gradient based optimization algorithms with the gradients being computed by back propagation. One of the challenges of deep learning is that the gradients with respect to the weights in one layer are highly dependent on the outputs of the neurons in the previous layer especially if these outputs change in a highly correlated way. Batch normalization Ioffe and Szegedy 2015 was proposed to reduce such undesirable covariate shift .', '3 Linear Layer. The Ô¨Åxed size vector f 1 Œ∏ can be fed to one or several standard neural network layers which perform afÔ¨Åne transformations over their inputs f l Œ∏ W l f l 1 Œ∏ bl 4 where W l Rnl hu nl 1 hu and bl Rnl hu are the parameters to be trained. The hyper parameter nl hu is usually called the number of hidden units of the lth layer. 2502NATURAL LANGUAGE PROCESSING ALMOST FROM SCRATCH HardTanh Layer. Several linear layers are often stacked interleaved with a non linearity func tion to extract highly non linear features. If no non linearity is introduced our network would be a simple linear model. We chose a hard version of the hyperbolic tangent as non linearity. It has the advantage of being slightly cheaper to compute compared to the exact hyperbolic tangent while leaving the generalization performance unchanged Collobert 2004 .']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999984722871884,1.0,0.9206451691960896,0.0384615384615384
333,What is cross entropy loss used for in PyTorch?,"Cross entropy loss is used in PyTorch to minimize the negative log probability of the correct class, or in other words, to maximize the probability of the correct class. It involves comparing the true probability distribution (p) with the computed model probability distribution (q) to calculate the cross entropy loss. In PyTorch, cross entropy loss is commonly used as a loss function.

Source: lecture02_Neural_Nets","['with cross entropy loss you use this in PyTorch Until now our objective was stated as to maximize the probability of the correct class y or equivalently we can minimize the negative log probability of that class Now restated in terms of cross entropy a concept from information theory Let the true probability distribution be p let our computed model probability be q The cross entropy is Assuming a ground truth or true or gold or target probability distribution that is 1 at the right class and 0 everywhere else p 0 0 1 0 0 then Because of one hot p the only term left is the negative log probability of the true class yi log ùëù ùë¶i ùë•i Cross entropy can be used in other ways with a more interesting p but for now just know that you ll want to use it as the loss in PyTorch 6Remember Stochastic Gradient Descent Update equation i.e. for each parameter In deep learning ùúÉincludes the data representation e.g.', 'Now we are ready to introduce cross entropy. The cross entropy is useful when cross entropy we don t know the actual probability distribution p that generated some data. It allows us to use some m which is a model of p i.e. an approximation to p . The cross entropy of m on p is deÔ¨Åned by H p m lim n 1 n X W L p w1 ... wn logm w1 ... wn 3.50 That is we draw sequences according to the probability distribution p but sum the log of their probabilities according to m. Again following the Shannon McMillan Breiman theorem for a stationary er godic process H p m lim n 1 n logm w1w2 ...wn 3.51 This means that as for entropy we can estimate the cross entropy of a model m on some distribution p by taking a single sequence that is long enough instead of summing over all possible sequences. What makes the cross entropy useful is that the cross entropy H p m is an up per bound on the entropy H p .', 'This is equal to the exponential of the cross entropy loss Normalized by number of words Inverse probability of corpus according to Language Model Lower perplexity is better 46RNNs greatly improved perplexity over what came before n gram model Increasingly complex RNNs Perplexity improves lower is better Source https research.fb.com building an efficient neural language model over a billion words 474.', '1.3 Least Squares Objective Recall that for the skip gram model we use softmax to compute the probability of word j appears in the context of word i Qij exp uT j vi W w 1 exp uT w vi Training proceeds in an on line stochastic fashion but the implied global cross entropy loss can be calculated as J i corpus j context i log Qij As the same words i and j can appear multiple times in the corpus it is more efÔ¨Åcient to Ô¨Årst group together the same values for i and j J W i 1 W j 1 Xij log Qij where the value of co occurring frequency is given by the co occurrence matrix X. One signiÔ¨Åcant drawback of the cross entropy loss is that it requires the distribution Q to be properly normalized which involves the expensive summation over the entire vocabulary.']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999999999999988,0.75,0.9565302679619626,0.1111111111111111
334,What is the significance of the backpropagation algorithm?,"The significance of the backpropagation algorithm lies in its ability to efficiently update the parameters of a neural network by propagating gradients backward through the network. This allows for the calculation of how each parameter contributes to the overall error, enabling the network to learn and improve its performance over time. Backpropagation is crucial for training neural networks as it helps in adjusting the weights and biases to minimize the error between the predicted output and the actual output, ultimately leading to better accuracy in tasks such as classification and regression.

Source: Lecture02 notes Backpropagation","['W k is the transfer matrix that maps the output from the k th layer to the input to the k 1 th. Thus W 1 W and W 2 UT using notation from Section 1.3. Let us begin Suppose the cost J 1 sc s is positive and we want to perform the update of parameter W 1 14 in Figure 5 and Figure 6 we must realize that W 1 14 only contributes to z 2 1 and thus a 2 1 . This fact is crucial to understanding backpropagation backpropagated gradients are only affected by values they contribute to. a 2 1 is consequently used in the forward computation of score by multiplication with W 2 1 . We can see from the max margin loss that J s J sc 1 Therefore we will work with s W 1 ij here for simplicity. Thus s W 1 ij W 2 a 2 W 1 ij W 2 i a 2 i W 1 ij W 2 i a 2 i', 'file name: lecture02 notes Backpropagation.  Neural networks. Forward computation. Backward propagation. Neuron Units. Max margin Loss. Gradient checks. Xavier parameter initialization. Learning rates. Adagrad. This set of notes introduces single and multilayer neural networks and how they can be used for classiÔ¨Åcation purposes. We then dis cuss how they can be trained using a distributed gradient descent technique known as backpropagation. We will see how the chain rule can be used to make parameter updates sequentially. After a rigorous mathematical discussion of neural networks we will discuss some practical tips and tricks in training neural networks involving neuron units non linearities gradient checks Xavier parameter ini tialization learning rates Adagrad etc. Lastly we will motivate the use of recurrent neural networks as a language model. 1 Neural Networks Foundations Figure 1 We see here how a non linear decision boundary separates the data very well. This is the prowess of neural networks.', 'Computational efÔ¨Åciency Having explored element wise updates as well as vector wise updates we must realize that the vectorized implementations run substantially faster in scientiÔ¨Åc computing environments such as MATLAB or Python using NumPy SciPy packages . Thus we should use vectorized implementation in prac tice. Furthermore we should also reduce redundant calculations in backpropagation for instance notice that Œ¥ k depends directly on Œ¥ k 1 . Thus we should ensure that when we update W k using Œ¥ k 1 we save Œ¥ k 1 to later derive Œ¥ k and we then repeat this for k 1 . . . 1 . Such a recursive procedure is what makes backpropa gation a computationally affordable procedure. 2 Neural Networks Tips and Tricks Having discussed the mathematical foundations of neural networks we will now dive into some tips and tricks commonly employed when using neural networks in practice. 2.1 Gradient Check In the last section we discussed in detail how to calculate error gradients updates for parameters in a neural network model via calculus based analytic methods.', 'The idea behind using this objective is to ensure that the score computed for true labeled data points is higher than the score computed for false labeled data points. Using the previous example if we call the score computed for the true labeled window Museums in Paris are amazing as s and the score computed for the false labeled window Not all museums in Paris as sc subscripted as c to signify that the window is corrupt . Then our objective function would be to maximize s sc or to minimize sc s . However we modify our objective to ensure that error is only computed if sc s sc s 0. The intuition behind doing this is that we only care the the true data point have a higher score than the false data point and that the rest does not matter. Thus we want our error to be sc s if sc s else 0. Thus our optimization objective is now minimize J max sc s 0 However the above optimization objective is risky in the sense that it does not attempt to create a margin of safety.']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.000000000000001,0.8,0.9451039448510355,0.0263157894736842
335,What does the Jacobian matrix represent in the context of neural networks?,"In the context of neural networks, the Jacobian matrix represents a matrix of partial derivatives. Specifically, it is used to generalize the gradient of a function with multiple outputs and inputs, where each entry in the Jacobian matrix corresponds to the partial derivative of an output with respect to an input. It helps in understanding how small changes in inputs affect the outputs of the function.

Source: lecture02_Neural_Nets.txt","['The Jacobian matrix xk 1 xk is given by WT recdiag œÉ xk . The 2 norm of this Jacobian is bounded by the product ofOn the diÔ¨Éculty of training Recurrent Neural Networks the norms of the two matrices see equation 6 . Due to our assumption this implies that it is smaller than 1. k xk 1 xk WT rec diag œÉ xk 1 Œ≥ Œ≥ 1 6 Let Œ∑ R be such that k xk 1 xk Œ∑ 1. The existence of Œ∑ is given by equation 6 .', '013 64.48Gradients Given a function with 1 output and n inputs Its gradient is a vector of partial derivatives with respect to each input 11Jacobian Matrix Generalization of the Gradient Given a function with m outputs and n inputs It s Jacobian is an m x n matrix of partial derivatives 12Chain Rule For composition of one variable functions multiply derivatives For multiple variables functions multiply Jacobians 13Example Jacobian Elementwise activation Function 14Example Jacobian Elementwise activation Function Function has n outputs and n inputs n by n Jacobian 15Example Jacobian Elementwise activation Function 16Example Jacobian Elementwise activation Function 17Example Jacobian Elementwise activation Function 18Other Jacobians Compute these at home for practice Check your answers with the lecture notes 19Other Jacobians Compute these at home for practice Check your answers with the lecture notes 20Other Jacobians Compute these at home for practice Check your answers with the lecture notes Fine print This is the correct Jacobian. Later we discuss the shape convention using it the answer would be h.', '. . 0 ith element Now let s compute J Wij J Wij J z z Wij Œ¥ z Wij m X k 1 Œ¥k zk Wij Œ¥ixj the only nonzero term in the sum is Œ¥i zi Wij . To get J W we want a ma trix where entry i j is Œ¥ixj. This matrix is equal to the outer product J W Œ¥T xT 6 Row vector time matrix with respect to the matrix z xW Œ¥ J z what is J W Œ¥ z W A similar computation to 5 shows that J W xT Œ¥ . 7 Cross entropy loss with respect to logits ÀÜ y softmax Œ∏ J CE y ÀÜ y what is J Œ∏ The gradient is J Œ∏ ÀÜ y y or ÀÜ y y T if y is a column vector . 4These identities will be enough to let you quickly compute the gradients for many neural networks. However it s important to know how to compute Jacobians for other functions as well in case they show up.', '21Other Jacobians Compute these at home for practice 22Back to our Neural Net x xmuseums xin xParis xare xamazing 23Back to our Neural Net Let s find Really we care about the gradient of the loss Jt but we will compute the gradient of the score for simplicity x xmuseums xin xParis xare xamazing 241. Break up equations into simple pieces Carefully define your variables and keep track of their dimensionality 252. Apply the chain rule 262. Apply the chain rule 272. Apply the chain rule 282. Apply the chain rule 293. Write out the Jacobians Useful Jacobians from previous slide 303. Write out the Jacobians ùíñùëá Useful Jacobians from previous slide 313. Write out the Jacobians ùíñùëá Useful Jacobians from previous slide 323. Write out the Jacobians ùíñùëá Useful Jacobians from previous slide 333.']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999999999999988,0.8,0.941650901154695,0.6666666666666666
336,How is the derivative with respect to a matrix described?,"The derivative with respect to a matrix is described as a matrix itself, where each entry in the matrix represents the partial derivative of the loss function with respect to the corresponding entry in the original matrix. This arrangement allows for easier computation and application of gradients in optimization algorithms like gradient descent.","['E Œ∏ X 1 t T Et Œ∏ 3 Et Œ∏ X 1 k t Et xt xt xk xk Œ∏ 4 xt xk Y t i k xi xi 1 Y t i k WT recdiag œÉ xi 1 5 xk Œ∏ refers to the immediate partial derivative of the state xk with respect to Œ∏ i.e. where xk 1 is taken as a constant with respect to Œ∏. SpeciÔ¨Åcally considering equation 2 the value of any row i of the matrix xk Wrec is just œÉ xk 1 . Equation 5 also provides the form of Jacobian matrix xi xi 1 for the speciÔ¨Åc parametrization given in equation 11 where diag converts a vector into a diagonal matrix and œÉ computes the derivative of œÉ in an element wise fash ion.', '. f1 xn . . . ... . . . fm x1 . . . fm xn That is f x ij fi xj which is just a standard non vector derivative . The Jacobian matrix will be useful for us because we can apply the chain rule to a vector valued function just by multiplying Jacobians. As a little illustration of this suppose we have a function f x f1 x f2 x taking a scalar to a vector of size 2 and a function g y g1 y1 y2 g2 y1 y2 taking a vector of size two to a vector of size two. Now let s compose them to get g x g1 f1 x f2 x g2 f1 x f2 x .', 'First suppose we have a loss function J a scalar and are computing its gradient with respect to a matrix W Rn m. Then we could think of J as a function of W taking nm inputs the entries of W to a single output J . This means the Jacobian J W would be a 1 nm vector. But in practice this is not a very useful way of arranging the gradient. It would be much nicer if the derivatives were in a n m matrix like this J W J W11 . . . J W1m . . . ... . . . J Wn1 . . . J Wnm Since this matrix has the same shape as W we could just subtract it times the learning rate from W when doing gradient descent. So in a slight abuse of notation let s Ô¨Ånd this matrix as J W instead. This way of arranging the gradients becomes complicated when computing z W . Unlike J z is a vector.', '4 An elementwise function applied a vector z f x what is z x Since f is being applied elementwise we have zi f xi . So z x ij zi xj xj f xi f xi if i j 0 if otherwise So we see that the Jacobian z x is a diagonal matrix where the entry at i i is the derivative of f applied to xi. We can write this as z x diag f x . Since multiplication by a diagonal matrix is the same as doing elementwise multiplication by the diagonal we could also write f x when applying the chain rule. 5 Matrix times column vector with respect to the matrix z W x Œ¥ J z what is J W J z z W Œ¥ z W This is a bit more complicated than the other identities. The reason for in cluding J z in the above problem formulation will become clear in a moment.']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999999999999996,0.5,0.8995012220907386,0.0454545454545454
337,What role does automatic differentiation play in modern deep learning frameworks?,"Automatic differentiation plays a crucial role in modern deep learning frameworks by efficiently computing gradients for optimization algorithms like stochastic gradient descent. It allows for automatic calculation of derivatives through complex neural network structures, enabling faster and more accurate training of deep learning models. This is essential for tasks such as backpropagation, where gradients need to be computed efficiently for updating the model parameters during training.

Source: lecture02_notes_Backpropagation","['2507COLLOBERT WESTON BOTTOU KARLEN KAVUKCUOGLU AND KUKSA 3.4.3 STOCHASTIC GRADIENT Maximizing 8 with stochastic gradient Bottou 1991 is achieved by iteratively selecting a random example x y and making a gradient step Œ∏ Œ∏ Œª log p y x Œ∏ Œ∏ 16 where Œª is a chosen learning rate. Our neural networks described in Figure 1 and Figure 2 are a succession of layers that correspond to successive composition of functions. The neural network is Ô¨Ånally composed with the word level log likelihood 11 or successively composed in the re cursion 14 if using the sentence level log likelihood 13 . Thus an analytical formulation of the derivative 16 can be computed by applying the differentiation chain rule through the network and through the word level log likelihood 11 or through the recurrence 14 . Remark 5 Differentiability Our cost functions are differentiable almost everywhere.', 'The regularizer we propose below prefers solutions for which the error signal preserves norm as it travels back in time ‚Ñ¶ X k ‚Ñ¶k X k E xk 1 xk 1 xk E xk 1 1 2 9 In order to be computationally eÔ¨Écient we only use the immediate partial derivative of ‚Ñ¶with respect to Wrec we consider that xk and E xk 1 as being constant with respect to Wrec when computing the derivative of ‚Ñ¶k as depicted in equation 10 . Note we use the parametrization of equation 11 . This can be done ef Ô¨Åciently because we get the values of E xk from BPTT. We use Theano to compute these gradients Bergstra et al. 2010 Bastien et al. 2012 .', 'The intuition is that to get a better approximation of the derivative slope around a point we need to examine the func tion f s behavior both to the left and right of that point. It can also be shown using Taylor s theorem that the centered difference formula has an error proportional to œµ2 which is quite small whereas the derivative deÔ¨Ånition is more error prone. Now a natural question you might ask is if this method is so pre cise why do we not use it to compute all of our network gradients instead of applying back propagation The simple answer as hinted earlier is inefÔ¨Åciency recall that every time we want to compute the gradient with respect to an element we need to make two forward passes through the network which will be computationally expen sive. Furthermore many large scale neural networks can contain millions of parameters and computing two passes per parameter is clearly not optimal. And since in optimization techniques such as SGD we must compute the gradients once per iteration for several thousands of iterations it is obvious that this method quickly grows intractable.', '2.3. The geometrical interpretation Let us consider a simple one hidden unit model equa tion 8 where we provide an initial state x0 and train the model to have a speciÔ¨Åc target value after 50 steps. Note that for simplicity we assume no input. xt wœÉ xt 1 b 8 Fig. 6 shows the error surface E50 œÉ x50 0.7 2 where x0 .5 and œÉ to be the sigmoid function. We can more easily analyze the behavior of this model by further simplifying it to be linear œÉ then being the identity function with b 0. xt x0wt from which it follows that xt w tx0wt 1 and 2xt w2 t t 1 x0wt 2 implying that when the Ô¨Årst derivative explodes so does the second derivative.']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.0000000000000002,0.2,0.9336383344613544,0.0
338,What is the purpose of manual gradient checking?,"The purpose of manual gradient checking is to verify the correctness of analytic gradients by comparing them with numerical gradients, which are much quicker to compute. This helps ensure that the gradients calculated during the training process are accurate and can help in debugging and improving models. 

Source: Lecture02_Neural_Nets.txt","['This inefÔ¨Åciency is why we only use gradient check to verify the correctness of our analytic gradients which are much quicker to compute. A standard implementation of gradient check is shown below Snippet 2.1 def eval_numerical_gradient f x a naive implementation of numerical gradient of f at x f should be a function that takes a single argument x is the point numpy array to evaluate the gradient at fx f x evaluate function value at original point grad np.zeros x.shape h 0.00001 iterate over all indexes in x it np.nditer x flags multi_index op_flags readwrite while not it.finished evaluate function at x h ix it.multi_index old_value x ix x ix old_value h increment by h fxh_left f x evaluate f x h x ix old_value h decrement by h fxh_right f x evaluate f x h x ix old_value restore to previous value very important compute the partial derivative grad ix fxh_left fxh_right 2 h the slope it.iternext step to next dimension return grad 2.', 'everything doing this everywhere was the key test Now much less needed you can use it to check layers are correctly implemented 78Summary We ve mastered the core technology of neural nets Backpropagation recursively and hence efficiently apply the chain rule along computation graph downstream gradient upstream gradient x local gradient Forward pass compute results of operations and save intermediate values Backward pass apply chain rule to compute gradients 79Why learn all these details about gradients 80 Modern deep learning frameworks compute gradients for you But why take a class on compilers or systems when they are implemented for you Understanding what is going on under the hood is useful Backpropagation doesn t always work perfectly out of the box Understanding why is crucial for debugging and improving models See Karpathy article https medium.com @karpathy yes you should understand backprop e2f06eab496b Example in future lecture exploding and vanishing gradients', 'We note that this deÔ¨Ånition of the numerical gradient follows very naturally from the deÔ¨Ånition of the derivative where in the scalar case f x f x œµ f x œµ Gradient checks are a great way to compare analytical and numerical gradients. Analytical gradients should be close and numerical gradients can be calculated using f Œ∏ J Œ∏ i J Œ∏ i 2œµ J Œ∏ i and J Œ∏ i can be evalu ated using two forward passes. An implementation of this can be seen in Snippet 2.1. Of course there is a slight difference the deÔ¨Ånition above only perturbs x in the positive direction to compute the gradient. While it would have been perfectly acceptable to deÔ¨Åne the numerical gradi ent in this way in practice it is often more precise and stable to use the centered difference formula where we perturb a parameter in both directions.', 'This makes the math a bit cumbersome. Even worse if we re implementing the model without automatic diÔ¨Äerentiation computing J Œ∏ twice will be ineÔ¨Écient. So it will help us to deÔ¨Åne some variables to represent the intermediate derivatives Œ¥1 J Œ∏ Œ¥2 J z These can be thought as the error signals passed down to Œ∏ and z when doing backpropagation.']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999999999999996,1.0,0.926099146812606,0.0909090909090909
339,Why is understanding gradients important despite automatic differentiation?,"Understanding gradients is important despite automatic differentiation because it allows us to verify the correctness of our analytic gradients through gradient checks. This verification process helps ensure the accuracy of the gradients used in training neural networks, as numerical gradients can be compared to analytical gradients to confirm their proximity. Additionally, understanding gradients provides insights into the optimization process and helps in diagnosing issues that may arise during training.","['The intuition is that to get a better approximation of the derivative slope around a point we need to examine the func tion f s behavior both to the left and right of that point. It can also be shown using Taylor s theorem that the centered difference formula has an error proportional to œµ2 which is quite small whereas the derivative deÔ¨Ånition is more error prone. Now a natural question you might ask is if this method is so pre cise why do we not use it to compute all of our network gradients instead of applying back propagation The simple answer as hinted earlier is inefÔ¨Åciency recall that every time we want to compute the gradient with respect to an element we need to make two forward passes through the network which will be computationally expen sive. Furthermore many large scale neural networks can contain millions of parameters and computing two passes per parameter is clearly not optimal. And since in optimization techniques such as SGD we must compute the gradients once per iteration for several thousands of iterations it is obvious that this method quickly grows intractable.', 'We note that this deÔ¨Ånition of the numerical gradient follows very naturally from the deÔ¨Ånition of the derivative where in the scalar case f x f x œµ f x œµ Gradient checks are a great way to compare analytical and numerical gradients. Analytical gradients should be close and numerical gradients can be calculated using f Œ∏ J Œ∏ i J Œ∏ i 2œµ J Œ∏ i and J Œ∏ i can be evalu ated using two forward passes. An implementation of this can be seen in Snippet 2.1. Of course there is a slight difference the deÔ¨Ånition above only perturbs x in the positive direction to compute the gradient. While it would have been perfectly acceptable to deÔ¨Åne the numerical gradi ent in this way in practice it is often more precise and stable to use the centered difference formula where we perturb a parameter in both directions.', 'This inefÔ¨Åciency is why we only use gradient check to verify the correctness of our analytic gradients which are much quicker to compute. A standard implementation of gradient check is shown below Snippet 2.1 def eval_numerical_gradient f x a naive implementation of numerical gradient of f at x f should be a function that takes a single argument x is the point numpy array to evaluate the gradient at fx f x evaluate function value at original point grad np.zeros x.shape h 0.00001 iterate over all indexes in x it np.nditer x flags multi_index op_flags readwrite while not it.finished evaluate function at x h ix it.multi_index old_value x ix x ix old_value h increment by h fxh_left f x evaluate f x h x ix old_value h decrement by h fxh_right f x evaluate f x h x ix old_value restore to previous value very important compute the partial derivative grad ix fxh_left fxh_right 2 h the slope it.iternext step to next dimension return grad 2.', 'This makes the math a bit cumbersome. Even worse if we re implementing the model without automatic diÔ¨Äerentiation computing J Œ∏ twice will be ineÔ¨Écient. So it will help us to deÔ¨Åne some variables to represent the intermediate derivatives Œ¥1 J Œ∏ Œ¥2 J z These can be thought as the error signals passed down to Œ∏ and z when doing backpropagation.']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9536899962692664,1.0,0.9183737179329236,0.0666666666666666
340,What is the core concept introduced in this class that leads to GPT-3 and ChatGPT?,The core concept introduced in this class that leads to GPT-3 and ChatGPT is the use of large-scale pre-training of deep neural networks on diverse datasets to enable the models to learn rich representations of language and perform well on a wide range of natural language processing tasks. This approach leverages the power of transformer-based architectures and self-supervised learning to achieve state-of-the-art performance in various NLP tasks.,"['Overall in context learning with GPT 3 shows mixed results on commonsense reasoning tasks with only small and inconsistent gains observed in the one and few shot learning settings for both PIQA and ARC but a signiÔ¨Åcant improvement is observed on OpenBookQA. GPT 3 sets SOTA on the new PIQA dataset in all evaluation settings. 3.6 Reading Comprehension Next we evaluate GPT 3 on the task of reading comprehension. We use a suite of 5 datasets including abstractive multiple choice and span based answer formats in both dialog and single question settings. We observe a wide spread in GPT 3 s performance across these datasets suggestive of varying capability with different answer formats. In general we observe GPT 3 is on par with initial baselines and early results trained using contextual representations on each respective dataset.', 'First despite the strong quantitative and qualitative improvements of GPT 3 particularly compared to its direct predecessor GPT 2 it still has notable weaknesses in text synthesis and several NLP tasks. On text synthesis although the overall quality is high GPT 3 samples still sometimes repeat themselves semantically at the document level start to lose coherence over sufÔ¨Åciently long passages contradict themselves and occasionally contain non sequitur sentences or paragraphs. We will release a collection of 500 uncurated unconditional samples to help provide a better sense of GPT 3 s limitations and strengths at text synthesis. Within the domain of discrete language tasks we have noticed informally that GPT 3 seems to have special difÔ¨Åculty with common sense physics despite doing well on some datasets such as PIQA BZB 19 that test this domain. SpeciÔ¨Åcally GPT 3 has difÔ¨Åculty with questions of the type If I put cheese into the fridge will it melt .', 'GPT 3 performs best within 3 points of the human baseline on CoQA RCM19 a free form conversational dataset and performs worst 13 F1 below an ELMo baseline on QuAC CHI 18 a dataset which requires modeling structured dialog acts and answer span selections of teacher student interactions. On DROP DWD 19 a dataset testing discrete reasoning and numeracy in the context of reading comprehension GPT 3 in a few shot setting outperforms the Ô¨Åne tuned BERT baseline from the original paper but is still well below both human performance and state of the art approaches which augment neural networks with symbolic systems RLL 19 . On SQuAD 2.0 RJL18 GPT 3 demonstrates its few shot learning capabilities improving by almost 10 F1 to 69.8 compared to a zero shot setting. This allows it to slightly outperform the best Ô¨Åne tuned result in the original paper.', 'Section 5 discusses limitations of GPT 3. Section 6 discusses broader impacts. Section 7 reviews related work and Section 8 concludes. 2 Approach Our basic pre training approach including model data and training is similar to the process described in RWC 19 with relatively straightforward scaling up of the model size dataset size and diversity and length of training. Our use of in context learning is also similar to RWC 19 but in this work we systematically explore different settings for learning within the context. Therefore we start this section by explicitly deÔ¨Åning and contrasting the different settings that we will be evaluating GPT 3 on or could in principle evaluate GPT 3 on. These settings can be seen as lying on a spectrum of how much task speciÔ¨Åc data they tend to rely on.']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9988607910943532,0.0,0.9096530675172704,0.0454545454545454
341,What is the purpose of Dropout as mentioned in the document?,"The purpose of Dropout, as mentioned in the document, is to prevent overfitting in neural networks by training exponentially many smaller networks at once and averaging over their predictions. Dropout helps the network learn more meaningful information from the data, reduces the likelihood of overfitting, and generally improves performance on the task at hand. During training, Dropout randomly sets a fraction of neurons to zero, and during testing, it uses all neurons in the network for forward pass computations.

Source: SEP 775 - Computational Natural Language Processing, lecture02_notes_Backpropagation","['The result is that the network typically learns more meaningful informa tion from the data is less likely to overÔ¨Åt and usually obtains higher performance overall on the task at hand. One intuitive reason why this technique should be so effective is that what dropout is doing is essentially doing is training exponentially many smaller networks at once and averaging over their predictions. Dropout applied to an artiÔ¨Åcial neural network. Image credits to Srivastava et al. In practice the way we introduce dropout is that we take the out put h of each layer of neurons and keep each neuron with prob ability p and else set it to 0. Then during back propagation we only pass gradients through neurons that were kept alive during the forward pass. Finally during testing we compute the forward pass using all of the neurons in the network.', 'kid stubbornly postponees pursuing job citing medical reasons despite tried reasonable compro mise negatively effecting forward employment mobility both personally and Ô¨Ånancially thoght wise want change this dumbass selÔ¨Åsh lazy atti tude now please help pls Table 29 Example of samples from our over optimized model overÔ¨Åt to one of our 1.3B reward models. 45', 'Lastly all the training examples are sorted by the length of paragraph and divided into mini batches of 32 examples each. We use Adamax for optimization as described in Kingma and Ba 5We use the Wikipedia Search API https www. mediawiki.org wiki API Search. 2014 . Dropout with p 0.3 is applied to word embeddings and all the hidden units of LSTMs. Result and analysis Table 4 presents our eval uation results on both development and test sets. SQuAD has been a very competitive machine comprehension benchmark since its creation and we only list the best performing systems in the ta ble. Our system single model can achieve 70.0 exact match and 79.0 F1 scores on the test set which surpasses all the published results and can match the top performance on the SQuAD leader board at the time of writing. Additionally we think that our model is conceptually simpler than most of the existing systems.', '2503COLLOBERT WESTON BOTTOU KARLEN KAVUKCUOGLU AND KUKSA 0 10 20 30 40 50 60 70 The proposed changes also would allow executives to report exercises of options later and less often . xx xx xx xx xx xx xx xx xx xx xx xx xx xx x x x x x x x x x x x x x x x x x xx xx xx xx xx xx xx xx xx xx xx xx xx x x x x x x x x xx xx xx xx xx xxxxx xx xx 0 10 20 30 40 50 60 70 The proposed changes also would allow executives to report exercises of options later and less often .']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9300257743842548,1.0,0.9061755887705052,0.0
342,What are some approaches to address the vanishing gradient problem in RNNs?,"Some approaches to address the vanishing gradient problem in RNNs include:
1. Using Long Short Term Memory (LSTM) networks proposed by Hochreiter and Schmidhuber in 1997.
2. Adding separate memory to RNNs like LSTMs.
3. Creating more direct and linear pass-through connections in the model.
4. Incorporating attention mechanisms and residual connections.

Source: Lecture03_RNNs_and_LLMs.txt","['press v28 pascanu13.pdfHow to fix the vanishing gradient problem The main problem is that it s too difficult for the RNN to learn to preserve information over many timesteps. In a vanilla RNN the hidden state is constantly being rewritten First off next time How about an RNN with separate memory which is added to LSTMs And then Creating more direct and linear pass through connections in model Attention residual connections etc. 60Long Short Term Memory RNNs LSTMs A type of RNN proposed by Hochreiter and Schmidhuber in 1997 as a solution to the problem of vanishing gradients Everyone cites that paper but really a crucial part of the modern LSTM is from Gers et al. 2000 Only started to be recognized as promising through the work of S s student Alex Graves c.', 'Problems with RNNs Vanishing and Exploding Gradients 48Vanishing gradient intuition 49Vanishing gradient intuition chain rule 50Vanishing gradient intuition chain rule 51Vanishing gradient intuition chain rule 52Vanishing gradient intuition What happens if these are small Vanishing gradient problem When these are small the gradient signal gets smaller and smaller as it backpropagates further 53Vanishing gradient proof sketch linear case Recall What if were the identity function 53 Source On the difficulty of training recurrent neural networks Pascanu et al 2013. http proceedings.mlr.press v28 pascanu13.pdf and supplemental materials at http proceedings.mlr.press v28 pascanu13 supp.pdf If Wh is small then this term gets exponentially problematic as becomes large chain rule Consider the gradient of the loss on step with respect to the hidden state on some previous step .', 'By employing the proposed regularization term we are able to improve test error even on tasks that are notOn the diÔ¨Éculty of training Recurrent Neural Networks dominated by long term contributions. 5. Summary and Conclusions We provided diÔ¨Äerent perspectives through which one can gain more insight into the exploding and vanishing gradients issue. To deal with the exploding gradients problem we propose a solution that involves clipping the norm of the exploded gradients when it is too large. The algorithm is motivated by the assumption that when gradients explode the curvature and higher or der derivatives explode as well and we are faced with a speciÔ¨Åc pattern in the error surface namely a val ley with a single steep wall. In order to deal with the vanishing gradient problem we use a regulariza tion term that forces the error signal not to vanish as it travels back in time. This regularization term forces the Jacobian matrices xi xi 1 to preserve norm only in relevant directions.', 'file name: On the difficulty of training Recurrent Neural Networks. On the diÔ¨Éculty of training Recurrent Neural Networks Razvan Pascanu pascanur@iro.umontreal.ca Universite de Montreal Tomas Mikolov t.mikolov@gmail.com Brno University Yoshua Bengio yoshua.bengio@umontreal.ca Universite de Montreal Abstract There are two widely known issues with prop erly training Recurrent Neural Networks the vanishing and the exploding gradient prob lems detailed in Bengio et al. 1994 . In this paper we attempt to improve the under standing of the underlying issues by explor ing these problems from an analytical a geo metric and a dynamical systems perspective. Our analysis is used to justify a simple yet ef fective solution. We propose a gradient norm clipping strategy to deal with exploding gra dients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section. 1.']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.0000000000000009,1.0,0.8753291092853799,0.0555555555555555
343,"What is perplexity, and how is it used in evaluating language models?","Perplexity is a metric used in evaluating language models, which is the inverse probability of a test set normalized by the number of words. It is a variant of raw probability and is used to measure how well a language model predicts a given set of words. A lower perplexity indicates that the language model is a better predictor of the words in the test set. Perplexity is calculated based on the likelihood of the test sequence according to the model, and it can be used to compare different n-gram models in terms of their predictive performance. It is important to note that perplexity should be computed without any prior knowledge of the test set or its vocabulary to ensure fair comparisons between language models.

Source: N_gram_Language_Models.txt","['Given a large corpus that we want to divide into training and test test data can either be taken from some continuous sequence of text inside the corpus or we can remove smaller stripes of text from randomly selected parts of our corpus and combine them into a test set. 3.2.1 Perplexity In practice we don t use raw probability as our metric for evaluating language mod els but a variant called perplexity. The perplexity sometimes called PPL for short perplexity of a language model on a test set is the inverse probability of the test set normalized by the number of words. For a test set W w1w2 ...wN perplexity W P w1w2 ...wN 1 N 3.14 N s 1 P w1w2 ...wN We can use the chain rule to expand the probability of W perplexity W N v u u t N Y i 1 1 P wi w1 ...wi 1 3.15 The perplexity of a test set W depends on which language model we use.', 'A trigram model is less surprised than a unigram model because it has a better idea of what words might come next and so it assigns them a higher probability. And the higher the probability the lower the perplexity since as Eq. 3.15 showed perplexity is related inversely to the likelihood of the test sequence according to the model . So a lower perplexity can tell us that a language model is a better predictor of the words in the test set. Note that in computing perplexities the n gram model P must be constructed without any knowledge of the test set or any prior knowledge of the vocabulary of the test set. Any kind of knowledge of the test set can cause the perplexity to be artiÔ¨Åcially low. The perplexity of two language models is only comparable if they use identical vocabularies. An intrinsic improvement in perplexity does not guarantee an extrinsic im provement in the performance of a language processing task like speech recognition10 CHAPTER 3 N GRAM LANGUAGE MODELS or machine translation.', 'In either case we then proceed to train the language model as before treating UNK like a regular word. The exact choice of UNK has an effect on metrics like perplexity. A language model can achieve low perplexity by choosing a small vocabulary and assigning the unknown word a high probability. Thus perplexities can only be compared across language models with the same vocabularies Buck et al. 2014 . 3.5 Smoothing What do we do with words that are in our vocabulary they are not unknown words but appear in a test set in an unseen context for example they appear after a word they never appeared after in training To keep a language model from assigning14 CHAPTER 3 N GRAM LANGUAGE MODELS zero probability to these unseen events we ll have to shave off a bit of probability mass from some more frequent events and give it to the events we ve never seen. This modiÔ¨Åcation is called smoothing or discounting.', 'We mentioned above that perplexity is a function of both the text and the lan guage model given a text W different language models will have different perplex ities. Because of this perplexity can be used to compare different n gram models. Let s look at an example in which we trained unigram bigram and trigram gram mars on 38 million words including start of sentence tokens from the Wall Street Journal using a 19 979 word vocabulary. We then computed the perplexity of each of these models on a test set of 1.5 million words using Eq. 3.16 for unigrams Eq. 3.17 for bigrams and the corresponding equation for trigrams. The table below shows the perplexity of a 1.5 million word WSJ test set according to each of these grammars. Unigram Bigram Trigram Perplexity 962 170 109 As we see above the more information the n gram gives us about the word sequence the higher the probability the n gram will assign to the string.']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9077103422875468,1.0,0.9523852678527592,0.0384615384615384
344,How does LSTM solve the vanishing gradient problem?,"LSTMs (Long Short Term Memory networks) were proposed as a solution to the vanishing gradient problem in RNNs. They introduce a special set of units called LSTM units which are linear and have a recurrent connection to itself fixed to 1. The flow of information into the unit and from the unit is guarded by input and output gates, and their behavior is learned. This architecture allows LSTMs to better preserve information over many timesteps, addressing the vanishing gradient problem commonly encountered in vanilla RNNs.

Source: lecture03_RNNs_and_LLMs.txt","['press v28 pascanu13.pdfHow to fix the vanishing gradient problem The main problem is that it s too difficult for the RNN to learn to preserve information over many timesteps. In a vanilla RNN the hidden state is constantly being rewritten First off next time How about an RNN with separate memory which is added to LSTMs And then Creating more direct and linear pass through connections in model Attention residual connections etc. 60Long Short Term Memory RNNs LSTMs A type of RNN proposed by Hochreiter and Schmidhuber in 1997 as a solution to the problem of vanishing gradients Everyone cites that paper but really a crucial part of the modern LSTM is from Gers et al. 2000 Only started to be recognized as promising through the work of S s student Alex Graves c.', 'SpeciÔ¨Åcally it introduces a special set of units called LSTM units which are linear and have a recurrent connection to itself which is Ô¨Åxed to 1. The Ô¨Çow of information into the unit and from the unit is guarded by an input and output gates their behaviour is learned . There are several variations of this basic structure. This solution does not address explicitly the exploding gradients problem. Sutskever et al. 2011 use the Hessian Free opti mizer in conjunction with structural damping a spe ciÔ¨Åc damping strategy of the Hessian. This approach seems to deal very well with the vanishing gradient though more detailed analysis is still missing. Pre sumably this method works because in high dimen sional spaces there is a high probability for long term components to be orthogonal to short term ones. This would allow the Hessian to rescale these components independently. In practice one can not guarantee that this property holds.', 'By employing the proposed regularization term we are able to improve test error even on tasks that are notOn the diÔ¨Éculty of training Recurrent Neural Networks dominated by long term contributions. 5. Summary and Conclusions We provided diÔ¨Äerent perspectives through which one can gain more insight into the exploding and vanishing gradients issue. To deal with the exploding gradients problem we propose a solution that involves clipping the norm of the exploded gradients when it is too large. The algorithm is motivated by the assumption that when gradients explode the curvature and higher or der derivatives explode as well and we are faced with a speciÔ¨Åc pattern in the error surface namely a val ley with a single steep wall. In order to deal with the vanishing gradient problem we use a regulariza tion term that forces the error signal not to vanish as it travels back in time. This regularization term forces the Jacobian matrices xi xi 1 to preserve norm only in relevant directions.', 'NaN which is easily detectable at runtime this issue is called the Gradient Explosion Problem. When the gradient value goes to zero however it can go undetected while dras tically reducing the learning quality of the model for far away words in the corpus this issue is called the Vanishing Gradient Problem. Due to vanishing gradients we don t know whether there is no dependency between steps t and t n in the data or we just cannot capture the true dependency due to this issue. To gain practical intuition about the vanishing gradient problem you may visit the following example website. 2.4 Solution to the Exploding Vanishing Gradients Now that we gained intuition about the nature of the vanishing gradi ents problem and how it manifests itself in deep neural networks let us focus on a simple and practical heuristic to solve these problems. To solve the problem of exploding gradients Thomas Mikolov Ô¨Årst introduced a simple heuristic solution that clips gradients to a small number whenever they explode.']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.8486510056069165,1.0,0.9075982437560228,0.0416666666666666
345,"What is the impact of bidirectional RNNs, and where are they applicable?","Bidirectional RNNs are impactful as they provide a contextual representation of a word with both left and right context, making them useful for tasks where understanding the entire input sequence is important. They are applicable when you have access to the entire input sequence, such as in tasks like speech recognition, machine translation, and summarization. However, bidirectional RNNs are not applicable to Language Modeling because in LM, only the left context is available. If you have the entire input sequence available, bidirectionality can be powerful and should be used, as seen in models like BERT (Bidirectional Encoder Representations from Transformers).

Source: Lecture03_RNNs_and_LLMs.txt","['Answer German 76 lots of neural architecture lots of neural architectureRNN LMs can be used to generate text e.g. speech recognition machine translation summarization RNN LM what s the the weather what s This is an example of a conditional language model. We ll see Machine Translation in much more detail next lectures. Input audio START conditioning 77Bidirectional RNNs was terribly exciting the movie Forward RNN Backward RNN Concatenated hidden states This contextual representation of terribly has both left and right context 78Bidirectional RNNs 79 Note bidirectional RNNs are only applicable if you have access to the entire input sequence They are not applicable to Language Modeling because in LM you only have left context available. If you do have entire input sequence e.g. any kind of encoding bidirectionality is powerful you should use it by default . For example BERT Bidirectional Encoder Representations from Transformers is a powerful pretrained contextual representation system built on bidirectionality.', 'Recall the goal of a RNN implementation is to en able propagating context information through faraway time steps. For example consider the following two sentences Sentence 1 Jane walked into the room. John walked in too. Jane said hi to ___ Sentence 2 Jane walked into the room. John walked in too. It was late in the day and everyone was walking home after a long day at work. Jane said hi to ___ In both sentences given their context one can tell the answer to both blank spots is most likely John . It is important that the RNN predicts the next word as John the second person who has appeared several time steps back in both contexts. Ideally this should be possible given what we know about RNNs so far. In practice however it turns out RNNs are more likely to correctly predict the blank spot in Sentence 1 than in Sentence 2. This is because during the back propagation phase the contribution of gradient values gradually vanishes as they propagate to earlier timesteps as we will show below.', 'Recurrent Neural Networks RNN A family of neural architectures hidden states input sequence any length Core idea Apply the same weights ùëärepeatedly outputs optional 28A Simple RNN Language Model the students opened their words one hot vectors books laptops word embeddings a zoo output distribution Note this input sequence could be much longer now hidden states is the initial hidden state 28RNN Language Models the students opened their books laptops a zoo RNN Advantages Can process any length input Computation for step t can in theory use information from many steps back Model size doesn t increase for longer input context Same weights applied on every timestep so there is symmetry in how inputs are processed. RNN Disadvantages Recurrent computation is slow In practice difficult to access information from many steps back More on these later 30Training an RNN Language Model Get a big corpus of text which is a sequence of words Feed into RNN LM compute output distribution for every step t. i.e.', 'h h h Figure 8 A bi directional RNN model h t f W xt V h t 1 b 17 h t f W xt V h t 1 b 18 ÀÜ yt g Uht c g U h t h t c 19 RNNs can also be multi layered. Figure 9 shows a multi layer bi directional RNN where each lower layer feeds the next layer. As shown in this Ô¨Ågure in this network architecture at time step t each interme diate neuron receives one set of parameters from the previous time step in the same RNN layer and two sets of parameters from the previous RNN hidden layer one input comes from the left to right RNN and the other from the right to left RNN.']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9149011561543609,1.0,0.9461914990882574,0.0454545454545454
346,Describe the concept of multi-layer or stacked RNNs and their benefit.,"Multi-layer or stacked RNNs involve applying multiple RNN layers on top of each other. This allows the network to compute more complex representations by having lower RNN layers compute lower-level features and higher RNN layers compute higher-level features. Stacked RNNs work better than having just one layer of high-dimensional encodings, enabling the network to learn more intricate patterns and relationships in the data. In practice, high-performing RNNs are usually multi-layered, with 2 to 4 layers being optimal for tasks like Neural Machine Translation. Skip connections or dense connections are often used to train deeper RNNs effectively. Transformer-based networks like BERT, on the other hand, are typically even deeper, with 12 or 24 layers. 

Source: SEP 775 - Computational Natural Language Processing, lecture03_RNNs_and_LLMs.txt","['You will learn more about transformers including BERT in a couple of weeksMulti layer RNNs RNNs are already deep on one dimension they unroll over many timesteps We can also make them deep in another dimension by applying multiple RNNs this is a multi layer RNN. This allows the network to compute more complex representations The lower RNNs should compute lower level features and the higher RNNs should compute higher level features. Multi layer RNNs are also called stacked RNNs. 80Multi layer RNNs was terribly exciting the movie RNN layer 1 RNN layer 2 81 RNN layer 3 The hidden states from RNN layer i are the inputs to RNN layer i 1Multi layer RNNs in practice 39 Multi layer or stacked RNNs allow a network to compute more complex representations they work better than just have one layer of high dimensional encodings The lower RNNs should compute lower level features and the higher RNNs should compute higher level features.', 'h h h Figure 8 A bi directional RNN model h t f W xt V h t 1 b 17 h t f W xt V h t 1 b 18 ÀÜ yt g Uht c g U h t h t c 19 RNNs can also be multi layered. Figure 9 shows a multi layer bi directional RNN where each lower layer feeds the next layer. As shown in this Ô¨Ågure in this network architecture at time step t each interme diate neuron receives one set of parameters from the previous time step in the same RNN layer and two sets of parameters from the previous RNN hidden layer one input comes from the left to right RNN and the other from the right to left RNN.', 'High performing RNNs are usually multi layer but aren t as deep as convolutional or feed forward networks For example In a 2017 paper Britz et al. find that for Neural Machine Translation 2 to 4 layers is best for the encoder RNN and 4 layers is best for the decoder RNN Often 2 layers is a lot better than 1 and 3 might be a little better than 2 Usually skip connections dense connections are needed to train deeper RNNs e.g. 8 layers Transformer based networks e.g. BERT are usually deeper like 12 or 24 layers.', 'For a RNN with 1000 recurrent layers the matrix would be 1000 1000 regardless of the corpus size. RNNs can be used for many tasks such as tagging e.g. part of speech named entity recognition sentence classiÔ¨Åcation e.g. senti ment classiÔ¨Åcation and encoder modules e.g. questiton answering machine translation and many other tasks . In the latter two applica tions we want a representation for the sentence which we can obtain by taking the element wise max or mean of all hidden states of the timesteps in that sentence. Note Figure 6 is an alternative representation of RNNs used in some publications. It represents the RNN hidden layer as a loop. ht Figure 6 The illustration of a RNN as a loop over time steps 2.3 Vanishing Gradient Gradient Explosion Problems Recurrent neural networks propagate weight matrices from one time step to the next.']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9594957877859814,1.0,0.9444214098310362,0.0714285714285714
347,How does Dropout work during training and testing?,"During training, Dropout works by randomly setting a fraction of the neurons in a layer to zero based on a specified probability \( p \). This helps prevent overfitting by training multiple smaller networks simultaneously and averaging their predictions. During testing, all neurons are used in the network to make predictions. It is important to ensure that the expected output of a neuron during testing is approximately the same as during training to maintain network behavior. This may involve adjusting the outputs of neurons during testing to maintain consistency with training.","['The result is that the network typically learns more meaningful informa tion from the data is less likely to overÔ¨Åt and usually obtains higher performance overall on the task at hand. One intuitive reason why this technique should be so effective is that what dropout is doing is essentially doing is training exponentially many smaller networks at once and averaging over their predictions. Dropout applied to an artiÔ¨Åcial neural network. Image credits to Srivastava et al. In practice the way we introduce dropout is that we take the out put h of each layer of neurons and keep each neuron with prob ability p and else set it to 0. Then during back propagation we only pass gradients through neurons that were kept alive during the forward pass. Finally during testing we compute the forward pass using all of the neurons in the network.', 'However a key sub tlety is that in order for dropout to work effectively the expected output of a neuron during testing should be approximately the same as it was during training else the magnitude of the outputs could be radically different and the behavior of the network is no longer well deÔ¨Åned. Thus we must typically divide the outputs of each neuron during testing by a certain value it is left as an exercise to the reader to determine what this value should be in order for the expected outputs during training and testing to be equivalent. 2.4 Neuron Units So far we have discussed neural networks that contain sigmoidal neurons to introduce nonlinearities however in many applications better networks can be designed using other activation functions. Some common choices are listed here with their function and gra dient deÔ¨Ånitions and these can be substituted with the sigmoidal functions discussed above.', 'Lastly all the training examples are sorted by the length of paragraph and divided into mini batches of 32 examples each. We use Adamax for optimization as described in Kingma and Ba 5We use the Wikipedia Search API https www. mediawiki.org wiki API Search. 2014 . Dropout with p 0.3 is applied to word embeddings and all the hidden units of LSTMs. Result and analysis Table 4 presents our eval uation results on both development and test sets. SQuAD has been a very competitive machine comprehension benchmark since its creation and we only list the best performing systems in the ta ble. Our system single model can achieve 70.0 exact match and 79.0 F1 scores on the test set which surpasses all the published results and can match the top performance on the SQuAD leader board at the time of writing. Additionally we think that our model is conceptually simpler than most of the existing systems.', 'We call this situation training on the test set. Training on the test set introduces a bias that makes the probabilities all look too high and causes huge inaccuracies in perplexity the probability based metric we introduce below. Sometimes we use a particular test set so often that we implicitly tune to its characteristics. We then need a fresh test set that is truly unseen. In such cases we call the initial test set the development test set or devset. How do we divide our development test data into training development and test sets We want our test set to be as large as possible since a small test set may be accidentally unrepresentative but we also8 CHAPTER 3 N GRAM LANGUAGE MODELS want as much training data as possible. At the minimum we would want to pick the smallest test set that gives us enough statistical power to measure a statistically signiÔ¨Åcant difference between two potential models. In practice we often just divide our data into 80 training 10 development and 10 test.']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9804445873957488,1.0,0.9299979676492676,0.0357142857142857
348,What is the role of parameter initialization in neural networks?,"Parameter initialization in neural networks plays a crucial role in achieving superlative performance. A good starting strategy is to initialize the weights to small random numbers normally distributed around 0. Different weight and bias initialization schemes can have a significant impact on training dynamics and the overall performance of the neural network. Proper parameter initialization helps in preventing issues like vanishing gradients, exploding gradients, and can lead to faster convergence and lower error rates during training. It is essential for maintaining activation variances and backpropagated gradient variances across layers, which are important for information flow in the network. 

Source: lecture02_notes_Backpropagation.txt","['We can then take the Singular Value Decomposition SVD of X to get matrices U S V. We then compute UX to project X into the basis deÔ¨Åned by the columns of U. We Ô¨Ånally divide each dimension of the result by the corresponding singular value in S to scale our data appropriately if a singular value is zero we can just divide by a small number instead . 2.6 Parameter Initialization A key step towards achieving superlative performance with a neu ral network is initializing the parameters in a reasonable way. A good starting strategy is to initialize the weights to small random numbers normally distributed around 0 and in practice this often words acceptably well. However in Understanding the dif ficulty of training deep feedforward neural networks 2010 Xavier et al study the effect of different weight and bias initialization schemes on training dynamics.', 'We should also note that such initialization does not prevent cross ing the boundary between basins of attraction which as shown could happen even though no bifurcation boundary is crossed. Teacher forcing is a more interesting yet a not very well understood solution. It can be seen as a way of initializing the model in the right regime and the rightOn the diÔ¨Éculty of training Recurrent Neural Networks region of space. It has been shown that in practice it can reduce the chance that gradients explode and even allow training generator models or models that work with unbounded amounts of memory Pascanu and Jaeger 2011 Doya and Yoshizawa 1991 . One important downside is that it requires a target to be deÔ¨Åned at every time step. In Hochreiter and Schmidhuber 1997 Graves et al. 2009 a solution is proposed for the vanishing gra dients problem where the structure of the model is changed.', 'The empirical Ô¨Åndings suggest that for sigmoid and tanh activation units faster convergence and lower error rates are achieved when the weights of a matrix W Rn l 1 n l are initialized randomly with a uniform distribution as follows W U s 6 n l n l 1 s 6 n l n l 1 Where n l is the number of input units to W fan in and n l 1 is the number of output units from W fan out . In this parameter initialization scheme bias units are initialized to 0. This approach attempts to maintain activation variances as well as backpropagated gradient variances across layers. Without such initialization the gradient variances which are a proxy for information generally decrease with backpropagation across layers. 2.7 Learning Strategies The rate magnitude of model parameter updates during training can be controlled using the learning rate.', 'Applying either weight normalization or batch normal ization using expected statistics is equivalent to have a different parameterization of the original feed forward neural network. Re parameterization in the ReLU network was studied in the Path normalized SGD Neyshabur et al. 2015 . Our proposed layer normalization method however is not a re parameterization of the original neural network. The layer normalized model thus has different invariance properties than the other methods that we will study in the following section. 5 Analysis In this section we investigate the invariance properties of different normalization schemes. 5.1 Invariance under weights and data transformations The proposed layer normalization is related to batch normalization and weight normalization. Al though their normalization scalars are computed differently these methods can be summarized as normalizing the summed inputs ai to a neuron through the two scalars ¬µ and œÉ. They also learn an adaptive bias b and gain g for each neuron after the normalization. hi f gi œÉi ai ¬µi bi 5 Note that for layer normalization and batch normalization ¬µ and œÉ is computed according to Eq.']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9841035568142736,1.0,0.9158377611248258,0.0416666666666666
349,"What are the differences between Vanilla RNNs, LSTMs, and GRUs?","Vanilla RNNs, LSTMs, and GRUs are all types of recurrent neural networks with some key differences. 

1. Vanilla RNNs: Vanilla RNNs suffer from the vanishing gradient problem, which makes it difficult for them to capture long-term dependencies in sequences. They have a simple architecture where the hidden state is updated at each time step based on the input and the previous hidden state.

2. LSTMs (Long Short-Term Memory): LSTMs were designed to address the vanishing gradient problem in Vanilla RNNs. They have a more complex architecture with three gates: input gate, forget gate, and output gate. These gates help control the flow of information into and out of the cell state, allowing LSTMs to capture long-term dependencies more effectively.

3. GRUs (Gated Recurrent Units): GRUs are a variation of LSTMs that aim to simplify the architecture while retaining similar performance. GRUs combine the forget and input gates into a single update gate and merge the cell state and hidden state. This simplification makes GRUs easier to train and has made them increasingly popular in practice.

In summary, Vanilla RNNs have a simple architecture but struggle with capturing long-term dependencies, while LSTMs and GRUs have more complex architectures with mechanisms to address the vanishing gradient problem and capture long-term dependencies effectively.

Source: lecture03_notes_LM_RNN.txt, Understanding_LSTM_Networks.txt","['Which of these variants is best Do the differences matter Greff et al. 2015 http arxiv.org pdf 1503.04069.pdf do a nice comparison of popular variants finding that they re all about the same. Jozefowicz et al. 2015 http jmlr.org proceedings papers v37 jozefowicz15.pdf tested more than ten thousand RNN architectures finding some that worked better than LSTMs on certain tasks. Conclusion Earlier I mentioned the remarkable results people are achieving with RNNs. Essentially all of these are achieved using LSTMs. They really work a lot better for most tasks Written down as a set of equations LSTMs look pretty intimidating.', 'We only forget when we re going to input something in its place. We only input new values to the state when we forget something older. A slightly more dramatic variation on the LSTM is the Gated Recurrent Unit or GRU introduced by Cho et al. 2014 http arxiv.org pdf 1406.1078v3.pdf . It combines the forget and input gates into a single update gate. It also merges the cell state and hidden state and makes some other changes. The resulting model is simpler than standard LSTM models and has been growing increasingly popular. These are only a few of the most notable LSTM variants. There are lots of others like Depth Gated RNNs by Yao et al. 2015 http arxiv.org pdf 1508.03790v2.pdf . There s also some completely different approach to tackling long term dependencies like Clockwork RNNs by Koutnik et al. 2014 http arxiv.org pdf 1402.3511v1.pdf .', 'Figure 12 The detailed internals of a GRU It is important to note that to train a GRU we need to learn all the different parameters W U W r U r W z U z . These follow the same backpropagation procedure we have seen in the past. 4 Long Short Term Memories Long Short Term Memories are another type of complex activation unit that differ a little from GRUs. The motivation for using these is similar to those for GRUs however the architecture of such units does differ.', 'You will learn about Transformers later they have a lot of skipping like connections Massive Exploration of Neural Machine Translation Architecutres Britz et al 2017. https arxiv.org pdf 1703.03906.pdfTerminology By the end of the course You will understand phrases like stacked bidirectional LSTMs with residual connections and self attention The RNN described in this lecture simple vanilla Elman RNN You learned about other RNN flavors like LSTM and GRU and multi layer RNNs 83']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9635808261014066,1.0,0.9566093320008846,0.05
350,What is the difference between traditional language models and conditioned language models?,"Traditional language models are based on predicting the next word in a sequence solely based on the previous words, without any additional conditioning. On the other hand, conditioned language models take into account additional information or conditions when generating the next word in the sequence, such as specific context or features related to the task at hand. This conditioning allows the model to generate more contextually relevant and accurate predictions compared to traditional language models.

Source: Language_Models_are_Few_Shot_Learners.txt","['For many purposes a pretrained multilingual model is more practical than a monolingual model since it avoids the need to build many 100 separate monolin gual models. And multilingual models can improve performance on low resourced languages by leveraging linguistic information from a similar language in the train ing data that happens to have more resources. Nonetheless when the number of languages grows very large multilingual models exhibit what has been called the curse of multilinguality Conneau et al. 2020 the performance on each language degrades compared to a model training on fewer languages. Another problem with multilingual models is that they have an accent grammatical structures in higher resource languages often English bleed into lower resource languages the vast amount of English language in training makes the model s representations for low resource languages slightly more English like Papadimitriou et al. 2023 .', 'They showed the advantages of ModiÔ¨Åed In terpolated Kneser Ney which became the standard baseline for n gram language modeling especially because they showed that caches and class based models pro vided only minor additional improvement. These papers are recommended for any reader with further interest in n gram language modeling. SRILM Stolcke 2002 and KenLM HeaÔ¨Åeld 2011 HeaÔ¨Åeld et al. 2013 are publicly available toolkits for building n gram language models. Modern language modeling is more commonly done with neural network lan guage models which solve the major problems with n grams the number of param eters increases exponentially as the n gram order increases and n grams have no way to generalize from training to test set. Neural language models instead project words into a continuous space in which words with similar contexts have similar representations. We ll introduce both feedforward language models Bengio et al.', 'We do submit to the test server on a small number of datasets SuperGLUE TriviaQA PiQa where we were able to make submission work and we submit only the 200B few shot results and report development set results for everything else. 3 Results In Figure 3.1 we display training curves for the 8 models described in Section 2. For this graph we also include 6 additional extra small models with as few as 100 000 parameters. As observed in KMH 20 language modeling performance follows a power law when making efÔ¨Åcient use of training compute. After extending this trend by two more orders of magnitude we observe only a slight if any departure from the power law. One might worry that these improvements in cross entropy loss come only from modeling spurious details of our training corpus. However we will see in the following sections that improvements in cross entropy loss lead to consistent performance gains across a broad spectrum of natural language tasks.', 'here are two examples of people acting brave please give a third example of bravery is often Figure 1.1 Language model meta learning. During unsupervised pre training a language model develops a broad set of skills and pattern recognition abilities. It then uses these abilities at inference time to rapidly adapt to or recognize the desired task. We use the term in context learning to describe the inner loop of this process which occurs within the forward pass upon each sequence. The sequences in this diagram are not intended to be representative of the data a model would see during pre training but are intended to show that there are sometimes repeated sub tasks embedded within a single sequence. 3Figure 1.2 Larger models make increasingly efÔ¨Åcient use of in context information. We show in context learning performance on a simple task requiring the model to remove random symbols from a word both with and without a natural language task description see Sec. 3.9.2 .']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999987736530204,0.6,0.9285923830390644,0.0416666666666666
351,How can LSTM networks be used in language modeling?,"LSTM networks can be used in language modeling by modeling the temporal interactions between words. In the context of language modeling, LSTM networks can be placed on top of the embeddings provided by previous layers to capture the sequential nature of the input text data. By using LSTM networks in language modeling, it becomes possible to capture long-term dependencies in the text data and generate more accurate predictions for the next word in a sequence.

Source: BI_DIRECTIONAL_ATTENTION_FLOW_FOR_MACHINE_COMPREHENSION","['Recap 69 Language Model A system that predicts the next word Recurrent Neural Network A family of neural networks that Take sequential input of any length Apply the same weights on each step Can optionally produce output on each step Recurrent Neural Network Language Model We ve shown that RNNs are a great way to build a LM despite some problems RNNs are also useful for much moreWhy should we care about Language Modeling 70 Language Modeling is a benchmark task that helps us measure our progress on predicting language use Language Modeling is a subcomponent of many NLP tasks especially those involving generating text or estimating the probability of text Predictive typing Speech recognition Handwriting recognition Spelling grammar correction Authorship identification Machine translation Summarization Dialogue etc. Everything else in NLP has now been rebuilt upon Language Modeling GPT 3 is an LMOther RNN ArchitecturesOther RNN uses RNNs can be used for sequence tagging e.g.', 'We report results with both the word level log likelihood WLL and the sentence level log likelihood SLL . We report with LMn performance of the networks trained from the language model embeddings Table 7 . Gen eralization performance is reported in per word accuracy PWA for POS and F1 score for other tasks. language models from the relatively fast training of the supervised networks. Once the language models are trained we can perform multiple experiments on the supervised networks in a rela tively short time. Note that our procedure is clearly linked to the semi supervised deep learning procedures of Hinton et al. 2006 Bengio et al. 2007 and Weston et al. 2008 . Table 8 clearly shows that this simple initialization signiÔ¨Åcantly boosts the generalization per formance of the supervised networks for each task. It is worth mentioning the larger language model led to even better performance. This suggests that we could still take advantage of even bigger unlabeled data sets.', '4.3 Training Language Models The language model network was trained by stochastic gradient minimization of the ranking crite rion 17 sampling a sentence word pair s w at each iteration. 16. However Klein and Manning 2002 describe a rare example of realistic unsupervised grammar induction using a cross entropy approach on binary branching parsing trees that is by forcing the system to generate a hierarchical representation. 2512NATURAL LANGUAGE PROCESSING ALMOST FROM SCRATCH Since training times for such large scale systems are counted in weeks it is not feasible to try many combinations of hyperparameters. It also makes sense to speed up the training time by initializing new networks with the embeddings computed by earlier networks. In particular we found it expedient to train a succession of networks using increasingly large dictionaries each network being initialized with the embeddings of the previous network. Successive dictionary sizes and switching times are chosen arbitrarily. Bengio et al. 2009 provides a more detailed discussion of this the as yet poorly understood curriculum process.', '3. Contextual Embedding Layer. We use a Long Short Term Memory Network LSTM Hochreiter Schmidhuber 1997 on top of the embeddings provided by the previous layers to model the temporal interactions between words. We place an LSTM in both directions and concatenate the outputs of the two LSTMs. Hence we obtain H R2d T from the context word vectors X and U R2d J from query word vectors Q. Note that each column vector of H and U is 2d dimensional because of the concatenation of the outputs of the forward and backward LSTMs each with d dimensional output. It is worth noting that the Ô¨Årst three layers of the model are computing features from the query and context at different levels of granularity akin to the multi stage feature computation of convolutional neural networks in the computer vision Ô¨Åeld. 4. Attention Flow Layer. Attention Ô¨Çow layer is responsible for linking and fusing information from the context and the query words.']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.0000000000000009,0.75,0.9516848412740676,0.2692307692307692
352,What are the methods of generation in conditioned language modeling?,"The methods of generation in conditioned language modeling include Sampling (trying to generate a random sentence according to the probability distribution) and Argmax (trying to generate the sentence with the highest probability). Ancestral Sampling involves randomly generating words one by one, which is an exact method for sampling from the distribution. Greedy Search involves picking the single highest probability word one by one, but it may not be exact for real problems as it tends to generate easy words first and prefers multiple common words to one rare word. Beam Search maintains several paths instead of picking just one high probability word. Model Ensembling involves combining predictions from multiple models to smooth over idiosyncrasies of individual models.

Source: lecture04_Conditioned_Generation","['file name: lecture04 Conditioned Generation. Hamidreza Mahyar mahyarh@mcmaster.ca Computational Natural Language Processing Conditioned GenerationLanguage Models Language models are generative models of text s P x The Malfoys said Hermione. Harry was watching him. He looked like Madame Maxime. When she strode up the wrong staircase to visit himself. I m afraid I ve definitely been suspended from power no chance indeed said Snape. He put his head back behind them and read groups as they crossed a corner and fluttered down onto their ink lamp and picked up his spoon. The doorbell rang. It was a lot cleaner down in London.', 'Instruction finetuning 3. Reinforcement Learning from Human Feedback RLHF 4. What s nextLecture Plan From Language Models to Assistants 13 1. Zero Shot ZS and Few Shot FS In Context Learning 2. Instruction finetuning 3. Reinforcement Learning from Human Feedback RLHF 4. What s nextLet s revisit the Generative Pretrained Transformer GPT models from OpenAI as an example Emergent abilities of large language models GPT 2018 GPT 117M parameters Radford et al. 2018 Transformer decoder with 12 layers. Trained on BooksCorpus over 7000 unique books 4.6GB text . Showed that language modeling at scale can be an effective pretraining technique for downstream tasks like natural language inference.', '2014 encoder decoder Transform can be different dimensions encoder transform decoder encoder Input at every time step Kalchbrenner Blunsom 2013 decoder decoder decoderMethods of GenerationThe Generation Problem We have a model of P Y X how do we use it to generate a sentence Two methods Sampling Try to generate a random sentence according to the probability distribution. Argmax Try to generate the sentence with the highest probability.Ancestral Sampling Randomly generate words one by one. An exact method for sampling from P X no further work needed.Greedy Search One by one pick the single highest probability word Not exact real problems Will often generate the easy words first Will prefer multiple common words to one rare wordBeam Search Instead of picking one high probability word maintain several paths Some in reading materials more in a later classModel EnsemblingEnsembling Why Multiple models make somewhat uncorrelated errors Models tend to be more uncertain when they are about to make errors Smooths over idiosyncrasies of the model LSTM1 predict1 I LSTM2 predict2 Combine predictions from multiple models s sLinear Interpolation Take', '4.3 Training Language Models The language model network was trained by stochastic gradient minimization of the ranking crite rion 17 sampling a sentence word pair s w at each iteration. 16. However Klein and Manning 2002 describe a rare example of realistic unsupervised grammar induction using a cross entropy approach on binary branching parsing trees that is by forcing the system to generate a hierarchical representation. 2512NATURAL LANGUAGE PROCESSING ALMOST FROM SCRATCH Since training times for such large scale systems are counted in weeks it is not feasible to try many combinations of hyperparameters. It also makes sense to speed up the training time by initializing new networks with the embeddings computed by earlier networks. In particular we found it expedient to train a succession of networks using increasingly large dictionaries each network being initialized with the embeddings of the previous network. Successive dictionary sizes and switching times are chosen arbitrarily. Bengio et al. 2009 provides a more detailed discussion of this the as yet poorly understood curriculum process.']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999976978441985,1.0,0.919888543440722,0.0344827586206896
353,What is beam search and how is it used in language generation?,"Beam search is a technique used in language generation to explore multiple potential sequences of words simultaneously. It involves keeping track of the top k candidate sequences at each step and expanding them further until a certain stopping criteria is met. Beam search is more efficient than exhaustive search but is not guaranteed to find the optimal solution. It helps in generating more coherent and context-aware responses by considering multiple possibilities at each step.

Source: lecture05_Seq2Seq","['top k on each step Beam search is not guaranteed to find optimal solution But much more efficient than exhaustive search 18Beam search decoding example Beam size k 2. Blue numbers START Calculate prob dist of next word 19Beam search decoding example Beam size k 2. Blue numbers START he I 0.7 log PLM he START 0.9 log PLM I START Take top k words and compute scores 20Beam search decoding example Beam size k 2. Blue numbers hit struck was got START he I 10 0.7 0.9 2.9 log PLM struck START he 0.7 1.6 log PLM was START I 0.9 1.8 log PLM got START I 0.9 1.7 log PLM hit START he 0.7 For each of the k hypotheses find top k next words and calculate scoresBeam search decoding example Beam size k 2.', 'Natural Language Understanding NLU Employ models like BERT for NLU tasks to accurately parse student inquiries determining their intent and extracting relevant details for precise responses. Retrieval Augmented Generation RAG Central to the NLP Engine RAG com bines retrieval methods to access information within a structured database of course materials with generative capabilities to formulate coherent and context aware respon ses . This approach ensures the AI TA delivers informative answers directly pulling from course content and synthesizing it into helpful responses. Response Generation Utilize the designed AI TA to generate responses that are not only accurate but also engaging and pedagogically effective. Interaction Interface Simplified Interface Development Construct an intuitive web or command line in terface that allows straightforward question submission and response retrieval ensuring ease of use for student interactions with the AI TA. McMaster University 1McMaster University SEP 775 Project 2 TV Show Script Generation Project Overview For this project you will harness the power of Natural Language Processing NLP and Machine Learning to create a script for a new episode of a TV show you enjoy.', '2014 encoder decoder Transform can be different dimensions encoder transform decoder encoder Input at every time step Kalchbrenner Blunsom 2013 decoder decoder decoderMethods of GenerationThe Generation Problem We have a model of P Y X how do we use it to generate a sentence Two methods Sampling Try to generate a random sentence according to the probability distribution. Argmax Try to generate the sentence with the highest probability.Ancestral Sampling Randomly generate words one by one. An exact method for sampling from P X no further work needed.Greedy Search One by one pick the single highest probability word Not exact real problems Will often generate the easy words first Will prefer multiple common words to one rare wordBeam Search Instead of picking one high probability word maintain several paths Some in reading materials more in a later classModel EnsemblingEnsembling Why Multiple models make somewhat uncorrelated errors Models tend to be more uncertain when they are about to make errors Smooths over idiosyncrasies of the model LSTM1 predict1 I LSTM2 predict2 Combine predictions from multiple models s sLinear Interpolation Take', 'On tasks that involve binary classiÔ¨Åcation we give the options more semantically meaningful names e.g. True or False rather than 0 or 1 and then treat the task like multiple choice we also sometimes frame the task similar to what is done by RSR 19 see Appendix G for details. On tasks with free form completion we use beam search with the same parameters as RSR 19 a beam width of 4 and a length penalty of Œ± 0.6. We score the model using F1 similarity score BLEU or exact match depending on what is standard for the dataset at hand. Final results are reported on the test set when publicly available for each model size and learning setting zero one and few shot . When the test set is private our model is often too large to Ô¨Åt on the test server so we report results on the development set.']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9218209045611296,1.0,0.9264503241266728,0.0555555555555555
354,Explain the concept of model ensembling in language models.,"Model ensembling in language models refers to the technique of combining multiple individual models to improve overall performance. By aggregating predictions from different models, ensembling can help reduce errors and enhance the robustness of the final model. This approach is commonly used in machine learning to boost accuracy and generalization by leveraging the diverse strengths of each individual model in the ensemble.

Source: Lecture notes on Computational Natural Language Processing.","['This approach includes ALBERT LCG 19 as well as general HVD15 and 39task speciÔ¨Åc SDCW19 JYS 19 KR16 approaches to distillation of language models. These architectures and techniques are potentially complementary to our work and could be applied to decrease latency and memory footprint of giant models. As Ô¨Åne tuned language models have neared human performance on many standard benchmark tasks considerable effort has been devoted to constructing more difÔ¨Åcult or open ended tasks including question answering KPR 19 IBGC 14 CCE 18 MCKS18 reading comprehension CHI 18 RCM19 and adversarially constructed datasets designed to be difÔ¨Åcult for existing language models SBBC19 NWD 19 . In this work we test our models on many of these datasets. Many previous efforts have focused speciÔ¨Åcally on question answering which constitutes a signiÔ¨Åcant fraction of the tasks we tested on.', 'Nonetheless because perplexity often correlates with such improvements it is commonly used as a quick check on an algorithm. But a model s improvement in perplexity should always be conÔ¨Årmed by an end to end evaluation of a real task before concluding the evaluation of the model. 3.3 Sampling sentences from a language model One important way to visualize what kind of knowledge a language model embodies is to sample from it. Sampling from a distribution means to choose random points sampling according to their likelihood. Thus sampling from a language model which rep resents a distribution over sentences means to generate some sentences choosing each sentence according to its likelihood as deÔ¨Åned by the model. Thus we are more likely to generate sentences that the model thinks have a high probability and less likely to generate sentences that the model thinks have a low probability. This technique of visualizing a language model by sampling was Ô¨Årst suggested very early on by Shannon 1951 and Miller and Selfridge 1950 . It s simplest to visualize how this works for the unigram case.', 'Below we evaluate the 8 models described in Section 2 the 175 billion parameter parameter GPT 3 and 7 smaller models on a wide range of datasets. We group the datasets into 9 categories representing roughly similar tasks. In Section 3.1 we evaluate on traditional language modeling tasks and tasks that are similar to language modeling such as Cloze tasks and sentence paragraph completion tasks. In Section 3.2 we evaluate on closed book question answering tasks tasks which require using the information stored in the model s parameters to answer general knowledge questions. In Section 3.3 we evaluate the model s ability to translate between languages especially one shot and few shot . In Section 3.4 we evaluate the model s performance on Winograd Schema like tasks. In Section 3.5 we evaluate on datasets that involve commonsense reasoning or question answering.', 'As for limitations of our study there is a degree of subjectivity in assigning tasks to clusters though we try to use accepted categorizations in the literature and we only explore the use of relatively short instructions of typically a single sentence c.f. detailed instructions given to crowd workers . A limitation for our evaluation is that individual examples might have appeared in the models pretraining data which includes web documents though in post hoc analysis Appendix C we do not Ô¨Ånd any evidence that data overlap substantially impacted the results. Finally the scale of FLAN 137B makes it costly to serve. Future work on instruction tuning could include gathering generating even more task clusters for Ô¨Ånetuning cross lingual experiments using FLAN to generate data for training downstream classiÔ¨Åers and using Ô¨Ånetuning to improve model behavior with respect to bias and fairness Solaiman Dennison 2021 . 7 CONCLUSIONS This paper has explored a simple method for improving the ability of language models at scale to perform zero shot tasks based purely on instructions.']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9582728788513124,0.0,0.9266841205450672,0.0
355,How does ensemble distillation differ from parameter averaging?,"Ensemble distillation involves training a model to copy the ensemble by matching the description over predicted words, aiming to make the same mistakes as an ensemble to increase accuracy notably. On the other hand, parameter averaging is a method where models are written out several times near the end of training, and the average of parameters is taken. This is a cheaper way to achieve some benefits of ensembling without the need to use multiple models at test time. 

Source: SEP775 - Computational Natural Language Processing, lecture04_Conditioned_Generation.","['e.g. Kim et al. 2016 Problem parameter averaging only works for models within the same run Knowledge distillation trains a model to copy the ensemble Specifically it tries to match the description over predicted words Why We want the model to make the same mistakes as an ensemble Shown to increase accuracy notablyStacking What if we have two very different models where prediction of outputs is done in very different ways e.g. a phrase based translation model and a neural MT model Niehues et al. 2017 Stacking uses the output of one system in calculating features for another systemCase Studies in Conditional Language ModelingFrom Images e.g. Karpathy et al. 2015 Input is image features output is text Use standard image encoders e.g.', 'predictions from multiple models s sLinear Interpolation Take a weighted average of the M model probabilities Second term often set to uniform distribution 1 MLog linear Interpolation Weighted combination of log probabilities normalize Interpolation coefficient often set to uniform distribution 1 MLinear or Log Linear Think of it in logic Linear Logical OR the interpolated model likes any choice that a model gives a high probability use models with models that capture different traits necessary when any model can assign zero probability Log Linear Logical AND interpolated model only likes choices where all models agree use when you want to restrict possible answersParameter Averaging Problem Ensembling means we have to use M models at test time increasing our time memory complexity Parameter averaging is a cheap way to get some good effects of ensembling Basically write out models several times near the end of training and take the average of parametersEnsemble Distillation e.g. Kim et al.', 'Yet it is often difÔ¨Åcult to Ô¨Ågure out which ideas are most responsible for the state of the art performance of a large ensemble. Because neural networks are nonconvex training runs with different initial parameters usually give different solutions. Table 11 reports results obtained for the CHUNK and NER task after ten training runs with random initial parameters. Voting the ten network outputs on a per tag basis voting ensemble leads to a small improvement over the average network performance. We have also tried a more sophisticated ensemble approach the ten network output scores before sentence level likelihood were combined with an additional linear layer 4 and then fed to a new sentence level likelihood 13 . The parameters of the combining layers were then trained on the existing training set while keeping the ten networks Ô¨Åxed joined ensemble . This approach did not improve on simple voting. These ensembles come of course at the expense of a ten fold increase of the running time. On the other hand multiple training times could be improved using smart sampling strategies Neal 1996 .', 'This difference indicates the potential gain of tuning a given hyperparameter as well as the risks of decreased performance when not tuning it. For example an entry of 9.2 in Table d means that the best model with cds 0.75 is 9.2 more accurate absolute than the best model with cds 1 i.e. on MSR s analogies using cds 0.75 instead of cds 1 improved PPMI s accuracy from .443 to .535. 223Acknowledgements This work was supported by the Google Research Award Program and the German Research Foun dation via the German Israeli Project Cooperation grant DA 1600 1 1 . We thank Marco Baroni and Jeffrey Pennington for their valuable comments. References Eneko Agirre Enrique Alfonseca Keith Hall Jana Kravalova Marius Pasca and Aitor Soroa. 2009. A study on similarity and relatedness using distribu tional and wordnet based approaches.']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9030353249240848,0.5,0.9678949646034108,0.0555555555555555
356,What are some common evaluation metrics for conditioned language models?,"Some common evaluation metrics for conditioned language models include BLEU, embedding-based metrics like BertScore, BLEURT, COMET, PRISM, and Perplexity. These metrics are used to evaluate the performance of conditioned language models in tasks such as machine translation and text generation.

Source: lecture04_Conditioned_Generation","['CNN Often pre trained on large databases such as ImageNetCheck out this review paperHow do we EvaluateBasic Evaluation Paradigm Use parallel test set Use system to generate translations Compare target translations w referenceHuman Evaluation Ask a human to do evaluation Final goal but slow expensive and sometimes inconsistentBLEU Works by comparing n gram overlap w reference Pros Easy to use good for measuring system improvement Cons Often doesn t match human eval bad for comparing very different systemsEmbedding based Metrics Recently many metrics based on neural models BertScore Find similarity between BERT embeddings unsupervised Zhang et al. 2020 BLEURT Train BERT to predict human evaluation scores Sellam et al. 2020 COMET Train model to predict human eval also using source sentence Rei et al. 2020 PRISM Model based on training paraphrasing model Thompson and Post 2020Perplexity Calculate the perplexity of the words in the held out set without doing generation Pros Naturally solves multiple reference problem Cons Doesn t consider decoding or actually generating output.', '8 Conclusion We presented a 175 billion parameter language model which shows strong performance on many NLP tasks and benchmarks in the zero shot one shot and few shot settings in some cases nearly matching the performance of 40state of the art Ô¨Åne tuned systems as well as generating high quality samples and strong qualitative performance at tasks deÔ¨Åned on the Ô¨Çy. We documented roughly predictable trends of scaling in performance without using Ô¨Åne tuning. We also discussed the social impacts of this class of model. Despite many limitations and weaknesses these results suggest that very large language models may be an important ingredient in the development of adaptable general language systems. Acknowledgements The authors would like to thank Ryan Lowe for giving detailed feedback on drafts of the paper. Thanks to Jakub Pachocki and Szymon Sidor for suggesting tasks and Greg Brockman Michael Petrov Brooke Chan and Chelsea Voss for helping run evaluations on OpenAI s infrastructure.', 'Thus our MT evaluation system requires two ingredients 1. a numerical translation closeness metric 2. a corpus of good quality human reference trans lations We fashion our closeness metric after the highly suc cessful word error rate metric used by the speech recognition community appropriately modiÔ¨Åed for multiple reference translations and allowing for le gitimate differences in word choice and word or der. The main idea is to use a weighted average of variable length phrase matches against the reference translations. This view gives rise to a family of met rics using various weighting schemes. We have se lected a promising baseline metric from this family. In Section 2 we describe the baseline metric in detail. In Section 3 we evaluate the performance of BLEU. In Section 4 we describe a human evaluation experiment. In Section 5 we compare our baseline metric performance with human evaluations. Computational Linguistics ACL Philadelphia July 2002 pp. 311 318.', 'Extrinsic evaluation is the only way to extrinsic evaluation know if a particular improvement in a component is really going to help the task at hand. Thus for speech recognition we can compare the performance of two language models by running the speech recognizer twice once with each language model and seeing which gives the more accurate transcription. Unfortunately running big NLP systems end to end is often very expensive. In stead it would be nice to have a metric that can be used to quickly evaluate potential improvements in a language model. An intrinsic evaluation metric is one that mea intrinsic evaluation sures the quality of a model independent of any application. For an intrinsic evaluation of a language model we need a test set. As with many of the statistical models in our Ô¨Åeld the probabilities of an n gram model come from the corpus it is trained on the training set or training corpus. We can then measure training set the quality of an n gram model by its performance on some unseen data called the test set or test corpus.']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.0000000000000002,0.4285714285714285,0.918986880440913,0.0384615384615384
357,What is the significance of stacking in model ensembling?,"Stacking is significant in model ensembling because it allows for combining predictions from multiple models that may be very different in terms of how they predict outputs. By using the output of one system to calculate features for another system, stacking can improve accuracy notably by leveraging the strengths of different models and capturing different traits. This approach is particularly useful when dealing with models that have varying prediction methods and can lead to better overall performance in ensembling.","['e.g. Kim et al. 2016 Problem parameter averaging only works for models within the same run Knowledge distillation trains a model to copy the ensemble Specifically it tries to match the description over predicted words Why We want the model to make the same mistakes as an ensemble Shown to increase accuracy notablyStacking What if we have two very different models where prediction of outputs is done in very different ways e.g. a phrase based translation model and a neural MT model Niehues et al. 2017 Stacking uses the output of one system in calculating features for another systemCase Studies in Conditional Language ModelingFrom Images e.g. Karpathy et al. 2015 Input is image features output is text Use standard image encoders e.g.', 'predictions from multiple models s sLinear Interpolation Take a weighted average of the M model probabilities Second term often set to uniform distribution 1 MLog linear Interpolation Weighted combination of log probabilities normalize Interpolation coefficient often set to uniform distribution 1 MLinear or Log Linear Think of it in logic Linear Logical OR the interpolated model likes any choice that a model gives a high probability use models with models that capture different traits necessary when any model can assign zero probability Log Linear Logical AND interpolated model only likes choices where all models agree use when you want to restrict possible answersParameter Averaging Problem Ensembling means we have to use M models at test time increasing our time memory complexity Parameter averaging is a cheap way to get some good effects of ensembling Basically write out models several times near the end of training and take the average of parametersEnsemble Distillation e.g. Kim et al.', '4.3 ROLE OF INSTRUCTIONS 20 30 40 50 60 FT no instruction Eval instruction FT dataset name Eval instruction FT dataset name Eval dataset name FT instruction Eval instruction FLAN 37.3 46.6 47.0 55.2 Zero shot performance 4 task cluster avg. Figure 8 Ablation study result using mod els with instructions removed from Ô¨Ånetun ing FT . In a Ô¨Ånal ablation study we explore the role of in structions during Ô¨Ånetuning as one possibility is that performance gains come entirely from multi task Ô¨Åne tuning and the model could perform just as well without instructions. We hence consider two Ô¨Ånetuning setups without instructions. In a no template setup only inputs and outputs were given to the model e.g. for transla tion the input would be The dog runs. and the output would be Le chien court. .', 'Figure 7 shows these results. We see that for the two models on the order of 100B pa rameters instruction tuning substantially im proves performance on held out tasks as is expected given the prior results in our pa per. The behavior on held out tasks for the 8B and smaller models however is thought provoking instruction tuning actually hurts performance on held out tasks. One potential explana tion for this result could be that for small scale models learning the 40 tasks used during instruction tuning Ô¨Ålls the entire model capacity causing these models to perform worse on new tasks. Under this potential explanation for the larger scale models instruction tuning Ô¨Ålls up some model capacity but also teaches these models how to follow instructions allowing them to generalize to new tasks with the remaining capacity.']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9829394607477268,1.0,0.9296690027617898,0.0714285714285714
358,How are images used as input for conditioned language models?,"Images can be used as input for conditioned language models in tasks combining modalities such as language-driven editing of images. This involves using the image information as a conditioning factor for the language model to generate or modify text based on the content of the images. Additionally, there is a goal to move beyond still images to videos in the context of model-based reinforcement learning.

Source: Final_Project\data\txt_from_pdf_with_file_name\Image_Transformer.txt","['In future work we would like to explore a broader variety of conditioning information including free form text as previously proposed Mansimov et al. 2015 and tasks combining modalities such as language driven editing of images. Fundamentally we aim to move beyond still images to video Kalchbrenner et al. 2016 and towards applications in model based reinforcement learning. References Ba Jimmy Lei Kiros Jamie Ryan and Hinton Geoffrey E. Layer normalization. arXiv preprint arXiv 1607.06450 2016. Bellemare Marc G. Srinivasan Sriram Ostrovski Georg Schaul Tom Saxton David and Munos R emi. Unifying count based exploration and intrinsic motivation. CoRR abs 1606.01868 2016. URL http arxiv.org abs 1606.01868. Bengio Yoshua and Bengio Samy. Modeling high dimensional discrete data with multi layer neural net works. In Neural Information Processing Systems pp. 400 406.', 'We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local restricted attention mechanisms to efficiently handle large inputs and outputs such as images audio and video. Making generation less sequential is another research goals of ours. The code we used to train and evaluate our models is available at https github.com tensorflow tensor2tensor. Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments corrections and inspiration. References 1 Jimmy Lei Ba Jamie Ryan Kiros and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv 1607.06450 2016. 2 Dzmitry Bahdanau Kyunghyun Cho and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. CoRR abs 1409.0473 2014. 3 Denny Britz Anna Goldie Minh Thang Luong and Quoc V. Le. Massive exploration of neural machine translation architectures.', 'Instruction finetuning 3. Reinforcement Learning from Human Feedback RLHF 4. What s nextLecture Plan From Language Models to Assistants 13 1. Zero Shot ZS and Few Shot FS In Context Learning 2. Instruction finetuning 3. Reinforcement Learning from Human Feedback RLHF 4. What s nextLet s revisit the Generative Pretrained Transformer GPT models from OpenAI as an example Emergent abilities of large language models GPT 2018 GPT 117M parameters Radford et al. 2018 Transformer decoder with 12 layers. Trained on BooksCorpus over 7000 unique books 4.6GB text . Showed that language modeling at scale can be an effective pretraining technique for downstream tasks like natural language inference.', '. . . . . . . . . . 39 7 Related Work 39 8 Conclusion 40 A Details of Common Crawl Filtering 43 B Details of Model Training 43 C Details of Test Set Contamination Studies 43 D Total Compute Used to Train Language Models 46 E Human Quality Assessment of Synthetic News Articles 46 F Additional Samples from GPT 3 48 G Details of Task Phrasing and SpeciÔ¨Åcations 50 H Results on All Tasks for All Model Sizes 63 21 Introduction Recent years have featured a trend towards pre trained language representations in NLP systems applied in increasingly Ô¨Çexible and task agnostic ways for downstream transfer.']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.954500832431075,1.0,0.8744325306557341,0.0277777777777777
359,What challenges exist in evaluating conditioned language models?,"One challenge in evaluating conditioned language models is the difficulty in collecting high-quality human demonstrations for supervised baselines due to the time and effort required. This makes it expensive to ensure the quality of the training set and limits the ability to collect equivalent baselines for comparison. Additionally, evaluating the quality of model outputs for tasks where humans cannot easily assess the performance poses a challenge in scaling human feedback effectively.

Source: Language Models are Few Shot Learners","['Our data collection procedure is also expensive compared to prior work the training set took thousands of labeler hours and required signiÔ¨Åcant researcher time to ensure quality. For this reason we were unable to collect baselines such as an equivalent amount of high quality human demonstrations for supervised baselines. See D for more discussion. We leave this ablation to future work. Nevertheless we believe reward modeling is more likely to scale to tasks where it is extremely skill intensive or time consuming to provide good demonstrations. Future directions. The methods in this paper could be applied to any task where humans can compare samples including dialogue machine translation question answering speech synthesis and music generation. We expect this method to be particularly important for generating long samples where the distributional shift and degeneracy of maximum likelihood samples can be problematic. It may be possible to improve sample efÔ¨Åciency by training to predict feedback across many tasks 42 . We are particularly interested in scaling human feedback to tasks where humans can t easily evaluate the quality of model outputs.', '8 Conclusion We presented a 175 billion parameter language model which shows strong performance on many NLP tasks and benchmarks in the zero shot one shot and few shot settings in some cases nearly matching the performance of 40state of the art Ô¨Åne tuned systems as well as generating high quality samples and strong qualitative performance at tasks deÔ¨Åned on the Ô¨Çy. We documented roughly predictable trends of scaling in performance without using Ô¨Åne tuning. We also discussed the social impacts of this class of model. Despite many limitations and weaknesses these results suggest that very large language models may be an important ingredient in the development of adaptable general language systems. Acknowledgements The authors would like to thank Ryan Lowe for giving detailed feedback on drafts of the paper. Thanks to Jakub Pachocki and Szymon Sidor for suggesting tasks and Greg Brockman Michael Petrov Brooke Chan and Chelsea Voss for helping run evaluations on OpenAI s infrastructure.', '. . . . . . . . . . 39 7 Related Work 39 8 Conclusion 40 A Details of Common Crawl Filtering 43 B Details of Model Training 43 C Details of Test Set Contamination Studies 43 D Total Compute Used to Train Language Models 46 E Human Quality Assessment of Synthetic News Articles 46 F Additional Samples from GPT 3 48 G Details of Task Phrasing and SpeciÔ¨Åcations 50 H Results on All Tasks for All Model Sizes 63 21 Introduction Recent years have featured a trend towards pre trained language representations in NLP systems applied in increasingly Ô¨Çexible and task agnostic ways for downstream transfer.', 'By contrast humans can generally perform a new language task from only a few examples or from simple instructions something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task agnostic few shot performance sometimes even reaching competitiveness with prior state of the art Ô¨Åne tuning approaches. SpeciÔ¨Åcally we train GPT 3 an autoregressive language model with 175 billion parameters 10x more than any previous non sparse language model and test its performance in the few shot setting. For all tasks GPT 3 is applied without any gradient updates or Ô¨Åne tuning with tasks and few shot demonstrations speciÔ¨Åed purely via text interaction with the model. GPT 3 achieves strong performance on many NLP datasets including translation question answering and cloze tasks as well as several tasks that require on the Ô¨Çy reasoning or domain adaptation such as unscrambling words using a novel word in a sentence or performing 3 digit arithmetic.']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9853673299566266,1.0,0.877252288015203,0.0
360,What is the instructor's name for this course?,"The instructor's name for this course is Hamidreza Mahyar. 
Source: NLP Winter 2024 Course Outline.","['COURSE SPECIFICS Course Description This course covers some fundamental concepts in natural language processing NLP and how they apply in real world use cases. Instruction Type Code Type Hours per term C Classroom instruction 39 L Laboratory workshop or fieldwork T Tutorial DE Distance education Total Hours 39 Resources ISBN Textbook Title Edition Author Publisher Natural Language Processing Jacob Eisenstein Speech and Language Processing Dan Jurafsky and James H. Martin Deep Learning Ian Goodfellow Yoshua Bengio and Aaron Courville Prerequisite s Proficiency in Python programming language Some Machine Learning and or Deep Learning knowledge is a plus Corequisite s Antirequisite s Course Specific Policies All work must be shown to get full credit. Specific policy of Projects Assignments submissionsProjects Assignments will be conducted online by using related software. Completed Projects Assignments should be uploaded to the drop box before midnight of the due date. Projects Assignments missed due to legitimate reasons must be completed later mutually agreed with the instructor.', 'file name: NLP Winter 2024 Course Outline. Course Outline 1. COURSE INFORMATION Session Offered Winter 2024 Course Name Introduction to Computational Natural Language Processing Course Code SEP 775 Date s and Time s of lectures Lectures In person Wednesday 3 30 pm 6 30 pm Program Name System and Technology Calendar Description This graduate course introduces fundamental concepts in computational natural language processing NLP and their applications to teaching students how to deal with textual data in Artificial Intelligence. This course demonstrates how machines can learn different tasks in natural language such as language modeling text generation machine translation and language understanding. In this regard we go over the most promising methods in this literature and the most recent state of the art techniques. Moreover this course explores different real world applications of NLP and helps students get hands on experience in this field. Instructor Hamidreza Mahyar E Mail mahyarh@mcmaster.ca Office Hours upon request 2.', 'file name: lecture01 wordvecs1. Hamidreza Mahyar mahyarh@mcmaster.ca Computational Natural Language Processing Overview of NLP and Word VectorsLecture Plan 2 Lecture 1 Introduction and Word Vectors 1. The course 2. Human language and word meaning 3. Word2vec introduction 4. Word2vec objective function gradients 5. Optimization basics 6. Looking at word vectors Key learning today The astounding result that word meaning can be represented rather well by a high dimensional vector of real numbersCourse logistics in brief 3 Instructor Hamidreza Mahyar Head TA Reza Namazi Time Wednesday 3 30pm 6 30pm We put a lot of other important information on Avenue to Learn. Please read itWhat do we hope to teach A.k.a. learning goals 4 1.', 'Supervising strong learners by amplifying weak experts. arXiv preprint arXiv 1810.08575 2018. 10 P. F. Christiano J. Leike T. Brown M. Martic S. Legg and D. Amodei. Deep reinforcement learning from human preferences. In Advances in Neural Information Processing Systems pages 4299 4307 2017. 11 P. Covington J. Adams and E. Sargin. Deep neural networks for youtube recommendations. In Proceedings of the 10th ACM conference on recommender systems pages 191 198 2016. 12 A. M. Dai and Q. V. Le. Semi supervised sequence learning. In Advances in neural information processing systems pages 3079 3087 2015. 13 J. Dodge G. Ilharco R. Schwartz A. Farhadi H. Hajishirzi and N. Smith. Fine tuning pretrained language models Weight initializations data orders and early stopping.']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999999999999996,1.0,0.9449756284974552,0.037037037037037
361,What is the email address of the instructor for this course?,"The email address of the instructor for the course ""Computational Natural Language Processing"" is mahyarh@mcmaster.ca.

Source: file_path: C:\University Graduate\2024 Winter\SEP775_NLP\Final_Project\data\txt_from_pdf_with_file_name\lecture07_Prompting_Instruction_Finetuning_and_RLHF_.txt","['COURSE SPECIFICS Course Description This course covers some fundamental concepts in natural language processing NLP and how they apply in real world use cases. Instruction Type Code Type Hours per term C Classroom instruction 39 L Laboratory workshop or fieldwork T Tutorial DE Distance education Total Hours 39 Resources ISBN Textbook Title Edition Author Publisher Natural Language Processing Jacob Eisenstein Speech and Language Processing Dan Jurafsky and James H. Martin Deep Learning Ian Goodfellow Yoshua Bengio and Aaron Courville Prerequisite s Proficiency in Python programming language Some Machine Learning and or Deep Learning knowledge is a plus Corequisite s Antirequisite s Course Specific Policies All work must be shown to get full credit. Specific policy of Projects Assignments submissionsProjects Assignments will be conducted online by using related software. Completed Projects Assignments should be uploaded to the drop box before midnight of the due date. Projects Assignments missed due to legitimate reasons must be completed later mutually agreed with the instructor.', 'file name: lecture07 Prompting Instruction Finetuning and RLHF . Hamidreza Mahyar mahyarh@mcmaster.ca Computational Natural Language Processing Prompting Instruction Finetuning and RLHFReminders Project proposals will be posted this week. A2 due Saturday 11 59PM We still recommend using Colab for the assignments in case you run into trouble e.g. you have exceeded Colab quota Please contact us 2Larger and larger models 3 https www.economist.com interactive briefing 2022 06 11 huge foundation models are turbo charging ai progress GPT 4 has 1.7 trillion parametersTrained on more and more data tokens seen during training https babylm.github.io 4 GPT 4 has seen 13 trillion tokensRecap What kinds of things does pretraining learn 5 Stanford University is located in California. Trivia I put fork down on the table. syntax The woman walked across the street checking for traffic over shoulder.', '. Students should be aware that when they access the electronic components of a course using these elements private information such as first and last names user names for the McMaster e mail accounts and program affiliation may become apparent to all other students in the same course. The available information is dependent on the technology used. Continuation in a course that uses on line elements will be deemed consent to this disclosure. If you have any questions or concerns about such disclosure please discuss this with the course instructor. ONLINE PROCTORING Some courses may use online proctoring software for tests and exams. This software may require students to turn on their video camera present identification monitor and record their computer activities and or lock restrict their browser or other applications software during tests or exams. This software may be required to be installed before the test exam begins. COMMUNICATIONS It is the student s responsibility to Maintain current contact information with the University including address phone numbers and emergency contact information. Use the University provided e mail address or maintain a valid forwarding e mail address.', 'In this paper we address the question can we train a better dense embedding model using only pairs of questions and passages or answers with out additional pretraining By leveraging the now standard BERT pretrained model Devlin et al. 2019 and a dual encoder architecture Bromley et al. 1994 we focus on developing the right training scheme using a relatively small number of question and passage pairs. Through a series of careful ablation studies our Ô¨Ånal solution is surprisingly simple the embedding is optimized for maximizing inner products of the question and relevant passage vectors with an objective compar ing all pairs of questions and passages in a batch. Our Dense Passage Retriever DPR is exception ally strong. It not only outperforms BM25 by a large margin 65.2 vs. 42.9 in Top 5 accuracy but also results in a substantial improvement on the end to end QA accuracy compared to ORQA 41.5 vs. 33.3 in the open Natural Questions setting Lee et al.']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.920081643441838,1.0,0.9023392805709692,0.0
362,When is this course held every week?,"The course is held every week on Tuesdays from 7 to 9 in the morning. 
Source: NLP_Winter_2024_Course_Outline","['EXTREME CIRCUMSTANCES The University reserves the right to change the dates and deadlines for any or all courses in extreme circumstances e.g. severe weather labour disruptions etc. . Changes will be communicated through regular McMaster communication channels such as McMaster Daily News A2L and or McMaster email.', '3. SUB TOPIC S Week 1 Overview Human language Word meanings Word2Vec and Optimization basics Week 2 Intro to neural networks Gradients and backpropagation Week 3 Recurrent Neural Networks and Language Models Week 4 Conditioned Generation Week 5 Part 1 Self attention and transformers Part 2 Seq2Seq models Week 6 Midterm Test Week 7 Midterm Recess No class Week 8 Prompting Instruction Finetuning and RLHF Week 9 Question Answering systems Week 10 Multimodal models Week 11 Project presentation and Q A Week 12 Project presentation and Q A Week 13 Project presentation and Q A Classes end April 10th 4. ASSESSMENT OF LEARNING Weight 4 Assignments 20 4 Quizzes 10 Midterm Test 35 Group Project 35 TOTAL 100Note that the percentage distribution of each module may be adjusted. Percentage grades will be converted to letter grades and grade points per the University calendar. 5.', 'So the answer is 01 01 1900. QUESTION Jane scheduled 3 apointments with 5 poeple for tomorrow Tue 7 9 1972 . What is the date one week ago from today in MM DD YYYY MODEL ANSWER INCORRECT Tomorrow is 7 9 1972. One week ago from today is 7 2 1972. So the answer is 7 2 1972. Ground truth is 07 01 1972. QUESTION Today is the palindrome day of 2020 because the MMDDYYYY format of the date is the same backwards as forwards. What is the date tomorrow in MM DD YYYY MODEL ANSWER INCORRECT Today is 02 29 2020. Tomorrow will be 03 01 2020. So the answer is 03 01 2020. Ground truth is 02 03 2020 .', 'What is the date one week from today in MM DD YYYY A If 2015 is coming in 36 hours then it is coming in 2 days. 2 days before 01 01 2015 is 12 30 2014 so today is 12 30 2014. So one week from today will be 01 05 2015. So the answer is 01 05 2015. Q The Ô¨Årst day of 2019 is a Tuesday and today is the Ô¨Årst Monday of 2019. What is the date today in MM DD YYYY A If the Ô¨Årst day of 2019 was Tuesday then 01 01 2019 was a Tuesday. Today is the Ô¨Årst monday would be six days later. So today is 01 07 2019. So the answer is 01 07 2019.']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.8115038661564274,0.0,0.8892131785218493,0.037037037037037
363,What is the name of the TA for this course?,"The name of the TA for this course is the AI Teaching Assistant (AI TA). It is designed to provide an interactive platform for students to ask questions and receive guidance on course materials, mirroring the support traditionally offered by human teaching assistants. The AI TA aims to enhance the learning experience through dynamic interaction and content understanding using NLP techniques. 

Source: SEP 775 - Computational Natural Language Processing.","['file name: SEP775 FinalProjects 2. McMaster University SEP 775 Final Projects Project 1 AI Teaching Assistant Project Overview The goal of this project is to create an AI Teaching Assistant AI TA . The AI TA aims to provide an interactive platform for students to ask questions and receive guidance on course materials mirroring the support traditionally offered by human teaching assistants. The project emphasizes the application of NLP techniques to create an educational tool that enhances the learning experience through dynamic interaction and content understanding. Project Objectives Automate FAQ Responses Develop the AI TA to autonomously address frequently asked questions related to course logistics assignments and content. Content Understanding and Query Handling Implement NLP strategies enabling the AI TA to interpret and respond to inquiries about the uploaded course material accurately. Dynamic Interaction Design the AI TA to interact with students conversationally providing information and resources pertinent to their queries. Technical Approach NLP Engine Components Language Model Integration Leverage pre trained NLP models e.g. BERT Llama for robust text understanding and generation tapping into their vast linguistic knowledge.', 'Natural Language Understanding NLU Employ models like BERT for NLU tasks to accurately parse student inquiries determining their intent and extracting relevant details for precise responses. Retrieval Augmented Generation RAG Central to the NLP Engine RAG com bines retrieval methods to access information within a structured database of course materials with generative capabilities to formulate coherent and context aware respon ses . This approach ensures the AI TA delivers informative answers directly pulling from course content and synthesizing it into helpful responses. Response Generation Utilize the designed AI TA to generate responses that are not only accurate but also engaging and pedagogically effective. Interaction Interface Simplified Interface Development Construct an intuitive web or command line in terface that allows straightforward question submission and response retrieval ensuring ease of use for student interactions with the AI TA. McMaster University 1McMaster University SEP 775 Project 2 TV Show Script Generation Project Overview For this project you will harness the power of Natural Language Processing NLP and Machine Learning to create a script for a new episode of a TV show you enjoy.', 'Transactions of the Association of Compu tational Linguistics TACL . Kenton Lee Ming Wei Chang and Kristina Toutanova. 2019. Latent retrieval for weakly supervised open domain question answering. In Association for Com putational Linguistics ACL pages 6086 6096. Mike Lewis Yinhan Liu Naman Goyal Mar jan Ghazvininejad Abdelrahman Mohamed Omer Levy Veselin Stoyanov and Luke Zettlemoyer. 2020a. BART Denoising sequence to sequence pre training for natural language generation translation and comprehension. In Association for Computa tional Linguistics ACL pages 7871 7880. Patrick Lewis Ethan Perez Aleksandara Piktus Fabio Petroni Vladimir Karpukhin Naman Goyal Heinrich K uttler Mike Lewis Wen tau Yih Tim Rockt aschel Sebastian Riedel and Douwe Kiela. 2020b.', 'URL https arxiv.org abs 1806.08730. John McCarthy. Programs with common sense. RLE and MIT computation center 1960. URL http jmc.stanford.edu articles mcc59 mcc59.pdf. Todor Mihaylov Peter Clark Tushar Khot and Ashish Sabharwal. Can a suit of armor conduct electricity A new dataset for open book question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing pp. 2381 2391 2018. URL https aclanthology.org D18 1260. Sewon Min Mike Lewis Luke Zettlemoyer and Hannaneh Hajishirzi. Metaicl Learning to learn in context. arXiv preprint arXiv 2110.15943 2021. URL https arxiv.org abs 2110. 15943.']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.0000000000000002,1.0,0.8209838448491584,0.0256410256410256
364,What is Word2vec?,"Word2vec is a framework for learning word vectors where every word in a fixed vocabulary is represented by a vector. It calculates the probability of words appearing together in a context and adjusts word vectors to maximize this probability, aiming to capture word meanings and relationships in a vector space. The model uses techniques like skip-gram and softmax function to learn word embeddings efficiently. Word2vec is widely used in Natural Language Processing tasks for tasks like word similarity, language modeling, and more. 

Source: Lecture01_wordvecs1.txt, lecture01_notes_Introduction.txt","['107 0.109 0.542 0.349 0.271 monetary 0.413 0.582 0.007 0.247 0.216 0.718 0.147 0.051 19Word meaning as a neural word vector visualization 0.286 0.792 0.177 0.107 0.109 0.542 0.349 0.271 0.487 expect 203. Word2vec Overview 21 Word2vec Mikolov et al. 2013 is a framework for learning word vectors Idea We have a large corpus body of text a long list of words Every word in a fixed vocabulary is represented by a vector Go through each position t in the text which has a center word c and context outside words o Use the similarity of the word vectors for c and o to calculate the probability of o given c or vice versa Keep adjusting the word vectors to maximize this probabilityWord2Vec Overview Example windows and process for computing ùëÉùë§ùë°', 'However many of the details of word2vec will hold true in methods that we ll proceed to further in the course so we ll focus our time on that. 3.2 Word2vec model and objective The word2vec model represents each word in a fixed vocabulary as a low dimensional much smaller than vocabulary size vector. It learns the value of each word s vector to be predictive via a simple function of the distribution of words in a usually short 2 4 words context. The model we ll describe here is called the skipgram word2vec algo rithm. Skipgram word2vec. As usual we have a finite vocabulary V. Let C O be random variables representing an unknown pair of C V a center word and O V an outside word appearing in the context of the center word . We ll use c o to refer to specific values of the random variables.', 'Let U R V d and V R V d. Note that each word in V is associated with a single row of U and one of V we think of this as resulting from an arbitrary ordering of V. The word2vec model is a probabilistic model specified as follows where uw refers to the row of U corresponding to word w V and likewise for V pU V o c exp u o vc w V exp u wvc 4 This may be familiar to you as the softmax function which takes arbitrary scores here one for each word in the vocabulary resulting from dot products and produces a probability distribution where larger scored things get higher probability. Note that the vector of probabilities over all words given a center word pU V c R V is a lot like a row of our old co occurrence matrix Xc. The story isn t over yet this is just a model. How do we estimate the values of the parameters U V We learn to minimize the cross entropy loss objective with the true distribution P O C min U V Eo c log pU V o c .', 'Illustration Use specific word pairs or groups to demonstrate differences in repre sentation by each model. McMaster University 1McMaster University SEP 775 Resources for Word2Vec and GloVe Word2Vec Pre trained Word2Vec embeddings can be found on repositories such as HuggingFace or directly using Gensim. GloVe Pre trained GloVe embeddings are available at the GloVe website or through HuggingFace. Submission Guidelines Submit all code in a ZIP file including Jupyter Notebooks and a detailed PDF report with written explanations and visualizations. Clearly label each part and question in your submissions. Deadline Feb. 7 2024 Rubric and Expectations Code Quality and Functionality 40 Code should be well organized commented and functioning as intended. The use of Python and relevant libraries e.g. Gensim for Word2Vec should demonstrate a good grasp of the tools. Analysis and Interpretation 30 Written explanations should be insightful de monstrating a deep understanding of the word vector models.']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999999999999994,1.0,0.955092505113232,0.0416666666666666
365,What are the main learning goals of the course?,"The main learning goals of the course include understanding the foundations of effective modern methods for deep learning applied to NLP, gaining knowledge about key methods used in NLP in 2024 such as word vectors, feed forward networks, recurrent networks, attention, encoder-decoder models, transformers, and large pre-trained language models, developing a big picture understanding of human languages and the challenges in understanding and producing them using computers, and exploring question answering techniques in NLP such as reading comprehension and open-domain textual question answering. 

Source: lecture01_wordvecs1.txt","['COURSE SPECIFICS Course Description This course covers some fundamental concepts in natural language processing NLP and how they apply in real world use cases. Instruction Type Code Type Hours per term C Classroom instruction 39 L Laboratory workshop or fieldwork T Tutorial DE Distance education Total Hours 39 Resources ISBN Textbook Title Edition Author Publisher Natural Language Processing Jacob Eisenstein Speech and Language Processing Dan Jurafsky and James H. Martin Deep Learning Ian Goodfellow Yoshua Bengio and Aaron Courville Prerequisite s Proficiency in Python programming language Some Machine Learning and or Deep Learning knowledge is a plus Corequisite s Antirequisite s Course Specific Policies All work must be shown to get full credit. Specific policy of Projects Assignments submissionsProjects Assignments will be conducted online by using related software. Completed Projects Assignments should be uploaded to the drop box before midnight of the due date. Projects Assignments missed due to legitimate reasons must be completed later mutually agreed with the instructor.', 'learning goals 4 1. The foundations of the effective modern methods for deep learning applied to NLP Basics first then key methods used in NLP in 2024 Word vectors feed forward networks recurrent networks attention encoder decoder models transformers large pre trained language models etc. 2. A big picture understanding of human languages and the difficulties in understanding and producing them via computers 3.', 'file name: lecture08 Question Answering. Hamidreza Mahyar mahyarh@mcmaster.ca Computational Natural Language Processing Question Answering2 Lecture plan 1. What is question answering 2. Reading comprehension How to answer questions over a single passage of text 3. Open domain textual question answering How to answer questions over a large collection of documents Assignment 3 is out Start working on your final project1. What is question answering Answer A Question Q The goal of question answering is to build systems that automatically answer questions posed by humans in a natural language The earliest QA systems dated back to 1960s Simmons et al.', '7. Critical Discussion Although we believe that this contribution represents a step towards the NLP from scratch objec tive we are keenly aware that both our goal and our means can be criticized. The main criticism of our goal can be summarized as follows. Over the years the NLP com munity has developed a considerable expertise in engineering effective NLP features. Why should they forget this painfully acquired expertise and instead painfully acquire the skills required to train large neural networks As mentioned in our introduction we observe that no single NLP task really covers the goals of NLP. Therefore we believe that task speciÔ¨Åc engineering i.e. that does not gen eralize to other tasks is not desirable. But we also recognize how much our neural networks owe to previous NLP task speciÔ¨Åc research. The main criticism of our means is easier to address.']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999999999999996,0.25,0.9369392317739578,0.0625
366,How is the course grade distributed?,"The course grade is distributed as follows: 4 Assignments (20%), 4 Quizzes (10%), Midterm Test (35%), and Group Project (35%). Please note that the percentage distribution of each module may be adjusted, and percentage grades will be converted to letter grades and grade points per the University calendar. Classes end on April 10th.","['He Ô¨Årst deÔ¨Ånes a ranking criterion Harris 1968 Section 4.1 Starting for convenience with very short sentence forms say ABC we choose a particular word choice for all the classes say BqCq except one in this case A for every pair of members Ai A j of that word class we ask how the sentence formed with one of the members that is AiBqCq compares as to acceptability with the sentence formed with the other member that is A jBqCq. These gradings are then used to compare sentence forms It now turns out that given the graded n tuples of words for a particular sentence form we can Ô¨Ånd other sentences forms of the same word classes in which the same n tuples of words produce the same grading of sentences. This is an indication that these two sentence forms exploit common words with the same syntac tic function and possibly the same meaning. This observation forms the empirical basis for the construction of operator grammars that describe real world natural languages such as English.', '3. SUB TOPIC S Week 1 Overview Human language Word meanings Word2Vec and Optimization basics Week 2 Intro to neural networks Gradients and backpropagation Week 3 Recurrent Neural Networks and Language Models Week 4 Conditioned Generation Week 5 Part 1 Self attention and transformers Part 2 Seq2Seq models Week 6 Midterm Test Week 7 Midterm Recess No class Week 8 Prompting Instruction Finetuning and RLHF Week 9 Question Answering systems Week 10 Multimodal models Week 11 Project presentation and Q A Week 12 Project presentation and Q A Week 13 Project presentation and Q A Classes end April 10th 4. ASSESSMENT OF LEARNING Weight 4 Assignments 20 4 Quizzes 10 Midterm Test 35 Group Project 35 TOTAL 100Note that the percentage distribution of each module may be adjusted. Percentage grades will be converted to letter grades and grade points per the University calendar. 5.', '4 73.2 79.8 63.2 85.4 85.0 81.4 89.1 77.2 BM25 DPR 76.6 79.8 71.0 85.2 71.5 83.8 84.5 80.5 92.7 81.3 Multi DPR 79.4 78.8 75.0 89.1 51.6 86.0 84.7 82.9 93.9 67.6 BM25 DPR 78.0 79.9 74.7 88.5 66.2 83.9 84.4 82.3 94.1 78.6 Table 2 Top 20 Top 100 retrieval accuracy on test sets measured as the percentage of top 20 100 retrieved passages that contain the answer.', 'We used this classiÔ¨Åer to score Common Crawl documents. We kept each document in our dataset iff np.random.pareto Œ± 1 document_score We chose Œ± 9 in order to take mostly documents the classiÔ¨Åer scored highly but still include some documents that were out of distribution. Œ± was chosen to match the distribution of scores from our classiÔ¨Åer on WebText. We found this re weighting increased quality as measured by loss on a range of out of distribution generative text samples. 2. To further improve model quality and prevent overÔ¨Åtting which becomes increasingly important as model capacity increases we fuzzily deduplicated documents i.e. removed documents with high overlap with other documents within each dataset using Spark s MinHashLSH implementation with 10 hashes using the same features as were used for classiÔ¨Åcation above. We also fuzzily removed WebText from Common Crawl. Overall this decreased dataset size by an average of 10 .']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999989989598372,0.75,0.9574133772704568,0.0
367,What is the policy for late submissions in this course?,"The policy for late submissions in this course is that students should submit their request for accommodation to their Faculty Office within 10 working days of the beginning of the term or to the Registrar's Office prior to their examinations. They should also contact their instructors as soon as possible to make alternative arrangements for classes, assignments, and tests. Submissions failing to meet requirements may be penalized. Students are advised to ensure that all submitted materials are well organized and clearly labeled for easy navigation. Additionally, the University reserves the right to change dates and deadlines for courses in extreme circumstances, such as severe weather or labor disruptions, and will communicate any changes through McMaster's regular communication channels. 

Source: NLP_Winter_2024_Course_Outline.txt","['Students should submit their request to their Faculty Office normally within 10 working days of the beginning of term in which they anticipate a need for accommodation or to the Registrar s Office prior to their examinations. Students should also contact their instructors as soon as possible to make alternative arrangements for classes assignments and tests. http www.mcmaster.ca policy Students AcademicStudies Studentcode.pdf COPYRIGHT AND RECORDING Students are advised that lectures demonstrations performances and any other course material provided by an instructor include copyright protected works. The Copyright Act and copyright law protect every original literary dramatic musical and artistic work including lectures by University instructors The recording of lectures tutorials or other methods of instruction may occur during a course. Recording may be done by either the instructor for the purpose of authorized distribution or by a student for the purpose of personal study. Students should be aware that their voice and or image may be recorded by others during the class. Please speak with the instructor if this is a concern for you.', 'Submissions failing to meet these requirements may be penalized. Please ensure that all submitted materials are well organized and clearly labeled for easy navigation. McMaster University 7', '. Students should be aware that when they access the electronic components of a course using these elements private information such as first and last names user names for the McMaster e mail accounts and program affiliation may become apparent to all other students in the same course. The available information is dependent on the technology used. Continuation in a course that uses on line elements will be deemed consent to this disclosure. If you have any questions or concerns about such disclosure please discuss this with the course instructor. ONLINE PROCTORING Some courses may use online proctoring software for tests and exams. This software may require students to turn on their video camera present identification monitor and record their computer activities and or lock restrict their browser or other applications software during tests or exams. This software may be required to be installed before the test exam begins. COMMUNICATIONS It is the student s responsibility to Maintain current contact information with the University including address phone numbers and emergency contact information. Use the University provided e mail address or maintain a valid forwarding e mail address.', 'EXTREME CIRCUMSTANCES The University reserves the right to change the dates and deadlines for any or all courses in extreme circumstances e.g. severe weather labour disruptions etc. . Changes will be communicated through regular McMaster communication channels such as McMaster Daily News A2L and or McMaster email.']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999999999999996,1.0,0.8654705941434966,0.217391304347826
368,What does the course hope to teach regarding word vectors?,The course hopes to teach that word meaning can be represented rather well by a high dimensional vector of real numbers.,"['file name: lecture01 wordvecs1. Hamidreza Mahyar mahyarh@mcmaster.ca Computational Natural Language Processing Overview of NLP and Word VectorsLecture Plan 2 Lecture 1 Introduction and Word Vectors 1. The course 2. Human language and word meaning 3. Word2vec introduction 4. Word2vec objective function gradients 5. Optimization basics 6. Looking at word vectors Key learning today The astounding result that word meaning can be represented rather well by a high dimensional vector of real numbersCourse logistics in brief 3 Instructor Hamidreza Mahyar Head TA Reza Namazi Time Wednesday 3 30pm 6 30pm We put a lot of other important information on Avenue to Learn. Please read itWhat do we hope to teach A.k.a. learning goals 4 1.', 'in 2006 we would like a classiÔ¨Åed output Jim Person bought 300 shares of Acme Corp. Organization in 2006 Time. Figure 5 We can classify word vectors using simple linear decision boundaries such as the one shown here 2 D word vectors using techniques such as logistic regression and SVMs For such problems we typically begin with a training set of the form x i y i N 1 where x i is a d dimensional word vector generated by some word embedding technique and y i is a C dimensional one hot vector which indicates the labels we wish to eventually predict sentiments other words named entities buy sell decisions etc. . In typical machine learning tasks we usually hold input data and target labels Ô¨Åxed and train weights using optimization techniques such as gradient descent L BFGS Newton s method etc. . In NLP applications however we introduce the idea of retraining the input word vectors when we train for extrinsic tasks. Let us discuss when and why we should consider doing this.', 'Then we search in the vector space for the word closest to X measured by cosine distance and use it as the answer to the question we discard the input question words during this search . When the word vectors are well trained it is possible to Ô¨Ånd the correct answer word smallest using this method. Finally we found that when we train high dimensional word vectors on a large amount of data the resulting vectors can be used to answer very subtle semantic relationships between words such as a city and the country it belongs to e.g. France is to Paris as Germany is to Berlin. Word vectors with such semantic relationships could be used to improve many existing NLP applications such as machine translation information retrieval and question answering systems and may enable other future applications yet to be invented. 5Table 1 Examples of Ô¨Åve types of semantic and nine types of syntactic questions in the Semantic Syntactic Word Relationship test set.', 'There followed a great deal of exploration of methods for obtaining distributional word vectors. Some interesting ideas worth noting include When we wish to apply neural networks to problems in NLP see Figure 4 it s useful to Ô¨Årst map each input word token to its vector and then feed the word vectors into the neural network model which performs a task like translation. The vectors can be Ô¨Åxed in advance or pretrained from a corpus using methods like those above often executed by someone else or they can be treated as parameters of the neural network model and adapted to the task speciÔ¨Åcally e.g. Collobert et al. 2011 . Finetuning refers to initializing the word vectors by pretraining then adapting them through task speciÔ¨Åc learning algorithms. The word vectors can also be initialized to random values then estimated solely through task learning which we might call learning from scratch. 6 We can use expert built data structures like WordNet as additional input to creating word vectors.']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9224102403839364,1.0,0.964419553866905,0.0344827586206896
369,How does Word2vec represent word meaning?,"Word2vec represents word meaning by assigning each word in a fixed vocabulary a vector. These vectors are learned based on the similarity of word vectors for a center word and its context words. By maximizing the probability of context words given the center word (or vice versa), Word2vec adjusts the word vectors to capture the meaning of words based on their usage in context.","['107 0.109 0.542 0.349 0.271 monetary 0.413 0.582 0.007 0.247 0.216 0.718 0.147 0.051 19Word meaning as a neural word vector visualization 0.286 0.792 0.177 0.107 0.109 0.542 0.349 0.271 0.487 expect 203. Word2vec Overview 21 Word2vec Mikolov et al. 2013 is a framework for learning word vectors Idea We have a large corpus body of text a long list of words Every word in a fixed vocabulary is represented by a vector Go through each position t in the text which has a center word c and context outside words o Use the similarity of the word vectors for c and o to calculate the probability of o given c or vice versa Keep adjusting the word vectors to maximize this probabilityWord2Vec Overview Example windows and process for computing ùëÉùë§ùë°', 'However many of the details of word2vec will hold true in methods that we ll proceed to further in the course so we ll focus our time on that. 3.2 Word2vec model and objective The word2vec model represents each word in a fixed vocabulary as a low dimensional much smaller than vocabulary size vector. It learns the value of each word s vector to be predictive via a simple function of the distribution of words in a usually short 2 4 words context. The model we ll describe here is called the skipgram word2vec algo rithm. Skipgram word2vec. As usual we have a finite vocabulary V. Let C O be random variables representing an unknown pair of C V a center word and O V an outside word appearing in the context of the center word . We ll use c o to refer to specific values of the random variables.', 'Where static embeddings represent the meaning of word types vocabulary entries contex tual embeddings represent the meaning of word instances instances of a particular10 CHAPTER 11 FINE TUNING AND MASKED LANGUAGE MODELS Input Embeddings Transformer Blocks CLS So long and thanks for all z1 zCLS z2 z3 z4 z5 z6 Figure 11.6 The output of a BERT style model is a contextual embedding vector zi for each input token xi. word type in a particular context. Thus where word2vec had a single vector for each word type contextual embeddings provide a single vector for each instance of that word type in its sentential context. Contextual embeddings can thus be used for tasks like measuring the semantic similarity of two words in context and are useful in linguistic tasks that require models of word meaning. 11.3.1 Contextual Embeddings and Word Sense Words are ambiguous the same word can be used to mean different things.', 'The distributional hypothsis the meaning of a word can be derived from the distribution of contexts in which it appears. That s the high level. But as always the details matter. What does it mean for a word to be near another word Right next to it Two away In the same document How does one represent this encoding and learn it Let s go through some options. 3.1 Co occurrence matrices and document contexts If you were asked to code up the idea represent a word by the distribution of words it appears near you might immediately have the following idea 1. Determine a vocabulary V. 2. Make a matrix of size V V of zeros. 3. Walk through a sequence of documents. For each document for each word w in the document add all the counts of the other words w in the document to the row corresponding to w at the column corresponding to w . 4. Normalize the rows by the sum.']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999999999999986,1.0,0.929588974275819,0.0416666666666666
370,What is the main focus of this lecture on neural networks?,"The main focus of this lecture on neural networks is on introducing single and multilayer neural networks for classification purposes, discussing how they can be trained using backpropagation, and providing practical tips and tricks for training neural networks. Additionally, the lecture motivates the use of recurrent neural networks as a language model.","['file name: lecture02 notes Backpropagation.  Neural networks. Forward computation. Backward propagation. Neuron Units. Max margin Loss. Gradient checks. Xavier parameter initialization. Learning rates. Adagrad. This set of notes introduces single and multilayer neural networks and how they can be used for classiÔ¨Åcation purposes. We then dis cuss how they can be trained using a distributed gradient descent technique known as backpropagation. We will see how the chain rule can be used to make parameter updates sequentially. After a rigorous mathematical discussion of neural networks we will discuss some practical tips and tricks in training neural networks involving neuron units non linearities gradient checks Xavier parameter ini tialization learning rates Adagrad etc. Lastly we will motivate the use of recurrent neural networks as a language model. 1 Neural Networks Foundations Figure 1 We see here how a non linear decision boundary separates the data very well. This is the prowess of neural networks.', 'A few examples of these include hidden Markov models Baum and Petrie 1966 conditional random fields Lafferty et al. 2001 recurrent neural networks Rumelhart et al. 1985 convolutional neural networks LeCun et al. 1989 and support vector machines Cortes and Vapnik 1995 . In this section we ll discuss a bit about the neural modeling ap proaches we ve discussed in Cs 224n so far and how their limitations and changes in the world inspired the modern as of 2023 zeitgeist of self attention and Transformer based architectures. 1.1 Notation and basics Let w1 n be a sequence where each wi V a finite vocabulary. We ll also overload w1 n to be a matrix of one hot vectors w1 n Rn V .', 'file name: lecture02 Neural Nets. Hamidreza Mahyar mahyarh@mcmaster.ca Computational Natural Language Processing Neural Networks Gradients and BackpropagationBinary classification for center word being location We do supervised training and want high score if it s a location ùêΩùë°ùúÉ ùúéùë† 1 1 ùëí ùë† x xmuseums xin xParis xare xamazing predicted model probability of class f Some element wise non linear function e.g.', 'Neural GPUs learn algorithms. In International Conference on Learning Representations ICLR 2016. 18 Nal Kalchbrenner Lasse Espeholt Karen Simonyan Aaron van den Oord Alex Graves and Ko ray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv 1610.10099v2 2017. 19 Yoon Kim Carl Denton Luong Hoang and Alexander M. Rush. Structured attention networks. In International Conference on Learning Representations 2017. 20 Diederik Kingma and Jimmy Ba. Adam A method for stochastic optimization. In ICLR 2015. 21 Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint arXiv 1703.10722 2017. 22 Zhouhan Lin Minwei Feng Cicero Nogueira dos Santos Mo Yu Bing Xiang Bowen Zhou and Yoshua Bengio.']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9997023364414662,1.0,0.9201956574808772,0.0263157894736842
371,What are the non-linear functions mentioned for neural networks?,"The non-linear functions mentioned for neural networks include logistic sigmoid, tanh, hard tanh, Swish, Rectified Linear Unit (ReLU), Leaky ReLU, Parametric ReLU, GELU. These non-linear functions are essential for neural networks to approximate complex functions and enable the use of non-linear decision boundaries for better classification.","['Non linearities old and new logistic sigmoid tanh hard tanh 1 0 1 1 Swish arXiv 1710.05941 swish ùë• ùë•. logistic ùë• Rectified Linear Unit ReLU ReLU ùëß max ùëß 0 Leaky ReLU Parametric ReLU 0 0 0 GELU arXiv 1606.08415 GELU ùë• ùë•. ùëÉùëã ùë• ùëã ùëÅ 0 1 ùë•. logistic 1.702ùë•Non linearities i.e. f on previous slide Why they re needed Neural networks do function approximation e.g. regression or classification Without non linearities deep neural networks can t do anything more than a linear transform Extra layers could just be compiled down into a single linear transform W1 W2 x Wx But with more layers that include non linearities they can approximate any complex function 5Training with cross entropy loss you use this in', '3.5 Non linear ClassiÔ¨Åers Figure 9 Here we see that many exam ples are wrongly classiÔ¨Åed even though the best linear decision boundary is chosen. This is due linear decision boundaries have limited model capacity for this dataset. We now introduce the need for non linear classiÔ¨Åcation models such as neural networks. We see in Figure 9 that a linear classiÔ¨Åer mis classiÔ¨Åes many datapoints. Using a non linear decision boundary as shown in Figure 10 we manage to classify all training points accu rately. Although oversimpliÔ¨Åed this is a classic case demonstrating the need for non linear decision boundaries. In the next set of notes we study neural networks as a class of non linear models that have performed particularly well in deep learning applications. Figure 10 Here we see that the non linear decision boundary allows for much better classiÔ¨Åcation of datapoints.', 'file name: lecture02 notes Backpropagation.  Neural networks. Forward computation. Backward propagation. Neuron Units. Max margin Loss. Gradient checks. Xavier parameter initialization. Learning rates. Adagrad. This set of notes introduces single and multilayer neural networks and how they can be used for classiÔ¨Åcation purposes. We then dis cuss how they can be trained using a distributed gradient descent technique known as backpropagation. We will see how the chain rule can be used to make parameter updates sequentially. After a rigorous mathematical discussion of neural networks we will discuss some practical tips and tricks in training neural networks involving neuron units non linearities gradient checks Xavier parameter ini tialization learning rates Adagrad etc. Lastly we will motivate the use of recurrent neural networks as a language model. 1 Neural Networks Foundations Figure 1 We see here how a non linear decision boundary separates the data very well. This is the prowess of neural networks.', '2 Background A feed forward neural network is a non linear mapping from a input pattern x to an output vector y. Consider the lth hidden layer in a deep feed forward neural network and let al be the vector representation of the summed inputs to the neurons in that layer. The summed inputs are computed through a linear projection with the weight matrix W l and the bottom up inputs hl given as follows al i wl i hl hl 1 i f al i bl i 1 where f is an element wise non linear function and wl i is the incoming weights to the ith hidden units and bl i is the scalar bias parameter. The parameters in the neural network are learnt using gradient based optimization algorithms with the gradients being computed by back propagation. One of the challenges of deep learning is that the gradients with respect to the weights in one layer are highly dependent on the outputs of the neurons in the previous layer especially if these outputs change in a highly correlated way. Batch normalization Ioffe and Szegedy 2015 was proposed to reduce such undesirable covariate shift .']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9911262334187996,1.0,0.9605650432120708,0.0285714285714285
372,What is the purpose of non-linearities in neural networks?,"The purpose of non-linearities in neural networks is to enable function approximation, such as regression or classification. Without non-linearities, deep neural networks can only perform a linear transform, limiting their capabilities. By introducing non-linearities, neural networks can approximate complex functions and perform more sophisticated tasks. Extra layers with non-linearities allow neural networks to approximate any complex function effectively.","['3.5 Non linear ClassiÔ¨Åers Figure 9 Here we see that many exam ples are wrongly classiÔ¨Åed even though the best linear decision boundary is chosen. This is due linear decision boundaries have limited model capacity for this dataset. We now introduce the need for non linear classiÔ¨Åcation models such as neural networks. We see in Figure 9 that a linear classiÔ¨Åer mis classiÔ¨Åes many datapoints. Using a non linear decision boundary as shown in Figure 10 we manage to classify all training points accu rately. Although oversimpliÔ¨Åed this is a classic case demonstrating the need for non linear decision boundaries. In the next set of notes we study neural networks as a class of non linear models that have performed particularly well in deep learning applications. Figure 10 Here we see that the non linear decision boundary allows for much better classiÔ¨Åcation of datapoints.', 'Non linearities old and new logistic sigmoid tanh hard tanh 1 0 1 1 Swish arXiv 1710.05941 swish ùë• ùë•. logistic ùë• Rectified Linear Unit ReLU ReLU ùëß max ùëß 0 Leaky ReLU Parametric ReLU 0 0 0 GELU arXiv 1606.08415 GELU ùë• ùë•. ùëÉùëã ùë• ùëã ùëÅ 0 1 ùë•. logistic 1.702ùë•Non linearities i.e. f on previous slide Why they re needed Neural networks do function approximation e.g. regression or classification Without non linearities deep neural networks can t do anything more than a linear transform Extra layers could just be compiled down into a single linear transform W1 W2 x Wx But with more layers that include non linearities they can approximate any complex function 5Training with cross entropy loss you use this in', '2 Background A feed forward neural network is a non linear mapping from a input pattern x to an output vector y. Consider the lth hidden layer in a deep feed forward neural network and let al be the vector representation of the summed inputs to the neurons in that layer. The summed inputs are computed through a linear projection with the weight matrix W l and the bottom up inputs hl given as follows al i wl i hl hl 1 i f al i bl i 1 where f is an element wise non linear function and wl i is the incoming weights to the ith hidden units and bl i is the scalar bias parameter. The parameters in the neural network are learnt using gradient based optimization algorithms with the gradients being computed by back propagation. One of the challenges of deep learning is that the gradients with respect to the weights in one layer are highly dependent on the outputs of the neurons in the previous layer especially if these outputs change in a highly correlated way. Batch normalization Ioffe and Szegedy 2015 was proposed to reduce such undesirable covariate shift .', '3 Linear Layer. The Ô¨Åxed size vector f 1 Œ∏ can be fed to one or several standard neural network layers which perform afÔ¨Åne transformations over their inputs f l Œ∏ W l f l 1 Œ∏ bl 4 where W l Rnl hu nl 1 hu and bl Rnl hu are the parameters to be trained. The hyper parameter nl hu is usually called the number of hidden units of the lth layer. 2502NATURAL LANGUAGE PROCESSING ALMOST FROM SCRATCH HardTanh Layer. Several linear layers are often stacked interleaved with a non linearity func tion to extract highly non linear features. If no non linearity is introduced our network would be a simple linear model. We chose a hard version of the hyperbolic tangent as non linearity. It has the advantage of being slightly cheaper to compute compared to the exact hyperbolic tangent while leaving the generalization performance unchanged Collobert 2004 .']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.0000000000000004,1.0,0.9236199617432752,0.0384615384615384
373,What is cross entropy loss used for in PyTorch?,"Cross entropy loss is used in PyTorch to minimize the negative log probability of the correct class, or in other words, to maximize the probability of the correct class. It involves comparing the true probability distribution (p) with the computed model probability distribution (q) to calculate the cross entropy. This loss function is commonly used in PyTorch for training neural networks. 

Source: lecture02_Neural_Nets.txt","['with cross entropy loss you use this in PyTorch Until now our objective was stated as to maximize the probability of the correct class y or equivalently we can minimize the negative log probability of that class Now restated in terms of cross entropy a concept from information theory Let the true probability distribution be p let our computed model probability be q The cross entropy is Assuming a ground truth or true or gold or target probability distribution that is 1 at the right class and 0 everywhere else p 0 0 1 0 0 then Because of one hot p the only term left is the negative log probability of the true class yi log ùëù ùë¶i ùë•i Cross entropy can be used in other ways with a more interesting p but for now just know that you ll want to use it as the loss in PyTorch 6Remember Stochastic Gradient Descent Update equation i.e. for each parameter In deep learning ùúÉincludes the data representation e.g.', 'Now we are ready to introduce cross entropy. The cross entropy is useful when cross entropy we don t know the actual probability distribution p that generated some data. It allows us to use some m which is a model of p i.e. an approximation to p . The cross entropy of m on p is deÔ¨Åned by H p m lim n 1 n X W L p w1 ... wn logm w1 ... wn 3.50 That is we draw sequences according to the probability distribution p but sum the log of their probabilities according to m. Again following the Shannon McMillan Breiman theorem for a stationary er godic process H p m lim n 1 n logm w1w2 ...wn 3.51 This means that as for entropy we can estimate the cross entropy of a model m on some distribution p by taking a single sequence that is long enough instead of summing over all possible sequences. What makes the cross entropy useful is that the cross entropy H p m is an up per bound on the entropy H p .', 'This is equal to the exponential of the cross entropy loss Normalized by number of words Inverse probability of corpus according to Language Model Lower perplexity is better 46RNNs greatly improved perplexity over what came before n gram model Increasingly complex RNNs Perplexity improves lower is better Source https research.fb.com building an efficient neural language model over a billion words 474.', '1.3 Least Squares Objective Recall that for the skip gram model we use softmax to compute the probability of word j appears in the context of word i Qij exp uT j vi W w 1 exp uT w vi Training proceeds in an on line stochastic fashion but the implied global cross entropy loss can be calculated as J i corpus j context i log Qij As the same words i and j can appear multiple times in the corpus it is more efÔ¨Åcient to Ô¨Årst group together the same values for i and j J W i 1 W j 1 Xij log Qij where the value of co occurring frequency is given by the co occurrence matrix X. One signiÔ¨Åcant drawback of the cross entropy loss is that it requires the distribution Q to be properly normalized which involves the expensive summation over the entire vocabulary.']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999999999999988,0.75,0.958897447384088,0.1111111111111111
374,What is the significance of the backpropagation algorithm?,"The significance of the backpropagation algorithm lies in its ability to efficiently update the parameters of a neural network by propagating gradients backwards through the network. This allows for the calculation of how each parameter contributes to the overall error, enabling the network to adjust its weights and improve its performance during the training process. Backpropagation is crucial for training deep neural networks and is based on the chain rule of calculus, making it a fundamental algorithm in the field of computational natural language processing.

Source: Lecture02_notes_Backpropagation","['W k is the transfer matrix that maps the output from the k th layer to the input to the k 1 th. Thus W 1 W and W 2 UT using notation from Section 1.3. Let us begin Suppose the cost J 1 sc s is positive and we want to perform the update of parameter W 1 14 in Figure 5 and Figure 6 we must realize that W 1 14 only contributes to z 2 1 and thus a 2 1 . This fact is crucial to understanding backpropagation backpropagated gradients are only affected by values they contribute to. a 2 1 is consequently used in the forward computation of score by multiplication with W 2 1 . We can see from the max margin loss that J s J sc 1 Therefore we will work with s W 1 ij here for simplicity. Thus s W 1 ij W 2 a 2 W 1 ij W 2 i a 2 i W 1 ij W 2 i a 2 i', 'file name: lecture02 notes Backpropagation.  Neural networks. Forward computation. Backward propagation. Neuron Units. Max margin Loss. Gradient checks. Xavier parameter initialization. Learning rates. Adagrad. This set of notes introduces single and multilayer neural networks and how they can be used for classiÔ¨Åcation purposes. We then dis cuss how they can be trained using a distributed gradient descent technique known as backpropagation. We will see how the chain rule can be used to make parameter updates sequentially. After a rigorous mathematical discussion of neural networks we will discuss some practical tips and tricks in training neural networks involving neuron units non linearities gradient checks Xavier parameter ini tialization learning rates Adagrad etc. Lastly we will motivate the use of recurrent neural networks as a language model. 1 Neural Networks Foundations Figure 1 We see here how a non linear decision boundary separates the data very well. This is the prowess of neural networks.', 'Computational efÔ¨Åciency Having explored element wise updates as well as vector wise updates we must realize that the vectorized implementations run substantially faster in scientiÔ¨Åc computing environments such as MATLAB or Python using NumPy SciPy packages . Thus we should use vectorized implementation in prac tice. Furthermore we should also reduce redundant calculations in backpropagation for instance notice that Œ¥ k depends directly on Œ¥ k 1 . Thus we should ensure that when we update W k using Œ¥ k 1 we save Œ¥ k 1 to later derive Œ¥ k and we then repeat this for k 1 . . . 1 . Such a recursive procedure is what makes backpropa gation a computationally affordable procedure. 2 Neural Networks Tips and Tricks Having discussed the mathematical foundations of neural networks we will now dive into some tips and tricks commonly employed when using neural networks in practice. 2.1 Gradient Check In the last section we discussed in detail how to calculate error gradients updates for parameters in a neural network model via calculus based analytic methods.', 'The idea behind using this objective is to ensure that the score computed for true labeled data points is higher than the score computed for false labeled data points. Using the previous example if we call the score computed for the true labeled window Museums in Paris are amazing as s and the score computed for the false labeled window Not all museums in Paris as sc subscripted as c to signify that the window is corrupt . Then our objective function would be to maximize s sc or to minimize sc s . However we modify our objective to ensure that error is only computed if sc s sc s 0. The intuition behind doing this is that we only care the the true data point have a higher score than the false data point and that the rest does not matter. Thus we want our error to be sc s if sc s else 0. Thus our optimization objective is now minimize J max sc s 0 However the above optimization objective is risky in the sense that it does not attempt to create a margin of safety.']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.000000000000001,0.8333333333333334,0.9426696050764758,0.0263157894736842
375,What does the Jacobian matrix represent in the context of neural networks?,"The Jacobian matrix in the context of neural networks represents a matrix of partial derivatives, where each element corresponds to the derivative of one output with respect to one input. It generalizes the concept of gradients for functions with multiple outputs and inputs, providing a comprehensive view of how each output is affected by changes in each input. The Jacobian matrix is crucial for understanding the sensitivity of a neural network's outputs to changes in its inputs and plays a key role in various computations involving derivatives and optimization techniques. 

Source: lecture02_Neural_Nets.txt","['The Jacobian matrix xk 1 xk is given by WT recdiag œÉ xk . The 2 norm of this Jacobian is bounded by the product ofOn the diÔ¨Éculty of training Recurrent Neural Networks the norms of the two matrices see equation 6 . Due to our assumption this implies that it is smaller than 1. k xk 1 xk WT rec diag œÉ xk 1 Œ≥ Œ≥ 1 6 Let Œ∑ R be such that k xk 1 xk Œ∑ 1. The existence of Œ∑ is given by equation 6 .', '013 64.48Gradients Given a function with 1 output and n inputs Its gradient is a vector of partial derivatives with respect to each input 11Jacobian Matrix Generalization of the Gradient Given a function with m outputs and n inputs It s Jacobian is an m x n matrix of partial derivatives 12Chain Rule For composition of one variable functions multiply derivatives For multiple variables functions multiply Jacobians 13Example Jacobian Elementwise activation Function 14Example Jacobian Elementwise activation Function Function has n outputs and n inputs n by n Jacobian 15Example Jacobian Elementwise activation Function 16Example Jacobian Elementwise activation Function 17Example Jacobian Elementwise activation Function 18Other Jacobians Compute these at home for practice Check your answers with the lecture notes 19Other Jacobians Compute these at home for practice Check your answers with the lecture notes 20Other Jacobians Compute these at home for practice Check your answers with the lecture notes Fine print This is the correct Jacobian. Later we discuss the shape convention using it the answer would be h.', '. . 0 ith element Now let s compute J Wij J Wij J z z Wij Œ¥ z Wij m X k 1 Œ¥k zk Wij Œ¥ixj the only nonzero term in the sum is Œ¥i zi Wij . To get J W we want a ma trix where entry i j is Œ¥ixj. This matrix is equal to the outer product J W Œ¥T xT 6 Row vector time matrix with respect to the matrix z xW Œ¥ J z what is J W Œ¥ z W A similar computation to 5 shows that J W xT Œ¥ . 7 Cross entropy loss with respect to logits ÀÜ y softmax Œ∏ J CE y ÀÜ y what is J Œ∏ The gradient is J Œ∏ ÀÜ y y or ÀÜ y y T if y is a column vector . 4These identities will be enough to let you quickly compute the gradients for many neural networks. However it s important to know how to compute Jacobians for other functions as well in case they show up.', '21Other Jacobians Compute these at home for practice 22Back to our Neural Net x xmuseums xin xParis xare xamazing 23Back to our Neural Net Let s find Really we care about the gradient of the loss Jt but we will compute the gradient of the score for simplicity x xmuseums xin xParis xare xamazing 241. Break up equations into simple pieces Carefully define your variables and keep track of their dimensionality 252. Apply the chain rule 262. Apply the chain rule 272. Apply the chain rule 282. Apply the chain rule 293. Write out the Jacobians Useful Jacobians from previous slide 303. Write out the Jacobians ùíñùëá Useful Jacobians from previous slide 313. Write out the Jacobians ùíñùëá Useful Jacobians from previous slide 323. Write out the Jacobians ùíñùëá Useful Jacobians from previous slide 333.']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999999999999988,0.8333333333333334,0.9400121475692766,0.6666666666666666
376,How is the derivative with respect to a matrix described?,"The derivative with respect to a matrix is described as a Jacobian matrix. This Jacobian matrix is useful for applying the chain rule to a vector-valued function by multiplying Jacobians. It is arranged as a matrix with the same shape as the original matrix, allowing for easier computation and application during gradient descent. Additionally, when an elementwise function is applied to a vector, the Jacobian matrix is represented as a diagonal matrix where each entry corresponds to the derivative of the function applied to the respective element. This diagonal matrix can be denoted as diag(f(x)). 
Source: gradient_notes.txt","['E Œ∏ X 1 t T Et Œ∏ 3 Et Œ∏ X 1 k t Et xt xt xk xk Œ∏ 4 xt xk Y t i k xi xi 1 Y t i k WT recdiag œÉ xi 1 5 xk Œ∏ refers to the immediate partial derivative of the state xk with respect to Œ∏ i.e. where xk 1 is taken as a constant with respect to Œ∏. SpeciÔ¨Åcally considering equation 2 the value of any row i of the matrix xk Wrec is just œÉ xk 1 . Equation 5 also provides the form of Jacobian matrix xi xi 1 for the speciÔ¨Åc parametrization given in equation 11 where diag converts a vector into a diagonal matrix and œÉ computes the derivative of œÉ in an element wise fash ion.', '. f1 xn . . . ... . . . fm x1 . . . fm xn That is f x ij fi xj which is just a standard non vector derivative . The Jacobian matrix will be useful for us because we can apply the chain rule to a vector valued function just by multiplying Jacobians. As a little illustration of this suppose we have a function f x f1 x f2 x taking a scalar to a vector of size 2 and a function g y g1 y1 y2 g2 y1 y2 taking a vector of size two to a vector of size two. Now let s compose them to get g x g1 f1 x f2 x g2 f1 x f2 x .', 'First suppose we have a loss function J a scalar and are computing its gradient with respect to a matrix W Rn m. Then we could think of J as a function of W taking nm inputs the entries of W to a single output J . This means the Jacobian J W would be a 1 nm vector. But in practice this is not a very useful way of arranging the gradient. It would be much nicer if the derivatives were in a n m matrix like this J W J W11 . . . J W1m . . . ... . . . J Wn1 . . . J Wnm Since this matrix has the same shape as W we could just subtract it times the learning rate from W when doing gradient descent. So in a slight abuse of notation let s Ô¨Ånd this matrix as J W instead. This way of arranging the gradients becomes complicated when computing z W . Unlike J z is a vector.', '4 An elementwise function applied a vector z f x what is z x Since f is being applied elementwise we have zi f xi . So z x ij zi xj xj f xi f xi if i j 0 if otherwise So we see that the Jacobian z x is a diagonal matrix where the entry at i i is the derivative of f applied to xi. We can write this as z x diag f x . Since multiplication by a diagonal matrix is the same as doing elementwise multiplication by the diagonal we could also write f x when applying the chain rule. 5 Matrix times column vector with respect to the matrix z W x Œ¥ J z what is J W J z z W Œ¥ z W This is a bit more complicated than the other identities. The reason for in cluding J z in the above problem formulation will become clear in a moment.']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.8484689584649364,1.0,0.8853883491797276,0.0454545454545454
377,What role does automatic differentiation play in modern deep learning frameworks?,"Automatic differentiation plays a crucial role in modern deep learning frameworks as it allows for efficient computation of gradients needed for training neural networks. It automates the process of calculating derivatives of complex functions, which is essential for optimizing the parameters of neural networks through techniques like backpropagation. By automatically computing gradients, deep learning frameworks can efficiently update model parameters during training, leading to faster convergence and better performance of neural networks. 

Source: lecture02_notes_Backpropagation","['2507COLLOBERT WESTON BOTTOU KARLEN KAVUKCUOGLU AND KUKSA 3.4.3 STOCHASTIC GRADIENT Maximizing 8 with stochastic gradient Bottou 1991 is achieved by iteratively selecting a random example x y and making a gradient step Œ∏ Œ∏ Œª log p y x Œ∏ Œ∏ 16 where Œª is a chosen learning rate. Our neural networks described in Figure 1 and Figure 2 are a succession of layers that correspond to successive composition of functions. The neural network is Ô¨Ånally composed with the word level log likelihood 11 or successively composed in the re cursion 14 if using the sentence level log likelihood 13 . Thus an analytical formulation of the derivative 16 can be computed by applying the differentiation chain rule through the network and through the word level log likelihood 11 or through the recurrence 14 . Remark 5 Differentiability Our cost functions are differentiable almost everywhere.', 'The regularizer we propose below prefers solutions for which the error signal preserves norm as it travels back in time ‚Ñ¶ X k ‚Ñ¶k X k E xk 1 xk 1 xk E xk 1 1 2 9 In order to be computationally eÔ¨Écient we only use the immediate partial derivative of ‚Ñ¶with respect to Wrec we consider that xk and E xk 1 as being constant with respect to Wrec when computing the derivative of ‚Ñ¶k as depicted in equation 10 . Note we use the parametrization of equation 11 . This can be done ef Ô¨Åciently because we get the values of E xk from BPTT. We use Theano to compute these gradients Bergstra et al. 2010 Bastien et al. 2012 .', 'The intuition is that to get a better approximation of the derivative slope around a point we need to examine the func tion f s behavior both to the left and right of that point. It can also be shown using Taylor s theorem that the centered difference formula has an error proportional to œµ2 which is quite small whereas the derivative deÔ¨Ånition is more error prone. Now a natural question you might ask is if this method is so pre cise why do we not use it to compute all of our network gradients instead of applying back propagation The simple answer as hinted earlier is inefÔ¨Åciency recall that every time we want to compute the gradient with respect to an element we need to make two forward passes through the network which will be computationally expen sive. Furthermore many large scale neural networks can contain millions of parameters and computing two passes per parameter is clearly not optimal. And since in optimization techniques such as SGD we must compute the gradients once per iteration for several thousands of iterations it is obvious that this method quickly grows intractable.', '2.3. The geometrical interpretation Let us consider a simple one hidden unit model equa tion 8 where we provide an initial state x0 and train the model to have a speciÔ¨Åc target value after 50 steps. Note that for simplicity we assume no input. xt wœÉ xt 1 b 8 Fig. 6 shows the error surface E50 œÉ x50 0.7 2 where x0 .5 and œÉ to be the sigmoid function. We can more easily analyze the behavior of this model by further simplifying it to be linear œÉ then being the identity function with b 0. xt x0wt from which it follows that xt w tx0wt 1 and 2xt w2 t t 1 x0wt 2 implying that when the Ô¨Årst derivative explodes so does the second derivative.']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.0000000000000002,0.0,0.924965723197108,0.0
378,What is the purpose of manual gradient checking?,"The purpose of manual gradient checking is to verify the correctness of analytic gradients by comparing them with numerical gradients, which are much quicker to compute. This helps ensure that the gradients calculated during the training process are accurate and can help in debugging and improving models. 

Source: SEP 775 - Computational Natural Language Processing, lecture02_notes_Backpropagation.txt","['This inefÔ¨Åciency is why we only use gradient check to verify the correctness of our analytic gradients which are much quicker to compute. A standard implementation of gradient check is shown below Snippet 2.1 def eval_numerical_gradient f x a naive implementation of numerical gradient of f at x f should be a function that takes a single argument x is the point numpy array to evaluate the gradient at fx f x evaluate function value at original point grad np.zeros x.shape h 0.00001 iterate over all indexes in x it np.nditer x flags multi_index op_flags readwrite while not it.finished evaluate function at x h ix it.multi_index old_value x ix x ix old_value h increment by h fxh_left f x evaluate f x h x ix old_value h decrement by h fxh_right f x evaluate f x h x ix old_value restore to previous value very important compute the partial derivative grad ix fxh_left fxh_right 2 h the slope it.iternext step to next dimension return grad 2.', 'everything doing this everywhere was the key test Now much less needed you can use it to check layers are correctly implemented 78Summary We ve mastered the core technology of neural nets Backpropagation recursively and hence efficiently apply the chain rule along computation graph downstream gradient upstream gradient x local gradient Forward pass compute results of operations and save intermediate values Backward pass apply chain rule to compute gradients 79Why learn all these details about gradients 80 Modern deep learning frameworks compute gradients for you But why take a class on compilers or systems when they are implemented for you Understanding what is going on under the hood is useful Backpropagation doesn t always work perfectly out of the box Understanding why is crucial for debugging and improving models See Karpathy article https medium.com @karpathy yes you should understand backprop e2f06eab496b Example in future lecture exploding and vanishing gradients', 'We note that this deÔ¨Ånition of the numerical gradient follows very naturally from the deÔ¨Ånition of the derivative where in the scalar case f x f x œµ f x œµ Gradient checks are a great way to compare analytical and numerical gradients. Analytical gradients should be close and numerical gradients can be calculated using f Œ∏ J Œ∏ i J Œ∏ i 2œµ J Œ∏ i and J Œ∏ i can be evalu ated using two forward passes. An implementation of this can be seen in Snippet 2.1. Of course there is a slight difference the deÔ¨Ånition above only perturbs x in the positive direction to compute the gradient. While it would have been perfectly acceptable to deÔ¨Åne the numerical gradi ent in this way in practice it is often more precise and stable to use the centered difference formula where we perturb a parameter in both directions.', 'This makes the math a bit cumbersome. Even worse if we re implementing the model without automatic diÔ¨Äerentiation computing J Œ∏ twice will be ineÔ¨Écient. So it will help us to deÔ¨Åne some variables to represent the intermediate derivatives Œ¥1 J Œ∏ Œ¥2 J z These can be thought as the error signals passed down to Œ∏ and z when doing backpropagation.']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999999999999996,0.8,0.9377563905716628,0.0909090909090909
379,Why is understanding gradients important despite automatic differentiation?,"Understanding gradients is important despite automatic differentiation because it allows us to verify the correctness of our analytic gradients efficiently. While automatic differentiation can compute gradients automatically, manually understanding gradients helps in ensuring the accuracy of the computations and can be crucial for debugging and optimizing machine learning models. Additionally, having a good grasp of gradients helps in interpreting how changes in model parameters affect the loss function, leading to better insights into the model's behavior and performance.","['The intuition is that to get a better approximation of the derivative slope around a point we need to examine the func tion f s behavior both to the left and right of that point. It can also be shown using Taylor s theorem that the centered difference formula has an error proportional to œµ2 which is quite small whereas the derivative deÔ¨Ånition is more error prone. Now a natural question you might ask is if this method is so pre cise why do we not use it to compute all of our network gradients instead of applying back propagation The simple answer as hinted earlier is inefÔ¨Åciency recall that every time we want to compute the gradient with respect to an element we need to make two forward passes through the network which will be computationally expen sive. Furthermore many large scale neural networks can contain millions of parameters and computing two passes per parameter is clearly not optimal. And since in optimization techniques such as SGD we must compute the gradients once per iteration for several thousands of iterations it is obvious that this method quickly grows intractable.', 'We note that this deÔ¨Ånition of the numerical gradient follows very naturally from the deÔ¨Ånition of the derivative where in the scalar case f x f x œµ f x œµ Gradient checks are a great way to compare analytical and numerical gradients. Analytical gradients should be close and numerical gradients can be calculated using f Œ∏ J Œ∏ i J Œ∏ i 2œµ J Œ∏ i and J Œ∏ i can be evalu ated using two forward passes. An implementation of this can be seen in Snippet 2.1. Of course there is a slight difference the deÔ¨Ånition above only perturbs x in the positive direction to compute the gradient. While it would have been perfectly acceptable to deÔ¨Åne the numerical gradi ent in this way in practice it is often more precise and stable to use the centered difference formula where we perturb a parameter in both directions.', 'This inefÔ¨Åciency is why we only use gradient check to verify the correctness of our analytic gradients which are much quicker to compute. A standard implementation of gradient check is shown below Snippet 2.1 def eval_numerical_gradient f x a naive implementation of numerical gradient of f at x f should be a function that takes a single argument x is the point numpy array to evaluate the gradient at fx f x evaluate function value at original point grad np.zeros x.shape h 0.00001 iterate over all indexes in x it np.nditer x flags multi_index op_flags readwrite while not it.finished evaluate function at x h ix it.multi_index old_value x ix x ix old_value h increment by h fxh_left f x evaluate f x h x ix old_value h decrement by h fxh_right f x evaluate f x h x ix old_value restore to previous value very important compute the partial derivative grad ix fxh_left fxh_right 2 h the slope it.iternext step to next dimension return grad 2.', 'This makes the math a bit cumbersome. Even worse if we re implementing the model without automatic diÔ¨Äerentiation computing J Œ∏ twice will be ineÔ¨Écient. So it will help us to deÔ¨Åne some variables to represent the intermediate derivatives Œ¥1 J Œ∏ Œ¥2 J z These can be thought as the error signals passed down to Œ∏ and z when doing backpropagation.']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999986325579928,0.75,0.9185771914986376,0.0666666666666666
380,What is the core concept introduced in this class that leads to GPT-3 and ChatGPT?,"The core concept introduced in this class that leads to GPT-3 and ChatGPT is the use of large-scale pre-training of deep neural networks on vast amounts of text data to enable the models to learn language representations and generate human-like text responses. This approach leverages the power of transformer-based architectures to capture complex patterns in language and improve performance on various natural language processing tasks. By pre-training on a diverse range of text data, these models can then be fine-tuned on specific tasks to achieve state-of-the-art results in tasks such as language modeling, text generation, and question answering. This methodology has been instrumental in the development of advanced language models like GPT-3 and ChatGPT, which have demonstrated impressive capabilities in understanding and generating human language.

Source: Lecture on Computational Natural Language Processing","['Overall in context learning with GPT 3 shows mixed results on commonsense reasoning tasks with only small and inconsistent gains observed in the one and few shot learning settings for both PIQA and ARC but a signiÔ¨Åcant improvement is observed on OpenBookQA. GPT 3 sets SOTA on the new PIQA dataset in all evaluation settings. 3.6 Reading Comprehension Next we evaluate GPT 3 on the task of reading comprehension. We use a suite of 5 datasets including abstractive multiple choice and span based answer formats in both dialog and single question settings. We observe a wide spread in GPT 3 s performance across these datasets suggestive of varying capability with different answer formats. In general we observe GPT 3 is on par with initial baselines and early results trained using contextual representations on each respective dataset.', 'First despite the strong quantitative and qualitative improvements of GPT 3 particularly compared to its direct predecessor GPT 2 it still has notable weaknesses in text synthesis and several NLP tasks. On text synthesis although the overall quality is high GPT 3 samples still sometimes repeat themselves semantically at the document level start to lose coherence over sufÔ¨Åciently long passages contradict themselves and occasionally contain non sequitur sentences or paragraphs. We will release a collection of 500 uncurated unconditional samples to help provide a better sense of GPT 3 s limitations and strengths at text synthesis. Within the domain of discrete language tasks we have noticed informally that GPT 3 seems to have special difÔ¨Åculty with common sense physics despite doing well on some datasets such as PIQA BZB 19 that test this domain. SpeciÔ¨Åcally GPT 3 has difÔ¨Åculty with questions of the type If I put cheese into the fridge will it melt .', 'GPT 3 performs best within 3 points of the human baseline on CoQA RCM19 a free form conversational dataset and performs worst 13 F1 below an ELMo baseline on QuAC CHI 18 a dataset which requires modeling structured dialog acts and answer span selections of teacher student interactions. On DROP DWD 19 a dataset testing discrete reasoning and numeracy in the context of reading comprehension GPT 3 in a few shot setting outperforms the Ô¨Åne tuned BERT baseline from the original paper but is still well below both human performance and state of the art approaches which augment neural networks with symbolic systems RLL 19 . On SQuAD 2.0 RJL18 GPT 3 demonstrates its few shot learning capabilities improving by almost 10 F1 to 69.8 compared to a zero shot setting. This allows it to slightly outperform the best Ô¨Åne tuned result in the original paper.', 'Section 5 discusses limitations of GPT 3. Section 6 discusses broader impacts. Section 7 reviews related work and Section 8 concludes. 2 Approach Our basic pre training approach including model data and training is similar to the process described in RWC 19 with relatively straightforward scaling up of the model size dataset size and diversity and length of training. Our use of in context learning is also similar to RWC 19 but in this work we systematically explore different settings for learning within the context. Therefore we start this section by explicitly deÔ¨Åning and contrasting the different settings that we will be evaluating GPT 3 on or could in principle evaluate GPT 3 on. These settings can be seen as lying on a spectrum of how much task speciÔ¨Åc data they tend to rely on.']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9988689864925634,0.6666666666666666,0.9061371532643564,0.0454545454545454
381,What is the purpose of Dropout as mentioned in the document?,"The purpose of Dropout, as mentioned in the document, is to prevent overfitting in neural networks by training exponentially many smaller networks at once and averaging over their predictions. Dropout helps the network learn more meaningful information from the data, reduces the likelihood of overfitting, and generally improves performance on the task at hand. During training, Dropout randomly sets a fraction of neurons to zero, and during backpropagation, gradients are only passed through the neurons that were kept active. During testing, all neurons in the network are used for the forward pass.","['The result is that the network typically learns more meaningful informa tion from the data is less likely to overÔ¨Åt and usually obtains higher performance overall on the task at hand. One intuitive reason why this technique should be so effective is that what dropout is doing is essentially doing is training exponentially many smaller networks at once and averaging over their predictions. Dropout applied to an artiÔ¨Åcial neural network. Image credits to Srivastava et al. In practice the way we introduce dropout is that we take the out put h of each layer of neurons and keep each neuron with prob ability p and else set it to 0. Then during back propagation we only pass gradients through neurons that were kept alive during the forward pass. Finally during testing we compute the forward pass using all of the neurons in the network.', 'kid stubbornly postponees pursuing job citing medical reasons despite tried reasonable compro mise negatively effecting forward employment mobility both personally and Ô¨Ånancially thoght wise want change this dumbass selÔ¨Åsh lazy atti tude now please help pls Table 29 Example of samples from our over optimized model overÔ¨Åt to one of our 1.3B reward models. 45', 'Lastly all the training examples are sorted by the length of paragraph and divided into mini batches of 32 examples each. We use Adamax for optimization as described in Kingma and Ba 5We use the Wikipedia Search API https www. mediawiki.org wiki API Search. 2014 . Dropout with p 0.3 is applied to word embeddings and all the hidden units of LSTMs. Result and analysis Table 4 presents our eval uation results on both development and test sets. SQuAD has been a very competitive machine comprehension benchmark since its creation and we only list the best performing systems in the ta ble. Our system single model can achieve 70.0 exact match and 79.0 F1 scores on the test set which surpasses all the published results and can match the top performance on the SQuAD leader board at the time of writing. Additionally we think that our model is conceptually simpler than most of the existing systems.', '2503COLLOBERT WESTON BOTTOU KARLEN KAVUKCUOGLU AND KUKSA 0 10 20 30 40 50 60 70 The proposed changes also would allow executives to report exercises of options later and less often . xx xx xx xx xx xx xx xx xx xx xx xx xx xx x x x x x x x x x x x x x x x x x xx xx xx xx xx xx xx xx xx xx xx xx xx x x x x x x x x xx xx xx xx xx xxxxx xx xx 0 10 20 30 40 50 60 70 The proposed changes also would allow executives to report exercises of options later and less often .']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9300597327790432,1.0,0.9178216635602812,0.0
382,What are some approaches to address the vanishing gradient problem in RNNs?,"Some approaches to address the vanishing gradient problem in RNNs include using Long Short Term Memory (LSTM) networks, which were proposed by Hochreiter and Schmidhuber in 1997 as a solution to the problem of vanishing gradients. Another approach is to create more direct and linear pass-through connections in the model, incorporate attention mechanisms, residual connections, and separate memory components in RNNs. Additionally, a regularization term can be used to force the error signal not to vanish as it propagates back in time, as mentioned in the paper ""On the difficulty of training Recurrent Neural Networks"" by Pascanu et al. (Source: lecture03_RNNs_and_LLMs.txt, On_the_difficulty_of_training_Recurrent_Neural_Networks.txt)","['press v28 pascanu13.pdfHow to fix the vanishing gradient problem The main problem is that it s too difficult for the RNN to learn to preserve information over many timesteps. In a vanilla RNN the hidden state is constantly being rewritten First off next time How about an RNN with separate memory which is added to LSTMs And then Creating more direct and linear pass through connections in model Attention residual connections etc. 60Long Short Term Memory RNNs LSTMs A type of RNN proposed by Hochreiter and Schmidhuber in 1997 as a solution to the problem of vanishing gradients Everyone cites that paper but really a crucial part of the modern LSTM is from Gers et al. 2000 Only started to be recognized as promising through the work of S s student Alex Graves c.', 'Problems with RNNs Vanishing and Exploding Gradients 48Vanishing gradient intuition 49Vanishing gradient intuition chain rule 50Vanishing gradient intuition chain rule 51Vanishing gradient intuition chain rule 52Vanishing gradient intuition What happens if these are small Vanishing gradient problem When these are small the gradient signal gets smaller and smaller as it backpropagates further 53Vanishing gradient proof sketch linear case Recall What if were the identity function 53 Source On the difficulty of training recurrent neural networks Pascanu et al 2013. http proceedings.mlr.press v28 pascanu13.pdf and supplemental materials at http proceedings.mlr.press v28 pascanu13 supp.pdf If Wh is small then this term gets exponentially problematic as becomes large chain rule Consider the gradient of the loss on step with respect to the hidden state on some previous step .', 'By employing the proposed regularization term we are able to improve test error even on tasks that are notOn the diÔ¨Éculty of training Recurrent Neural Networks dominated by long term contributions. 5. Summary and Conclusions We provided diÔ¨Äerent perspectives through which one can gain more insight into the exploding and vanishing gradients issue. To deal with the exploding gradients problem we propose a solution that involves clipping the norm of the exploded gradients when it is too large. The algorithm is motivated by the assumption that when gradients explode the curvature and higher or der derivatives explode as well and we are faced with a speciÔ¨Åc pattern in the error surface namely a val ley with a single steep wall. In order to deal with the vanishing gradient problem we use a regulariza tion term that forces the error signal not to vanish as it travels back in time. This regularization term forces the Jacobian matrices xi xi 1 to preserve norm only in relevant directions.', 'file name: On the difficulty of training Recurrent Neural Networks. On the diÔ¨Éculty of training Recurrent Neural Networks Razvan Pascanu pascanur@iro.umontreal.ca Universite de Montreal Tomas Mikolov t.mikolov@gmail.com Brno University Yoshua Bengio yoshua.bengio@umontreal.ca Universite de Montreal Abstract There are two widely known issues with prop erly training Recurrent Neural Networks the vanishing and the exploding gradient prob lems detailed in Bengio et al. 1994 . In this paper we attempt to improve the under standing of the underlying issues by explor ing these problems from an analytical a geo metric and a dynamical systems perspective. Our analysis is used to justify a simple yet ef fective solution. We propose a gradient norm clipping strategy to deal with exploding gra dients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section. 1.']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.0000000000000009,1.0,0.8698019911322374,0.0555555555555555
383,"What is perplexity, and how is it used in evaluating language models?","Perplexity is a metric used to evaluate language models in natural language processing. It is the inverse probability of a test set normalized by the number of words in the test set. A lower perplexity indicates that a language model is a better predictor of the words in the test set. It is calculated based on the likelihood of the test sequence according to the model. Perplexity allows us to compare different language models, and it is essential to construct language models without any knowledge of the test set or the vocabulary of the test set to avoid artificially low perplexity values. Additionally, perplexity can be influenced by the choice of unknown word handling and vocabulary size in the language model. Smoothing techniques are used to adjust probabilities in language models to prevent assigning zero probability to unseen events, which can affect perplexity values.

Source: Lecture notes ""N-gram Language Models"" in SEP 775 - Computational Natural Language Processing.","['Given a large corpus that we want to divide into training and test test data can either be taken from some continuous sequence of text inside the corpus or we can remove smaller stripes of text from randomly selected parts of our corpus and combine them into a test set. 3.2.1 Perplexity In practice we don t use raw probability as our metric for evaluating language mod els but a variant called perplexity. The perplexity sometimes called PPL for short perplexity of a language model on a test set is the inverse probability of the test set normalized by the number of words. For a test set W w1w2 ...wN perplexity W P w1w2 ...wN 1 N 3.14 N s 1 P w1w2 ...wN We can use the chain rule to expand the probability of W perplexity W N v u u t N Y i 1 1 P wi w1 ...wi 1 3.15 The perplexity of a test set W depends on which language model we use.', 'A trigram model is less surprised than a unigram model because it has a better idea of what words might come next and so it assigns them a higher probability. And the higher the probability the lower the perplexity since as Eq. 3.15 showed perplexity is related inversely to the likelihood of the test sequence according to the model . So a lower perplexity can tell us that a language model is a better predictor of the words in the test set. Note that in computing perplexities the n gram model P must be constructed without any knowledge of the test set or any prior knowledge of the vocabulary of the test set. Any kind of knowledge of the test set can cause the perplexity to be artiÔ¨Åcially low. The perplexity of two language models is only comparable if they use identical vocabularies. An intrinsic improvement in perplexity does not guarantee an extrinsic im provement in the performance of a language processing task like speech recognition10 CHAPTER 3 N GRAM LANGUAGE MODELS or machine translation.', 'In either case we then proceed to train the language model as before treating UNK like a regular word. The exact choice of UNK has an effect on metrics like perplexity. A language model can achieve low perplexity by choosing a small vocabulary and assigning the unknown word a high probability. Thus perplexities can only be compared across language models with the same vocabularies Buck et al. 2014 . 3.5 Smoothing What do we do with words that are in our vocabulary they are not unknown words but appear in a test set in an unseen context for example they appear after a word they never appeared after in training To keep a language model from assigning14 CHAPTER 3 N GRAM LANGUAGE MODELS zero probability to these unseen events we ll have to shave off a bit of probability mass from some more frequent events and give it to the events we ve never seen. This modiÔ¨Åcation is called smoothing or discounting.', 'We mentioned above that perplexity is a function of both the text and the lan guage model given a text W different language models will have different perplex ities. Because of this perplexity can be used to compare different n gram models. Let s look at an example in which we trained unigram bigram and trigram gram mars on 38 million words including start of sentence tokens from the Wall Street Journal using a 19 979 word vocabulary. We then computed the perplexity of each of these models on a test set of 1.5 million words using Eq. 3.16 for unigrams Eq. 3.17 for bigrams and the corresponding equation for trigrams. The table below shows the perplexity of a 1.5 million word WSJ test set according to each of these grammars. Unigram Bigram Trigram Perplexity 962 170 109 As we see above the more information the n gram gives us about the word sequence the higher the probability the n gram will assign to the string.']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9077103422875468,1.0,0.9496013156134974,0.0384615384615384
384,How does LSTM solve the vanishing gradient problem?,"LSTMs (Long Short Term Memory networks) solve the vanishing gradient problem by introducing a special set of units called LSTM units, which are linear and have a recurrent connection to itself fixed to 1. The flow of information into the unit and from the unit is controlled by input and output gates, and their behavior is learned. This design allows LSTMs to better preserve information over many timesteps, addressing the vanishing gradient problem effectively. Additionally, LSTMs have variations in their basic structure that further enhance their ability to tackle the vanishing gradient issue. (Source: lecture03_RNNs_and_LLMs.txt)","['press v28 pascanu13.pdfHow to fix the vanishing gradient problem The main problem is that it s too difficult for the RNN to learn to preserve information over many timesteps. In a vanilla RNN the hidden state is constantly being rewritten First off next time How about an RNN with separate memory which is added to LSTMs And then Creating more direct and linear pass through connections in model Attention residual connections etc. 60Long Short Term Memory RNNs LSTMs A type of RNN proposed by Hochreiter and Schmidhuber in 1997 as a solution to the problem of vanishing gradients Everyone cites that paper but really a crucial part of the modern LSTM is from Gers et al. 2000 Only started to be recognized as promising through the work of S s student Alex Graves c.', 'SpeciÔ¨Åcally it introduces a special set of units called LSTM units which are linear and have a recurrent connection to itself which is Ô¨Åxed to 1. The Ô¨Çow of information into the unit and from the unit is guarded by an input and output gates their behaviour is learned . There are several variations of this basic structure. This solution does not address explicitly the exploding gradients problem. Sutskever et al. 2011 use the Hessian Free opti mizer in conjunction with structural damping a spe ciÔ¨Åc damping strategy of the Hessian. This approach seems to deal very well with the vanishing gradient though more detailed analysis is still missing. Pre sumably this method works because in high dimen sional spaces there is a high probability for long term components to be orthogonal to short term ones. This would allow the Hessian to rescale these components independently. In practice one can not guarantee that this property holds.', 'By employing the proposed regularization term we are able to improve test error even on tasks that are notOn the diÔ¨Éculty of training Recurrent Neural Networks dominated by long term contributions. 5. Summary and Conclusions We provided diÔ¨Äerent perspectives through which one can gain more insight into the exploding and vanishing gradients issue. To deal with the exploding gradients problem we propose a solution that involves clipping the norm of the exploded gradients when it is too large. The algorithm is motivated by the assumption that when gradients explode the curvature and higher or der derivatives explode as well and we are faced with a speciÔ¨Åc pattern in the error surface namely a val ley with a single steep wall. In order to deal with the vanishing gradient problem we use a regulariza tion term that forces the error signal not to vanish as it travels back in time. This regularization term forces the Jacobian matrices xi xi 1 to preserve norm only in relevant directions.', 'NaN which is easily detectable at runtime this issue is called the Gradient Explosion Problem. When the gradient value goes to zero however it can go undetected while dras tically reducing the learning quality of the model for far away words in the corpus this issue is called the Vanishing Gradient Problem. Due to vanishing gradients we don t know whether there is no dependency between steps t and t n in the data or we just cannot capture the true dependency due to this issue. To gain practical intuition about the vanishing gradient problem you may visit the following example website. 2.4 Solution to the Exploding Vanishing Gradients Now that we gained intuition about the nature of the vanishing gradi ents problem and how it manifests itself in deep neural networks let us focus on a simple and practical heuristic to solve these problems. To solve the problem of exploding gradients Thomas Mikolov Ô¨Årst introduced a simple heuristic solution that clips gradients to a small number whenever they explode.']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9693533216118292,1.0,0.9266232159467768,0.0416666666666666
385,"What is the impact of bidirectional RNNs, and where are they applicable?","Bidirectional RNNs are impactful as they allow for capturing both left and right context information simultaneously, providing a more comprehensive contextual representation. They are applicable when you have access to the entire input sequence, making them powerful for tasks where bidirectionality is beneficial, such as speech recognition, machine translation, and summarization. However, bidirectional RNNs are not applicable to Language Modeling because in LM, only left context is available. If you do have the entire input sequence, bidirectionality is recommended, as seen in models like BERT (Bidirectional Encoder Representations from Transformers) which leverage bidirectional representations effectively.","['Answer German 76 lots of neural architecture lots of neural architectureRNN LMs can be used to generate text e.g. speech recognition machine translation summarization RNN LM what s the the weather what s This is an example of a conditional language model. We ll see Machine Translation in much more detail next lectures. Input audio START conditioning 77Bidirectional RNNs was terribly exciting the movie Forward RNN Backward RNN Concatenated hidden states This contextual representation of terribly has both left and right context 78Bidirectional RNNs 79 Note bidirectional RNNs are only applicable if you have access to the entire input sequence They are not applicable to Language Modeling because in LM you only have left context available. If you do have entire input sequence e.g. any kind of encoding bidirectionality is powerful you should use it by default . For example BERT Bidirectional Encoder Representations from Transformers is a powerful pretrained contextual representation system built on bidirectionality.', 'Recall the goal of a RNN implementation is to en able propagating context information through faraway time steps. For example consider the following two sentences Sentence 1 Jane walked into the room. John walked in too. Jane said hi to ___ Sentence 2 Jane walked into the room. John walked in too. It was late in the day and everyone was walking home after a long day at work. Jane said hi to ___ In both sentences given their context one can tell the answer to both blank spots is most likely John . It is important that the RNN predicts the next word as John the second person who has appeared several time steps back in both contexts. Ideally this should be possible given what we know about RNNs so far. In practice however it turns out RNNs are more likely to correctly predict the blank spot in Sentence 1 than in Sentence 2. This is because during the back propagation phase the contribution of gradient values gradually vanishes as they propagate to earlier timesteps as we will show below.', 'Recurrent Neural Networks RNN A family of neural architectures hidden states input sequence any length Core idea Apply the same weights ùëärepeatedly outputs optional 28A Simple RNN Language Model the students opened their words one hot vectors books laptops word embeddings a zoo output distribution Note this input sequence could be much longer now hidden states is the initial hidden state 28RNN Language Models the students opened their books laptops a zoo RNN Advantages Can process any length input Computation for step t can in theory use information from many steps back Model size doesn t increase for longer input context Same weights applied on every timestep so there is symmetry in how inputs are processed. RNN Disadvantages Recurrent computation is slow In practice difficult to access information from many steps back More on these later 30Training an RNN Language Model Get a big corpus of text which is a sequence of words Feed into RNN LM compute output distribution for every step t. i.e.', 'h h h Figure 8 A bi directional RNN model h t f W xt V h t 1 b 17 h t f W xt V h t 1 b 18 ÀÜ yt g Uht c g U h t h t c 19 RNNs can also be multi layered. Figure 9 shows a multi layer bi directional RNN where each lower layer feeds the next layer. As shown in this Ô¨Ågure in this network architecture at time step t each interme diate neuron receives one set of parameters from the previous time step in the same RNN layer and two sets of parameters from the previous RNN hidden layer one input comes from the left to right RNN and the other from the right to left RNN.']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9343972452259388,1.0,0.9559654163178416,0.0454545454545454
386,Describe the concept of multi-layer or stacked RNNs and their benefit.,"Multi-layer or stacked RNNs involve applying multiple RNN layers on top of each other. This allows the network to compute more complex representations by having lower RNN layers compute lower level features and higher RNN layers compute higher level features. Stacked RNNs work better than having just one layer of high-dimensional encodings, enabling the network to learn more intricate patterns and relationships in the data. In practice, high-performing RNNs are usually multi-layered, with 2 to 4 layers being optimal for tasks like Neural Machine Translation. Skip connections or dense connections are often used to train deeper RNNs effectively. Source: Lecture03_RNNs_and_LLMs.","['You will learn more about transformers including BERT in a couple of weeksMulti layer RNNs RNNs are already deep on one dimension they unroll over many timesteps We can also make them deep in another dimension by applying multiple RNNs this is a multi layer RNN. This allows the network to compute more complex representations The lower RNNs should compute lower level features and the higher RNNs should compute higher level features. Multi layer RNNs are also called stacked RNNs. 80Multi layer RNNs was terribly exciting the movie RNN layer 1 RNN layer 2 81 RNN layer 3 The hidden states from RNN layer i are the inputs to RNN layer i 1Multi layer RNNs in practice 39 Multi layer or stacked RNNs allow a network to compute more complex representations they work better than just have one layer of high dimensional encodings The lower RNNs should compute lower level features and the higher RNNs should compute higher level features.', 'h h h Figure 8 A bi directional RNN model h t f W xt V h t 1 b 17 h t f W xt V h t 1 b 18 ÀÜ yt g Uht c g U h t h t c 19 RNNs can also be multi layered. Figure 9 shows a multi layer bi directional RNN where each lower layer feeds the next layer. As shown in this Ô¨Ågure in this network architecture at time step t each interme diate neuron receives one set of parameters from the previous time step in the same RNN layer and two sets of parameters from the previous RNN hidden layer one input comes from the left to right RNN and the other from the right to left RNN.', 'High performing RNNs are usually multi layer but aren t as deep as convolutional or feed forward networks For example In a 2017 paper Britz et al. find that for Neural Machine Translation 2 to 4 layers is best for the encoder RNN and 4 layers is best for the decoder RNN Often 2 layers is a lot better than 1 and 3 might be a little better than 2 Usually skip connections dense connections are needed to train deeper RNNs e.g. 8 layers Transformer based networks e.g. BERT are usually deeper like 12 or 24 layers.', 'For a RNN with 1000 recurrent layers the matrix would be 1000 1000 regardless of the corpus size. RNNs can be used for many tasks such as tagging e.g. part of speech named entity recognition sentence classiÔ¨Åcation e.g. senti ment classiÔ¨Åcation and encoder modules e.g. questiton answering machine translation and many other tasks . In the latter two applica tions we want a representation for the sentence which we can obtain by taking the element wise max or mean of all hidden states of the timesteps in that sentence. Note Figure 6 is an alternative representation of RNNs used in some publications. It represents the RNN hidden layer as a loop. ht Figure 6 The illustration of a RNN as a loop over time steps 2.3 Vanishing Gradient Gradient Explosion Problems Recurrent neural networks propagate weight matrices from one time step to the next.']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9575957298707172,1.0,0.9621506083095652,0.0714285714285714
387,How does Dropout work during training and testing?,"During training, Dropout works by randomly setting a fraction of the neurons in a layer to zero based on a specified probability \( p \), while during testing, all neurons are used. This technique helps prevent overfitting by training multiple smaller networks simultaneously and averaging their predictions during training. During backpropagation, gradients are only passed through the neurons that were kept active during the forward pass. To ensure Dropout works effectively, the expected output of a neuron during testing should be similar to that during training, requiring adjustments such as scaling the outputs during testing. 

Source: Lecture02_notes_Backpropagation","['The result is that the network typically learns more meaningful informa tion from the data is less likely to overÔ¨Åt and usually obtains higher performance overall on the task at hand. One intuitive reason why this technique should be so effective is that what dropout is doing is essentially doing is training exponentially many smaller networks at once and averaging over their predictions. Dropout applied to an artiÔ¨Åcial neural network. Image credits to Srivastava et al. In practice the way we introduce dropout is that we take the out put h of each layer of neurons and keep each neuron with prob ability p and else set it to 0. Then during back propagation we only pass gradients through neurons that were kept alive during the forward pass. Finally during testing we compute the forward pass using all of the neurons in the network.', 'However a key sub tlety is that in order for dropout to work effectively the expected output of a neuron during testing should be approximately the same as it was during training else the magnitude of the outputs could be radically different and the behavior of the network is no longer well deÔ¨Åned. Thus we must typically divide the outputs of each neuron during testing by a certain value it is left as an exercise to the reader to determine what this value should be in order for the expected outputs during training and testing to be equivalent. 2.4 Neuron Units So far we have discussed neural networks that contain sigmoidal neurons to introduce nonlinearities however in many applications better networks can be designed using other activation functions. Some common choices are listed here with their function and gra dient deÔ¨Ånitions and these can be substituted with the sigmoidal functions discussed above.', 'Lastly all the training examples are sorted by the length of paragraph and divided into mini batches of 32 examples each. We use Adamax for optimization as described in Kingma and Ba 5We use the Wikipedia Search API https www. mediawiki.org wiki API Search. 2014 . Dropout with p 0.3 is applied to word embeddings and all the hidden units of LSTMs. Result and analysis Table 4 presents our eval uation results on both development and test sets. SQuAD has been a very competitive machine comprehension benchmark since its creation and we only list the best performing systems in the ta ble. Our system single model can achieve 70.0 exact match and 79.0 F1 scores on the test set which surpasses all the published results and can match the top performance on the SQuAD leader board at the time of writing. Additionally we think that our model is conceptually simpler than most of the existing systems.', 'We call this situation training on the test set. Training on the test set introduces a bias that makes the probabilities all look too high and causes huge inaccuracies in perplexity the probability based metric we introduce below. Sometimes we use a particular test set so often that we implicitly tune to its characteristics. We then need a fresh test set that is truly unseen. In such cases we call the initial test set the development test set or devset. How do we divide our development test data into training development and test sets We want our test set to be as large as possible since a small test set may be accidentally unrepresentative but we also8 CHAPTER 3 N GRAM LANGUAGE MODELS want as much training data as possible. At the minimum we would want to pick the smallest test set that gives us enough statistical power to measure a statistically signiÔ¨Åcant difference between two potential models. In practice we often just divide our data into 80 training 10 development and 10 test.']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.988222493203864,1.0,0.9187157595170112,0.0357142857142857
388,What is the role of parameter initialization in neural networks?,"Parameter initialization in neural networks plays a crucial role in achieving superlative performance. A good starting strategy is to initialize the weights to small random numbers normally distributed around 0. Different weight and bias initialization schemes can have a significant impact on training dynamics and can affect convergence speed and error rates. Proper initialization helps in maintaining activation variances and backpropagated gradient variances across layers, which are essential for effective training. Additionally, initialization methods can help address issues like vanishing gradients and exploding gradients, ultimately influencing the overall performance of the neural network. 

Source: lecture02_notes_Backpropagation","['We can then take the Singular Value Decomposition SVD of X to get matrices U S V. We then compute UX to project X into the basis deÔ¨Åned by the columns of U. We Ô¨Ånally divide each dimension of the result by the corresponding singular value in S to scale our data appropriately if a singular value is zero we can just divide by a small number instead . 2.6 Parameter Initialization A key step towards achieving superlative performance with a neu ral network is initializing the parameters in a reasonable way. A good starting strategy is to initialize the weights to small random numbers normally distributed around 0 and in practice this often words acceptably well. However in Understanding the dif ficulty of training deep feedforward neural networks 2010 Xavier et al study the effect of different weight and bias initialization schemes on training dynamics.', 'We should also note that such initialization does not prevent cross ing the boundary between basins of attraction which as shown could happen even though no bifurcation boundary is crossed. Teacher forcing is a more interesting yet a not very well understood solution. It can be seen as a way of initializing the model in the right regime and the rightOn the diÔ¨Éculty of training Recurrent Neural Networks region of space. It has been shown that in practice it can reduce the chance that gradients explode and even allow training generator models or models that work with unbounded amounts of memory Pascanu and Jaeger 2011 Doya and Yoshizawa 1991 . One important downside is that it requires a target to be deÔ¨Åned at every time step. In Hochreiter and Schmidhuber 1997 Graves et al. 2009 a solution is proposed for the vanishing gra dients problem where the structure of the model is changed.', 'The empirical Ô¨Åndings suggest that for sigmoid and tanh activation units faster convergence and lower error rates are achieved when the weights of a matrix W Rn l 1 n l are initialized randomly with a uniform distribution as follows W U s 6 n l n l 1 s 6 n l n l 1 Where n l is the number of input units to W fan in and n l 1 is the number of output units from W fan out . In this parameter initialization scheme bias units are initialized to 0. This approach attempts to maintain activation variances as well as backpropagated gradient variances across layers. Without such initialization the gradient variances which are a proxy for information generally decrease with backpropagation across layers. 2.7 Learning Strategies The rate magnitude of model parameter updates during training can be controlled using the learning rate.', 'Applying either weight normalization or batch normal ization using expected statistics is equivalent to have a different parameterization of the original feed forward neural network. Re parameterization in the ReLU network was studied in the Path normalized SGD Neyshabur et al. 2015 . Our proposed layer normalization method however is not a re parameterization of the original neural network. The layer normalized model thus has different invariance properties than the other methods that we will study in the following section. 5 Analysis In this section we investigate the invariance properties of different normalization schemes. 5.1 Invariance under weights and data transformations The proposed layer normalization is related to batch normalization and weight normalization. Al though their normalization scalars are computed differently these methods can be summarized as normalizing the summed inputs ai to a neuron through the two scalars ¬µ and œÉ. They also learn an adaptive bias b and gain g for each neuron after the normalization. hi f gi œÉi ai ¬µi bi 5 Note that for layer normalization and batch normalization ¬µ and œÉ is computed according to Eq.']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9872464643577216,1.0,0.914341453482722,0.0416666666666666
389,"What are the differences between Vanilla RNNs, LSTMs, and GRUs?","Vanilla RNNs, LSTMs, and GRUs are all types of recurrent neural networks, but they differ in their architectures and capabilities. 

1. Vanilla RNNs: These are the simplest form of recurrent neural networks. They suffer from the vanishing gradient problem, which makes it difficult for them to capture long-term dependencies in sequences.

2. LSTMs (Long Short-Term Memory): LSTMs were designed to address the vanishing gradient problem in Vanilla RNNs. They have a more complex architecture with mechanisms like input, forget, and output gates, which allow them to retain long-term dependencies in sequences better.

3. GRUs (Gated Recurrent Units): GRUs are a variation of LSTMs that combine the forget and input gates into a single update gate. They also merge the cell state and hidden state, making the model simpler compared to LSTMs. GRUs have been found to be effective in certain tasks and are growing in popularity.

Each of these variants has its own strengths and weaknesses, and researchers have been exploring different architectures to improve the performance of recurrent neural networks for various NLP tasks.

Source: lecture03_RNNs_and_LLMs.txt","['Which of these variants is best Do the differences matter Greff et al. 2015 http arxiv.org pdf 1503.04069.pdf do a nice comparison of popular variants finding that they re all about the same. Jozefowicz et al. 2015 http jmlr.org proceedings papers v37 jozefowicz15.pdf tested more than ten thousand RNN architectures finding some that worked better than LSTMs on certain tasks. Conclusion Earlier I mentioned the remarkable results people are achieving with RNNs. Essentially all of these are achieved using LSTMs. They really work a lot better for most tasks Written down as a set of equations LSTMs look pretty intimidating.', 'We only forget when we re going to input something in its place. We only input new values to the state when we forget something older. A slightly more dramatic variation on the LSTM is the Gated Recurrent Unit or GRU introduced by Cho et al. 2014 http arxiv.org pdf 1406.1078v3.pdf . It combines the forget and input gates into a single update gate. It also merges the cell state and hidden state and makes some other changes. The resulting model is simpler than standard LSTM models and has been growing increasingly popular. These are only a few of the most notable LSTM variants. There are lots of others like Depth Gated RNNs by Yao et al. 2015 http arxiv.org pdf 1508.03790v2.pdf . There s also some completely different approach to tackling long term dependencies like Clockwork RNNs by Koutnik et al. 2014 http arxiv.org pdf 1402.3511v1.pdf .', 'Figure 12 The detailed internals of a GRU It is important to note that to train a GRU we need to learn all the different parameters W U W r U r W z U z . These follow the same backpropagation procedure we have seen in the past. 4 Long Short Term Memories Long Short Term Memories are another type of complex activation unit that differ a little from GRUs. The motivation for using these is similar to those for GRUs however the architecture of such units does differ.', 'You will learn about Transformers later they have a lot of skipping like connections Massive Exploration of Neural Machine Translation Architecutres Britz et al 2017. https arxiv.org pdf 1703.03906.pdfTerminology By the end of the course You will understand phrases like stacked bidirectional LSTMs with residual connections and self attention The RNN described in this lecture simple vanilla Elman RNN You learned about other RNN flavors like LSTM and GRU and multi layer RNNs 83']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9635808261014066,1.0,0.9538745996375216,0.05
390,What is the difference between traditional language models and conditioned language models?,"Traditional language models are trained to predict the next word in a sequence based on the history of words, while conditioned language models take additional input, such as a prompt or a specific context, to generate text that is more tailored to that input. Conditioned language models can produce more targeted and contextually relevant outputs compared to traditional language models.

Source: Language_Models_are_Few_Shot_Learners lecture.","['For many purposes a pretrained multilingual model is more practical than a monolingual model since it avoids the need to build many 100 separate monolin gual models. And multilingual models can improve performance on low resourced languages by leveraging linguistic information from a similar language in the train ing data that happens to have more resources. Nonetheless when the number of languages grows very large multilingual models exhibit what has been called the curse of multilinguality Conneau et al. 2020 the performance on each language degrades compared to a model training on fewer languages. Another problem with multilingual models is that they have an accent grammatical structures in higher resource languages often English bleed into lower resource languages the vast amount of English language in training makes the model s representations for low resource languages slightly more English like Papadimitriou et al. 2023 .', 'They showed the advantages of ModiÔ¨Åed In terpolated Kneser Ney which became the standard baseline for n gram language modeling especially because they showed that caches and class based models pro vided only minor additional improvement. These papers are recommended for any reader with further interest in n gram language modeling. SRILM Stolcke 2002 and KenLM HeaÔ¨Åeld 2011 HeaÔ¨Åeld et al. 2013 are publicly available toolkits for building n gram language models. Modern language modeling is more commonly done with neural network lan guage models which solve the major problems with n grams the number of param eters increases exponentially as the n gram order increases and n grams have no way to generalize from training to test set. Neural language models instead project words into a continuous space in which words with similar contexts have similar representations. We ll introduce both feedforward language models Bengio et al.', 'We do submit to the test server on a small number of datasets SuperGLUE TriviaQA PiQa where we were able to make submission work and we submit only the 200B few shot results and report development set results for everything else. 3 Results In Figure 3.1 we display training curves for the 8 models described in Section 2. For this graph we also include 6 additional extra small models with as few as 100 000 parameters. As observed in KMH 20 language modeling performance follows a power law when making efÔ¨Åcient use of training compute. After extending this trend by two more orders of magnitude we observe only a slight if any departure from the power law. One might worry that these improvements in cross entropy loss come only from modeling spurious details of our training corpus. However we will see in the following sections that improvements in cross entropy loss lead to consistent performance gains across a broad spectrum of natural language tasks.', 'here are two examples of people acting brave please give a third example of bravery is often Figure 1.1 Language model meta learning. During unsupervised pre training a language model develops a broad set of skills and pattern recognition abilities. It then uses these abilities at inference time to rapidly adapt to or recognize the desired task. We use the term in context learning to describe the inner loop of this process which occurs within the forward pass upon each sequence. The sequences in this diagram are not intended to be representative of the data a model would see during pre training but are intended to show that there are sometimes repeated sub tasks embedded within a single sequence. 3Figure 1.2 Larger models make increasingly efÔ¨Åcient use of in context information. We show in context learning performance on a simple task requiring the model to remove random symbols from a word both with and without a natural language task description see Sec. 3.9.2 .']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999989821844766,0.0,0.946004803442728,0.0416666666666666
391,How can LSTM networks be used in language modeling?,"LSTM networks can be used in language modeling to model the temporal interactions between words. By placing an LSTM on top of the embeddings provided by previous layers, LSTM networks can capture long-term dependencies in the sequential data, making them effective for tasks like predicting the next word in a sentence or generating text based on the context. Additionally, LSTM networks can be used bidirectionally by placing LSTMs in both directions and concatenating their outputs to capture context from both past and future words effectively. This allows LSTM networks to capture complex patterns and dependencies in language data, making them suitable for language modeling tasks.
Source: BI_DIRECTIONAL_ATTENTION_FLOW_FOR_MACHINE_COMPREHENSION.txt","['Recap 69 Language Model A system that predicts the next word Recurrent Neural Network A family of neural networks that Take sequential input of any length Apply the same weights on each step Can optionally produce output on each step Recurrent Neural Network Language Model We ve shown that RNNs are a great way to build a LM despite some problems RNNs are also useful for much moreWhy should we care about Language Modeling 70 Language Modeling is a benchmark task that helps us measure our progress on predicting language use Language Modeling is a subcomponent of many NLP tasks especially those involving generating text or estimating the probability of text Predictive typing Speech recognition Handwriting recognition Spelling grammar correction Authorship identification Machine translation Summarization Dialogue etc. Everything else in NLP has now been rebuilt upon Language Modeling GPT 3 is an LMOther RNN ArchitecturesOther RNN uses RNNs can be used for sequence tagging e.g.', 'We report results with both the word level log likelihood WLL and the sentence level log likelihood SLL . We report with LMn performance of the networks trained from the language model embeddings Table 7 . Gen eralization performance is reported in per word accuracy PWA for POS and F1 score for other tasks. language models from the relatively fast training of the supervised networks. Once the language models are trained we can perform multiple experiments on the supervised networks in a rela tively short time. Note that our procedure is clearly linked to the semi supervised deep learning procedures of Hinton et al. 2006 Bengio et al. 2007 and Weston et al. 2008 . Table 8 clearly shows that this simple initialization signiÔ¨Åcantly boosts the generalization per formance of the supervised networks for each task. It is worth mentioning the larger language model led to even better performance. This suggests that we could still take advantage of even bigger unlabeled data sets.', '4.3 Training Language Models The language model network was trained by stochastic gradient minimization of the ranking crite rion 17 sampling a sentence word pair s w at each iteration. 16. However Klein and Manning 2002 describe a rare example of realistic unsupervised grammar induction using a cross entropy approach on binary branching parsing trees that is by forcing the system to generate a hierarchical representation. 2512NATURAL LANGUAGE PROCESSING ALMOST FROM SCRATCH Since training times for such large scale systems are counted in weeks it is not feasible to try many combinations of hyperparameters. It also makes sense to speed up the training time by initializing new networks with the embeddings computed by earlier networks. In particular we found it expedient to train a succession of networks using increasingly large dictionaries each network being initialized with the embeddings of the previous network. Successive dictionary sizes and switching times are chosen arbitrarily. Bengio et al. 2009 provides a more detailed discussion of this the as yet poorly understood curriculum process.', '3. Contextual Embedding Layer. We use a Long Short Term Memory Network LSTM Hochreiter Schmidhuber 1997 on top of the embeddings provided by the previous layers to model the temporal interactions between words. We place an LSTM in both directions and concatenate the outputs of the two LSTMs. Hence we obtain H R2d T from the context word vectors X and U R2d J from query word vectors Q. Note that each column vector of H and U is 2d dimensional because of the concatenation of the outputs of the forward and backward LSTMs each with d dimensional output. It is worth noting that the Ô¨Årst three layers of the model are computing features from the query and context at different levels of granularity akin to the multi stage feature computation of convolutional neural networks in the computer vision Ô¨Åeld. 4. Attention Flow Layer. Attention Ô¨Çow layer is responsible for linking and fusing information from the context and the query words.']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9841818755895096,1.0,0.9472354495859204,0.2692307692307692
392,What are the methods of generation in conditioned language modeling?,"The methods of generation in conditioned language modeling are Sampling (trying to generate a random sentence according to the probability distribution) and Argmax (trying to generate the sentence with the highest probability). Ancestral Sampling involves randomly generating words one by one, which is an exact method for sampling from P X without needing further work. Greedy Search involves picking the single highest probability word one by one, but it may not be exact and often generates easy words first. Beam Search maintains several paths instead of picking just one high probability word. Model Ensembling involves combining predictions from multiple models to smooth over idiosyncrasies of the model.

Source: lecture04_Conditioned_Generation.","['file name: lecture04 Conditioned Generation. Hamidreza Mahyar mahyarh@mcmaster.ca Computational Natural Language Processing Conditioned GenerationLanguage Models Language models are generative models of text s P x The Malfoys said Hermione. Harry was watching him. He looked like Madame Maxime. When she strode up the wrong staircase to visit himself. I m afraid I ve definitely been suspended from power no chance indeed said Snape. He put his head back behind them and read groups as they crossed a corner and fluttered down onto their ink lamp and picked up his spoon. The doorbell rang. It was a lot cleaner down in London.', 'Instruction finetuning 3. Reinforcement Learning from Human Feedback RLHF 4. What s nextLecture Plan From Language Models to Assistants 13 1. Zero Shot ZS and Few Shot FS In Context Learning 2. Instruction finetuning 3. Reinforcement Learning from Human Feedback RLHF 4. What s nextLet s revisit the Generative Pretrained Transformer GPT models from OpenAI as an example Emergent abilities of large language models GPT 2018 GPT 117M parameters Radford et al. 2018 Transformer decoder with 12 layers. Trained on BooksCorpus over 7000 unique books 4.6GB text . Showed that language modeling at scale can be an effective pretraining technique for downstream tasks like natural language inference.', '2014 encoder decoder Transform can be different dimensions encoder transform decoder encoder Input at every time step Kalchbrenner Blunsom 2013 decoder decoder decoderMethods of GenerationThe Generation Problem We have a model of P Y X how do we use it to generate a sentence Two methods Sampling Try to generate a random sentence according to the probability distribution. Argmax Try to generate the sentence with the highest probability.Ancestral Sampling Randomly generate words one by one. An exact method for sampling from P X no further work needed.Greedy Search One by one pick the single highest probability word Not exact real problems Will often generate the easy words first Will prefer multiple common words to one rare wordBeam Search Instead of picking one high probability word maintain several paths Some in reading materials more in a later classModel EnsemblingEnsembling Why Multiple models make somewhat uncorrelated errors Models tend to be more uncertain when they are about to make errors Smooths over idiosyncrasies of the model LSTM1 predict1 I LSTM2 predict2 Combine predictions from multiple models s sLinear Interpolation Take', '4.3 Training Language Models The language model network was trained by stochastic gradient minimization of the ranking crite rion 17 sampling a sentence word pair s w at each iteration. 16. However Klein and Manning 2002 describe a rare example of realistic unsupervised grammar induction using a cross entropy approach on binary branching parsing trees that is by forcing the system to generate a hierarchical representation. 2512NATURAL LANGUAGE PROCESSING ALMOST FROM SCRATCH Since training times for such large scale systems are counted in weeks it is not feasible to try many combinations of hyperparameters. It also makes sense to speed up the training time by initializing new networks with the embeddings computed by earlier networks. In particular we found it expedient to train a succession of networks using increasingly large dictionaries each network being initialized with the embeddings of the previous network. Successive dictionary sizes and switching times are chosen arbitrarily. Bengio et al. 2009 provides a more detailed discussion of this the as yet poorly understood curriculum process.']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999985757308452,1.0,0.9233023981184654,0.0344827586206896
393,What is beam search and how is it used in language generation?,"Beam search is a technique used in language generation where, instead of picking just one high probability word at each step, several paths are maintained. This allows for exploring multiple possibilities simultaneously, which can lead to better overall results. Beam search is not guaranteed to find the optimal solution but is much more efficient than exhaustive search. It involves calculating the probability distribution of the next word, taking the top k words, computing scores, and finding the top k next words for each hypothesis. By considering multiple options at each step, beam search helps in generating more coherent and context-aware responses in natural language generation tasks.

Source: lecture05_Seq2Seq.txt","['top k on each step Beam search is not guaranteed to find optimal solution But much more efficient than exhaustive search 18Beam search decoding example Beam size k 2. Blue numbers START Calculate prob dist of next word 19Beam search decoding example Beam size k 2. Blue numbers START he I 0.7 log PLM he START 0.9 log PLM I START Take top k words and compute scores 20Beam search decoding example Beam size k 2. Blue numbers hit struck was got START he I 10 0.7 0.9 2.9 log PLM struck START he 0.7 1.6 log PLM was START I 0.9 1.8 log PLM got START I 0.9 1.7 log PLM hit START he 0.7 For each of the k hypotheses find top k next words and calculate scoresBeam search decoding example Beam size k 2.', 'Natural Language Understanding NLU Employ models like BERT for NLU tasks to accurately parse student inquiries determining their intent and extracting relevant details for precise responses. Retrieval Augmented Generation RAG Central to the NLP Engine RAG com bines retrieval methods to access information within a structured database of course materials with generative capabilities to formulate coherent and context aware respon ses . This approach ensures the AI TA delivers informative answers directly pulling from course content and synthesizing it into helpful responses. Response Generation Utilize the designed AI TA to generate responses that are not only accurate but also engaging and pedagogically effective. Interaction Interface Simplified Interface Development Construct an intuitive web or command line in terface that allows straightforward question submission and response retrieval ensuring ease of use for student interactions with the AI TA. McMaster University 1McMaster University SEP 775 Project 2 TV Show Script Generation Project Overview For this project you will harness the power of Natural Language Processing NLP and Machine Learning to create a script for a new episode of a TV show you enjoy.', '2014 encoder decoder Transform can be different dimensions encoder transform decoder encoder Input at every time step Kalchbrenner Blunsom 2013 decoder decoder decoderMethods of GenerationThe Generation Problem We have a model of P Y X how do we use it to generate a sentence Two methods Sampling Try to generate a random sentence according to the probability distribution. Argmax Try to generate the sentence with the highest probability.Ancestral Sampling Randomly generate words one by one. An exact method for sampling from P X no further work needed.Greedy Search One by one pick the single highest probability word Not exact real problems Will often generate the easy words first Will prefer multiple common words to one rare wordBeam Search Instead of picking one high probability word maintain several paths Some in reading materials more in a later classModel EnsemblingEnsembling Why Multiple models make somewhat uncorrelated errors Models tend to be more uncertain when they are about to make errors Smooths over idiosyncrasies of the model LSTM1 predict1 I LSTM2 predict2 Combine predictions from multiple models s sLinear Interpolation Take', 'On tasks that involve binary classiÔ¨Åcation we give the options more semantically meaningful names e.g. True or False rather than 0 or 1 and then treat the task like multiple choice we also sometimes frame the task similar to what is done by RSR 19 see Appendix G for details. On tasks with free form completion we use beam search with the same parameters as RSR 19 a beam width of 4 and a length penalty of Œ± 0.6. We score the model using F1 similarity score BLEU or exact match depending on what is standard for the dataset at hand. Final results are reported on the test set when publicly available for each model size and learning setting zero one and few shot . When the test set is private our model is often too large to Ô¨Åt on the test server so we report results on the development set.']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9218209045611296,1.0,0.9434360694305683,0.0555555555555555
394,Explain the concept of model ensembling in language models.,"Model ensembling in language models refers to the technique of combining multiple individual models to improve overall performance. By aggregating predictions from different models, ensembling can help reduce overfitting, increase robustness, and enhance the overall accuracy of the model. This approach is commonly used in machine learning to leverage the strengths of diverse models and produce more reliable results. In the context of language models, ensembling can involve combining various architectures, techniques, or approaches to enhance the model's capabilities in tasks such as question answering, reading comprehension, and language translation. It can also help in handling different types of datasets and improving the model's generalization ability.","['This approach includes ALBERT LCG 19 as well as general HVD15 and 39task speciÔ¨Åc SDCW19 JYS 19 KR16 approaches to distillation of language models. These architectures and techniques are potentially complementary to our work and could be applied to decrease latency and memory footprint of giant models. As Ô¨Åne tuned language models have neared human performance on many standard benchmark tasks considerable effort has been devoted to constructing more difÔ¨Åcult or open ended tasks including question answering KPR 19 IBGC 14 CCE 18 MCKS18 reading comprehension CHI 18 RCM19 and adversarially constructed datasets designed to be difÔ¨Åcult for existing language models SBBC19 NWD 19 . In this work we test our models on many of these datasets. Many previous efforts have focused speciÔ¨Åcally on question answering which constitutes a signiÔ¨Åcant fraction of the tasks we tested on.', 'Nonetheless because perplexity often correlates with such improvements it is commonly used as a quick check on an algorithm. But a model s improvement in perplexity should always be conÔ¨Årmed by an end to end evaluation of a real task before concluding the evaluation of the model. 3.3 Sampling sentences from a language model One important way to visualize what kind of knowledge a language model embodies is to sample from it. Sampling from a distribution means to choose random points sampling according to their likelihood. Thus sampling from a language model which rep resents a distribution over sentences means to generate some sentences choosing each sentence according to its likelihood as deÔ¨Åned by the model. Thus we are more likely to generate sentences that the model thinks have a high probability and less likely to generate sentences that the model thinks have a low probability. This technique of visualizing a language model by sampling was Ô¨Årst suggested very early on by Shannon 1951 and Miller and Selfridge 1950 . It s simplest to visualize how this works for the unigram case.', 'Below we evaluate the 8 models described in Section 2 the 175 billion parameter parameter GPT 3 and 7 smaller models on a wide range of datasets. We group the datasets into 9 categories representing roughly similar tasks. In Section 3.1 we evaluate on traditional language modeling tasks and tasks that are similar to language modeling such as Cloze tasks and sentence paragraph completion tasks. In Section 3.2 we evaluate on closed book question answering tasks tasks which require using the information stored in the model s parameters to answer general knowledge questions. In Section 3.3 we evaluate the model s ability to translate between languages especially one shot and few shot . In Section 3.4 we evaluate the model s performance on Winograd Schema like tasks. In Section 3.5 we evaluate on datasets that involve commonsense reasoning or question answering.', 'As for limitations of our study there is a degree of subjectivity in assigning tasks to clusters though we try to use accepted categorizations in the literature and we only explore the use of relatively short instructions of typically a single sentence c.f. detailed instructions given to crowd workers . A limitation for our evaluation is that individual examples might have appeared in the models pretraining data which includes web documents though in post hoc analysis Appendix C we do not Ô¨Ånd any evidence that data overlap substantially impacted the results. Finally the scale of FLAN 137B makes it costly to serve. Future work on instruction tuning could include gathering generating even more task clusters for Ô¨Ånetuning cross lingual experiments using FLAN to generate data for training downstream classiÔ¨Åers and using Ô¨Ånetuning to improve model behavior with respect to bias and fairness Solaiman Dennison 2021 . 7 CONCLUSIONS This paper has explored a simple method for improving the ability of language models at scale to perform zero shot tasks based purely on instructions.']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9582858448573888,0.0,0.9199510803771248,0.0
395,How does ensemble distillation differ from parameter averaging?,"Ensemble distillation involves training a model to copy the ensemble by matching the description over predicted words to increase accuracy notably, while parameter averaging is a cheap way to get some good effects of ensembling by writing out models several times near the end of training and taking the average of parameters. Ensemble distillation focuses on matching the ensemble's behavior, while parameter averaging focuses on averaging parameters of multiple models.","['e.g. Kim et al. 2016 Problem parameter averaging only works for models within the same run Knowledge distillation trains a model to copy the ensemble Specifically it tries to match the description over predicted words Why We want the model to make the same mistakes as an ensemble Shown to increase accuracy notablyStacking What if we have two very different models where prediction of outputs is done in very different ways e.g. a phrase based translation model and a neural MT model Niehues et al. 2017 Stacking uses the output of one system in calculating features for another systemCase Studies in Conditional Language ModelingFrom Images e.g. Karpathy et al. 2015 Input is image features output is text Use standard image encoders e.g.', 'predictions from multiple models s sLinear Interpolation Take a weighted average of the M model probabilities Second term often set to uniform distribution 1 MLog linear Interpolation Weighted combination of log probabilities normalize Interpolation coefficient often set to uniform distribution 1 MLinear or Log Linear Think of it in logic Linear Logical OR the interpolated model likes any choice that a model gives a high probability use models with models that capture different traits necessary when any model can assign zero probability Log Linear Logical AND interpolated model only likes choices where all models agree use when you want to restrict possible answersParameter Averaging Problem Ensembling means we have to use M models at test time increasing our time memory complexity Parameter averaging is a cheap way to get some good effects of ensembling Basically write out models several times near the end of training and take the average of parametersEnsemble Distillation e.g. Kim et al.', 'Yet it is often difÔ¨Åcult to Ô¨Ågure out which ideas are most responsible for the state of the art performance of a large ensemble. Because neural networks are nonconvex training runs with different initial parameters usually give different solutions. Table 11 reports results obtained for the CHUNK and NER task after ten training runs with random initial parameters. Voting the ten network outputs on a per tag basis voting ensemble leads to a small improvement over the average network performance. We have also tried a more sophisticated ensemble approach the ten network output scores before sentence level likelihood were combined with an additional linear layer 4 and then fed to a new sentence level likelihood 13 . The parameters of the combining layers were then trained on the existing training set while keeping the ten networks Ô¨Åxed joined ensemble . This approach did not improve on simple voting. These ensembles come of course at the expense of a ten fold increase of the running time. On the other hand multiple training times could be improved using smart sampling strategies Neal 1996 .', 'This difference indicates the potential gain of tuning a given hyperparameter as well as the risks of decreased performance when not tuning it. For example an entry of 9.2 in Table d means that the best model with cds 0.75 is 9.2 more accurate absolute than the best model with cds 1 i.e. on MSR s analogies using cds 0.75 instead of cds 1 improved PPMI s accuracy from .443 to .535. 223Acknowledgements This work was supported by the Google Research Award Program and the German Research Foun dation via the German Israeli Project Cooperation grant DA 1600 1 1 . We thank Marco Baroni and Jeffrey Pennington for their valuable comments. References Eneko Agirre Enrique Alfonseca Keith Hall Jana Kravalova Marius Pasca and Aitor Soroa. 2009. A study on similarity and relatedness using distribu tional and wordnet based approaches.']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.987278043698859,1.0,0.9797652472945128,0.0555555555555555
396,What are some common evaluation metrics for conditioned language models?,"Some common evaluation metrics for conditioned language models include BLEU, embedding-based metrics like BertScore, BLEURT, COMET, PRISM, and perplexity. BLEU works by comparing n-gram overlap with a reference, while embedding-based metrics use neural models to find similarity between embeddings or train models to predict human evaluation scores. Perplexity calculates the perplexity of words in a held-out set without generating output, which can help solve the multiple reference problem.","['CNN Often pre trained on large databases such as ImageNetCheck out this review paperHow do we EvaluateBasic Evaluation Paradigm Use parallel test set Use system to generate translations Compare target translations w referenceHuman Evaluation Ask a human to do evaluation Final goal but slow expensive and sometimes inconsistentBLEU Works by comparing n gram overlap w reference Pros Easy to use good for measuring system improvement Cons Often doesn t match human eval bad for comparing very different systemsEmbedding based Metrics Recently many metrics based on neural models BertScore Find similarity between BERT embeddings unsupervised Zhang et al. 2020 BLEURT Train BERT to predict human evaluation scores Sellam et al. 2020 COMET Train model to predict human eval also using source sentence Rei et al. 2020 PRISM Model based on training paraphrasing model Thompson and Post 2020Perplexity Calculate the perplexity of the words in the held out set without doing generation Pros Naturally solves multiple reference problem Cons Doesn t consider decoding or actually generating output.', '8 Conclusion We presented a 175 billion parameter language model which shows strong performance on many NLP tasks and benchmarks in the zero shot one shot and few shot settings in some cases nearly matching the performance of 40state of the art Ô¨Åne tuned systems as well as generating high quality samples and strong qualitative performance at tasks deÔ¨Åned on the Ô¨Çy. We documented roughly predictable trends of scaling in performance without using Ô¨Åne tuning. We also discussed the social impacts of this class of model. Despite many limitations and weaknesses these results suggest that very large language models may be an important ingredient in the development of adaptable general language systems. Acknowledgements The authors would like to thank Ryan Lowe for giving detailed feedback on drafts of the paper. Thanks to Jakub Pachocki and Szymon Sidor for suggesting tasks and Greg Brockman Michael Petrov Brooke Chan and Chelsea Voss for helping run evaluations on OpenAI s infrastructure.', 'Thus our MT evaluation system requires two ingredients 1. a numerical translation closeness metric 2. a corpus of good quality human reference trans lations We fashion our closeness metric after the highly suc cessful word error rate metric used by the speech recognition community appropriately modiÔ¨Åed for multiple reference translations and allowing for le gitimate differences in word choice and word or der. The main idea is to use a weighted average of variable length phrase matches against the reference translations. This view gives rise to a family of met rics using various weighting schemes. We have se lected a promising baseline metric from this family. In Section 2 we describe the baseline metric in detail. In Section 3 we evaluate the performance of BLEU. In Section 4 we describe a human evaluation experiment. In Section 5 we compare our baseline metric performance with human evaluations. Computational Linguistics ACL Philadelphia July 2002 pp. 311 318.', 'Extrinsic evaluation is the only way to extrinsic evaluation know if a particular improvement in a component is really going to help the task at hand. Thus for speech recognition we can compare the performance of two language models by running the speech recognizer twice once with each language model and seeing which gives the more accurate transcription. Unfortunately running big NLP systems end to end is often very expensive. In stead it would be nice to have a metric that can be used to quickly evaluate potential improvements in a language model. An intrinsic evaluation metric is one that mea intrinsic evaluation sures the quality of a model independent of any application. For an intrinsic evaluation of a language model we need a test set. As with many of the statistical models in our Ô¨Åeld the probabilities of an n gram model come from the corpus it is trained on the training set or training corpus. We can then measure training set the quality of an n gram model by its performance on some unseen data called the test set or test corpus.']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.0000000000000002,1.0,0.9512813523387216,0.0384615384615384
397,What is the significance of stacking in model ensembling?,"Stacking is significant in model ensembling when we have two very different models that predict outputs in different ways. It involves using the output of one system to calculate features for another system, which can be beneficial in cases where models capture different traits. Stacking helps improve accuracy by combining the strengths of different models to make more robust predictions.","['e.g. Kim et al. 2016 Problem parameter averaging only works for models within the same run Knowledge distillation trains a model to copy the ensemble Specifically it tries to match the description over predicted words Why We want the model to make the same mistakes as an ensemble Shown to increase accuracy notablyStacking What if we have two very different models where prediction of outputs is done in very different ways e.g. a phrase based translation model and a neural MT model Niehues et al. 2017 Stacking uses the output of one system in calculating features for another systemCase Studies in Conditional Language ModelingFrom Images e.g. Karpathy et al. 2015 Input is image features output is text Use standard image encoders e.g.', 'predictions from multiple models s sLinear Interpolation Take a weighted average of the M model probabilities Second term often set to uniform distribution 1 MLog linear Interpolation Weighted combination of log probabilities normalize Interpolation coefficient often set to uniform distribution 1 MLinear or Log Linear Think of it in logic Linear Logical OR the interpolated model likes any choice that a model gives a high probability use models with models that capture different traits necessary when any model can assign zero probability Log Linear Logical AND interpolated model only likes choices where all models agree use when you want to restrict possible answersParameter Averaging Problem Ensembling means we have to use M models at test time increasing our time memory complexity Parameter averaging is a cheap way to get some good effects of ensembling Basically write out models several times near the end of training and take the average of parametersEnsemble Distillation e.g. Kim et al.', '4.3 ROLE OF INSTRUCTIONS 20 30 40 50 60 FT no instruction Eval instruction FT dataset name Eval instruction FT dataset name Eval dataset name FT instruction Eval instruction FLAN 37.3 46.6 47.0 55.2 Zero shot performance 4 task cluster avg. Figure 8 Ablation study result using mod els with instructions removed from Ô¨Ånetun ing FT . In a Ô¨Ånal ablation study we explore the role of in structions during Ô¨Ånetuning as one possibility is that performance gains come entirely from multi task Ô¨Åne tuning and the model could perform just as well without instructions. We hence consider two Ô¨Ånetuning setups without instructions. In a no template setup only inputs and outputs were given to the model e.g. for transla tion the input would be The dog runs. and the output would be Le chien court. .', 'Figure 7 shows these results. We see that for the two models on the order of 100B pa rameters instruction tuning substantially im proves performance on held out tasks as is expected given the prior results in our pa per. The behavior on held out tasks for the 8B and smaller models however is thought provoking instruction tuning actually hurts performance on held out tasks. One potential explana tion for this result could be that for small scale models learning the 40 tasks used during instruction tuning Ô¨Ålls the entire model capacity causing these models to perform worse on new tasks. Under this potential explanation for the larger scale models instruction tuning Ô¨Ålls up some model capacity but also teaches these models how to follow instructions allowing them to generalize to new tasks with the remaining capacity.']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9610025004152436,1.0,0.9324588797483988,0.0714285714285714
398,How are images used as input for conditioned language models?,"Images can be used as input for conditioned language models in tasks combining modalities such as language-driven editing of images. This involves exploring a broader variety of conditioning information, including free-form text, to enhance the model's understanding and performance. Additionally, there is a goal to move beyond still images to video and towards applications in model-based reinforcement learning, indicating a potential integration of images with language models for more complex tasks.","['In future work we would like to explore a broader variety of conditioning information including free form text as previously proposed Mansimov et al. 2015 and tasks combining modalities such as language driven editing of images. Fundamentally we aim to move beyond still images to video Kalchbrenner et al. 2016 and towards applications in model based reinforcement learning. References Ba Jimmy Lei Kiros Jamie Ryan and Hinton Geoffrey E. Layer normalization. arXiv preprint arXiv 1607.06450 2016. Bellemare Marc G. Srinivasan Sriram Ostrovski Georg Schaul Tom Saxton David and Munos R emi. Unifying count based exploration and intrinsic motivation. CoRR abs 1606.01868 2016. URL http arxiv.org abs 1606.01868. Bengio Yoshua and Bengio Samy. Modeling high dimensional discrete data with multi layer neural net works. In Neural Information Processing Systems pp. 400 406.', 'We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local restricted attention mechanisms to efficiently handle large inputs and outputs such as images audio and video. Making generation less sequential is another research goals of ours. The code we used to train and evaluate our models is available at https github.com tensorflow tensor2tensor. Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments corrections and inspiration. References 1 Jimmy Lei Ba Jamie Ryan Kiros and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv 1607.06450 2016. 2 Dzmitry Bahdanau Kyunghyun Cho and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. CoRR abs 1409.0473 2014. 3 Denny Britz Anna Goldie Minh Thang Luong and Quoc V. Le. Massive exploration of neural machine translation architectures.', 'Instruction finetuning 3. Reinforcement Learning from Human Feedback RLHF 4. What s nextLecture Plan From Language Models to Assistants 13 1. Zero Shot ZS and Few Shot FS In Context Learning 2. Instruction finetuning 3. Reinforcement Learning from Human Feedback RLHF 4. What s nextLet s revisit the Generative Pretrained Transformer GPT models from OpenAI as an example Emergent abilities of large language models GPT 2018 GPT 117M parameters Radford et al. 2018 Transformer decoder with 12 layers. Trained on BooksCorpus over 7000 unique books 4.6GB text . Showed that language modeling at scale can be an effective pretraining technique for downstream tasks like natural language inference.', '. . . . . . . . . . 39 7 Related Work 39 8 Conclusion 40 A Details of Common Crawl Filtering 43 B Details of Model Training 43 C Details of Test Set Contamination Studies 43 D Total Compute Used to Train Language Models 46 E Human Quality Assessment of Synthetic News Articles 46 F Additional Samples from GPT 3 48 G Details of Task Phrasing and SpeciÔ¨Åcations 50 H Results on All Tasks for All Model Sizes 63 21 Introduction Recent years have featured a trend towards pre trained language representations in NLP systems applied in increasingly Ô¨Çexible and task agnostic ways for downstream transfer.']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.925072543403722,1.0,0.897544731762151,0.0
399,What challenges exist in evaluating conditioned language models?,"One challenge in evaluating conditioned language models is the need for significant human effort and time to ensure quality in the data collection procedure, making it expensive compared to prior work. This can result in difficulties in collecting baselines such as high-quality human demonstrations for supervised baselines. Additionally, evaluating the quality of model outputs in tasks where humans can't easily assess the output's quality poses another challenge.","['Our data collection procedure is also expensive compared to prior work the training set took thousands of labeler hours and required signiÔ¨Åcant researcher time to ensure quality. For this reason we were unable to collect baselines such as an equivalent amount of high quality human demonstrations for supervised baselines. See D for more discussion. We leave this ablation to future work. Nevertheless we believe reward modeling is more likely to scale to tasks where it is extremely skill intensive or time consuming to provide good demonstrations. Future directions. The methods in this paper could be applied to any task where humans can compare samples including dialogue machine translation question answering speech synthesis and music generation. We expect this method to be particularly important for generating long samples where the distributional shift and degeneracy of maximum likelihood samples can be problematic. It may be possible to improve sample efÔ¨Åciency by training to predict feedback across many tasks 42 . We are particularly interested in scaling human feedback to tasks where humans can t easily evaluate the quality of model outputs.', '8 Conclusion We presented a 175 billion parameter language model which shows strong performance on many NLP tasks and benchmarks in the zero shot one shot and few shot settings in some cases nearly matching the performance of 40state of the art Ô¨Åne tuned systems as well as generating high quality samples and strong qualitative performance at tasks deÔ¨Åned on the Ô¨Çy. We documented roughly predictable trends of scaling in performance without using Ô¨Åne tuning. We also discussed the social impacts of this class of model. Despite many limitations and weaknesses these results suggest that very large language models may be an important ingredient in the development of adaptable general language systems. Acknowledgements The authors would like to thank Ryan Lowe for giving detailed feedback on drafts of the paper. Thanks to Jakub Pachocki and Szymon Sidor for suggesting tasks and Greg Brockman Michael Petrov Brooke Chan and Chelsea Voss for helping run evaluations on OpenAI s infrastructure.', '. . . . . . . . . . 39 7 Related Work 39 8 Conclusion 40 A Details of Common Crawl Filtering 43 B Details of Model Training 43 C Details of Test Set Contamination Studies 43 D Total Compute Used to Train Language Models 46 E Human Quality Assessment of Synthetic News Articles 46 F Additional Samples from GPT 3 48 G Details of Task Phrasing and SpeciÔ¨Åcations 50 H Results on All Tasks for All Model Sizes 63 21 Introduction Recent years have featured a trend towards pre trained language representations in NLP systems applied in increasingly Ô¨Çexible and task agnostic ways for downstream transfer.', 'By contrast humans can generally perform a new language task from only a few examples or from simple instructions something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task agnostic few shot performance sometimes even reaching competitiveness with prior state of the art Ô¨Åne tuning approaches. SpeciÔ¨Åcally we train GPT 3 an autoregressive language model with 175 billion parameters 10x more than any previous non sparse language model and test its performance in the few shot setting. For all tasks GPT 3 is applied without any gradient updates or Ô¨Åne tuning with tasks and few shot demonstrations speciÔ¨Åed purely via text interaction with the model. GPT 3 achieves strong performance on many NLP datasets including translation question answering and cloze tasks as well as several tasks that require on the Ô¨Çy reasoning or domain adaptation such as unscrambling words using a novel word in a sentence or performing 3 digit arithmetic.']",BAAI/bge-small-en-v1.5,256,10,4,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9853827242012354,1.0,0.8844920487847937,0.0
400,What is the instructor's name for this course?,"The instructor's name for the course is Hamidreza Mahyar. 

Source: NLP_Winter_2024_Course_Outline.txt","['Instructor Hamidreza Mahyar E Mail mahyarh@mcmaster.ca Office Hours upon request 2. COURSE SPECIFICS Course Description This course covers some fundamental concepts in natural language processing NLP and how they apply in real world use cases.', 'Announcements made in class or placed on Avenue are considered to have been communicated to all students including those individuals that are not in class. Instructor has the right to submit work to software to identify plagiarism. 3. SUB TOPIC S Week 1 Overview Human language Word meanings Word2Vec and Optimization basics Week 2 Intro to neural networks Gradients and backpropagation Week 3 Recurrent Neural Networks and Language Models Week', 'file name: lecture07 Prompting Instruction Finetuning and RLHF . Hamidreza Mahyar mahyarh@mcmaster.ca Computational Natural Language Processing Prompting Instruction Finetuning and RLHFReminders Project proposals will be posted this week. A2 due Saturday 11 59PM We still recommend using Colab for the assignments in case you run into trouble e.g.', 'file name: lecture08 Question Answering. Hamidreza Mahyar mahyarh@mcmaster.ca Computational Natural Language Processing Question Answering2 Lecture plan 1. What is question answering 2. Reading comprehension How to answer questions over a single passage of text 3. Open domain textual question answering How to answer questions over a large collection of documents Assignment 3 is out Start working on your final project1.', 'Course Outline 1. COURSE INFORMATION Session Offered Winter 2024 Course Name Introduction to Computational Natural Language Processing Course Code SEP 775 Date s and Time s of lectures Lectures In person Wednesday 3 30 pm 6 30 pm Program Name System and Technology Calendar Description This graduate course introduces fundamental concepts in computational natural language processing NLP and their applications to teaching students how to deal with textual data in Artificial Intelligence.', 'Martin Deep Learning Ian Goodfellow Yoshua Bengio and Aaron Courville Prerequisite s Proficiency in Python programming language Some Machine Learning and or Deep Learning knowledge is a plus Corequisite s Antirequisite s Course Specific Policies All work must be shown to get full credit. Specific policy of Projects Assignments submissionsProjects Assignments will be conducted online by using related software.', 'file name: lecture01 wordvecs1. Hamidreza Mahyar mahyarh@mcmaster.ca Computational Natural Language Processing Overview of NLP and Word VectorsLecture Plan 2 Lecture 1 Introduction and Word Vectors 1. The course 2. Human language and word meaning 3. Word2vec introduction 4. Word2vec objective function gradients 5. Optimization basics 6.', '. . pm and the question vector q as input and simply train two classiÔ¨Åers independently for predicting the two ends of the span.']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9927351287276536,1.0,0.9471542628224192,0.375
401,What is the email address of the instructor for this course?,"The email address of the instructor for this course is mahyarh@mcmaster.ca. 

Source: Course Outline - NLP Winter 2024","['Instructor Hamidreza Mahyar E Mail mahyarh@mcmaster.ca Office Hours upon request 2. COURSE SPECIFICS Course Description This course covers some fundamental concepts in natural language processing NLP and how they apply in real world use cases.', 'Accept that forwarded e mails may be lost and that e mail is considered received if sent via the student s @mcmaster.ca alias. Check the McMaster Avenue email and course websites on a regular basis during the term. CONDUCT EXPECTATIONS As a McMaster student you have the right to experience and the responsibility to demonstrate respectful and dignified interactions within all of our living learning and working communities.', 'file name: lecture07 Prompting Instruction Finetuning and RLHF . Hamidreza Mahyar mahyarh@mcmaster.ca Computational Natural Language Processing Prompting Instruction Finetuning and RLHFReminders Project proposals will be posted this week. A2 due Saturday 11 59PM We still recommend using Colab for the assignments in case you run into trouble e.g.', '. . pm and the question vector q as input and simply train two classiÔ¨Åers independently for predicting the two ends of the span.', 'e mail Avenue to Learn A2L LearnLink web pages capa Moodle ThinkingCap etc. . Students should be aware that when they access the electronic components of a course using these elements private information such as first and last names user names for the McMaster e mail accounts and program affiliation may become apparent to all other students in the same course. The available information is dependent on the technology used.', 'Announcements made in class or placed on Avenue are considered to have been communicated to all students including those individuals that are not in class. Instructor has the right to submit work to software to identify plagiarism. 3. SUB TOPIC S Week 1 Overview Human language Word meanings Word2Vec and Optimization basics Week 2 Intro to neural networks Gradients and backpropagation Week 3 Recurrent Neural Networks and Language Models Week', 'Instruction Type Code Type Hours per term C Classroom instruction 39 L Laboratory workshop or fieldwork T Tutorial DE Distance education Total Hours 39 Resources ISBN Textbook Title Edition Author Publisher Natural Language Processing Jacob Eisenstein Speech and Language Processing Dan Jurafsky and James H.', '2019 and a dual encoder architecture Bromley et al. 1994 we focus on developing the right training scheme using a relatively small number of question and passage pairs. Through a series of careful ablation studies our Ô¨Ånal solution is surprisingly simple the embedding is optimized for maximizing inner products of the question and relevant passage vectors with an objective compar ing all pairs of questions and passages in a batch.']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.0000000000000002,1.0,0.9268467921493144,0.2631578947368421
402,When is this course held every week?,"The course is held in person on Wednesdays from 3:30 pm to 6:30 pm every week. 

Source: NLP_Winter_2024_Course_Outline","['2 days before 01 01 2015 is 12 30 2014 so today is 12 30 2014. So one week from today will be 01 05 2015. So the answer is 01 05 2015.', 'So today is 01 07 2019. So the answer is 01 07 2019. Q The concert was scheduled to be on 06 01 1943 but was delayed by one day to today.', 'Announcements made in class or placed on Avenue are considered to have been communicated to all students including those individuals that are not in class. Instructor has the right to submit work to software to identify plagiarism. 3. SUB TOPIC S Week 1 Overview Human language Word meanings Word2Vec and Optimization basics Week 2 Intro to neural networks Gradients and backpropagation Week 3 Recurrent Neural Networks and Language Models Week', 'QUESTION Jane scheduled 3 apointments with 5 poeple for tomorrow Tue 7 9 1972 . What is the date one week ago from today in MM DD YYYY MODEL ANSWER INCORRECT Tomorrow is 7 9 1972. One week ago from today is 7 2 1972.', 'EXTREME CIRCUMSTANCES The University reserves the right to change the dates and deadlines for any or all courses in extreme circumstances e.g. severe weather labour disruptions etc. . Changes will be communicated through regular McMaster communication channels such as McMaster Daily News A2L and or McMaster email.', '3 Recurrent Neural Networks and Language Models Week 4 Conditioned Generation Week 5 Part 1 Self attention and transformers Part 2 Seq2Seq models Week 6 Midterm Test Week 7 Midterm Recess No class Week 8 Prompting Instruction Finetuning and RLHF Week 9 Question Answering systems Week 10 Multimodal models Week 11 Project presentation and Q A Week 12 Project presentation and Q', 'Q The Ô¨Årst day of 2019 is a Tuesday and today is the Ô¨Årst Monday of 2019. What is the date today in MM DD YYYY A If the Ô¨Årst day of 2019 was Tuesday then 01 01 2019 was a Tuesday. Today is the Ô¨Årst monday would be six days later.', 'Course Outline 1. COURSE INFORMATION Session Offered Winter 2024 Course Name Introduction to Computational Natural Language Processing Course Code SEP 775 Date s and Time s of lectures Lectures In person Wednesday 3 30 pm 6 30 pm Program Name System and Technology Calendar Description This graduate course introduces fundamental concepts in computational natural language processing NLP and their applications to teaching students how to deal with textual data in Artificial Intelligence.']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9109590891375872,0.6666666666666666,0.9104204997551758,0.0454545454545454
403,What is the name of the TA for this course?,The Teaching Assistant (TA) for this course is an AI Teaching Assistant (AI TA) designed to provide an interactive platform for students to ask questions and receive guidance on course materials.,"['The project emphasizes the application of NLP techniques to create an educational tool that enhances the learning experience through dynamic interaction and content understanding. Project Objectives Automate FAQ Responses Develop the AI TA to autonomously address frequently asked questions related to course logistics assignments and content. Content Understanding and Query Handling Implement NLP strategies enabling the AI TA to interpret and respond to inquiries about the uploaded course material accurately.', 'file name: SEP775 FinalProjects 2. McMaster University SEP 775 Final Projects Project 1 AI Teaching Assistant Project Overview The goal of this project is to create an AI Teaching Assistant AI TA . The AI TA aims to provide an interactive platform for students to ask questions and receive guidance on course materials mirroring the support traditionally offered by human teaching assistants.', 'Response Generation Utilize the designed AI TA to generate responses that are not only accurate but also engaging and pedagogically effective. Interaction Interface Simplified Interface Development Construct an intuitive web or command line in terface that allows straightforward question submission and response retrieval ensuring ease of use for student interactions with the AI TA.', '2016. From particular to general A preliminary case study of transfer learning in reading compre hension. Machine Intelligence Workshop NIPS . Diederik Kingma and Jimmy Ba. 2014. Adam A method for stochastic optimization. arXiv preprint arXiv 1412.6980 . Kenton Lee Tom Kwiatkowski Ankur Parikh and Di panjan Das.', '2020. Span BERT Improving pre training by representing and predicting spans. Transactions of the Association of Computational Linguistics TACL . Mandar Joshi Eunsol Choi Daniel S Weld and Luke Zettlemoyer. 2017. TriviaQA A large scale dis tantly supervised challenge dataset for reading com prehension. In Association for Computational Lin guistics ACL .', 'Course Outline 1. COURSE INFORMATION Session Offered Winter 2024 Course Name Introduction to Computational Natural Language Processing Course Code SEP 775 Date s and Time s of lectures Lectures In person Wednesday 3 30 pm 6 30 pm Program Name System and Technology Calendar Description This graduate course introduces fundamental concepts in computational natural language processing NLP and their applications to teaching students how to deal with textual data in Artificial Intelligence.', 'Propose future research directions or applications of the TANDA methodology in other NLP tasks. McMaster University 1McMaster University SEP 775 Submission Guidelines Submit all code in a ZIP file including Jupyter Notebooks and a detailed PDF report with written explanations and visualizations. Clearly label each part and question in your submissions. Deadline March 13 2024 Evaluation Criteria Depth and clarity of theoretical understanding and literature review.', 'file name: SEP775 Assignment3. McMaster University SEP 775 Assignment 3 TANDA Approach for QA System Enhancement Objectives Explore TANDA Transfer And Adapt methodology to improve Question Answering QA systems using pre trained Transformer models focusing on sequential fine tuning te chniques. 1. Introduction and Theory 15 Summarize the TANDA methodology s principles focusing on its novelty and the rationale behind sequential fine tuning.']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.8572388090201634,1.0,0.8318607660037336,0.0303030303030303
404,What is Word2vec?,"Word2vec is a framework for learning word vectors, where each word in a fixed vocabulary is represented as a low-dimensional vector. It is a model used to train and discover latent vector representations of natural language words in a semantic space. It involves constructing matrices U and V and using a softmax function to produce a probability distribution based on dot products of word vectors. Word2vec has been widely used in natural language processing tasks for generating word embeddings.","['The word2vec model is a probabilistic model specified as follows where uw refers to the row of U corresponding to word w V and likewise for V pU V o c exp u o vc w V exp u wvc 4 This may be familiar to you as the softmax function which takes arbitrary scores here one for each word in the vocabulary resulting from dot products and produces a probability distribution where larger scored things get higher probability.', 'However many of the details of word2vec will hold true in methods that we ll proceed to further in the course so we ll focus our time on that. 3.2 Word2vec model and objective The word2vec model represents each word in a fixed vocabulary as a low dimensional much smaller than vocabulary size vector.', '718 0.147 0.051 19Word meaning as a neural word vector visualization 0.286 0.792 0.177 0.107 0.109 0.542 0.349 0.271 0.487 expect 203. Word2vec Overview 21 Word2vec Mikolov et al. 2013 is a framework for learning word vectors Idea We have a large corpus body of text', 'It consistently outperforms word2vec on the word analogy task given the same corpus vocabulary window size and training time. It achieves better results faster and also obtains the best results irrespective of speed. 2 Evaluation of Word Vectors So far we have discussed methods such as the Word2Vec and GloVe methods to train and discover latent vector representations of natural language words in a semantic space.', '3.3 Estimating a word2vec model from a corpus How do we train word2vec in practice Specifying the word2vec model is relatively transparent from the math we ve given above one constructs matrices U and V and can write out the math of the probability.', 'To do this in practice we will need to tune many hyperparameters in the Word2Vec subsystem such as the dimension of the word vector representation . While the idealistic approach is to retrain the entire system after any parametric changes in the Word2Vec subsystem this is impractical from an engineering standpoint because the machine learning system in step 3 is typi cally a deep neural network with millions of parameters that takes very long to train.', 'McMaster University 1McMaster University SEP 775 Resources for Word2Vec and GloVe Word2Vec Pre trained Word2Vec embeddings can be found on repositories such as HuggingFace or directly using Gensim. GloVe Pre trained GloVe embeddings are available at the GloVe website or through HuggingFace.', '4 for a plot of the learning curve. 4.7 Model Analysis Comparison with word2vec A rigorous quantitative comparison of GloVe with word2vec is complicated by the existence of many parameters that have a strong effect on per formance.']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999999999999994,1.0,0.9560615848626344,0.0666666666666666
405,What are the main learning goals of the course?,"The main learning goals of the course include:
1. Understanding the foundations of effective modern methods for deep learning applied to NLP, such as word vectors, feed forward networks, recurrent networks, attention, encoder-decoder models, transformers, and large pre-trained language models.
2. Developing a big picture understanding of human languages and the challenges in understanding and producing them via computers.
3. Building systems in PyTorch for some NLP tasks.

Source: lecture01_wordvecs1.txt","['The main criticism of our goal can be summarized as follows. Over the years the NLP com munity has developed a considerable expertise in engineering effective NLP features. Why should they forget this painfully acquired expertise and instead painfully acquire the skills required to train large neural networks As mentioned in our introduction we observe that no single NLP task really covers the goals of NLP.', 'learning goals 4 1. The foundations of the effective modern methods for deep learning applied to NLP Basics first then key methods used in NLP in 2024 Word vectors feed forward networks recurrent networks attention encoder decoder models transformers large pre trained language models etc. 2. A big picture understanding of human languages and the difficulties in understanding and producing them via computers 3. An understanding of and ability to build systems in PyTorch for some of', '13.6 Ambiguous We don t agree with the crowd workers answer or the question does not have a unique answer. Q What is the main goal of criminal punishment Sen. Achieving crime control via incapacitation and deterrence is a major goal of criminal punish ment. 6.1 Table 3 We manually labeled 192 examples into one or more of the above categories.', 'What s next Lecture Plan From Language Models to AssistantsLimitations of instruction finetuning One limitation of instruction finetuning is obvious it s expensive to collect ground truth data for tasks. But there are other subtler limitations too. Can you think of any Problem 1 tasks like open ended creative generation have no right answer. Write me a story about a dog and her pet grasshopper.', 'Martin Deep Learning Ian Goodfellow Yoshua Bengio and Aaron Courville Prerequisite s Proficiency in Python programming language Some Machine Learning and or Deep Learning knowledge is a plus Corequisite s Antirequisite s Course Specific Policies All work must be shown to get full credit. Specific policy of Projects Assignments submissionsProjects Assignments will be conducted online by using related software.', 'Zero Shot ZS and Few Shot FS In Context Learning No finetuning needed prompt engineering e.g. CoT can improve performance Limits to what you can fit in context Complex tasks will probably need gradient steps 2. Instruction finetuning 3. Reinforcement Learning from Human Feedback RLHF 4.', 'The additional pretraining tasks are likely more useful only when the target training sets are small. Al though the results of DPR on WQ and TREC in the single dataset setting are less competitive adding more question answer pairs helps boost the perfor mance achieving the new state of the art.', 'Reinforcement Learning from Human Feedback RLHF 51 Lecture Plan From Language Models to Assistants 4. What s next1. Zero Shot ZS and Few Shot FS In Context Learning No finetuning needed prompt engineering e.g. CoT can improve performance Limits to what you can fit in context Complex tasks will probably need gradient steps 2.']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999999999999996,0.6666666666666666,0.945017365098558,0.0555555555555555
406,How is the course grade distributed?,"The course grade is distributed as follows:
- Assignments: 20%
- Quizzes: 10%
- Midterm Test: 35%
- Group Project: 35%
- Total: 100%
Please note that the percentage distribution of each module may be adjusted.","['These gradings are then used to compare sentence forms It now turns out that given the graded n tuples of words for a particular sentence form we can Ô¨Ånd other sentences forms of the same word classes in which the same n tuples of words produce the same grading of sentences. This is an indication that these two sentence forms exploit common words with the same syntac tic function and possibly the same meaning.', '1 x i x i 1 x i 2 The gradient will of course need to be distributed to update the corresponding word vectors in implementation. 3.5 Non linear ClassiÔ¨Åers Figure 9 Here we see that many exam ples are wrongly classiÔ¨Åed even though the best linear decision boundary is chosen. This is due linear decision boundaries have limited model capacity for this dataset.', 'We used this classiÔ¨Åer to score Common Crawl documents. We kept each document in our dataset iff np.random.pareto Œ± 1 document_score We chose Œ± 9 in order to take mostly documents the classiÔ¨Åer scored highly but still include some documents that were out of distribution. Œ± was chosen to match the distribution of scores from our classiÔ¨Åer on WebText.', 'The prob lem now is that we usually cannot directly com pare scores from different rankings Aslam and Montague 2001 as their scores are not guaran teed to have the same ranges. An even worse case is the following scenario. Assume we use rank correlation as our metric. As a consequence we need our gold ranking to deÔ¨Åne an order on all the word pairs.', 'and Q A Week 12 Project presentation and Q A Week 13 Project presentation and Q A Classes end April 10th 4. ASSESSMENT OF LEARNING Weight 4 Assignments 20 4 Quizzes 10 Midterm Test 35 Group Project 35 TOTAL 100Note that the percentage distribution of each module may be adjusted.', 'Percentage grades will be converted to letter grades and grade points per the University calendar. 5. LEARNING OUTCOMES By the end of this course the students will know about Introduction to Deep Learning Word representations Language models Pre trained language models Text generation Seq2Seq models Prompting Feedback systems Multimodal models NLP applications e.g. Q A systems 6.', '0 79.9 74.7 88.5 66.2 83.9 84.4 82.3 94.1 78.6 Table 2 Top 20 Top 100 retrieval accuracy on test sets measured as the percentage of top 20 100 retrieved passages that contain the answer.', 'If none of the top 100 retrieved passages has the answer the ques tion will be discarded. For SQuAD and Natural Questions since the original passages have been split and processed differently than our pool of candidate passages we match and replace each gold passage with the corresponding passage in the candidate pool.7 We discard the questions when the matching is failed due to different Wikipedia versions or pre processing.']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999999999999996,0.5,0.958408602129042,0.0526315789473684
407,What is the policy for late submissions in this course?,"The policy for late submissions in this course is that there are NO free late days, and there is a penalty of 1 off the course grade per day late. Assignments are not accepted more than 3 days late unless permission is given in advance.","['to build systems in PyTorch for some of the major problems in NLP Word meaning dependency parsing machine translation question answeringCourse work and grading policy 5 4 Assignments 20 4 Quizzes 10 Midterm 35 Final Group Project 1 3 people 34 Late day policy NO free late days 1 off course grade per day late Assignments not accepted more than 3 days late unless given permission in advance Collaboration policy Understand allowed collaboration and how', 'Students should submit their request to their Faculty Office normally within 10 working days of the beginning of term in which they anticipate a need for accommodation or to the Registrar s Office prior to their examinations. Students should also contact their instructors as soon as possible to make alternative arrangements for classes assignments and tests.', 'Continuation in a course that uses on line elements will be deemed consent to this disclosure. If you have any questions or concerns about such disclosure please discuss this with the course instructor. ONLINE PROCTORING Some courses may use online proctoring software for tests and exams.', 'Additional Materials Any additional materials used or created for the project such as scripts for data preprocessing trained models or Jupyter notebooks should be included in the submission. Submissions failing to meet these requirements may be penalized. Please ensure that all submitted materials are well organized and clearly labeled for easy navigation. McMaster University 7', 'Completed Projects Assignments should be uploaded to the drop box before midnight of the due date. Projects Assignments missed due to legitimate reasons must be completed later mutually agreed with the instructor. Departmental Policies The use of cell phones iPods laptops and other personal electronic devices are prohibited from the classroom during the class time unless the instructor makes an explicit exception.', 'Adherence to Guidelines 10 Submissions should follow the provided guidelines including format labeling and adherence to the deadline. McMaster University 2', 'All submitted work is subject to normal verification that standards of academicintegrity have been upheld e.g. on line search other software etc. . For more details about McMaster s use of Turnitin.com please go to www.mcmaster.ca academicintegrity. COURSES WITH AN ON LINE ELEMENT Some courses may use on line elements e.g.', 'Propose future research directions or applications of the TANDA methodology in other NLP tasks. McMaster University 1McMaster University SEP 775 Submission Guidelines Submit all code in a ZIP file including Jupyter Notebooks and a detailed PDF report with written explanations and visualizations. Clearly label each part and question in your submissions. Deadline March 13 2024 Evaluation Criteria Depth and clarity of theoretical understanding and literature review.']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9748806789661276,0.6666666666666666,0.9634404438494484,0.0434782608695652
408,What does the course hope to teach regarding word vectors?,The course hopes to teach that word meaning can be represented rather well by a high-dimensional vector of real numbers.,"['Optimization basics 6. Looking at word vectors Key learning today The astounding result that word meaning can be represented rather well by a high dimensional vector of real numbersCourse logistics in brief 3 Instructor Hamidreza Mahyar Head TA Reza Namazi Time Wednesday 3 30pm 6 30pm We put a lot of other important information on Avenue to Learn. Please read itWhat do we hope to teach A.k.a.', 'While there has been later substantial amount of work that focuses on learning word vectors we consider the approach proposed in 13 to be the simplest one. Note that related models have been proposed also much earlier 26 8 .', '3.2 Retraining Word Vectors As we have discussed so far the word vectors we use for extrinsic tasks are initialized by optimizing them over a simpler intrinsic task. In many cases these pretrained word vectors are a good proxy for optimal word vectors for the extrinsic task and they perform well at the extrinsic task. However it is also possible that the pretrained word vectors could be trained further i.e.', 'file name: lecture01 wordvecs1. Hamidreza Mahyar mahyarh@mcmaster.ca Computational Natural Language Processing Overview of NLP and Word VectorsLecture Plan 2 Lecture 1 Introduction and Word Vectors 1. The course 2. Human language and word meaning 3. Word2vec introduction 4. Word2vec objective function gradients 5. Optimization basics 6.', 'In this work we directly extend this architecture and focus just on the Ô¨Årst step where the word vectors are learned using a simple model. It was later shown that the word vectors can be used to signiÔ¨Åcantly improve and simplify many NLP applications 4 5 29 .', 'Collobert and Weston 2008 decoupled the word vector training from the downstream training objectives which paved the way for Collobert et al. 2011 to use the full context of a word for learning the word represen tations rather than just the preceding context as is the case with language models.', 'The word vectors can also be initialized to random values then estimated solely through task learning which we might call learning from scratch. 6 We can use expert built data structures like WordNet as additional input to creating word vectors.', 'Motivation Let us consider an example where our Ô¨Ånal goal is to create a question answering system which uses word vectors as inputs. One approach of doing so would be to train a machine learning system that 1. Takes words as inputs 2. Converts them to word vectors 3. Uses word vectors as inputs for an elaborate machine learning system 4.']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9127950218144932,0.5,0.9637591978828216,0.0625
409,How does Word2vec represent word meaning?,"Word2vec represents each word in a fixed vocabulary as a low-dimensional vector that is much smaller than the vocabulary size. The model uses matrices U and V to represent words and calculates probabilities based on dot products to create a probability distribution where larger scored items have higher probabilities. This allows Word2vec to capture word meanings in a semantic space efficiently.

Source: lecture01_notes_Introduction.txt","['However many of the details of word2vec will hold true in methods that we ll proceed to further in the course so we ll focus our time on that. 3.2 Word2vec model and objective The word2vec model represents each word in a fixed vocabulary as a low dimensional much smaller than vocabulary size vector.', 'The word2vec model is a probabilistic model specified as follows where uw refers to the row of U corresponding to word w V and likewise for V pU V o c exp u o vc w V exp u wvc 4 This may be familiar to you as the softmax function which takes arbitrary scores here one for each word in the vocabulary resulting from dot products and produces a probability distribution where larger scored things get higher probability.', '718 0.147 0.051 19Word meaning as a neural word vector visualization 0.286 0.792 0.177 0.107 0.109 0.542 0.349 0.271 0.487 expect 203. Word2vec Overview 21 Word2vec Mikolov et al. 2013 is a framework for learning word vectors Idea We have a large corpus body of text', 'Just as we used static embeddings like word2vec in Chapter 6 to represent the meaning of words we can use contextual embeddings as representations of word meanings in context for any task that might require a model of word meaning.', '3.3 Estimating a word2vec model from a corpus How do we train word2vec in practice Specifying the word2vec model is relatively transparent from the math we ve given above one constructs matrices U and V and can write out the math of the probability.', 'It consistently outperforms word2vec on the word analogy task given the same corpus vocabulary window size and training time. It achieves better results faster and also obtains the best results irrespective of speed. 2 Evaluation of Word Vectors So far we have discussed methods such as the Word2Vec and GloVe methods to train and discover latent vector representations of natural language words in a semantic space.', '4 for a plot of the learning curve. 4.7 Model Analysis Comparison with word2vec A rigorous quantitative comparison of GloVe with word2vec is complicated by the existence of many parameters that have a strong effect on per formance.', 'Performance is lower for extremely low dimensional word vec tors Lower dimensional word vectors are not able to capture the dif ferent meanings of the different words in the corpus. This can be viewed as a high bias problem where our model complexity is too low. For instance let us consider the words king queen man woman . Intuitively we would need to use two dimensions such as gender and leadership to encode these into 2 bit word vec tors.']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9228405965490182,1.0,0.9209510461686634,0.0625
410,What is the main focus of this lecture on neural networks?,"The main focus of this lecture on neural networks is to study neural networks as a class of non-linear models that have performed well in deep learning applications, introducing single and multilayer neural networks for classification purposes, discussing the need for non-linear classifiers due to data not being linearly separable, and motivating the use of recurrent neural networks as a language model.","['In the next set of notes we study neural networks as a class of non linear models that have performed particularly well in deep learning applications. Figure 10 Here we see that the non linear decision boundary allows for much better classiÔ¨Åcation of datapoints.', 'file name: lecture02 notes Backpropagation.  Neural networks. Forward computation. Backward propagation. Neuron Units. Max margin Loss. Gradient checks. Xavier parameter initialization. Learning rates. Adagrad. This set of notes introduces single and multilayer neural networks and how they can be used for classiÔ¨Åcation purposes.', 'However in reality human neural networks are so much more capable and complex from artiÔ¨Åcial neural net works that it is usually better to not draw too many parallels between the two. We established in our previous discussions the need for non linear classiÔ¨Åers since most data are not linearly separable and thus our classiÔ¨Åcation performance on them is limited.', 'Source nodes inputs Interior nodes operations 44Computation Graphs and Backpropagation Software represents our neural net equations as a graph Source nodes inputs Interior nodes operations Edges pass along result of the operation 45Computation Graphs and Backpropagation Software represents our neural net equations as a graph Source nodes inputs Interior nodes operations Edges pass along result of the operation Forward Propagation 46Backpropagation Then go backwards along edges Pass along gradients', 'Lastly we will motivate the use of recurrent neural networks as a language model. 1 Neural Networks Foundations Figure 1 We see here how a non linear decision boundary separates the data very well. This is the prowess of neural networks. Fun Fact Neural networks are biologically in spired classiÔ¨Åers which is why they are often called artiÔ¨Åcial neural networks to distinguish them from the organic kind.', '20 X. Glorot and Y. Bengio. Understanding the difÔ¨Åculty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artiÔ¨Åcial intelligence and statistics pages 249 256 2010. 21 B. Hancock A. Bordes P. E. Mazare and J. Weston.', '2007 . Optimization and applications of echo state networks with leaky integrator neurons. Neural Networks 20 3 335 352. LukoÀá seviÀá cius M. and Jaeger H. 2009 . Reservoir computing approaches to recurrent neural network training. Computer Science Review 3 3 127 149.', 'In this section we ll discuss a bit about the neural modeling ap proaches we ve discussed in Cs 224n so far and how their limitations and changes in the world inspired the modern as of 2023 zeitgeist of self attention and Transformer based architectures. 1.1 Notation and basics Let w1 n be a sequence where each wi V a finite vocabulary.']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9995544102996656,1.0,0.8812087053049467,0.03125
411,What are the non-linear functions mentioned for neural networks?,"The non-linear functions mentioned for neural networks include the squashing sigmoid function, transfer function, and activation functions other than sigmoidal neurons.","['We will see that a neuron can be one of many functions that allows for non linearities to accrue in the network. lently formulate a 1 1 exp wT b x 1 Figure 2 This image captures how in a sigmoid neuron the input vector x is Ô¨Årst scaled summed added to a bias unit and then passed to the squashing sigmoid function.', 'In the next set of notes we study neural networks as a class of non linear models that have performed particularly well in deep learning applications. Figure 10 Here we see that the non linear decision boundary allows for much better classiÔ¨Åcation of datapoints.', 'œÜf a b 11 where f is the transfer function that is the analog of the non linearity in neural networks f is the derivative of the transfer function Œ∑ is a real valued function and c is the log parti tion function. œÜ is a constant that scales the output variance.', 'logistic 1.702ùë•Non linearities i.e. f on previous slide Why they re needed Neural networks do function approximation e.g. regression or classification Without non linearities deep neural networks can t do anything more than a linear transform Extra layers could just be compiled down into a single linear transform W1 W2 x Wx But with more layers that include non linearities they can approximate any complex function 5Training with cross', 'We now introduce the need for non linear classiÔ¨Åcation models such as neural networks. We see in Figure 9 that a linear classiÔ¨Åer mis classiÔ¨Åes many datapoints. Using a non linear decision boundary as shown in Figure 10 we manage to classify all training points accu rately. Although oversimpliÔ¨Åed this is a classic case demonstrating the need for non linear decision boundaries.', 'Thus we must typically divide the outputs of each neuron during testing by a certain value it is left as an exercise to the reader to determine what this value should be in order for the expected outputs during training and testing to be equivalent. 2.4 Neuron Units So far we have discussed neural networks that contain sigmoidal neurons to introduce nonlinearities however in many applications better networks can be designed using other activation functions.', 'However in reality human neural networks are so much more capable and complex from artiÔ¨Åcial neural net works that it is usually better to not draw too many parallels between the two. We established in our previous discussions the need for non linear classiÔ¨Åers since most data are not linearly separable and thus our classiÔ¨Åcation performance on them is limited.', 'Neural networks are a family of classiÔ¨Åers with non linear decision boundary as seen in Figure 1. Now that we know the sort of decision boundaries neural networks create let us see how they manage doing so. 1.1 A Neuron A neuron is a generic computational unit that takes n inputs and produces a single output. What differentiates the outputs of different neurons is their parameters also referred to as their weights .']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.990252641413291,0.6666666666666666,0.9223247483606754,0.0526315789473684
412,What is the purpose of non-linearities in neural networks?,"The purpose of non-linearities in neural networks is to allow for more complex function approximation, such as regression or classification. Without non-linearities, deep neural networks can only perform a linear transform, limiting their capabilities. By incorporating non-linearities, neural networks can approximate any complex function, enabling them to handle more intricate patterns and relationships in the data.","['In the next set of notes we study neural networks as a class of non linear models that have performed particularly well in deep learning applications. Figure 10 Here we see that the non linear decision boundary allows for much better classiÔ¨Åcation of datapoints.', 'We now introduce the need for non linear classiÔ¨Åcation models such as neural networks. We see in Figure 9 that a linear classiÔ¨Åer mis classiÔ¨Åes many datapoints. Using a non linear decision boundary as shown in Figure 10 we manage to classify all training points accu rately. Although oversimpliÔ¨Åed this is a classic case demonstrating the need for non linear decision boundaries.', 'We will see that a neuron can be one of many functions that allows for non linearities to accrue in the network. lently formulate a 1 1 exp wT b x 1 Figure 2 This image captures how in a sigmoid neuron the input vector x is Ô¨Årst scaled summed added to a bias unit and then passed to the squashing sigmoid function.', 'logistic 1.702ùë•Non linearities i.e. f on previous slide Why they re needed Neural networks do function approximation e.g. regression or classification Without non linearities deep neural networks can t do anything more than a linear transform Extra layers could just be compiled down into a single linear transform W1 W2 x Wx But with more layers that include non linearities they can approximate any complex function 5Training with cross', 'However in reality human neural networks are so much more capable and complex from artiÔ¨Åcial neural net works that it is usually better to not draw too many parallels between the two. We established in our previous discussions the need for non linear classiÔ¨Åers since most data are not linearly separable and thus our classiÔ¨Åcation performance on them is limited.', 'nonlinearities in self attention Note that there are no elementwise nonlinearities in self attention stacking more self attention layers just re averages value vectors Why Easy fix add a feed forward network to post process each output vector. ùëöùëñ ùëÄùêøùëÉoutputùëñ ùëä2 ReLU ùëä1 outputùëñ ùëè1 ùëè2 ùë§1 The ùë§2 chef', 'The summed inputs are computed through a linear projection with the weight matrix W l and the bottom up inputs hl given as follows al i wl i hl hl 1 i f al i bl i 1 where f is an element wise non linear function and wl i is the incoming weights to the ith hidden units and bl i is the scalar bias parameter. The parameters in the neural network are learnt using gradient based optimization algorithms with the gradients being computed by back propagation.', 'œÜf a b 11 where f is the transfer function that is the analog of the non linearity in neural networks f is the derivative of the transfer function Œ∑ is a real valued function and c is the log parti tion function. œÜ is a constant that scales the output variance.']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999984722871876,1.0,0.9269009151808631,0.0588235294117647
413,What is cross entropy loss used for in PyTorch?,"Cross entropy loss is used in PyTorch to minimize the negative log probability of the correct class or to maximize the probability of the correct class. It is used as the loss function for training neural networks in PyTorch. Cross entropy is a concept from information theory that compares the true probability distribution with the computed model probability distribution. It is particularly useful when the actual probability distribution that generated the data is unknown. By using cross entropy, we can train models effectively by updating the weights based on the difference between the predicted probabilities and the actual probabilities.","['can approximate any complex function 5Training with cross entropy loss you use this in PyTorch Until now our objective was stated as to maximize the probability of the correct class y or equivalently we can minimize the negative log probability of that class Now restated in terms of cross entropy a concept from information theory Let the true probability distribution be p let our computed model probability be q The cross entropy is Assuming a ground truth or true or gold or target probability distribution that', 'know that you ll want to use it as the loss in PyTorch 6Remember Stochastic Gradient Descent Update equation i.e. for each parameter In deep learning ùúÉincludes the data representation e.g. word vectors too How can we compute ùúÉùêΩ ùúÉ 1. By hand 2.', 'To summarize by making some incorrect but convenient simplifying assump tions we can compute the entropy of some stochastic process by taking a very long sample of the output and computing its average log probability. Now we are ready to introduce cross entropy. The cross entropy is useful when cross entropy we don t know the actual probability distribution p that generated some data. It allows us to use some m which is a model of p i.e.', 'Using the Cross entropy loss function we calculate the loss of such a training example as C j 1 yj log p yj 1 x C j 1 yj log exp Wj x C c 1 exp Wc x Of course the above summation will be a sum over C 1 zero values since yj is 1 only at a single index at least for now implying that x belongs to only 1 correct class.', 'What makes the cross entropy useful is that the cross entropy H p m is an up per bound on the entropy H p .', 'the same values for i and j J W i 1 W j 1 Xij log Qij where the value of co occurring frequency is given by the co occurrence matrix X. One signiÔ¨Åcant drawback of the cross entropy loss is that it requires the distribution Q to be properly normalized which involves the expensive summation over the entire vocabulary.', 'Training proceeds in the usual way cross entropy loss between the softmax output and the correct answer is used to drive the learning that produces WC. A key difference from what we ve seen earlier with neural classiÔ¨Åers is that this loss can be used to not only learn the weights of the classiÔ¨Åer but also to update the weights for the pretrained language model itself.', 'After extending this trend by two more orders of magnitude we observe only a slight if any departure from the power law. One might worry that these improvements in cross entropy loss come only from modeling spurious details of our training corpus. However we will see in the following sections that improvements in cross entropy loss lead to consistent performance gains across a broad spectrum of natural language tasks.']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.990019201628698,0.8333333333333334,0.9681746193753412,0.0625
414,What is the significance of the backpropagation algorithm?,"The significance of the backpropagation algorithm lies in its ability to efficiently calculate loss gradients for parameters in a neural network model by utilizing the chain rule of differentiation. This allows for updating parameters using gradient descent or its variants, such as SGD, which is crucial for optimizing the network's performance through learning from the data provided.

Source: Lecture02_Notes_Backpropagation","['The idea behind using this objective is to ensure that the score computed for true labeled data points is higher than the score computed for false labeled data points. Using the previous example if we call the score computed for the true labeled window Museums in Paris are amazing as s and the score computed for the false labeled window Not all museums in Paris as sc subscripted as c to signify that the window is corrupt .', 'Now a natural question you might ask is if this method is so pre cise why do we not use it to compute all of our network gradients instead of applying back propagation The simple answer as hinted earlier is inefÔ¨Åciency recall that every time we want to compute the gradient with respect to an element we need to make two forward passes through the network which will be computationally expen sive.', '. . 1 . Such a recursive procedure is what makes backpropa gation a computationally affordable procedure. 2 Neural Networks Tips and Tricks Having discussed the mathematical foundations of neural networks we will now dive into some tips and tricks commonly employed when using neural networks in practice. 2.1 Gradient Check In the last section we discussed in detail how to calculate error gradients updates for parameters in a neural network model via calculus based analytic methods.', 'Since we typically update parameters using gradient descent or a variant such as SGD we typically need the gradient information for any parameter as required in the update equation Œ∏ t 1 Œ∏ t Œ± Œ∏ t J Backpropagation is technique that allows us to use the chain rule of differentiation to calculate loss gradients for any parameter used in the feed forward computation on the model.', 'Let us begin Suppose the cost J 1 sc s is positive and we want to perform the update of parameter W 1 14 in Figure 5 and Figure 6 we must realize that W 1 14 only contributes to z 2 1 and thus a 2 1 . This fact is crucial to understanding backpropagation backpropagated gradients are only affected by values they contribute to.', 'But what is the intuition behind doing so Let us consider the following named entity recognition NER problem in NLP as an example Museums in Paris are amazing Dimensions for a single hidden layer neural network If we represent each word using a 4 dimensional word vector and we use a 5 word window as input then the input x R20.', 'Formally Œ∏t i Œ∏t 1 i Œ± q t œÑ 1 g2 œÑ i gt i where gt i Œ∏t i Jt Œ∏ In this technique we see that if the RMS of the history of gradients is extremely low the learning rate is very high. A simple implemen tation of this technique is Snippet 2.', 'W k is the transfer matrix that maps the output from the k th layer to the input to the k 1 th Thus W 1 W and W 2 U to put this new generalized notation in perspective of Section 1.3. Backpropagation Notation xi is an input to the neural network. s is the output of the neural net work.']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999991389486104,1.0,0.945738731866164,0.0588235294117647
415,What does the Jacobian matrix represent in the context of neural networks?,"The Jacobian matrix in the context of neural networks represents a matrix of partial derivatives that captures the relationship between the outputs and inputs of a function. It is used to compute how small changes in inputs affect the outputs of the function, providing crucial information for tasks like gradient computation and backpropagation in neural network training.","['m outputs and n inputs It s Jacobian is an m x n matrix of partial derivatives 12Chain Rule For composition of one variable functions multiply derivatives For multiple variables functions multiply Jacobians 13Example Jacobian Elementwise activation Function 14Example Jacobian Elementwise activation Function Function has n outputs and n inputs n by n Jacobian 15Example Jacobian Elementwise activation Function 16Example Jacobian Elementwise activation Function 17Example Jacobian Elementwise', 'Note that we assume the parametrization given by equation 11 . The Jacobian matrix xk 1 xk is given by WT recdiag œÉ xk . The 2 norm of this Jacobian is bounded by the product ofOn the diÔ¨Éculty of training Recurrent Neural Networks the norms of the two matrices see equation 6 .', 'Equation 5 also provides the form of Jacobian matrix xi xi 1 for the speciÔ¨Åc parametrization given in equation 11 where diag converts a vector into a diagonal matrix and œÉ computes the derivative of œÉ in an element wise fash ion.', '2.1. The mechanics To understand this phenomenon we need to look at the form of each temporal component and in particular at the matrix factors xt xk see equation 5 that take the form of a product of t k Jacobian matrices.', 'is the same In general our nets have regular layer structure and so we can use matrices and Jacobians Back Prop in General Computation Graph Inputs Single scalar output 73Automatic Differentiation The gradient computation can be automatically inferred from the symbolic expression of the fprop Each node type needs to know how to compute its output and how to compute the gradient wrt its inputs given the gradient wrt its output Modern DL frameworks Tensorflow PyTorch etc. do', 'input a bit At x 1 it changes about 3 times as much 1.013 1.03 At x 4 it changes about 48 times as much 4.013 64.48Gradients Given a function with 1 output and n inputs Its gradient is a vector of partial derivatives with respect to each input 11Jacobian Matrix Generalization of the Gradient Given a function with m outputs and n inputs It s Jacobian is', 'look like 1 output nm inputs 1 by nm Jacobian Inconvenient to then do 38Derivative with respect to Matrix Output shape What does look like 1 output nm inputs 1 by nm Jacobian Inconvenient to then do Instead we leave pure math and use the shape convention the shape of the gradient is the shape of the parameters So is n by m 39Derivative with respect to Matrix What is is going to be in our', 'activation Function 17Example Jacobian Elementwise activation Function 18Other Jacobians Compute these at home for practice Check your answers with the lecture notes 19Other Jacobians Compute these at home for practice Check your answers with the lecture notes 20Other Jacobians Compute these at home for practice Check your answers with the lecture notes Fine print This is the correct Jacobian. Later we discuss the shape convention using it the answer would be h.']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999983242513932,1.0,0.941466899465182,0.0833333333333333
416,How is the derivative with respect to a matrix described?,"The derivative with respect to a matrix is described as the direction to locally move the matrix in order to maximally increase the value of the scalar function. In practice, the initial matrix is drawn randomly, and the derivative is represented as a matrix that has the same shape as the original matrix, allowing for adjustments during gradient descent.

Source: Lecture01_notes_Introduction.txt","['Briefly the gradient think derivative U f of a scalar function f with respect to a parameter matrix U represents the direction to locally move U in in order to maximally increase the value of f. So in practice we do something like drawing the initial U 0 and V 0 randomly as U V N 0 0.', 'fm x1 . . . fm xn That is f x ij fi xj which is just a standard non vector derivative . The Jacobian matrix will be useful for us because we can apply the chain rule to a vector valued function just by multiplying Jacobians.', '1 Matrix times column vector with respect to the column vector z W x what is z x Suppose W Rn m. Then we can think of z as a function of x taking an m dimensional vector to an n dimensional vector.', 'It would be much nicer if the derivatives were in a n m matrix like this J W J W11 . . . J W1m . . . ... . . . J Wn1 . . . J Wnm Since this matrix has the same shape as W we could just subtract it times the learning rate from W when doing gradient descent.', 'Since multiplication by a diagonal matrix is the same as doing elementwise multiplication by the diagonal we could also write f x when applying the chain rule. 5 Matrix times column vector with respect to the matrix z W x Œ¥ J z what is J W J z z W Œ¥ z W This is a bit more complicated than the other identities. The reason for in cluding J z in the above problem formulation will become clear in a moment.', 'As a little illustration of this suppose we have a function f x f1 x f2 x taking a scalar to a vector of size 2 and a function g y g1 y1 y2 g2 y1 y2 taking a vector of size two to a vector of size two. Now let s compose them to get g x g1 f1 x f2 x g2 f1 x f2 x . Using the regular chain rule we can compute the derivative', 'This matrix is equal to the outer product J W Œ¥T xT 6 Row vector time matrix with respect to the matrix z xW Œ¥ J z what is J W Œ¥ z W A similar computation to 5 shows that J W xT Œ¥ .', 'So we see that z x W 2 Row vector times matrix with respect to the row vector z xW what is z x A computation similar to 1 shows that z x W T . 3 A vector with itself z x what is z x We have zi xi.']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999999999999996,1.0,0.8990773463613326,0.0625
417,What role does automatic differentiation play in modern deep learning frameworks?,"Automatic differentiation plays a crucial role in modern deep learning frameworks as it allows each module to independently compute derivatives with respect to its inputs and trainable parameters. This capability enables the easy building of variants of neural networks and facilitates gradient computations for training deep learning models efficiently. Additionally, automatic differentiation helps in verifying the correctness of analytic gradients, which is essential for ensuring the accuracy of the training process.","['Given derivatives with respect to its outputs each module can independently compute derivatives with respect to its inputs and with respect to its trainable parameters as proposed by Bottou and Gallinari 1991 . This allows us to easily build variants of our networks. For details about gradient computations see Appendix A. Remark 7 Tricks Many tricks have been reported for training neural networks LeCun et al. 1998 .', 'However in Understanding the dif ficulty of training deep feedforward neural networks 2010 Xavier et al study the effect of different weight and bias initialization schemes on training dynamics. The empirical Ô¨Åndings suggest that for sigmoid and tanh activation units faster convergence and lower error rates are achieved when the weights of a matrix W Rn l 1 n l are initialized randomly with a uniform distribution as follows W U s 6 n l n l 1 s', '1 Introduction Deep neural networks trained with some version of Stochastic Gradient Descent have been shown to substantially outperform previous approaches on various supervised learning tasks in computer vision Krizhevsky et al. 2012 and speech processing Hinton et al. 2012 . But state of the art deep neural networks often require many days of training.', 'Here we now introduce a tech nique of numerically approximating these gradients though too computationally inefÔ¨Åcient to be used directly for training the net works this method will allow us to very precisely estimate the derivative with respect to any parameter it can thus serve as a useful sanity check on the correctness of our analytic derivatives.', 'One of the challenges of deep learning is that the gradients with respect to the weights in one layer are highly dependent on the outputs of the neurons in the previous layer especially if these outputs change in a highly correlated way. Batch normalization Ioffe and Szegedy 2015 was proposed to reduce such undesirable covariate shift . The method normalizes the summed inputs to each hidden unit over the training cases. SpeciÔ¨Åcally for the ith summed input in', 'Furthermore many large scale neural networks can contain millions of parameters and computing two passes per parameter is clearly not optimal. And since in optimization techniques such as SGD we must compute the gradients once per iteration for several thousands of iterations it is obvious that this method quickly grows intractable. This inefÔ¨Åciency is why we only use gradient check to verify the correctness of our analytic gradients which are much quicker to compute.', 'The algorithm is motivated by the assumption that when gradients explode the curvature and higher or der derivatives explode as well and we are faced with a speciÔ¨Åc pattern in the error surface namely a val ley with a single steep wall. In order to deal with the vanishing gradient problem we use a regulariza tion term that forces the error signal not to vanish as it travels back in time.', 'The derivative for the ReLU is either 0 or 1. This way gradients would Ô¨Çow through the neurons whose deriva tive is 1 without getting attenuated while propagating back through time steps. 2.5 Deep Bidirectional RNNs So far we have focused on RNNs that condition on past words to predict the next word in the sequence.']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.0000000000000002,0.8,0.9215098413519476,0.05
418,What is the purpose of manual gradient checking?,"The purpose of manual gradient checking is to verify the correctness of analytic gradients by comparing them with numerical gradients, which are much quicker to compute. This helps ensure the accuracy of the gradients used in training neural networks.","['We note that this deÔ¨Ånition of the numerical gradient follows very naturally from the deÔ¨Ånition of the derivative where in the scalar case f x f x œµ f x œµ Gradient checks are a great way to compare analytical and numerical gradients.', 'A standard implementation of gradient check is shown below Snippet 2.1 def eval_numerical_gradient f x a naive implementation of numerical gradient of f at x f should be a function that takes a single argument x is the point numpy array to evaluate the gradient at fx f x evaluate function value at original point grad np.zeros x.shape h 0.00001 iterate over all indexes in x it np.', 'Furthermore many large scale neural networks can contain millions of parameters and computing two passes per parameter is clearly not optimal. And since in optimization techniques such as SGD we must compute the gradients once per iteration for several thousands of iterations it is obvious that this method quickly grows intractable. This inefÔ¨Åciency is why we only use gradient check to verify the correctness of our analytic gradients which are much quicker to compute.', 'Analytical gradients should be close and numerical gradients can be calculated using f Œ∏ J Œ∏ i J Œ∏ i 2œµ J Œ∏ i and J Œ∏ i can be evalu ated using two forward passes. An implementation of this can be seen in Snippet 2.1. Of course there is a slight difference the deÔ¨Ånition above only perturbs x in the positive direction to compute the gradient.', 'frameworks Tensorflow PyTorch etc. do backpropagation for you but mainly leave layer node writer to hand calculate the local derivative 74Backprop Implementations 75Implementation forward backward API 76Implementation forward backward API 77Manual Gradient checking Numeric Gradient For small h 1e 4 Easy to implement correctly But approximate and very slow You have to recompute f for every parameter of our model Useful for checking your implementation In the old days we hand', 'for checking your implementation In the old days we hand wrote everything doing this everywhere was the key test Now much less needed you can use it to check layers are correctly implemented 78Summary We ve mastered the core technology of neural nets Backpropagation recursively and hence efficiently apply the chain rule along computation graph downstream gradient upstream gradient x local gradient Forward pass compute results of operations and save intermediate values Backward pass apply chain rule to compute gradients 79Why learn all these details', 'This makes the math a bit cumbersome. Even worse if we re implementing the model without automatic diÔ¨Äerentiation computing J Œ∏ twice will be ineÔ¨Écient. So it will help us to deÔ¨Åne some variables to represent the intermediate derivatives Œ¥1 J Œ∏ Œ¥2 J z These can be thought as the error signals passed down to Œ∏ and z when doing backpropagation. We can compute them as follows Œ¥1 J', 'Here we now introduce a tech nique of numerically approximating these gradients though too computationally inefÔ¨Åcient to be used directly for training the net works this method will allow us to very precisely estimate the derivative with respect to any parameter it can thus serve as a useful sanity check on the correctness of our analytic derivatives.']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999999999999996,0.4,0.9416352335583432,0.0666666666666666
419,Why is understanding gradients important despite automatic differentiation?,"Understanding gradients is important despite automatic differentiation because it allows us to have a deeper insight into how neural networks are trained and optimized. By understanding gradients, we can debug and improve models, deal with issues like vanishing or exploding gradients, and have more control over the training process. It also helps us verify the correctness of our analytic gradients and ensure the stability and efficiency of the training process.","['We note that this deÔ¨Ånition of the numerical gradient follows very naturally from the deÔ¨Ånition of the derivative where in the scalar case f x f x œµ f x œµ Gradient checks are a great way to compare analytical and numerical gradients.', 'Here we now introduce a tech nique of numerically approximating these gradients though too computationally inefÔ¨Åcient to be used directly for training the net works this method will allow us to very precisely estimate the derivative with respect to any parameter it can thus serve as a useful sanity check on the correctness of our analytic derivatives.', 'Briefly the gradient think derivative U f of a scalar function f with respect to a parameter matrix U represents the direction to locally move U in in order to maximally increase the value of f. So in practice we do something like drawing the initial U 0 and V 0 randomly as U V N 0 0.', 'Furthermore many large scale neural networks can contain millions of parameters and computing two passes per parameter is clearly not optimal. And since in optimization techniques such as SGD we must compute the gradients once per iteration for several thousands of iterations it is obvious that this method quickly grows intractable. This inefÔ¨Åciency is why we only use gradient check to verify the correctness of our analytic gradients which are much quicker to compute.', 'to compute gradients 79Why learn all these details about gradients 80 Modern deep learning frameworks compute gradients for you But why take a class on compilers or systems when they are implemented for you Understanding what is going on under the hood is useful Backpropagation doesn t always work perfectly out of the box Understanding why is crucial for debugging and improving models See Karpathy article https medium.', 'It would be much nicer if the derivatives were in a n m matrix like this J W J W11 . . . J W1m . . . ... . . . J Wn1 . . . J Wnm Since this matrix has the same shape as W we could just subtract it times the learning rate from W when doing gradient descent.', 'The algorithm is motivated by the assumption that when gradients explode the curvature and higher or der derivatives explode as well and we are faced with a speciÔ¨Åc pattern in the error surface namely a val ley with a single steep wall. In order to deal with the vanishing gradient problem we use a regulariza tion term that forces the error signal not to vanish as it travels back in time.', 'This makes the math a bit cumbersome. Even worse if we re implementing the model without automatic diÔ¨Äerentiation computing J Œ∏ twice will be ineÔ¨Écient. So it will help us to deÔ¨Åne some variables to represent the intermediate derivatives Œ¥1 J Œ∏ Œ¥2 J z These can be thought as the error signals passed down to Œ∏ and z when doing backpropagation. We can compute them as follows Œ¥1 J']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9691266641795108,0.6666666666666666,0.938497155117018,0.0666666666666666
420,What is the core concept introduced in this class that leads to GPT-3 and ChatGPT?,"The core concept introduced in this class that leads to GPT-3 and ChatGPT is the use of large-scale language models trained on vast amounts of text data to perform natural language processing tasks with impressive capabilities and performance levels. These models, such as GPT-3 (Generative Pre-trained Transformer 3) and ChatGPT, leverage deep learning techniques and transformer architectures to understand and generate human-like text responses based on input prompts. They have shown significant advancements in various NLP tasks, including text generation, reading comprehension, and language understanding, by learning patterns and structures from the massive corpus of text data they are trained on.

Source: Lecture slides on Computational Natural Language Processing.","['Finally given the broad spectrum of capabilities displayed by GPT 3 we discuss concerns about bias fairness and broader societal impacts and attempt a preliminary analysis of GPT 3 s characteristics in this regard. The remainder of this paper is organized as follows. In Section 2 we describe our approach and methods for training GPT 3 and evaluating it.', '3.6 Reading Comprehension Next we evaluate GPT 3 on the task of reading comprehension. We use a suite of 5 datasets including abstractive multiple choice and span based answer formats in both dialog and single question settings. We observe a wide spread in GPT 3 s performance across these datasets suggestive of varying capability with different answer formats.', 'GPT 3 performs best within 3 points of the human baseline on CoQA RCM19 a free form conversational dataset and performs worst 13 F1 below an ELMo baseline on QuAC CHI 18 a dataset which requires modeling structured dialog acts and answer span selections of teacher student interactions.', 'We will release a collection of 500 uncurated unconditional samples to help provide a better sense of GPT 3 s limitations and strengths at text synthesis. Within the domain of discrete language tasks we have noticed informally that GPT 3 seems to have special difÔ¨Åculty with common sense physics despite doing well on some datasets such as PIQA BZB 19 that test this domain.', 'However it is very likely that incorporating these algorithmic advances could improve GPT 3 s performance on downstream tasks especially in the Ô¨Åne tuning setting and combining GPT 3 s scale with these algorithmic techniques is a promising direction for future work.', 'Second we conduct experiments using the publicly available GPT 3 API for four model scales text ada 001 text babbage 001 text curie 001 text davinci 002 . Finally we make exact inputs targets and predictions for LaMDA 137B for each task available as a zip Ô¨Åle in the supplementary material.', 'The ability of GPT 3 to generate several paragraphs of synthetic content that people Ô¨Ånd difÔ¨Åcult to distinguish from human written text in 3.9.4 represents a concerning milestone in this regard.', 'In this work we do not Ô¨Åne tune GPT 3 because our focus is on task agnostic performance but GPT 3 can be Ô¨Åne tuned in principle and this is a promising direction for future work.']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9994303955471764,0.1666666666666666,0.9086697733820818,0.0714285714285714
421,What is the purpose of Dropout as mentioned in the document?,"The purpose of Dropout, as mentioned in the document, is to act as another form of regularization by randomly dropping (setting to zero) neurons in the forward pass. This technique helps prevent neural networks from overfitting by training exponentially many smaller networks at once and averaging over their predictions. Additionally, Dropout is effective in ensuring that the expected output of a neuron during testing is approximately the same as it was during training to maintain network behavior.","['The result is that the network typically learns more meaningful informa tion from the data is less likely to overÔ¨Åt and usually obtains higher performance overall on the task at hand. One intuitive reason why this technique should be so effective is that what dropout is doing is essentially doing is training exponentially many smaller networks at once and averaging over their predictions. Dropout applied to an artiÔ¨Åcial neural network.', 'However a key sub tlety is that in order for dropout to work effectively the expected output of a neuron during testing should be approximately the same as it was during training else the magnitude of the outputs could be radically different and the behavior of the network is no longer well deÔ¨Åned.', 'Consider the question what causes pre cipitation to fall posed on the passage in Figure 1. In order to answer the question one might Ô¨Årst lo cate the relevant part of the passage precipitation ... falls under gravity then reason that under refers to a cause not location and thus determine the cor rect answer gravity .', 'kid stubbornly postponees pursuing job citing medical reasons despite tried reasonable compro mise negatively effecting forward employment mobility both personally and Ô¨Ånancially thoght wise want change this dumbass selÔ¨Åsh lazy atti tude now please help pls Table 29 Example of samples from our over optimized model overÔ¨Åt to one of our 1.3B reward models. 45', 'A summary has poor coverage if someone reading only the summary would be missing several important pieces of information about the situation in the post. A summary with good coverage should also match the purpose of the original post e.g. to ask for advice . Rubric Score of 1 The summary contains no information relevant to the post.', 'For example using the conÔ¨Ådence thresholds mentioned above we found that while lower conÔ¨Ådence labels were less useful than high conÔ¨Ådence labels for improving reward model accuracy they were still better to include than to omit. Similarly leaving out workers with poorer agreement rates did not help.', 'In the next section we discuss dropout which effectively acts as another form of regularization by randomly dropping i.e. setting to zero neurons in the forward pass. 2.3 Dropout Dropout is a powerful technique for regularization Ô¨Årst introduced by Srivastava et al. in Dropout A Simple Way to Prevent Neural Net works from OverÔ¨Åtting.', 'We conÔ¨Årmed that every matching training document contained only the source passage and none of the questions and answers in the dataset. The more likely explanation for the decrease in performance is that the 6 of examples that remain after Ô¨Åltering come from a slightly different distribution than the dirty examples.']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9653191454129152,0.75,0.934301496737238,0.0588235294117647
422,What are some approaches to address the vanishing gradient problem in RNNs?,"Some approaches to address the vanishing gradient problem in RNNs include using a gradient norm clipping strategy to deal with exploding gradients, implementing a soft constraint for the vanishing gradients problem, using a regularization term that represents a preference for parameter values that prevent gradients from increasing or decreasing too much in magnitude, and utilizing the Long Short Term Memory (LSTM) architecture proposed by Hochreiter and Schmidhuber in 1997 as a solution to the vanishing gradients issue. Additionally, the Hessian Free optimizer in conjunction with structural damping has been used to address the vanishing gradient problem effectively.

Source: Lecture03_RNNs_and_LLMs.txt, On_the_difficulty_of_training_Recurrent_Neural_Networks.txt","['2013. http proceedings.mlr.press v28 pascanu13.pdfHow to fix the vanishing gradient problem The main problem is that it s too difficult for the RNN to learn to preserve information over many timesteps.', 'Our analysis is used to justify a simple yet ef fective solution. We propose a gradient norm clipping strategy to deal with exploding gra dients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section. 1. Introduction A recurrent neural network RNN e.g. Fig.', 'Below we discuss the math ematical reasoning behind the vanishing gradient problem. Consider Equations 5 and 6 at a time step t to compute the RNN error dE dW we sum the error at each time step. That is dEt dW for every time step t is computed and accumulated.', '3.3. Vanishing gradient regularization We opt to address the vanishing gradients problem us ing a regularization term that represents a preference for parameter values such that back propagated gra dients neither increase or decrease too much in mag nitude.', 'Sutskever et al. 2011 use the Hessian Free opti mizer in conjunction with structural damping a spe ciÔ¨Åc damping strategy of the Hessian. This approach seems to deal very well with the vanishing gradient though more detailed analysis is still missing.', 'with RNNs Vanishing and Exploding Gradients 48Vanishing gradient intuition 49Vanishing gradient intuition chain rule 50Vanishing gradient intuition chain rule 51Vanishing gradient intuition chain rule 52Vanishing gradient intuition What happens if these are small Vanishing gradient problem When these are small the gradient signal gets smaller and smaller as it backpropagates further 53Vanishing gradient proof sketch linear case Recall What if were the identity', '60Long Short Term Memory RNNs LSTMs A type of RNN proposed by Hochreiter and Schmidhuber in 1997 as a solution to the problem of vanishing gradients Everyone cites that paper but really a crucial part of the modern LSTM is from Gers et al. 2000 Only started to be recognized as promising through the work of S s student Alex Graves c.', 'To gain practical intuition about the vanishing gradient problem you may visit the following example website. 2.4 Solution to the Exploding Vanishing Gradients Now that we gained intuition about the nature of the vanishing gradi ents problem and how it manifests itself in deep neural networks let us focus on a simple and practical heuristic to solve these problems.']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.0000000000000009,1.0,0.8136257889393534,0.0555555555555555
423,"What is perplexity, and how is it used in evaluating language models?","Perplexity is a metric used in evaluating language models, which is the inverse probability of a test set normalized by the number of words. It is a measure of how well a probability distribution or probability model predicts a sample. In practice, perplexity is used instead of raw probability for evaluating language models because it provides a more interpretable measure of model performance. Lower perplexity values indicate that the language model is better at predicting the words in the test set. Perplexity is closely related to the information theoretic notion of entropy and can be used to compare different n-gram models. It is important to note that perplexity should be computed without any prior knowledge of the test set or its vocabulary to ensure fair comparisons between language models.

Source: N_gram_Language_Models.txt","['3.2.1 Perplexity In practice we don t use raw probability as our metric for evaluating language mod els but a variant called perplexity. The perplexity sometimes called PPL for short perplexity of a language model on a test set is the inverse probability of the test set normalized by the number of words. For a test set W w1w2 .wN perplexity W P w1w2 .', 'The exact choice of UNK has an effect on metrics like perplexity. A language model can achieve low perplexity by choosing a small vocabulary and assigning the unknown word a high probability. Thus perplexities can only be compared across language models with the same vocabularies Buck et al. 2014 . 3.', 'We leave this exact calculation as exercise 3.12. We see in Section 3.8 that perplexity is also closely related to the information theoretic notion of entropy. We mentioned above that perplexity is a function of both the text and the lan guage model given a text W different language models will have different perplex ities. Because of this perplexity can be used to compare different n gram models.', 'An intrinsic improvement in perplexity does not guarantee an extrinsic im provement in the performance of a language processing task like speech recognition10 CHAPTER 3 N GRAM LANGUAGE MODELS or machine translation. Nonetheless because perplexity often correlates with such improvements it is commonly used as a quick check on an algorithm.', 'And the higher the probability the lower the perplexity since as Eq. 3.15 showed perplexity is related inversely to the likelihood of the test sequence according to the model . So a lower perplexity can tell us that a language model is a better predictor of the words in the test set.', 'Note that in computing perplexities the n gram model P must be constructed without any knowledge of the test set or any prior knowledge of the vocabulary of the test set. Any kind of knowledge of the test set can cause the perplexity to be artiÔ¨Åcially low. The perplexity of two language models is only comparable if they use identical vocabularies.', 'A better n gram model is one that assigns a higher probability to the test data and perplexity is a normalized version of the probability of the test set. The perplexity measure actually arises from the information theoretic concept of cross entropy which explains otherwise mysterious properties of perplexity why the inverse probability for example and its relationship to entropy. Entropy is a Entropy measure of information.', 'The perplexity of a test set according to a language model is the geometric mean of the inverse test set probability computed by the model. Smoothing algorithms provide a more sophisticated way to estimate the prob ability of n grams. Commonly used smoothing algorithms for n grams rely on lower order n gram counts through backoff or interpolation. Both backoff and interpolation require discounting to create a probability dis tribution.']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9199947816905886,0.875,0.9532236084792016,0.5185185185185185
424,How does LSTM solve the vanishing gradient problem?,"LSTM (Long Short-Term Memory) solves the vanishing gradient problem by introducing a special set of units that are linear and have a recurrent connection to itself which is fixed to 1. This architecture makes it much easier for an RNN to preserve information over many timesteps, ensuring that information is not lost due to vanishing gradients. By using LSTM units, the model can better capture long-term dependencies in the data and mitigate the vanishing gradient issue effectively.

Source: On_the_difficulty_of_training_Recurrent_Neural_Networks.txt","['Sutskever et al. 2011 use the Hessian Free opti mizer in conjunction with structural damping a spe ciÔ¨Åc damping strategy of the Hessian. This approach seems to deal very well with the vanishing gradient though more detailed analysis is still missing.', 'The algorithm is motivated by the assumption that when gradients explode the curvature and higher or der derivatives explode as well and we are faced with a speciÔ¨Åc pattern in the error surface namely a val ley with a single steep wall. In order to deal with the vanishing gradient problem we use a regulariza tion term that forces the error signal not to vanish as it travels back in time.', 'Below we discuss the math ematical reasoning behind the vanishing gradient problem. Consider Equations 5 and 6 at a time step t to compute the RNN error dE dW we sum the error at each time step. That is dEt dW for every time step t is computed and accumulated.', 'When the gradient value goes to zero however it can go undetected while dras tically reducing the learning quality of the model for far away words in the corpus this issue is called the Vanishing Gradient Problem. Due to vanishing gradients we don t know whether there is no dependency between steps t and t n in the data or we just cannot capture the true dependency due to this issue.', 'In Hochreiter and Schmidhuber 1997 Graves et al. 2009 a solution is proposed for the vanishing gra dients problem where the structure of the model is changed. SpeciÔ¨Åcally it introduces a special set of units called LSTM units which are linear and have a recurrent connection to itself which is Ô¨Åxed to 1.', '3.3. Vanishing gradient regularization We opt to address the vanishing gradients problem us ing a regularization term that represents a preference for parameter values such that back propagated gra dients neither increase or decrease too much in mag nitude.', 'To gain practical intuition about the vanishing gradient problem you may visit the following example website. 2.4 Solution to the Exploding Vanishing Gradients Now that we gained intuition about the nature of the vanishing gradi ents problem and how it manifests itself in deep neural networks let us focus on a simple and practical heuristic to solve these problems.', 'github.io posts 2015 08 Understanding LSTMs 65How does LSTM solve vanishing gradients 66 The LSTM architecture makes it much easier for an RNN to preserve information over many timesteps e.g. if the forget gate is set to 1 for a cell dimension and the input gate set to 0 then the information of that cell is preserved indefinitely. In contrast it s harder for a vanilla RNN to']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.0000000000000009,1.0,0.9378156051785748,0.0588235294117647
425,"What is the impact of bidirectional RNNs, and where are they applicable?","Bidirectional RNNs provide a contextual representation that includes both left and right context, making them useful for tasks where information from both directions is important. However, bidirectional RNNs are only applicable when you have access to the entire input sequence. They are not applicable to Language Modeling because in LM, only the left context is available.

Source: lecture03_RNNs_and_LLMs","['Input audio START conditioning 77Bidirectional RNNs was terribly exciting the movie Forward RNN Backward RNN Concatenated hidden states This contextual representation of terribly has both left and right context 78Bidirectional RNNs 79 Note bidirectional RNNs are only applicable if you have access to the entire input sequence They are not applicable to Language Modeling because in LM you only have left context available.', 'The Problem of Long Term Dependencies One of the appeals of RNNs is the idea that they might be able to connect previous information to the present task such as using previous video frames might inform the understanding of the present frame. If RNNs could do this they d be extremely useful. But can they It depends. Sometimes we only need to look at recent information to perform the present task.', 'Perplexity 2J 9 2.2 Advantages Disadvantages and Applications of RNNs RNNs have several advantages 1. They can process input sequences of any length 2. The model size does not increase for longer input sequence lengths 3. Computation for step t can in theory use information from many steps back. 4.', 'Unfortunately as that gap grows RNNs become unable to learn to connect the information. In theory RNNs are absolutely capable of handling such long term dependencies. A human could carefully pick parameters for them to solve toy problems of this form. Sadly in practice RNNs don t seem to be able to learn them.', 'Linear interaction distance. A related issue with RNNs is the diffi culty with which distant tokens in a sequence can interact with each other. By interact we mean that the presence of one token already observed in the past gainfully affects the processing of another token.', 'A simple form of RNN is as follows ht œÉ W ht 1 Uxt 4 Dependence on the sequence index where ht Rd U Rd d and W Rd d. By 2017 the intuition was that there were twofold issues with the recurrent neural net work form and they both had to do with the the depenence on the sequence index often called the dependence on time highlighted in Equation 4.', 'h h h Figure 8 A bi directional RNN model h t f W xt V h t 1 b 17 h t f W xt V h t 1 b 18 ÀÜ yt g Uht c g U h t h t c 19 RNNs can also be multi layered. Figure 9 shows a multi layer bi directional RNN where each lower layer feeds the next layer.', 'input context Same weights applied on every timestep so there is symmetry in how inputs are processed. RNN Disadvantages Recurrent computation is slow In practice difficult to access information from many steps back More on these later 30Training an RNN Language Model Get a big corpus of text which is a sequence of words Feed into RNN LM compute output distribution for every step t. i.e. predict probability dist of every word given words so far']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.922093222854596,1.0,0.9543262300668288,0.3809523809523809
426,Describe the concept of multi-layer or stacked RNNs and their benefit.,"Multi-layer or stacked RNNs allow a network to compute more complex representations by having lower RNNs compute lower level features and higher RNNs compute higher level features. This architecture works better than just having one layer of high dimensional encodings. In practice, multi-layer RNNs are found to be high performing, although they are not as deep as convolutional or feed-forward networks. Stacked RNNs are beneficial for tasks like Neural Machine Translation, where having 2 to 4 layers for the encoder RNN and 4 layers for the decoder RNN is optimal. Additionally, skip connections or dense connections are often needed to train deeper RNNs effectively.

Source: Lecture03_RNNs_and_LLMs","['This allows the network to compute more complex representations The lower RNNs should compute lower level features and the higher RNNs should compute higher level features. Multi layer RNNs are also called stacked RNNs. 80Multi layer RNNs was terribly exciting the movie RNN layer 1 RNN layer 2 81 RNN layer 3 The hidden states from RNN layer i are the inputs to RNN layer i', 'layer i are the inputs to RNN layer i 1Multi layer RNNs in practice 39 Multi layer or stacked RNNs allow a network to compute more complex representations they work better than just have one layer of high dimensional encodings The lower RNNs should compute lower level features and the higher RNNs should compute higher level features.', 'h h h Figure 8 A bi directional RNN model h t f W xt V h t 1 b 17 h t f W xt V h t 1 b 18 ÀÜ yt g Uht c g U h t h t c 19 RNNs can also be multi layered. Figure 9 shows a multi layer bi directional RNN where each lower layer feeds the next layer.', 'RNNs should compute higher level features. High performing RNNs are usually multi layer but aren t as deep as convolutional or feed forward networks For example In a 2017 paper Britz et al.', 'org pdf 1703.03906.pdfTerminology By the end of the course You will understand phrases like stacked bidirectional LSTMs with residual connections and self attention The RNN described in this lecture simple vanilla Elman RNN You learned about other RNN flavors like LSTM and GRU and multi layer RNNs 83', 'To maintain two hidden layers at any time this network consumes twice as much memory space for its weight and bias parameters. The Ô¨Ånal classiÔ¨Åcation result ÀÜ yt is generated through combining the score results produced by both RNN hidden layers. Fig ure 8 shows the bi directional network architecture and Equations 17 and 18 show the mathematical formulation behind setting up the bi directional RNN hidden layer.', 'Perplexity 2J 9 2.2 Advantages Disadvantages and Applications of RNNs RNNs have several advantages 1. They can process input sequences of any length 2. The model size does not increase for longer input sequence lengths 3. Computation for step t can in theory use information from many steps back. 4.', 'find that for Neural Machine Translation 2 to 4 layers is best for the encoder RNN and 4 layers is best for the decoder RNN Often 2 layers is a lot better than 1 and 3 might be a little better than 2 Usually skip connections dense connections are needed to train deeper RNNs e.g. 8 layers Transformer based networks e.g.']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9535767576920824,0.8333333333333334,0.9575155034705304,0.0555555555555555
427,How does Dropout work during training and testing?,"During training, dropout works by randomly dropping (setting to zero) neurons in the forward pass with a probability \( p \). This helps prevent overfitting by training the network to be more robust. During testing, the output of each neuron is kept the same as it was during training, but the weights are scaled by \( \frac{1}{p} \) to account for the fact that not all neurons are active. This ensures that the expected output during testing is approximately the same as during training, maintaining the network's behavior. By averaging over predictions from exponentially many smaller networks trained with dropout, the network learns more meaningful information, is less likely to overfit, and usually performs better on the task at hand.

Source: Lecture02 - Backpropagation","['However a key sub tlety is that in order for dropout to work effectively the expected output of a neuron during testing should be approximately the same as it was during training else the magnitude of the outputs could be radically different and the behavior of the network is no longer well deÔ¨Åned.', 'Image credits to Srivastava et al. In practice the way we introduce dropout is that we take the out put h of each layer of neurons and keep each neuron with prob ability p and else set it to 0. Then during back propagation we only pass gradients through neurons that were kept alive during the forward pass. Finally during testing we compute the forward pass using all of the neurons in the network.', 'The result is that the network typically learns more meaningful informa tion from the data is less likely to overÔ¨Åt and usually obtains higher performance overall on the task at hand. One intuitive reason why this technique should be so effective is that what dropout is doing is essentially doing is training exponentially many smaller networks at once and averaging over their predictions. Dropout applied to an artiÔ¨Åcial neural network.', 'In the next section we discuss dropout which effectively acts as another form of regularization by randomly dropping i.e. setting to zero neurons in the forward pass. 2.3 Dropout Dropout is a powerful technique for regularization Ô¨Årst introduced by Srivastava et al. in Dropout A Simple Way to Prevent Neural Net works from OverÔ¨Åtting.', 'Training on the test set introduces a bias that makes the probabilities all look too high and causes huge inaccuracies in perplexity the probability based metric we introduce below. Sometimes we use a particular test set so often that we implicitly tune to its characteristics. We then need a fresh test set that is truly unseen. In such cases we call the initial test set the development test set or devset.', 'Lastly all the training examples are sorted by the length of paragraph and divided into mini batches of 32 examples each. We use Adamax for optimization as described in Kingma and Ba 5We use the Wikipedia Search API https www. mediawiki.org wiki API Search. 2014 . Dropout with p 0.3 is applied to word embeddings and all the hidden units of LSTMs.', '5 except p 0.15 for input layer via dropout mask During testing Multiply all weights by 1 ùëù No other dropout ùë•2 ùë•3 0 ùë§1 ùë§2 ùë§3 ùë§4 ùëè 1 ùë¶ ùë§1ùë•1 ùë§2ùë•2 ùë§3ùë•3 ùë•', 'While GPT 3 takes a step towards test time sample efÔ¨Åciency closer to that of humans one shot or zero shot it still sees much more text during pre training than a human sees in the their lifetime Lin20 . Improving pre training sample efÔ¨Åciency is an important direction for future work and might come from grounding in the physical world to provide additional information or from algorithmic improvements.']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9915023089199764,0.7142857142857143,0.926297096543248,0.0454545454545454
428,What is the role of parameter initialization in neural networks?,"The role of parameter initialization in neural networks is crucial for achieving optimal performance. Proper initialization of weights and biases helps in maintaining activation variances and backpropagated gradient variances across layers, leading to faster convergence and lower error rates during training. A common strategy is to initialize weights to small random numbers normally distributed around 0, which has been found to work acceptably well in practice. Different initialization schemes can have a significant impact on the training dynamics and overall performance of the neural network.","['s 6 n l n l 1 s 6 n l n l 1 Where n l is the number of input units to W fan in and n l 1 is the number of output units from W fan out . In this parameter initialization scheme bias units are initialized to 0. This approach attempts to maintain activation variances as well as backpropagated gradient variances across layers.', 'However in Understanding the dif ficulty of training deep feedforward neural networks 2010 Xavier et al study the effect of different weight and bias initialization schemes on training dynamics. The empirical Ô¨Åndings suggest that for sigmoid and tanh activation units faster convergence and lower error rates are achieved when the weights of a matrix W Rn l 1 n l are initialized randomly with a uniform distribution as follows W U s 6 n l n l 1 s', '2.6 Parameter Initialization A key step towards achieving superlative performance with a neu ral network is initializing the parameters in a reasonable way. A good starting strategy is to initialize the weights to small random numbers normally distributed around 0 and in practice this often words acceptably well.', '2015 . Our proposed layer normalization method however is not a re parameterization of the original neural network. The layer normalized model thus has different invariance properties than the other methods that we will study in the following section. 5 Analysis In this section we investigate the invariance properties of different normalization schemes. 5.1 Invariance under weights and data transformations The proposed layer normalization is related to batch normalization and weight normalization.', 'Similar to the re centering of the weight matrix in layer normalization we can also show that batch normalization is invariant to re centering of the dataset. 5.2 Geometry of parameter space during learning We have investigated the invariance of the model s prediction under re centering and re scaling of the parameters. Learning however can behave very differently under different parameterizations even though the models express the same underlying function.', 'In weight normalization instead of the variance the L2 norm of the incoming weights is used to normalize the summed inputs to a neuron. Applying either weight normalization or batch normal ization using expected statistics is equivalent to have a different parameterization of the original feed forward neural network. Re parameterization in the ReLU network was studied in the Path normalized SGD Neyshabur et al. 2015 .', 'For Ô¨Åne tuning the BERT model is Ô¨Årst initialized with the pre trained parameters and all of the param eters are Ô¨Åne tuned using labeled data from the downstream tasks. Each downstream task has sep arate Ô¨Åne tuned models even though they are ini tialized with the same pre trained parameters.', 'The summed inputs are computed through a linear projection with the weight matrix W l and the bottom up inputs hl given as follows al i wl i hl hl 1 i f al i bl i 1 where f is an element wise non linear function and wl i is the incoming weights to the ith hidden units and bl i is the scalar bias parameter. The parameters in the neural network are learnt using gradient based optimization algorithms with the gradients being computed by back propagation.']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999999999999996,1.0,0.9304737457039004,0.0454545454545454
429,"What are the differences between Vanilla RNNs, LSTMs, and GRUs?","Vanilla RNNs have a limitation of vanishing gradients, which makes it difficult to retain information over long sequences. LSTMs (Long Short-Term Memory) were proposed as a solution to the vanishing gradient problem in RNNs. LSTMs have more complex units with input, forget, and output gates that help in preserving information over many time steps. GRUs (Gated Recurrent Units) are similar to LSTMs but have a simpler architecture with fewer gates, making them computationally less expensive. GRUs combine the forget and input gates into a single ""update gate"" and merge the cell state and hidden state. Overall, LSTMs and GRUs are improvements over Vanilla RNNs in terms of handling long-term dependencies and mitigating the vanishing gradient problem.

Source: lecture03_RNNs_and_LLMs.txt","['The motivation for using these is similar to those for GRUs however the architecture of such units does differ. Let us Ô¨Årst take a look at the mathematical formulation of LSTM units before diving into the intuition behind this design it œÉ W i xt U i ht 1 Input gate ft œÉ W f xt U f ht 1 Forget gate ot œÉ W o xt U o ht 1 Output Exposure gate ct tanh W c xt U c ht', 'In a vanilla RNN the hidden state is constantly being rewritten First off next time How about an RNN with separate memory which is added to LSTMs And then Creating more direct and linear pass through connections in model Attention residual connections etc.', 'github.io posts 2015 08 Understanding LSTMs 65How does LSTM solve vanishing gradients 66 The LSTM architecture makes it much easier for an RNN to preserve information over many timesteps e.g. if the forget gate is set to 1 for a cell dimension and the input gate set to 0 then the information of that cell is preserved indefinitely. In contrast it s harder for a vanilla RNN to', 'org pdf 1703.03906.pdfTerminology By the end of the course You will understand phrases like stacked bidirectional LSTMs with residual connections and self attention The RNN described in this lecture simple vanilla Elman RNN You learned about other RNN flavors like LSTM and GRU and multi layer RNNs 83', 'There are lots of others like Depth Gated RNNs by Yao et al. 2015 http arxiv.org pdf 1508.03790v2.pdf . There s also some completely different approach to tackling long term dependencies like Clockwork RNNs by Koutnik et al. 2014 http arxiv.org pdf 1402.3511v1.pdf .', '60Long Short Term Memory RNNs LSTMs A type of RNN proposed by Hochreiter and Schmidhuber in 1997 as a solution to the problem of vanishing gradients Everyone cites that paper but really a crucial part of the modern LSTM is from Gers et al. 2000 Only started to be recognized as promising through the work of S s student Alex Graves c.', 'Jozefowicz et al. 2015 http jmlr.org proceedings papers v37 jozefowicz15.pdf tested more than ten thousand RNN architectures finding some that worked better than LSTMs on certain tasks. Conclusion Earlier I mentioned the remarkable results people are achieving with RNNs. Essentially all of these are achieved using LSTMs.', 'A new family of neural networks Recurrent Neural Networks RNNs This is the most important concept in the class It leads to GPT 3 and ChatGPT Important and used in Ass1 but not the only way to build LMs0 model power Training error Test error Classic view Regularization works to prevent overfitting when we have a lot of features or later a very powerful deep model etc.']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9144657012831092,0.6666666666666666,0.952634292663389,0.0769230769230769
430,What is the difference between traditional language models and conditioned language models?,"Traditional language models typically consider only a finite window of previous words for conditioning the model, while conditioned language models, such as Recurrent Neural Networks (RNN), are capable of conditioning the model on all previous words in the corpus. This allows conditioned language models to have a broader context for generating text.","['2 Recurrent Neural Networks RNN Unlike the conventional translation models where only a Ô¨Ånite win dow of previous words would be considered for conditioning the language model Recurrent Neural Networks RNN are capable of conditioning the model on all previous words in the corpus. xt 1 xt xt 1 ht 1 ht ht 1 W W yt 1 yt yt 1 Figure 3 A Recurrent Neural Network RNN .', '3.4. While they both model English like sentences there is clearly no overlap in generated sentences and little overlap even in small phrases. Statistical models are likely to be pretty use less as predictors if the training sets and the test sets are as different as Shakespeare and WSJ.', 'As a result our experiments do not include any bidirectional architectures or other training objectives such as denoising. This is a noticeable difference from much of the recent literature which has documented improved Ô¨Åne tuning performance when using these approaches over standard language models RSR 19 . Thus our design decision comes at the cost of potentially worse performance on tasks which empirically beneÔ¨Åt from bidirectionality.', 'file name: lecture04 Conditioned Generation. Hamidreza Mahyar mahyarh@mcmaster.ca Computational Natural Language Processing Conditioned GenerationLanguage Models Language models are generative models of text s P x The Malfoys said Hermione. Harry was watching him. He looked like Madame Maxime. When she strode up the wrong staircase to visit himself.', 'For many purposes a pretrained multilingual model is more practical than a monolingual model since it avoids the need to build many 100 separate monolin gual models. And multilingual models can improve performance on low resourced languages by leveraging linguistic information from a similar language in the train ing data that happens to have more resources.', 'Our approach of stufÔ¨Ång the model s context with previous examples is most structurally similar to RL2 and also resembles HYC01 in that an inner loop of adaptation takes place through computation in the model s activations across timesteps without updating the weights while an outer loop in this case just language model pre training updates the weights and implicitly learns the ability to adapt to or at least recognize tasks deÔ¨Åned at inference time.', 'We documented roughly predictable trends of scaling in performance without using Ô¨Åne tuning. We also discussed the social impacts of this class of model. Despite many limitations and weaknesses these results suggest that very large language models may be an important ingredient in the development of adaptable general language systems. Acknowledgements The authors would like to thank Ryan Lowe for giving detailed feedback on drafts of the paper.', 'While n gram models are much simpler than state of the art neural language models based on the RNNs and trans formers we will introduce in Chapter 9 they are an important foundational tool for understanding the fundamental concepts of language modeling.']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.0000000000000002,,0.9091977423424288,0.0476190476190476
431,How can LSTM networks be used in language modeling?,LSTM networks can be used in language modeling to model the temporal interactions between words by placing an LSTM in both directions and concatenating the outputs of the two LSTMs. This helps in capturing long-term dependencies in text and improving the ability to predict language use effectively.,"['of any length Apply the same weights on each step Can optionally produce output on each step Recurrent Neural Network Language Model We ve shown that RNNs are a great way to build a LM despite some problems RNNs are also useful for much moreWhy should we care about Language Modeling 70 Language Modeling is a benchmark task that helps us measure our progress on predicting language use Language Modeling is a subcomponent of many NLP tasks especially those involving generating', '3. Contextual Embedding Layer. We use a Long Short Term Memory Network LSTM Hochreiter Schmidhuber 1997 on top of the embeddings provided by the previous layers to model the temporal interactions between words. We place an LSTM in both directions and concatenate the outputs of the two LSTMs.', 'Your model should include an embedding layer at least one RNN layer and a fully connected layer for output. Refer to the Recurrent Neural Networks RNN section of the lectures for guidance on the architecture. Use the Long Short Term Memory RNNs LSTMs section as a reference to enhance your model with LSTM cells to improve its ability to capture long term dependencies in text. 2.', 'LSTM neural networks for language modeling. In Proc. of Interspeech 2012. 14Peter D. Turney and Patrick Pantel. From frequency to meaning Vector space models of semantics. Journal of ArtiÔ¨Åcial Intelligence Research 37 1 141 188 2010.', 'We report with LMn performance of the networks trained from the language model embeddings Table 7 . Gen eralization performance is reported in per word accuracy PWA for POS and F1 score for other tasks. language models from the relatively fast training of the supervised networks. Once the language models are trained we can perform multiple experiments on the supervised networks in a rela tively short time.', '2.2 Recurrent Neural Net Language Model RNNLM Recurrent neural network based language model has been proposed to overcome certain limitations of the feedforward NNLM such as the need to specify the context length the order of the model N and because theoretically RNNs can efÔ¨Åciently represent more complex patterns than the shallow neural networks 15 2 .', 'All the networks were trained separately on each task using the sentence level likelihood SLL . The word embeddings were initialized to LM2 embeddings and then Ô¨Åne tuned for each task. We summarize features used by our implementation in Table 14 and we report performance achieved on each task in Table 15.', 'Hopefully walking through them step by step in this essay has made them a bit more approachable.1 24 24 3 23 PM Understanding LSTM Networks colah s blog 7 7 Attention and Augmented Recurrent Neural Networks On Distill Conv Nets A Modular Perspective Neural Networks Manifolds and Topology Deep Learning NLP and Representations LSTMs were a big step in what we can accomplish with RNNs.']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9841818755895096,1.0,0.9597952630206164,0.217391304347826
432,What are the methods of generation in conditioned language modeling?,"The methods of generation in conditioned language modeling include NL Generation, Translation, Summarization, Response Generation, Image Captioning, and Speech Recognition.","['file name: lecture04 Conditioned Generation. Hamidreza Mahyar mahyarh@mcmaster.ca Computational Natural Language Processing Conditioned GenerationLanguage Models Language models are generative models of text s P x The Malfoys said Hermione. Harry was watching him. He looked like Madame Maxime. When she strode up the wrong staircase to visit himself.', 'generate text according to some specification Input X Structured Data English Document Utterance Image Speech Output Y Text NL Description Japanese Short Description Response Text Transcript Task NL Generation Translation Summarization Response Generation Image Captioning Speech RecognitionFormulation and ModelingCalculating the Probability of a SentenceConditional Language ModelsOne Type of Language Model LSTM LSTM LSTM LSTM predict hate predict this predict movie predict s LSTM Mikolov et al.', 'Future directions. The methods in this paper could be applied to any task where humans can compare samples including dialogue machine translation question answering speech synthesis and music generation. We expect this method to be particularly important for generating long samples where the distributional shift and degeneracy of maximum likelihood samples can be problematic. It may be possible to improve sample efÔ¨Åciency by training to predict feedback across many tasks 42 .', '7871 7880 2020. URL https aclanthology.org 2020.acl main.703. Xiang Lisa Li and Percy Liang. PreÔ¨Åx tuning Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing Volume 1 Long Papers pp.', 'The generative model is trained on one billion words. These approaches should be seen as a linear counterpart of our work. Using multilayer models vastly expands the parameter sharing opportunities see Section 5 . Our approach simply consists of initializing the word lookup tables of the supervised networks with the embeddings computed by the language models. Supervised training is then performed as in Section 3.5.', '3.9.4 News Article Generation Previous work on generative language models qualitatively tested their ability to generate synthetic news articles by conditional sampling from the model given a human written prompt consisting of a plausible Ô¨Årst sentence for a news story RWC 19 .', '2 Recurrent Neural Networks RNN Unlike the conventional translation models where only a Ô¨Ånite win dow of previous words would be considered for conditioning the language model Recurrent Neural Networks RNN are capable of conditioning the model on all previous words in the corpus. xt 1 xt xt 1 ht 1 ht ht 1 W W yt 1 yt yt 1 Figure 3 A Recurrent Neural Network RNN .', 'We report with LMn performance of the networks trained from the language model embeddings Table 7 . Gen eralization performance is reported in per word accuracy PWA for POS and F1 score for other tasks. language models from the relatively fast training of the supervised networks. Once the language models are trained we can perform multiple experiments on the supervised networks in a rela tively short time.']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9899391568765684,0.6666666666666666,0.8704792720507712,0.037037037037037
433,What is beam search and how is it used in language generation?,"Beam search is a technique used in natural language processing for generating sequences of words, such as in machine translation or text generation tasks. It involves keeping track of the k most probable partial translations or hypotheses at each step of the decoding process. Each hypothesis has a score, typically its log probability, and the search algorithm aims to find high-scoring hypotheses by exploring multiple paths simultaneously. Beam search is more efficient than exhaustive search but is not guaranteed to find the optimal solution. In practice, a beam size of around 5 to 10 is commonly used in neural machine translation tasks. It helps in generating more fluent and coherent sequences by considering multiple possible options at each step.

Source: lecture05_Seq2Seq.txt","['Core idea On each step of decoder keep track of the k most probable partial translations which we call hypotheses k is the beam size in practice around 5 to 10 in NMT A hypothesis has a score which is its log probability Scores are all negative and higher score is better We search for high scoring hypotheses tracking top k on each step Beam search is not guaranteed to find optimal solution But much more efficient than exhaustive search 18Beam search decoding example Beam size k', 'On tasks with free form completion we use beam search with the same parameters as RSR 19 a beam width of 4 and a length penalty of Œ± 0.6. We score the model using F1 similarity score BLEU or exact match depending on what is standard for the dataset at hand. Final results are reported on the test set when publicly available for each model size and learning setting zero one and few shot .', 'Argmax Try to generate the sentence with the highest probability.Ancestral Sampling Randomly generate words one by one. An exact method for sampling from P X no further work needed.Greedy Search One by one pick the single highest probability word Not exact real problems Will often generate the easy words first Will prefer multiple common words to one rare wordBeam Search Instead of picking one high probability word maintain several paths Some in reading materials more in a later classModel', '2020 which were optimized for GPT 3 without performing any prompt engineering to optimize them on our model. For simplicity we use greedy search for all generative tasks compared with beam search used in Brown et al. 2020 .', 'Quality can often be improved signiÔ¨Åcantly by non uniform sampling strategies such as beam search 51 but these can lead to repetition and other undesirable artifacts 69 23 . Optimizing for quality may be a principled approach to overcoming these problems. Our goal in this paper is to advance methods for training language models on objectives that more closely capture the behavior we care about.', 'search 18Beam search decoding example Beam size k 2. Blue numbers START Calculate prob dist of next word 19Beam search decoding example Beam size k 2. Blue numbers START he I 0.7 log PLM he START 0.9 log PLM I START Take top k words and compute scores 20Beam search decoding example Beam size k 2. Blue numbers hit struck was got START he I 10 0.', 'Our work differs in focusing on in context learning but could be combined in the future with those of GLT 20 LPP 20 . Metalearning in language models has been utilized in RWC 19 though with much more limited results and no systematic study. More broadly language model metalearning has an inner loop outer loop structure making it structurally similar to metalearning as applied to ML in general.', '6 For each of the k hypotheses find top k next words and calculate scoresBeam search decoding example Beam size k 2.']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9218209045611296,1.0,0.9349835053759858,0.0526315789473684
434,Explain the concept of model ensembling in language models.,"Model ensembling in language models involves using multiple models together to improve performance. This can be done through techniques like parameter averaging, where the parameters of multiple models are averaged to create a more robust model. Ensemble Distillation is another approach where the knowledge from multiple models is distilled into a single model for better performance. By combining the strengths of different models, ensembling can help enhance the overall effectiveness of language models.

Source: lecture04_Conditioned_Generation","['agree use when you want to restrict possible answersParameter Averaging Problem Ensembling means we have to use M models at test time increasing our time memory complexity Parameter averaging is a cheap way to get some good effects of ensembling Basically write out models several times near the end of training and take the average of parametersEnsemble Distillation e.g. Kim et al.', 'Surprisingly grammatical but incoherent. We need to consider more than three words at a time if we want to model language well. But increasing n worsens sparsity problem and increases model sizeHow to build a neural language model Recall the Language Modeling task Input sequence of words Output prob. dist. of the next word How about a window based neural model We saw this applied to Named Entity Recognition in Lecture 2 LOCATION', 'We documented roughly predictable trends of scaling in performance without using Ô¨Åne tuning. We also discussed the social impacts of this class of model. Despite many limitations and weaknesses these results suggest that very large language models may be an important ingredient in the development of adaptable general language systems. Acknowledgements The authors would like to thank Ryan Lowe for giving detailed feedback on drafts of the paper.', 'But a model s improvement in perplexity should always be conÔ¨Årmed by an end to end evaluation of a real task before concluding the evaluation of the model. 3.3 Sampling sentences from a language model One important way to visualize what kind of knowledge a language model embodies is to sample from it. Sampling from a distribution means to choose random points sampling according to their likelihood.', 'Our approach of stufÔ¨Ång the model s context with previous examples is most structurally similar to RL2 and also resembles HYC01 in that an inner loop of adaptation takes place through computation in the model s activations across timesteps without updating the weights while an outer loop in this case just language model pre training updates the weights and implicitly learns the ability to adapt to or at least recognize tasks deÔ¨Åned at inference time.', 'applied to Named Entity Recognition in Lecture 2 LOCATION in Paris are amazing museums 24A fixed window neural Language Model their as the proctor started the clock discard the students opened fixed window 25A fixed window neural Language Model the students opened their books laptops concatenated word embeddings words one hot vectors hidden layer a zoo output distribution 26A fixed window neural Language Model the students opened their books laptops a zoo Improvements over n gram LM No sparsity problem', 'The literature on this is also extensive QMZH19 HZJ 19 so we offer only a few brief comments on future directions speciÔ¨Åc to large language models. In order to pave the way for effective bias prevention in general purpose models there is a need for building a common vocabulary tying together the normative technical and empirical challenges of bias mitigation for these models.', 'We also provide an in depth ablation study of our model on the SQuAD development set vi sualize the intermediate feature spaces in our model and analyse its performance as compared to a more traditional language model for machine comprehension Rajpurkar et al. 2016 . 2 MODEL Our machine comprehension model is a hierarchical multi stage process and consists of six layers Figure 1 1.']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9687264226750372,0.0,0.8844563703903516,0.0555555555555555
435,How does ensemble distillation differ from parameter averaging?,"Ensemble distillation differs from parameter averaging in that ensemble distillation trains a model to copy the ensemble by matching the description over predicted words, aiming to make the same mistakes as an ensemble to increase accuracy notably. On the other hand, parameter averaging is a method where models are written out several times near the end of training, and the average of parameters is taken to achieve some of the benefits of ensembling in a cheaper way.","['e.g. Kim et al. 2016 Problem parameter averaging only works for models within the same run Knowledge distillation trains a model to copy the ensemble Specifically it tries to match the description over predicted words Why We want the model to make the same mistakes as an ensemble Shown to increase accuracy notablyStacking What if we have two very different models where prediction of outputs is done in very different ways e.g.', 'agree use when you want to restrict possible answersParameter Averaging Problem Ensembling means we have to use M models at test time increasing our time memory complexity Parameter averaging is a cheap way to get some good effects of ensembling Basically write out models several times near the end of training and take the average of parametersEnsemble Distillation e.g. Kim et al.', 'Yet it is often difÔ¨Åcult to Ô¨Ågure out which ideas are most responsible for the state of the art performance of a large ensemble. Because neural networks are nonconvex training runs with different initial parameters usually give different solutions. Table 11 reports results obtained for the CHUNK and NER task after ten training runs with random initial parameters.', 'Training Strategy Distillation Objective Design a loss function that effectively combines the traditio nal task specific loss and a distillation loss that measures the discrepancy between the teacher s and student s hidden states. Dataset Selection Utilize task specific datasets like the Stanford Sentiment Treebank SST for sentiment analysis CoNLL 2003 for NER and WMT datasets for machine translation.', 'Some in reading materials more in a later classModel EnsemblingEnsembling Why Multiple models make somewhat uncorrelated errors Models tend to be more uncertain when they are about to make errors Smooths over idiosyncrasies of the model LSTM1 predict1 I LSTM2 predict2 Combine predictions from multiple models s sLinear Interpolation Take a weighted average of the M model probabilities Second term often set to uniform distribution 1 MLog linear Interpolation Weighted', 'The parameters of the combining layers were then trained on the existing training set while keeping the ten networks Ô¨Åxed joined ensemble . This approach did not improve on simple voting. These ensembles come of course at the expense of a ten fold increase of the running time. On the other hand multiple training times could be improved using smart sampling strategies Neal 1996 .', 'The learning rate for each parameter depends on the history of gradient updates of that parameter in a way such that parameters with a scarce history of updates are updated faster using a larger learning rate. In other words parameters that have not been updated much in the past are likelier to have higher learning rates now.', 'Table 8 The added value versus the risk of setting each hyperparameter. The Ô¨Ågures show the differences in performance between the best achievable conÔ¨Ågurations when restricting a hyperparameter to different values. This difference indicates the potential gain of tuning a given hyperparameter as well as the risks of decreased performance when not tuning it.']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.0,1.0,0.9803909348401546,0.0588235294117647
436,What are some common evaluation metrics for conditioned language models?,"Some common evaluation metrics for conditioned language models include BLEU scores for generative tasks like machine translation, accuracy for classification tasks such as sentiment analysis and NER (Named Entity Recognition), and metrics based on neural models like BertScore. These metrics are used to measure the effectiveness and performance of conditioned language models in various NLP tasks.","['Indeed existing automatic metrics for evaluating summary quality such as ROUGE 39 have received criticism for poor correlation with human judgments 55 45 6 33 . We follow the works of 3 73 who Ô¨Åne tune language models from human feedback using reward learning 35 . We Ô¨Årst collect a dataset of human preferences between pairs of summaries then train a reward model RM via supervised learning to predict the human preferred summary.', 'out this review paperHow do we EvaluateBasic Evaluation Paradigm Use parallel test set Use system to generate translations Compare target translations w referenceHuman Evaluation Ask a human to do evaluation Final goal but slow expensive and sometimes inconsistentBLEU Works by comparing n gram overlap w reference Pros Easy to use good for measuring system improvement Cons Often doesn t match human eval bad for comparing very different systemsEmbedding based Metrics Recently many metrics based on neural models BertScore Find similarity between', 'Training and Distillation Fine tune the student model on the selected dataset while also learning from the teacher model s hidden state outputs to achieve similar perfor mance. McMaster University 4McMaster University SEP 775 Evaluation Metrics Performance Metrics Use accuracy for classification tasks sentiment analysis NER and BLEU scores for generative tasks machine translation to measure the student mo del s effectiveness.', 'Thus for speech recognition we can compare the performance of two language models by running the speech recognizer twice once with each language model and seeing which gives the more accurate transcription. Unfortunately running big NLP systems end to end is often very expensive. In stead it would be nice to have a metric that can be used to quickly evaluate potential improvements in a language model.', '8 Conclusion We presented a 175 billion parameter language model which shows strong performance on many NLP tasks and benchmarks in the zero shot one shot and few shot settings in some cases nearly matching the performance of 40state of the art Ô¨Åne tuned systems as well as generating high quality samples and strong qualitative performance at tasks deÔ¨Åned on the Ô¨Çy.', 'Thus our MT evaluation system requires two ingredients 1. a numerical translation closeness metric 2. a corpus of good quality human reference trans lations We fashion our closeness metric after the highly suc cessful word error rate metric used by the speech recognition community appropriately modiÔ¨Åed for multiple reference translations and allowing for le gitimate differences in word choice and word or der.', 'As a result our experiments do not include any bidirectional architectures or other training objectives such as denoising. This is a noticeable difference from much of the recent literature which has documented improved Ô¨Åne tuning performance when using these approaches over standard language models RSR 19 . Thus our design decision comes at the cost of potentially worse performance on tasks which empirically beneÔ¨Åt from bidirectionality.', 'We show that instruction tuning Ô¨Ånetuning language models on a collection of datasets described via instructions substantially improves zero shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction tune it on over 60 NLP datasets verbalized via natural language instruction templates. We evaluate this instruction tuned model which we call FLAN on unseen task types.']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999993746832612,1.0,0.9196842567417058,0.2105263157894736
437,What is the significance of stacking in model ensembling?,"Stacking in model ensembling is significant because it involves using the output of one system to calculate features for another system. This approach helps to combine predictions from multiple models, especially when the models are very different and predict outputs in different ways. By leveraging stacking, it becomes possible to improve the overall accuracy by incorporating diverse perspectives and leveraging the strengths of each individual model in the ensemble.","['agree use when you want to restrict possible answersParameter Averaging Problem Ensembling means we have to use M models at test time increasing our time memory complexity Parameter averaging is a cheap way to get some good effects of ensembling Basically write out models several times near the end of training and take the average of parametersEnsemble Distillation e.g. Kim et al.', 'a phrase based translation model and a neural MT model Niehues et al. 2017 Stacking uses the output of one system in calculating features for another systemCase Studies in Conditional Language ModelingFrom Images e.g. Karpathy et al. 2015 Input is image features output is text Use standard image encoders e.g. CNN Often pre trained on large databases such as ImageNetCheck out this review paperHow do we EvaluateBasic', 'Some in reading materials more in a later classModel EnsemblingEnsembling Why Multiple models make somewhat uncorrelated errors Models tend to be more uncertain when they are about to make errors Smooths over idiosyncrasies of the model LSTM1 predict1 I LSTM2 predict2 Combine predictions from multiple models s sLinear Interpolation Take a weighted average of the M model probabilities Second term often set to uniform distribution 1 MLog linear Interpolation Weighted', 'Figure 8 Ablation study result using mod els with instructions removed from Ô¨Ånetun ing FT . In a Ô¨Ånal ablation study we explore the role of in structions during Ô¨Ånetuning as one possibility is that performance gains come entirely from multi task Ô¨Åne tuning and the model could perform just as well without instructions. We hence consider two Ô¨Ånetuning setups without instructions.', 'e.g. Kim et al. 2016 Problem parameter averaging only works for models within the same run Knowledge distillation trains a model to copy the ensemble Specifically it tries to match the description over predicted words Why We want the model to make the same mistakes as an ensemble Shown to increase accuracy notablyStacking What if we have two very different models where prediction of outputs is done in very different ways e.g.', 'The small effect of templates is striking given our original motivation that composing ten templates per task would mitigate overÔ¨Åtting to any particular template. This results serves to underscore however the unpredictability of Ô¨Ånetuning large language models as one hypothesis is that models at such scale do not easily overÔ¨Åt to a Ô¨Ånetuning single task.', 'Figure 7 shows these results. We see that for the two models on the order of 100B pa rameters instruction tuning substantially im proves performance on held out tasks as is expected given the prior results in our pa per. The behavior on held out tasks for the 8B and smaller models however is thought provoking instruction tuning actually hurts performance on held out tasks.', 'See Appendix B for the details of each variation. In this appendix section we experimentally demonstrate how different choices of the similarity function Œ± Equation 1 and the fusion function Œ≤ Equation 2 impact the performance of our model. Each variation is deÔ¨Åned as following Eqn. 1 dot product. Dot product Œ± is deÔ¨Åned as Œ± h u h u 6 where indicates matrix transpose.']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9817039090335614,0.8,0.9324545203804562,0.0588235294117647
438,How are images used as input for conditioned language models?,"Images can be used as input for conditioned language models by representing the image classes as learned dimensional embeddings per class and adding the respective embedding to the input representation of every input position together with the positional encodings. This approach allows for conditioning on image class in models like the Image Transformer, which can achieve similar log likelihoods as in unconditioned generation when trained on datasets like CIFAR 10.","['We further hope to have provided additional evidence that even in the light of generative adversarial networks likelihood based models of images is very much a promising area for further research as is using network architectures such as the Image Transformer in GANs. In future work we would like to explore a broader variety of conditioning information including free form text as previously proposed Mansimov et al. 2015 and tasks combining modalities such as language driven editing of images.', 'As a result our experiments do not include any bidirectional architectures or other training objectives such as denoising. This is a noticeable difference from much of the recent literature which has documented improved Ô¨Åne tuning performance when using these approaches over standard language models RSR 19 . Thus our design decision comes at the cost of potentially worse performance on tasks which empirically beneÔ¨Åt from bidirectionality.', 'a phrase based translation model and a neural MT model Niehues et al. 2017 Stacking uses the output of one system in calculating features for another systemCase Studies in Conditional Language ModelingFrom Images e.g. Karpathy et al. 2015 Input is image features output is text Use standard image encoders e.g. CNN Often pre trained on large databases such as ImageNetCheck out this review paperHow do we EvaluateBasic', 'For DMOL we only evaluated learned coordinate embeddings. 5.2. Conditioning on Image Class We represent the image classes as learned d dimensional embeddings per class and simply add the respective em bedding to the input representation of every input position together with the positional encodings. We trained the class conditioned Image Transformer on CIFAR 10 achieving very similar log likelihoods as in un conditioned generation.', 'in the embedding parameters LCG 19 . Many of these techniques provide signiÔ¨Åcant gains on downstream tasks. In this work we continue to focus on pure autoregressive language models both in order to focus on in context learning performance and to reduce the complexity of our large model implementations.', 'We show that instruction tuning Ô¨Ånetuning language models on a collection of datasets described via instructions substantially improves zero shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction tune it on over 60 NLP datasets verbalized via natural language instruction templates. We evaluate this instruction tuned model which we call FLAN on unseen task types.', '6. Conclusion In this work we demonstrate that models based on self attention can operate effectively on modalities other than text and through local self attention scale to signiÔ¨Åcantly larger structures than sentences. With fewer layers its larger receptive Ô¨Åelds allow the Image Transformer to signiÔ¨Åcantly improve over the state of the art in unconditional probabilis tic image modeling of comparatively complex images from ImageNet as well as super resolution.', 'Instruction tuning is a simple method that as depicted in Figure 2 combines appealing aspects of both the pretrain Ô¨Ånetune and prompting paradigms by using supervision via Ô¨Ånetuning to improve language model s responses to inference time text interactions. Our empirical results demonstrate promising abilities of language models to perform tasks described purely via instructions.']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9918168868462132,1.0,0.8988141036659114,0.05
439,What challenges exist in evaluating conditioned language models?,"One challenge in evaluating conditioned language models is the presence of biases that can affect the model's performance and generalization abilities. These biases can impact the model's predictions and its ability to adapt to new tasks effectively. Additionally, there may be limitations in characterizing biases in large-scale generative models, which requires continuous research and the exploration of different methodological approaches to address these challenges.

Source: Language Models_are_Few_Shot_Learners","['In this paper we test this hypothesis by training a 175 billion parameter autoregressive language model which we call GPT 3 and measuring its in context learning abilities. SpeciÔ¨Åcally we evaluate GPT 3 on over two dozen NLP datasets as well as several novel tasks designed to test rapid adaptation to tasks unlikely to be directly contained in the training set.', '386.2.4 Future Bias and Fairness Challenges We have presented this preliminary analysis to share some of the biases we found in order to motivate further research and to highlight the inherent difÔ¨Åculties in characterizing biases in large scale generative models we expect this to be an area of continuous research for us and are excited to discuss different methodological approaches with the community.', 'We documented roughly predictable trends of scaling in performance without using Ô¨Åne tuning. We also discussed the social impacts of this class of model. Despite many limitations and weaknesses these results suggest that very large language models may be an important ingredient in the development of adaptable general language systems. Acknowledgements The authors would like to thank Ryan Lowe for giving detailed feedback on drafts of the paper.', 'The literature on this is also extensive QMZH19 HZJ 19 so we offer only a few brief comments on future directions speciÔ¨Åc to large language models. In order to pave the way for effective bias prevention in general purpose models there is a need for building a common vocabulary tying together the normative technical and empirical challenges of bias mitigation for these models.', 'As a result our experiments do not include any bidirectional architectures or other training objectives such as denoising. This is a noticeable difference from much of the recent literature which has documented improved Ô¨Åne tuning performance when using these approaches over standard language models RSR 19 . Thus our design decision comes at the cost of potentially worse performance on tasks which empirically beneÔ¨Åt from bidirectionality.', 'Promising future directions in this vein might include learning the objective function from humans ZSW 19a Ô¨Åne tuning with reinforcement learning or adding additional modalities such as images to provide grounding and a better model of the world CLY 19 . Another limitation broadly shared by language models is poor sample efÔ¨Åciency during pre training.', 'By presenting a broad characterization of GPT 3 s strengths and weaknesses including these limitations we hope to stimulate study of few shot learning in language models and draw attention to where progress is most needed. A heuristic sense of the overall results can be seen in Figure 1.3 which aggregates the various tasks though it should not be seen as a rigorous or meaningful benchmark in itself .', 'As Ô¨Åne tuned language models have neared human performance on many standard benchmark tasks considerable effort has been devoted to constructing more difÔ¨Åcult or open ended tasks including question answering KPR 19 IBGC 14 CCE 18 MCKS18 reading comprehension CHI 18 RCM19 and adversarially constructed datasets designed to be difÔ¨Åcult for existing language models SBBC19 NWD 19 .']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI teaching Assistant for the SEP 775 course. 
You will provide an interactive platform for students to ask questions and receive guidance on course materials.
Your goal is to answer questions as accurately as possible based on the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9853673299566266,1.0,0.8377355513783987,0.0588235294117647
440,What is the instructor's name for this course?,"The instructor's name for this course is Hamidreza Mahyar. 
Source: NLP_Winter_2024_Course_Outline.txt","['Instructor Hamidreza Mahyar E Mail mahyarh@mcmaster.ca Office Hours upon request 2. COURSE SPECIFICS Course Description This course covers some fundamental concepts in natural language processing NLP and how they apply in real world use cases.', 'Announcements made in class or placed on Avenue are considered to have been communicated to all students including those individuals that are not in class. Instructor has the right to submit work to software to identify plagiarism. 3. SUB TOPIC S Week 1 Overview Human language Word meanings Word2Vec and Optimization basics Week 2 Intro to neural networks Gradients and backpropagation Week 3 Recurrent Neural Networks and Language Models Week', 'file name: lecture07 Prompting Instruction Finetuning and RLHF . Hamidreza Mahyar mahyarh@mcmaster.ca Computational Natural Language Processing Prompting Instruction Finetuning and RLHFReminders Project proposals will be posted this week. A2 due Saturday 11 59PM We still recommend using Colab for the assignments in case you run into trouble e.g.', 'file name: lecture08 Question Answering. Hamidreza Mahyar mahyarh@mcmaster.ca Computational Natural Language Processing Question Answering2 Lecture plan 1. What is question answering 2. Reading comprehension How to answer questions over a single passage of text 3. Open domain textual question answering How to answer questions over a large collection of documents Assignment 3 is out Start working on your final project1.', 'Course Outline 1. COURSE INFORMATION Session Offered Winter 2024 Course Name Introduction to Computational Natural Language Processing Course Code SEP 775 Date s and Time s of lectures Lectures In person Wednesday 3 30 pm 6 30 pm Program Name System and Technology Calendar Description This graduate course introduces fundamental concepts in computational natural language processing NLP and their applications to teaching students how to deal with textual data in Artificial Intelligence.', 'Martin Deep Learning Ian Goodfellow Yoshua Bengio and Aaron Courville Prerequisite s Proficiency in Python programming language Some Machine Learning and or Deep Learning knowledge is a plus Corequisite s Antirequisite s Course Specific Policies All work must be shown to get full credit. Specific policy of Projects Assignments submissionsProjects Assignments will be conducted online by using related software.', 'file name: lecture01 wordvecs1. Hamidreza Mahyar mahyarh@mcmaster.ca Computational Natural Language Processing Overview of NLP and Word VectorsLecture Plan 2 Lecture 1 Introduction and Word Vectors 1. The course 2. Human language and word meaning 3. Word2vec introduction 4. Word2vec objective function gradients 5. Optimization basics 6.', '. . pm and the question vector q as input and simply train two classiÔ¨Åers independently for predicting the two ends of the span.']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999999999999997,1.0,0.9472431237876591,0.375
441,What is the email address of the instructor for this course?,"The email address of the instructor for this course is mahyarh@mcmaster.ca. 

Source: NLP_Winter_2024_Course_Outline.txt","['Instructor Hamidreza Mahyar E Mail mahyarh@mcmaster.ca Office Hours upon request 2. COURSE SPECIFICS Course Description This course covers some fundamental concepts in natural language processing NLP and how they apply in real world use cases.', 'Accept that forwarded e mails may be lost and that e mail is considered received if sent via the student s @mcmaster.ca alias. Check the McMaster Avenue email and course websites on a regular basis during the term. CONDUCT EXPECTATIONS As a McMaster student you have the right to experience and the responsibility to demonstrate respectful and dignified interactions within all of our living learning and working communities.', 'file name: lecture07 Prompting Instruction Finetuning and RLHF . Hamidreza Mahyar mahyarh@mcmaster.ca Computational Natural Language Processing Prompting Instruction Finetuning and RLHFReminders Project proposals will be posted this week. A2 due Saturday 11 59PM We still recommend using Colab for the assignments in case you run into trouble e.g.', '. . pm and the question vector q as input and simply train two classiÔ¨Åers independently for predicting the two ends of the span.', 'e mail Avenue to Learn A2L LearnLink web pages capa Moodle ThinkingCap etc. . Students should be aware that when they access the electronic components of a course using these elements private information such as first and last names user names for the McMaster e mail accounts and program affiliation may become apparent to all other students in the same course. The available information is dependent on the technology used.', 'Announcements made in class or placed on Avenue are considered to have been communicated to all students including those individuals that are not in class. Instructor has the right to submit work to software to identify plagiarism. 3. SUB TOPIC S Week 1 Overview Human language Word meanings Word2Vec and Optimization basics Week 2 Intro to neural networks Gradients and backpropagation Week 3 Recurrent Neural Networks and Language Models Week', 'Instruction Type Code Type Hours per term C Classroom instruction 39 L Laboratory workshop or fieldwork T Tutorial DE Distance education Total Hours 39 Resources ISBN Textbook Title Edition Author Publisher Natural Language Processing Jacob Eisenstein Speech and Language Processing Dan Jurafsky and James H.', '2019 and a dual encoder architecture Bromley et al. 1994 we focus on developing the right training scheme using a relatively small number of question and passage pairs. Through a series of careful ablation studies our Ô¨Ånal solution is surprisingly simple the embedding is optimized for maximizing inner products of the question and relevant passage vectors with an objective compar ing all pairs of questions and passages in a batch.']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.0000000000000002,1.0,0.938617973815404,0.21052631578947367
442,When is this course held every week?,"The course ""Introduction to Computational Natural Language Processing"" (SEP 775) is held in person every Wednesday from 3:30 pm to 6:30 pm.","['2 days before 01 01 2015 is 12 30 2014 so today is 12 30 2014. So one week from today will be 01 05 2015. So the answer is 01 05 2015.', 'So today is 01 07 2019. So the answer is 01 07 2019. Q The concert was scheduled to be on 06 01 1943 but was delayed by one day to today.', 'Announcements made in class or placed on Avenue are considered to have been communicated to all students including those individuals that are not in class. Instructor has the right to submit work to software to identify plagiarism. 3. SUB TOPIC S Week 1 Overview Human language Word meanings Word2Vec and Optimization basics Week 2 Intro to neural networks Gradients and backpropagation Week 3 Recurrent Neural Networks and Language Models Week', 'QUESTION Jane scheduled 3 apointments with 5 poeple for tomorrow Tue 7 9 1972 . What is the date one week ago from today in MM DD YYYY MODEL ANSWER INCORRECT Tomorrow is 7 9 1972. One week ago from today is 7 2 1972.', 'EXTREME CIRCUMSTANCES The University reserves the right to change the dates and deadlines for any or all courses in extreme circumstances e.g. severe weather labour disruptions etc. . Changes will be communicated through regular McMaster communication channels such as McMaster Daily News A2L and or McMaster email.', '3 Recurrent Neural Networks and Language Models Week 4 Conditioned Generation Week 5 Part 1 Self attention and transformers Part 2 Seq2Seq models Week 6 Midterm Test Week 7 Midterm Recess No class Week 8 Prompting Instruction Finetuning and RLHF Week 9 Question Answering systems Week 10 Multimodal models Week 11 Project presentation and Q A Week 12 Project presentation and Q', 'Q The Ô¨Årst day of 2019 is a Tuesday and today is the Ô¨Årst Monday of 2019. What is the date today in MM DD YYYY A If the Ô¨Årst day of 2019 was Tuesday then 01 01 2019 was a Tuesday. Today is the Ô¨Årst monday would be six days later.', 'Course Outline 1. COURSE INFORMATION Session Offered Winter 2024 Course Name Introduction to Computational Natural Language Processing Course Code SEP 775 Date s and Time s of lectures Lectures In person Wednesday 3 30 pm 6 30 pm Program Name System and Technology Calendar Description This graduate course introduces fundamental concepts in computational natural language processing NLP and their applications to teaching students how to deal with textual data in Artificial Intelligence.']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.8459265441514411,1.0,0.8855559242760931,0.045454545454545456
443,What is the name of the TA for this course?,"The name of the TA for this course is AI Teaching Assistant (AI TA). It aims to provide an interactive platform for students to ask questions and receive guidance on course materials, similar to the support offered by human teaching assistants. 

Source: SEP 775 - Computational Natural Language Processing","['The project emphasizes the application of NLP techniques to create an educational tool that enhances the learning experience through dynamic interaction and content understanding. Project Objectives Automate FAQ Responses Develop the AI TA to autonomously address frequently asked questions related to course logistics assignments and content. Content Understanding and Query Handling Implement NLP strategies enabling the AI TA to interpret and respond to inquiries about the uploaded course material accurately.', 'file name: SEP775 FinalProjects 2. McMaster University SEP 775 Final Projects Project 1 AI Teaching Assistant Project Overview The goal of this project is to create an AI Teaching Assistant AI TA . The AI TA aims to provide an interactive platform for students to ask questions and receive guidance on course materials mirroring the support traditionally offered by human teaching assistants.', 'Response Generation Utilize the designed AI TA to generate responses that are not only accurate but also engaging and pedagogically effective. Interaction Interface Simplified Interface Development Construct an intuitive web or command line in terface that allows straightforward question submission and response retrieval ensuring ease of use for student interactions with the AI TA.', '2016. From particular to general A preliminary case study of transfer learning in reading compre hension. Machine Intelligence Workshop NIPS . Diederik Kingma and Jimmy Ba. 2014. Adam A method for stochastic optimization. arXiv preprint arXiv 1412.6980 . Kenton Lee Tom Kwiatkowski Ankur Parikh and Di panjan Das.', '2020. Span BERT Improving pre training by representing and predicting spans. Transactions of the Association of Computational Linguistics TACL . Mandar Joshi Eunsol Choi Daniel S Weld and Luke Zettlemoyer. 2017. TriviaQA A large scale dis tantly supervised challenge dataset for reading com prehension. In Association for Computational Lin guistics ACL .', 'Course Outline 1. COURSE INFORMATION Session Offered Winter 2024 Course Name Introduction to Computational Natural Language Processing Course Code SEP 775 Date s and Time s of lectures Lectures In person Wednesday 3 30 pm 6 30 pm Program Name System and Technology Calendar Description This graduate course introduces fundamental concepts in computational natural language processing NLP and their applications to teaching students how to deal with textual data in Artificial Intelligence.', 'Propose future research directions or applications of the TANDA methodology in other NLP tasks. McMaster University 1McMaster University SEP 775 Submission Guidelines Submit all code in a ZIP file including Jupyter Notebooks and a detailed PDF report with written explanations and visualizations. Clearly label each part and question in your submissions. Deadline March 13 2024 Evaluation Criteria Depth and clarity of theoretical understanding and literature review.', 'file name: SEP775 Assignment3. McMaster University SEP 775 Assignment 3 TANDA Approach for QA System Enhancement Objectives Explore TANDA Transfer And Adapt methodology to improve Question Answering QA systems using pre trained Transformer models focusing on sequential fine tuning te chniques. 1. Introduction and Theory 15 Summarize the TANDA methodology s principles focusing on its novelty and the rationale behind sequential fine tuning.']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.999999614852789,1.0,0.8259342972610522,0.030303030303030304
444,What is Word2vec?,Word2vec is a framework for learning word vectors that represents each word in a fixed vocabulary as a low-dimensional vector. It is a model used for training and discovering latent vector representations of natural language words in a semantic space. It is widely used in natural language processing tasks for capturing word meanings and relationships within a corpus of text.,"['The word2vec model is a probabilistic model specified as follows where uw refers to the row of U corresponding to word w V and likewise for V pU V o c exp u o vc w V exp u wvc 4 This may be familiar to you as the softmax function which takes arbitrary scores here one for each word in the vocabulary resulting from dot products and produces a probability distribution where larger scored things get higher probability.', 'However many of the details of word2vec will hold true in methods that we ll proceed to further in the course so we ll focus our time on that. 3.2 Word2vec model and objective The word2vec model represents each word in a fixed vocabulary as a low dimensional much smaller than vocabulary size vector.', '718 0.147 0.051 19Word meaning as a neural word vector visualization 0.286 0.792 0.177 0.107 0.109 0.542 0.349 0.271 0.487 expect 203. Word2vec Overview 21 Word2vec Mikolov et al. 2013 is a framework for learning word vectors Idea We have a large corpus body of text', 'It consistently outperforms word2vec on the word analogy task given the same corpus vocabulary window size and training time. It achieves better results faster and also obtains the best results irrespective of speed. 2 Evaluation of Word Vectors So far we have discussed methods such as the Word2Vec and GloVe methods to train and discover latent vector representations of natural language words in a semantic space.', '3.3 Estimating a word2vec model from a corpus How do we train word2vec in practice Specifying the word2vec model is relatively transparent from the math we ve given above one constructs matrices U and V and can write out the math of the probability.', 'To do this in practice we will need to tune many hyperparameters in the Word2Vec subsystem such as the dimension of the word vector representation . While the idealistic approach is to retrain the entire system after any parametric changes in the Word2Vec subsystem this is impractical from an engineering standpoint because the machine learning system in step 3 is typi cally a deep neural network with millions of parameters that takes very long to train.', 'McMaster University 1McMaster University SEP 775 Resources for Word2Vec and GloVe Word2Vec Pre trained Word2Vec embeddings can be found on repositories such as HuggingFace or directly using Gensim. GloVe Pre trained GloVe embeddings are available at the GloVe website or through HuggingFace.', '4 for a plot of the learning curve. 4.7 Model Analysis Comparison with word2vec A rigorous quantitative comparison of GloVe with word2vec is complicated by the existence of many parameters that have a strong effect on per formance.']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999999999999994,,0.9567460780805264,0.06666666666666667
445,What are the main learning goals of the course?,"The main learning goals of the course include understanding the foundations of effective modern methods for deep learning applied to NLP, gaining a big picture understanding of human languages and the challenges in processing them with computers, and developing the ability to build systems in PyTorch for tasks such as word vectors, feedforward networks, recurrent networks, attention mechanisms, encoder-decoder models, transformers, and large pre-trained language models. Additionally, the course aims to cover topics like prompting, instruction finetuning, reinforcement learning from human feedback, zero-shot and few-shot learning, and the limitations and challenges associated with these techniques.","['The main criticism of our goal can be summarized as follows. Over the years the NLP com munity has developed a considerable expertise in engineering effective NLP features. Why should they forget this painfully acquired expertise and instead painfully acquire the skills required to train large neural networks As mentioned in our introduction we observe that no single NLP task really covers the goals of NLP.', 'learning goals 4 1. The foundations of the effective modern methods for deep learning applied to NLP Basics first then key methods used in NLP in 2024 Word vectors feed forward networks recurrent networks attention encoder decoder models transformers large pre trained language models etc. 2. A big picture understanding of human languages and the difficulties in understanding and producing them via computers 3. An understanding of and ability to build systems in PyTorch for some of', '13.6 Ambiguous We don t agree with the crowd workers answer or the question does not have a unique answer. Q What is the main goal of criminal punishment Sen. Achieving crime control via incapacitation and deterrence is a major goal of criminal punish ment. 6.1 Table 3 We manually labeled 192 examples into one or more of the above categories.', 'What s next Lecture Plan From Language Models to AssistantsLimitations of instruction finetuning One limitation of instruction finetuning is obvious it s expensive to collect ground truth data for tasks. But there are other subtler limitations too. Can you think of any Problem 1 tasks like open ended creative generation have no right answer. Write me a story about a dog and her pet grasshopper.', 'Martin Deep Learning Ian Goodfellow Yoshua Bengio and Aaron Courville Prerequisite s Proficiency in Python programming language Some Machine Learning and or Deep Learning knowledge is a plus Corequisite s Antirequisite s Course Specific Policies All work must be shown to get full credit. Specific policy of Projects Assignments submissionsProjects Assignments will be conducted online by using related software.', 'Zero Shot ZS and Few Shot FS In Context Learning No finetuning needed prompt engineering e.g. CoT can improve performance Limits to what you can fit in context Complex tasks will probably need gradient steps 2. Instruction finetuning 3. Reinforcement Learning from Human Feedback RLHF 4.', 'The additional pretraining tasks are likely more useful only when the target training sets are small. Al though the results of DPR on WQ and TREC in the single dataset setting are less competitive adding more question answer pairs helps boost the perfor mance achieving the new state of the art.', 'Reinforcement Learning from Human Feedback RLHF 51 Lecture Plan From Language Models to Assistants 4. What s next1. Zero Shot ZS and Few Shot FS In Context Learning No finetuning needed prompt engineering e.g. CoT can improve performance Limits to what you can fit in context Complex tasks will probably need gradient steps 2.']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999999999999997,1.0,0.9728279832508053,0.05555555555555555
446,How is the course grade distributed?,"The course grade is distributed as follows: 4 Assignments (20%), 4 Quizzes (10%), Midterm Test (35%), and Group Project (35%). The total weight for the course grade is 100%. It is important to note that the percentage distribution of each module may be adjusted.","['These gradings are then used to compare sentence forms It now turns out that given the graded n tuples of words for a particular sentence form we can Ô¨Ånd other sentences forms of the same word classes in which the same n tuples of words produce the same grading of sentences. This is an indication that these two sentence forms exploit common words with the same syntac tic function and possibly the same meaning.', '1 x i x i 1 x i 2 The gradient will of course need to be distributed to update the corresponding word vectors in implementation. 3.5 Non linear ClassiÔ¨Åers Figure 9 Here we see that many exam ples are wrongly classiÔ¨Åed even though the best linear decision boundary is chosen. This is due linear decision boundaries have limited model capacity for this dataset.', 'We used this classiÔ¨Åer to score Common Crawl documents. We kept each document in our dataset iff np.random.pareto Œ± 1 document_score We chose Œ± 9 in order to take mostly documents the classiÔ¨Åer scored highly but still include some documents that were out of distribution. Œ± was chosen to match the distribution of scores from our classiÔ¨Åer on WebText.', 'The prob lem now is that we usually cannot directly com pare scores from different rankings Aslam and Montague 2001 as their scores are not guaran teed to have the same ranges. An even worse case is the following scenario. Assume we use rank correlation as our metric. As a consequence we need our gold ranking to deÔ¨Åne an order on all the word pairs.', 'and Q A Week 12 Project presentation and Q A Week 13 Project presentation and Q A Classes end April 10th 4. ASSESSMENT OF LEARNING Weight 4 Assignments 20 4 Quizzes 10 Midterm Test 35 Group Project 35 TOTAL 100Note that the percentage distribution of each module may be adjusted.', 'Percentage grades will be converted to letter grades and grade points per the University calendar. 5. LEARNING OUTCOMES By the end of this course the students will know about Introduction to Deep Learning Word representations Language models Pre trained language models Text generation Seq2Seq models Prompting Feedback systems Multimodal models NLP applications e.g. Q A systems 6.', '0 79.9 74.7 88.5 66.2 83.9 84.4 82.3 94.1 78.6 Table 2 Top 20 Top 100 retrieval accuracy on test sets measured as the percentage of top 20 100 retrieved passages that contain the answer.', 'If none of the top 100 retrieved passages has the answer the ques tion will be discarded. For SQuAD and Natural Questions since the original passages have been split and processed differently than our pool of candidate passages we match and replace each gold passage with the corresponding passage in the candidate pool.7 We discard the questions when the matching is failed due to different Wikipedia versions or pre processing.']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999991154165837,0.3333333333333333,0.9681676939176167,0.05263157894736842
447,What is the policy for late submissions in this course?,"The policy for late submissions in this course is that there are NO free late days, and there is a penalty of 1 off course grade per day late. Assignments are not accepted more than 3 days late unless permission is given in advance. Students should make alternative arrangements with the instructor for late submissions. 

Source: NLP_Winter_2024_Course_Outline.txt","['to build systems in PyTorch for some of the major problems in NLP Word meaning dependency parsing machine translation question answeringCourse work and grading policy 5 4 Assignments 20 4 Quizzes 10 Midterm 35 Final Group Project 1 3 people 34 Late day policy NO free late days 1 off course grade per day late Assignments not accepted more than 3 days late unless given permission in advance Collaboration policy Understand allowed collaboration and how', 'Students should submit their request to their Faculty Office normally within 10 working days of the beginning of term in which they anticipate a need for accommodation or to the Registrar s Office prior to their examinations. Students should also contact their instructors as soon as possible to make alternative arrangements for classes assignments and tests.', 'Continuation in a course that uses on line elements will be deemed consent to this disclosure. If you have any questions or concerns about such disclosure please discuss this with the course instructor. ONLINE PROCTORING Some courses may use online proctoring software for tests and exams.', 'Additional Materials Any additional materials used or created for the project such as scripts for data preprocessing trained models or Jupyter notebooks should be included in the submission. Submissions failing to meet these requirements may be penalized. Please ensure that all submitted materials are well organized and clearly labeled for easy navigation. McMaster University 7', 'Completed Projects Assignments should be uploaded to the drop box before midnight of the due date. Projects Assignments missed due to legitimate reasons must be completed later mutually agreed with the instructor. Departmental Policies The use of cell phones iPods laptops and other personal electronic devices are prohibited from the classroom during the class time unless the instructor makes an explicit exception.', 'Adherence to Guidelines 10 Submissions should follow the provided guidelines including format labeling and adherence to the deadline. McMaster University 2', 'All submitted work is subject to normal verification that standards of academicintegrity have been upheld e.g. on line search other software etc. . For more details about McMaster s use of Turnitin.com please go to www.mcmaster.ca academicintegrity. COURSES WITH AN ON LINE ELEMENT Some courses may use on line elements e.g.', 'Propose future research directions or applications of the TANDA methodology in other NLP tasks. McMaster University 1McMaster University SEP 775 Submission Guidelines Submit all code in a ZIP file including Jupyter Notebooks and a detailed PDF report with written explanations and visualizations. Clearly label each part and question in your submissions. Deadline March 13 2024 Evaluation Criteria Depth and clarity of theoretical understanding and literature review.']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9832537859774183,0.25,0.9376665691180891,0.043478260869565216
448,What does the course hope to teach regarding word vectors?,The course hopes to teach that word meaning can be represented rather well by a high-dimensional vector of real numbers.,"['Optimization basics 6. Looking at word vectors Key learning today The astounding result that word meaning can be represented rather well by a high dimensional vector of real numbersCourse logistics in brief 3 Instructor Hamidreza Mahyar Head TA Reza Namazi Time Wednesday 3 30pm 6 30pm We put a lot of other important information on Avenue to Learn. Please read itWhat do we hope to teach A.k.a.', 'While there has been later substantial amount of work that focuses on learning word vectors we consider the approach proposed in 13 to be the simplest one. Note that related models have been proposed also much earlier 26 8 .', '3.2 Retraining Word Vectors As we have discussed so far the word vectors we use for extrinsic tasks are initialized by optimizing them over a simpler intrinsic task. In many cases these pretrained word vectors are a good proxy for optimal word vectors for the extrinsic task and they perform well at the extrinsic task. However it is also possible that the pretrained word vectors could be trained further i.e.', 'file name: lecture01 wordvecs1. Hamidreza Mahyar mahyarh@mcmaster.ca Computational Natural Language Processing Overview of NLP and Word VectorsLecture Plan 2 Lecture 1 Introduction and Word Vectors 1. The course 2. Human language and word meaning 3. Word2vec introduction 4. Word2vec objective function gradients 5. Optimization basics 6.', 'In this work we directly extend this architecture and focus just on the Ô¨Årst step where the word vectors are learned using a simple model. It was later shown that the word vectors can be used to signiÔ¨Åcantly improve and simplify many NLP applications 4 5 29 .', 'Collobert and Weston 2008 decoupled the word vector training from the downstream training objectives which paved the way for Collobert et al. 2011 to use the full context of a word for learning the word represen tations rather than just the preceding context as is the case with language models.', 'The word vectors can also be initialized to random values then estimated solely through task learning which we might call learning from scratch. 6 We can use expert built data structures like WordNet as additional input to creating word vectors.', 'Motivation Let us consider an example where our Ô¨Ånal goal is to create a question answering system which uses word vectors as inputs. One approach of doing so would be to train a machine learning system that 1. Takes words as inputs 2. Converts them to word vectors 3. Uses word vectors as inputs for an elaborate machine learning system 4.']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9031701111073178,0.5,0.9637591978828217,0.0625
449,How does Word2vec represent word meaning?,Word2vec represents each word in a fixed vocabulary as a low-dimensional vector that is much smaller than the vocabulary size. The model assigns a vector representation to each word in a way that captures the meaning of the word based on its context in the corpus. This vector representation allows for semantic relationships between words to be captured and utilized in various natural language processing tasks.,"['However many of the details of word2vec will hold true in methods that we ll proceed to further in the course so we ll focus our time on that. 3.2 Word2vec model and objective The word2vec model represents each word in a fixed vocabulary as a low dimensional much smaller than vocabulary size vector.', 'The word2vec model is a probabilistic model specified as follows where uw refers to the row of U corresponding to word w V and likewise for V pU V o c exp u o vc w V exp u wvc 4 This may be familiar to you as the softmax function which takes arbitrary scores here one for each word in the vocabulary resulting from dot products and produces a probability distribution where larger scored things get higher probability.', '718 0.147 0.051 19Word meaning as a neural word vector visualization 0.286 0.792 0.177 0.107 0.109 0.542 0.349 0.271 0.487 expect 203. Word2vec Overview 21 Word2vec Mikolov et al. 2013 is a framework for learning word vectors Idea We have a large corpus body of text', 'Just as we used static embeddings like word2vec in Chapter 6 to represent the meaning of words we can use contextual embeddings as representations of word meanings in context for any task that might require a model of word meaning.', '3.3 Estimating a word2vec model from a corpus How do we train word2vec in practice Specifying the word2vec model is relatively transparent from the math we ve given above one constructs matrices U and V and can write out the math of the probability.', 'It consistently outperforms word2vec on the word analogy task given the same corpus vocabulary window size and training time. It achieves better results faster and also obtains the best results irrespective of speed. 2 Evaluation of Word Vectors So far we have discussed methods such as the Word2Vec and GloVe methods to train and discover latent vector representations of natural language words in a semantic space.', '4 for a plot of the learning curve. 4.7 Model Analysis Comparison with word2vec A rigorous quantitative comparison of GloVe with word2vec is complicated by the existence of many parameters that have a strong effect on per formance.', 'Performance is lower for extremely low dimensional word vec tors Lower dimensional word vectors are not able to capture the dif ferent meanings of the different words in the corpus. This can be viewed as a high bias problem where our model complexity is too low. For instance let us consider the words king queen man woman . Intuitively we would need to use two dimensions such as gender and leadership to encode these into 2 bit word vec tors.']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9362376495948294,1.0,0.9250991091429055,0.0625
450,What is the main focus of this lecture on neural networks?,"The main focus of this lecture on neural networks is on understanding neural networks as a class of non-linear models that have performed well in deep learning applications, introducing single and multilayer neural networks for classification purposes, emphasizing the need for non-linear classifiers due to the limited classification performance on data that are not linearly separable, and motivating the use of recurrent neural networks as a language model. Additionally, the lecture discusses neural network foundations, the representation of neural net equations as a graph, and the prowess of neural networks in creating non-linear decision boundaries for better data separation.","['In the next set of notes we study neural networks as a class of non linear models that have performed particularly well in deep learning applications. Figure 10 Here we see that the non linear decision boundary allows for much better classiÔ¨Åcation of datapoints.', 'file name: lecture02 notes Backpropagation.  Neural networks. Forward computation. Backward propagation. Neuron Units. Max margin Loss. Gradient checks. Xavier parameter initialization. Learning rates. Adagrad. This set of notes introduces single and multilayer neural networks and how they can be used for classiÔ¨Åcation purposes.', 'However in reality human neural networks are so much more capable and complex from artiÔ¨Åcial neural net works that it is usually better to not draw too many parallels between the two. We established in our previous discussions the need for non linear classiÔ¨Åers since most data are not linearly separable and thus our classiÔ¨Åcation performance on them is limited.', 'Source nodes inputs Interior nodes operations 44Computation Graphs and Backpropagation Software represents our neural net equations as a graph Source nodes inputs Interior nodes operations Edges pass along result of the operation 45Computation Graphs and Backpropagation Software represents our neural net equations as a graph Source nodes inputs Interior nodes operations Edges pass along result of the operation Forward Propagation 46Backpropagation Then go backwards along edges Pass along gradients', 'Lastly we will motivate the use of recurrent neural networks as a language model. 1 Neural Networks Foundations Figure 1 We see here how a non linear decision boundary separates the data very well. This is the prowess of neural networks. Fun Fact Neural networks are biologically in spired classiÔ¨Åers which is why they are often called artiÔ¨Åcial neural networks to distinguish them from the organic kind.', '20 X. Glorot and Y. Bengio. Understanding the difÔ¨Åculty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artiÔ¨Åcial intelligence and statistics pages 249 256 2010. 21 B. Hancock A. Bordes P. E. Mazare and J. Weston.', '2007 . Optimization and applications of echo state networks with leaky integrator neurons. Neural Networks 20 3 335 352. LukoÀá seviÀá cius M. and Jaeger H. 2009 . Reservoir computing approaches to recurrent neural network training. Computer Science Review 3 3 127 149.', 'In this section we ll discuss a bit about the neural modeling ap proaches we ve discussed in Cs 224n so far and how their limitations and changes in the world inspired the modern as of 2023 zeitgeist of self attention and Transformer based architectures. 1.1 Notation and basics Let w1 n be a sequence where each wi V a finite vocabulary.']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9995535046621988,1.0,0.8776016314927766,0.03125
451,What are the non-linear functions mentioned for neural networks?,"The non-linear functions mentioned for neural networks include the squashing sigmoid function, transfer function, and activation functions such as sigmoidal neurons. These functions introduce non-linearities in neural networks, allowing them to approximate complex functions and create non-linear decision boundaries for tasks like regression or classification.","['We will see that a neuron can be one of many functions that allows for non linearities to accrue in the network. lently formulate a 1 1 exp wT b x 1 Figure 2 This image captures how in a sigmoid neuron the input vector x is Ô¨Årst scaled summed added to a bias unit and then passed to the squashing sigmoid function.', 'In the next set of notes we study neural networks as a class of non linear models that have performed particularly well in deep learning applications. Figure 10 Here we see that the non linear decision boundary allows for much better classiÔ¨Åcation of datapoints.', 'œÜf a b 11 where f is the transfer function that is the analog of the non linearity in neural networks f is the derivative of the transfer function Œ∑ is a real valued function and c is the log parti tion function. œÜ is a constant that scales the output variance.', 'logistic 1.702ùë•Non linearities i.e. f on previous slide Why they re needed Neural networks do function approximation e.g. regression or classification Without non linearities deep neural networks can t do anything more than a linear transform Extra layers could just be compiled down into a single linear transform W1 W2 x Wx But with more layers that include non linearities they can approximate any complex function 5Training with cross', 'We now introduce the need for non linear classiÔ¨Åcation models such as neural networks. We see in Figure 9 that a linear classiÔ¨Åer mis classiÔ¨Åes many datapoints. Using a non linear decision boundary as shown in Figure 10 we manage to classify all training points accu rately. Although oversimpliÔ¨Åed this is a classic case demonstrating the need for non linear decision boundaries.', 'Thus we must typically divide the outputs of each neuron during testing by a certain value it is left as an exercise to the reader to determine what this value should be in order for the expected outputs during training and testing to be equivalent. 2.4 Neuron Units So far we have discussed neural networks that contain sigmoidal neurons to introduce nonlinearities however in many applications better networks can be designed using other activation functions.', 'However in reality human neural networks are so much more capable and complex from artiÔ¨Åcial neural net works that it is usually better to not draw too many parallels between the two. We established in our previous discussions the need for non linear classiÔ¨Åers since most data are not linearly separable and thus our classiÔ¨Åcation performance on them is limited.', 'Neural networks are a family of classiÔ¨Åers with non linear decision boundary as seen in Figure 1. Now that we know the sort of decision boundaries neural networks create let us see how they manage doing so. 1.1 A Neuron A neuron is a generic computational unit that takes n inputs and produces a single output. What differentiates the outputs of different neurons is their parameters also referred to as their weights .']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9911262334187995,0.8,0.9120109702329149,0.05263157894736842
452,What is the purpose of non-linearities in neural networks?,"The purpose of non-linearities in neural networks is to allow for more complex function approximation, such as regression or classification. Without non-linearities, deep neural networks can only perform a linear transform, limiting their ability to model and learn from data effectively. By introducing non-linearities, neural networks can approximate any complex function, enabling them to handle tasks that require more sophisticated decision boundaries and mappings between inputs and outputs.","['In the next set of notes we study neural networks as a class of non linear models that have performed particularly well in deep learning applications. Figure 10 Here we see that the non linear decision boundary allows for much better classiÔ¨Åcation of datapoints.', 'We now introduce the need for non linear classiÔ¨Åcation models such as neural networks. We see in Figure 9 that a linear classiÔ¨Åer mis classiÔ¨Åes many datapoints. Using a non linear decision boundary as shown in Figure 10 we manage to classify all training points accu rately. Although oversimpliÔ¨Åed this is a classic case demonstrating the need for non linear decision boundaries.', 'We will see that a neuron can be one of many functions that allows for non linearities to accrue in the network. lently formulate a 1 1 exp wT b x 1 Figure 2 This image captures how in a sigmoid neuron the input vector x is Ô¨Årst scaled summed added to a bias unit and then passed to the squashing sigmoid function.', 'logistic 1.702ùë•Non linearities i.e. f on previous slide Why they re needed Neural networks do function approximation e.g. regression or classification Without non linearities deep neural networks can t do anything more than a linear transform Extra layers could just be compiled down into a single linear transform W1 W2 x Wx But with more layers that include non linearities they can approximate any complex function 5Training with cross', 'However in reality human neural networks are so much more capable and complex from artiÔ¨Åcial neural net works that it is usually better to not draw too many parallels between the two. We established in our previous discussions the need for non linear classiÔ¨Åers since most data are not linearly separable and thus our classiÔ¨Åcation performance on them is limited.', 'nonlinearities in self attention Note that there are no elementwise nonlinearities in self attention stacking more self attention layers just re averages value vectors Why Easy fix add a feed forward network to post process each output vector. ùëöùëñ ùëÄùêøùëÉoutputùëñ ùëä2 ReLU ùëä1 outputùëñ ùëè1 ùëè2 ùë§1 The ùë§2 chef', 'The summed inputs are computed through a linear projection with the weight matrix W l and the bottom up inputs hl given as follows al i wl i hl hl 1 i f al i bl i 1 where f is an element wise non linear function and wl i is the incoming weights to the ith hidden units and bl i is the scalar bias parameter. The parameters in the neural network are learnt using gradient based optimization algorithms with the gradients being computed by back propagation.', 'œÜf a b 11 where f is the transfer function that is the analog of the non linearity in neural networks f is the derivative of the transfer function Œ∑ is a real valued function and c is the log parti tion function. œÜ is a constant that scales the output variance.']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.0000000000000004,1.0,0.9267540781334255,0.058823529411764705
453,What is cross entropy loss used for in PyTorch?,"Cross entropy loss is used in PyTorch to maximize the probability of the correct class or equivalently minimize the negative log probability of that class. It is a concept from information theory where the true probability distribution is denoted as p and the computed model probability is denoted as q. Cross entropy is used when the actual probability distribution that generated some data is unknown, allowing the use of a model (m) of that distribution. It is also an upper bound on entropy. Additionally, in PyTorch, cross entropy loss is used in training to drive the learning process and update the weights of the model.","['can approximate any complex function 5Training with cross entropy loss you use this in PyTorch Until now our objective was stated as to maximize the probability of the correct class y or equivalently we can minimize the negative log probability of that class Now restated in terms of cross entropy a concept from information theory Let the true probability distribution be p let our computed model probability be q The cross entropy is Assuming a ground truth or true or gold or target probability distribution that', 'know that you ll want to use it as the loss in PyTorch 6Remember Stochastic Gradient Descent Update equation i.e. for each parameter In deep learning ùúÉincludes the data representation e.g. word vectors too How can we compute ùúÉùêΩ ùúÉ 1. By hand 2.', 'To summarize by making some incorrect but convenient simplifying assump tions we can compute the entropy of some stochastic process by taking a very long sample of the output and computing its average log probability. Now we are ready to introduce cross entropy. The cross entropy is useful when cross entropy we don t know the actual probability distribution p that generated some data. It allows us to use some m which is a model of p i.e.', 'Using the Cross entropy loss function we calculate the loss of such a training example as C j 1 yj log p yj 1 x C j 1 yj log exp Wj x C c 1 exp Wc x Of course the above summation will be a sum over C 1 zero values since yj is 1 only at a single index at least for now implying that x belongs to only 1 correct class.', 'What makes the cross entropy useful is that the cross entropy H p m is an up per bound on the entropy H p .', 'the same values for i and j J W i 1 W j 1 Xij log Qij where the value of co occurring frequency is given by the co occurrence matrix X. One signiÔ¨Åcant drawback of the cross entropy loss is that it requires the distribution Q to be properly normalized which involves the expensive summation over the entire vocabulary.', 'Training proceeds in the usual way cross entropy loss between the softmax output and the correct answer is used to drive the learning that produces WC. A key difference from what we ve seen earlier with neural classiÔ¨Åers is that this loss can be used to not only learn the weights of the classiÔ¨Åer but also to update the weights for the pretrained language model itself.', 'After extending this trend by two more orders of magnitude we observe only a slight if any departure from the power law. One might worry that these improvements in cross entropy loss come only from modeling spurious details of our training corpus. However we will see in the following sections that improvements in cross entropy loss lead to consistent performance gains across a broad spectrum of natural language tasks.']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999190167003905,0.8333333333333334,0.9573442044937273,0.0625
454,What is the significance of the backpropagation algorithm?,"The significance of the backpropagation algorithm lies in its ability to efficiently calculate loss gradients for parameters in a neural network model by using the chain rule of differentiation during the feedforward computation. This allows for updating parameters using gradient descent or its variants, such as SGD, which is crucial for training neural networks effectively. Backpropagation makes it computationally affordable to adjust the network's parameters based on the error gradients, contributing to the optimization process and improving the model's performance.","['The idea behind using this objective is to ensure that the score computed for true labeled data points is higher than the score computed for false labeled data points. Using the previous example if we call the score computed for the true labeled window Museums in Paris are amazing as s and the score computed for the false labeled window Not all museums in Paris as sc subscripted as c to signify that the window is corrupt .', 'Now a natural question you might ask is if this method is so pre cise why do we not use it to compute all of our network gradients instead of applying back propagation The simple answer as hinted earlier is inefÔ¨Åciency recall that every time we want to compute the gradient with respect to an element we need to make two forward passes through the network which will be computationally expen sive.', '. . 1 . Such a recursive procedure is what makes backpropa gation a computationally affordable procedure. 2 Neural Networks Tips and Tricks Having discussed the mathematical foundations of neural networks we will now dive into some tips and tricks commonly employed when using neural networks in practice. 2.1 Gradient Check In the last section we discussed in detail how to calculate error gradients updates for parameters in a neural network model via calculus based analytic methods.', 'Since we typically update parameters using gradient descent or a variant such as SGD we typically need the gradient information for any parameter as required in the update equation Œ∏ t 1 Œ∏ t Œ± Œ∏ t J Backpropagation is technique that allows us to use the chain rule of differentiation to calculate loss gradients for any parameter used in the feed forward computation on the model.', 'Let us begin Suppose the cost J 1 sc s is positive and we want to perform the update of parameter W 1 14 in Figure 5 and Figure 6 we must realize that W 1 14 only contributes to z 2 1 and thus a 2 1 . This fact is crucial to understanding backpropagation backpropagated gradients are only affected by values they contribute to.', 'But what is the intuition behind doing so Let us consider the following named entity recognition NER problem in NLP as an example Museums in Paris are amazing Dimensions for a single hidden layer neural network If we represent each word using a 4 dimensional word vector and we use a 5 word window as input then the input x R20.', 'Formally Œ∏t i Œ∏t 1 i Œ± q t œÑ 1 g2 œÑ i gt i where gt i Œ∏t i Jt Œ∏ In this technique we see that if the RMS of the history of gradients is extremely low the learning rate is very high. A simple implemen tation of this technique is Snippet 2.', 'W k is the transfer matrix that maps the output from the k th layer to the input to the k 1 th Thus W 1 W and W 2 U to put this new generalized notation in perspective of Section 1.3. Backpropagation Notation xi is an input to the neural network. s is the output of the neural net work.']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.000000000000001,1.0,0.9536464960245421,0.058823529411764705
455,What does the Jacobian matrix represent in the context of neural networks?,"In the context of neural networks, the Jacobian matrix represents a matrix of partial derivatives that describes how the outputs of a function change with respect to changes in its inputs. It is used to compute gradients for functions with multiple outputs and inputs, allowing for efficient optimization during training. The Jacobian matrix helps in understanding the sensitivity of the network's outputs to changes in its parameters or inputs. 

Source: lecture02_Neural_Nets","['m outputs and n inputs It s Jacobian is an m x n matrix of partial derivatives 12Chain Rule For composition of one variable functions multiply derivatives For multiple variables functions multiply Jacobians 13Example Jacobian Elementwise activation Function 14Example Jacobian Elementwise activation Function Function has n outputs and n inputs n by n Jacobian 15Example Jacobian Elementwise activation Function 16Example Jacobian Elementwise activation Function 17Example Jacobian Elementwise', 'Note that we assume the parametrization given by equation 11 . The Jacobian matrix xk 1 xk is given by WT recdiag œÉ xk . The 2 norm of this Jacobian is bounded by the product ofOn the diÔ¨Éculty of training Recurrent Neural Networks the norms of the two matrices see equation 6 .', 'Equation 5 also provides the form of Jacobian matrix xi xi 1 for the speciÔ¨Åc parametrization given in equation 11 where diag converts a vector into a diagonal matrix and œÉ computes the derivative of œÉ in an element wise fash ion.', '2.1. The mechanics To understand this phenomenon we need to look at the form of each temporal component and in particular at the matrix factors xt xk see equation 5 that take the form of a product of t k Jacobian matrices.', 'is the same In general our nets have regular layer structure and so we can use matrices and Jacobians Back Prop in General Computation Graph Inputs Single scalar output 73Automatic Differentiation The gradient computation can be automatically inferred from the symbolic expression of the fprop Each node type needs to know how to compute its output and how to compute the gradient wrt its inputs given the gradient wrt its output Modern DL frameworks Tensorflow PyTorch etc. do', 'input a bit At x 1 it changes about 3 times as much 1.013 1.03 At x 4 it changes about 48 times as much 4.013 64.48Gradients Given a function with 1 output and n inputs Its gradient is a vector of partial derivatives with respect to each input 11Jacobian Matrix Generalization of the Gradient Given a function with m outputs and n inputs It s Jacobian is', 'look like 1 output nm inputs 1 by nm Jacobian Inconvenient to then do 38Derivative with respect to Matrix Output shape What does look like 1 output nm inputs 1 by nm Jacobian Inconvenient to then do Instead we leave pure math and use the shape convention the shape of the gradient is the shape of the parameters So is n by m 39Derivative with respect to Matrix What is is going to be in our', 'activation Function 17Example Jacobian Elementwise activation Function 18Other Jacobians Compute these at home for practice Check your answers with the lecture notes 19Other Jacobians Compute these at home for practice Check your answers with the lecture notes 20Other Jacobians Compute these at home for practice Check your answers with the lecture notes Fine print This is the correct Jacobian. Later we discuss the shape convention using it the answer would be h.']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999999999999988,0.8,0.9414776365711045,0.08333333333333333
456,How is the derivative with respect to a matrix described?,"The derivative with respect to a matrix is described as representing the direction to locally move the matrix in order to maximally increase the value of the scalar function. In practice, this involves drawing the initial matrix randomly and adjusting it based on the calculated derivative.","['Briefly the gradient think derivative U f of a scalar function f with respect to a parameter matrix U represents the direction to locally move U in in order to maximally increase the value of f. So in practice we do something like drawing the initial U 0 and V 0 randomly as U V N 0 0.', 'fm x1 . . . fm xn That is f x ij fi xj which is just a standard non vector derivative . The Jacobian matrix will be useful for us because we can apply the chain rule to a vector valued function just by multiplying Jacobians.', '1 Matrix times column vector with respect to the column vector z W x what is z x Suppose W Rn m. Then we can think of z as a function of x taking an m dimensional vector to an n dimensional vector.', 'It would be much nicer if the derivatives were in a n m matrix like this J W J W11 . . . J W1m . . . ... . . . J Wn1 . . . J Wnm Since this matrix has the same shape as W we could just subtract it times the learning rate from W when doing gradient descent.', 'Since multiplication by a diagonal matrix is the same as doing elementwise multiplication by the diagonal we could also write f x when applying the chain rule. 5 Matrix times column vector with respect to the matrix z W x Œ¥ J z what is J W J z z W Œ¥ z W This is a bit more complicated than the other identities. The reason for in cluding J z in the above problem formulation will become clear in a moment.', 'As a little illustration of this suppose we have a function f x f1 x f2 x taking a scalar to a vector of size 2 and a function g y g1 y1 y2 g2 y1 y2 taking a vector of size two to a vector of size two. Now let s compose them to get g x g1 f1 x f2 x g2 f1 x f2 x . Using the regular chain rule we can compute the derivative', 'This matrix is equal to the outer product J W Œ¥T xT 6 Row vector time matrix with respect to the matrix z xW Œ¥ J z what is J W Œ¥ z W A similar computation to 5 shows that J W xT Œ¥ .', 'So we see that z x W 2 Row vector times matrix with respect to the row vector z xW what is z x A computation similar to 1 shows that z x W T . 3 A vector with itself z x what is z x We have zi xi.']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9630155031779594,1.0,0.8912128636204886,0.0625
457,What role does automatic differentiation play in modern deep learning frameworks?,"Automatic differentiation plays a crucial role in modern deep learning frameworks by allowing each module to independently compute derivatives with respect to its inputs and trainable parameters. This feature enables easy building of variants of neural networks and helps in efficient gradient computations, which are essential for training deep learning models effectively. Additionally, automatic differentiation helps in verifying the correctness of analytic gradients, ensuring the accuracy of the training process.","['Given derivatives with respect to its outputs each module can independently compute derivatives with respect to its inputs and with respect to its trainable parameters as proposed by Bottou and Gallinari 1991 . This allows us to easily build variants of our networks. For details about gradient computations see Appendix A. Remark 7 Tricks Many tricks have been reported for training neural networks LeCun et al. 1998 .', 'However in Understanding the dif ficulty of training deep feedforward neural networks 2010 Xavier et al study the effect of different weight and bias initialization schemes on training dynamics. The empirical Ô¨Åndings suggest that for sigmoid and tanh activation units faster convergence and lower error rates are achieved when the weights of a matrix W Rn l 1 n l are initialized randomly with a uniform distribution as follows W U s 6 n l n l 1 s', '1 Introduction Deep neural networks trained with some version of Stochastic Gradient Descent have been shown to substantially outperform previous approaches on various supervised learning tasks in computer vision Krizhevsky et al. 2012 and speech processing Hinton et al. 2012 . But state of the art deep neural networks often require many days of training.', 'Here we now introduce a tech nique of numerically approximating these gradients though too computationally inefÔ¨Åcient to be used directly for training the net works this method will allow us to very precisely estimate the derivative with respect to any parameter it can thus serve as a useful sanity check on the correctness of our analytic derivatives.', 'One of the challenges of deep learning is that the gradients with respect to the weights in one layer are highly dependent on the outputs of the neurons in the previous layer especially if these outputs change in a highly correlated way. Batch normalization Ioffe and Szegedy 2015 was proposed to reduce such undesirable covariate shift . The method normalizes the summed inputs to each hidden unit over the training cases. SpeciÔ¨Åcally for the ith summed input in', 'Furthermore many large scale neural networks can contain millions of parameters and computing two passes per parameter is clearly not optimal. And since in optimization techniques such as SGD we must compute the gradients once per iteration for several thousands of iterations it is obvious that this method quickly grows intractable. This inefÔ¨Åciency is why we only use gradient check to verify the correctness of our analytic gradients which are much quicker to compute.', 'The algorithm is motivated by the assumption that when gradients explode the curvature and higher or der derivatives explode as well and we are faced with a speciÔ¨Åc pattern in the error surface namely a val ley with a single steep wall. In order to deal with the vanishing gradient problem we use a regulariza tion term that forces the error signal not to vanish as it travels back in time.', 'The derivative for the ReLU is either 0 or 1. This way gradients would Ô¨Çow through the neurons whose deriva tive is 1 without getting attenuated while propagating back through time steps. 2.5 Deep Bidirectional RNNs So far we have focused on RNNs that condition on past words to predict the next word in the sequence.']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.0000000000000002,0.8571428571428571,0.9220775158548289,0.05
458,What is the purpose of manual gradient checking?,"The purpose of manual gradient checking is to verify the correctness of our analytic gradients, as it is much quicker to compute compared to numerical gradients. This is particularly important in large-scale neural networks with millions of parameters, where computing two passes per parameter is not optimal and can quickly become intractable. By using gradient checks, we can ensure that our analytic gradients are accurate without the computational inefficiencies of numerical gradient calculations.","['We note that this deÔ¨Ånition of the numerical gradient follows very naturally from the deÔ¨Ånition of the derivative where in the scalar case f x f x œµ f x œµ Gradient checks are a great way to compare analytical and numerical gradients.', 'A standard implementation of gradient check is shown below Snippet 2.1 def eval_numerical_gradient f x a naive implementation of numerical gradient of f at x f should be a function that takes a single argument x is the point numpy array to evaluate the gradient at fx f x evaluate function value at original point grad np.zeros x.shape h 0.00001 iterate over all indexes in x it np.', 'Furthermore many large scale neural networks can contain millions of parameters and computing two passes per parameter is clearly not optimal. And since in optimization techniques such as SGD we must compute the gradients once per iteration for several thousands of iterations it is obvious that this method quickly grows intractable. This inefÔ¨Åciency is why we only use gradient check to verify the correctness of our analytic gradients which are much quicker to compute.', 'Analytical gradients should be close and numerical gradients can be calculated using f Œ∏ J Œ∏ i J Œ∏ i 2œµ J Œ∏ i and J Œ∏ i can be evalu ated using two forward passes. An implementation of this can be seen in Snippet 2.1. Of course there is a slight difference the deÔ¨Ånition above only perturbs x in the positive direction to compute the gradient.', 'frameworks Tensorflow PyTorch etc. do backpropagation for you but mainly leave layer node writer to hand calculate the local derivative 74Backprop Implementations 75Implementation forward backward API 76Implementation forward backward API 77Manual Gradient checking Numeric Gradient For small h 1e 4 Easy to implement correctly But approximate and very slow You have to recompute f for every parameter of our model Useful for checking your implementation In the old days we hand', 'for checking your implementation In the old days we hand wrote everything doing this everywhere was the key test Now much less needed you can use it to check layers are correctly implemented 78Summary We ve mastered the core technology of neural nets Backpropagation recursively and hence efficiently apply the chain rule along computation graph downstream gradient upstream gradient x local gradient Forward pass compute results of operations and save intermediate values Backward pass apply chain rule to compute gradients 79Why learn all these details', 'This makes the math a bit cumbersome. Even worse if we re implementing the model without automatic diÔ¨Äerentiation computing J Œ∏ twice will be ineÔ¨Écient. So it will help us to deÔ¨Åne some variables to represent the intermediate derivatives Œ¥1 J Œ∏ Œ¥2 J z These can be thought as the error signals passed down to Œ∏ and z when doing backpropagation. We can compute them as follows Œ¥1 J', 'Here we now introduce a tech nique of numerically approximating these gradients though too computationally inefÔ¨Åcient to be used directly for training the net works this method will allow us to very precisely estimate the derivative with respect to any parameter it can thus serve as a useful sanity check on the correctness of our analytic derivatives.']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999999999999997,0.4,0.9214501795705239,0.06666666666666667
459,Why is understanding gradients important despite automatic differentiation?,"Understanding gradients is important despite automatic differentiation because having a good grasp of gradients helps in debugging and improving models. It allows us to comprehend what is happening under the hood, which is crucial when backpropagation doesn't work perfectly out of the box. Additionally, understanding gradients is essential for verifying the correctness of analytic gradients through techniques like gradient checks. This knowledge is valuable for ensuring the accuracy and efficiency of the training process, especially in large-scale neural networks with millions of parameters where computing gradients manually for every iteration is not optimal. Therefore, understanding gradients provides deeper insights into the optimization process and can lead to more effective model development and troubleshooting. 

Source: SEP 775 - Computational Natural Language Processing, lecture02_Neural_Nets.txt","['We note that this deÔ¨Ånition of the numerical gradient follows very naturally from the deÔ¨Ånition of the derivative where in the scalar case f x f x œµ f x œµ Gradient checks are a great way to compare analytical and numerical gradients.', 'Here we now introduce a tech nique of numerically approximating these gradients though too computationally inefÔ¨Åcient to be used directly for training the net works this method will allow us to very precisely estimate the derivative with respect to any parameter it can thus serve as a useful sanity check on the correctness of our analytic derivatives.', 'Briefly the gradient think derivative U f of a scalar function f with respect to a parameter matrix U represents the direction to locally move U in in order to maximally increase the value of f. So in practice we do something like drawing the initial U 0 and V 0 randomly as U V N 0 0.', 'Furthermore many large scale neural networks can contain millions of parameters and computing two passes per parameter is clearly not optimal. And since in optimization techniques such as SGD we must compute the gradients once per iteration for several thousands of iterations it is obvious that this method quickly grows intractable. This inefÔ¨Åciency is why we only use gradient check to verify the correctness of our analytic gradients which are much quicker to compute.', 'to compute gradients 79Why learn all these details about gradients 80 Modern deep learning frameworks compute gradients for you But why take a class on compilers or systems when they are implemented for you Understanding what is going on under the hood is useful Backpropagation doesn t always work perfectly out of the box Understanding why is crucial for debugging and improving models See Karpathy article https medium.', 'It would be much nicer if the derivatives were in a n m matrix like this J W J W11 . . . J W1m . . . ... . . . J Wn1 . . . J Wnm Since this matrix has the same shape as W we could just subtract it times the learning rate from W when doing gradient descent.', 'The algorithm is motivated by the assumption that when gradients explode the curvature and higher or der derivatives explode as well and we are faced with a speciÔ¨Åc pattern in the error surface namely a val ley with a single steep wall. In order to deal with the vanishing gradient problem we use a regulariza tion term that forces the error signal not to vanish as it travels back in time.', 'This makes the math a bit cumbersome. Even worse if we re implementing the model without automatic diÔ¨Äerentiation computing J Œ∏ twice will be ineÔ¨Écient. So it will help us to deÔ¨Åne some variables to represent the intermediate derivatives Œ¥1 J Œ∏ Œ¥2 J z These can be thought as the error signals passed down to Œ∏ and z when doing backpropagation. We can compute them as follows Œ¥1 J']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9536899962692664,0.7142857142857143,0.9399121853754226,0.06666666666666667
460,What is the core concept introduced in this class that leads to GPT-3 and ChatGPT?,"The core concept introduced in this class that leads to models like GPT-3 and ChatGPT is the use of large-scale language models trained on vast amounts of text data to perform various natural language processing tasks. These models leverage deep learning techniques to understand and generate human-like text, enabling them to excel in tasks such as language generation, reading comprehension, and text synthesis. They have the ability to learn from context and generate coherent and contextually relevant responses, making them powerful tools in the field of natural language processing.

Source: SEP 775 - Computational Natural Language Processing","['Finally given the broad spectrum of capabilities displayed by GPT 3 we discuss concerns about bias fairness and broader societal impacts and attempt a preliminary analysis of GPT 3 s characteristics in this regard. The remainder of this paper is organized as follows. In Section 2 we describe our approach and methods for training GPT 3 and evaluating it.', '3.6 Reading Comprehension Next we evaluate GPT 3 on the task of reading comprehension. We use a suite of 5 datasets including abstractive multiple choice and span based answer formats in both dialog and single question settings. We observe a wide spread in GPT 3 s performance across these datasets suggestive of varying capability with different answer formats.', 'GPT 3 performs best within 3 points of the human baseline on CoQA RCM19 a free form conversational dataset and performs worst 13 F1 below an ELMo baseline on QuAC CHI 18 a dataset which requires modeling structured dialog acts and answer span selections of teacher student interactions.', 'We will release a collection of 500 uncurated unconditional samples to help provide a better sense of GPT 3 s limitations and strengths at text synthesis. Within the domain of discrete language tasks we have noticed informally that GPT 3 seems to have special difÔ¨Åculty with common sense physics despite doing well on some datasets such as PIQA BZB 19 that test this domain.', 'However it is very likely that incorporating these algorithmic advances could improve GPT 3 s performance on downstream tasks especially in the Ô¨Åne tuning setting and combining GPT 3 s scale with these algorithmic techniques is a promising direction for future work.', 'Second we conduct experiments using the publicly available GPT 3 API for four model scales text ada 001 text babbage 001 text curie 001 text davinci 002 . Finally we make exact inputs targets and predictions for LaMDA 137B for each task available as a zip Ô¨Åle in the supplementary material.', 'The ability of GPT 3 to generate several paragraphs of synthetic content that people Ô¨Ånd difÔ¨Åcult to distinguish from human written text in 3.9.4 represents a concerning milestone in this regard.', 'In this work we do not Ô¨Åne tune GPT 3 because our focus is on task agnostic performance but GPT 3 can be Ô¨Åne tuned in principle and this is a promising direction for future work.']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.987917170570614,0.8333333333333334,0.9070298072531569,0.07142857142857142
461,What is the purpose of Dropout as mentioned in the document?,"The purpose of Dropout, as mentioned in the document, is to act as another form of regularization by randomly dropping neurons in the forward pass, which effectively helps prevent neural networks from overfitting by training exponentially many smaller networks at once and averaging over their predictions. Additionally, Dropout is a powerful technique that can lead to more meaningful learning from data, reduce the likelihood of overfitting, and improve overall performance on the task at hand.","['The result is that the network typically learns more meaningful informa tion from the data is less likely to overÔ¨Åt and usually obtains higher performance overall on the task at hand. One intuitive reason why this technique should be so effective is that what dropout is doing is essentially doing is training exponentially many smaller networks at once and averaging over their predictions. Dropout applied to an artiÔ¨Åcial neural network.', 'However a key sub tlety is that in order for dropout to work effectively the expected output of a neuron during testing should be approximately the same as it was during training else the magnitude of the outputs could be radically different and the behavior of the network is no longer well deÔ¨Åned.', 'Consider the question what causes pre cipitation to fall posed on the passage in Figure 1. In order to answer the question one might Ô¨Årst lo cate the relevant part of the passage precipitation ... falls under gravity then reason that under refers to a cause not location and thus determine the cor rect answer gravity .', 'kid stubbornly postponees pursuing job citing medical reasons despite tried reasonable compro mise negatively effecting forward employment mobility both personally and Ô¨Ånancially thoght wise want change this dumbass selÔ¨Åsh lazy atti tude now please help pls Table 29 Example of samples from our over optimized model overÔ¨Åt to one of our 1.3B reward models. 45', 'A summary has poor coverage if someone reading only the summary would be missing several important pieces of information about the situation in the post. A summary with good coverage should also match the purpose of the original post e.g. to ask for advice . Rubric Score of 1 The summary contains no information relevant to the post.', 'For example using the conÔ¨Ådence thresholds mentioned above we found that while lower conÔ¨Ådence labels were less useful than high conÔ¨Ådence labels for improving reward model accuracy they were still better to include than to omit. Similarly leaving out workers with poorer agreement rates did not help.', 'In the next section we discuss dropout which effectively acts as another form of regularization by randomly dropping i.e. setting to zero neurons in the forward pass. 2.3 Dropout Dropout is a powerful technique for regularization Ô¨Årst introduced by Srivastava et al. in Dropout A Simple Way to Prevent Neural Net works from OverÔ¨Åtting.', 'We conÔ¨Årmed that every matching training document contained only the source passage and none of the questions and answers in the dataset. The more likely explanation for the decrease in performance is that the 6 of examples that remain after Ô¨Åltering come from a slightly different distribution than the dirty examples.']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9653191454129152,1.0,0.9281964389763891,0.058823529411764705
462,What are some approaches to address the vanishing gradient problem in RNNs?,"Some approaches to address the vanishing gradient problem in RNNs include using a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem, as well as opting for a regularization term that represents a preference for parameter values such that backpropagated gradients neither increase nor decrease too much in magnitude. Additionally, using the Hessian Free optimizer in conjunction with structural damping has been suggested as a strategy that seems to deal well with the vanishing gradient issue, although more detailed analysis is still needed. LSTM (Long Short Term Memory) RNNs, proposed by Hochreiter and Schmidhuber in 1997, have also been recognized as a solution to the vanishing gradient problem, with crucial contributions from Gers et al. in 2000. 

Source: Lecture03_RNNs_and_LLMs, On_the_difficulty_of_training_Recurrent_Neural_Networks","['2013. http proceedings.mlr.press v28 pascanu13.pdfHow to fix the vanishing gradient problem The main problem is that it s too difficult for the RNN to learn to preserve information over many timesteps.', 'Our analysis is used to justify a simple yet ef fective solution. We propose a gradient norm clipping strategy to deal with exploding gra dients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section. 1. Introduction A recurrent neural network RNN e.g. Fig.', 'Below we discuss the math ematical reasoning behind the vanishing gradient problem. Consider Equations 5 and 6 at a time step t to compute the RNN error dE dW we sum the error at each time step. That is dEt dW for every time step t is computed and accumulated.', '3.3. Vanishing gradient regularization We opt to address the vanishing gradients problem us ing a regularization term that represents a preference for parameter values such that back propagated gra dients neither increase or decrease too much in mag nitude.', 'Sutskever et al. 2011 use the Hessian Free opti mizer in conjunction with structural damping a spe ciÔ¨Åc damping strategy of the Hessian. This approach seems to deal very well with the vanishing gradient though more detailed analysis is still missing.', 'with RNNs Vanishing and Exploding Gradients 48Vanishing gradient intuition 49Vanishing gradient intuition chain rule 50Vanishing gradient intuition chain rule 51Vanishing gradient intuition chain rule 52Vanishing gradient intuition What happens if these are small Vanishing gradient problem When these are small the gradient signal gets smaller and smaller as it backpropagates further 53Vanishing gradient proof sketch linear case Recall What if were the identity', '60Long Short Term Memory RNNs LSTMs A type of RNN proposed by Hochreiter and Schmidhuber in 1997 as a solution to the problem of vanishing gradients Everyone cites that paper but really a crucial part of the modern LSTM is from Gers et al. 2000 Only started to be recognized as promising through the work of S s student Alex Graves c.', 'To gain practical intuition about the vanishing gradient problem you may visit the following example website. 2.4 Solution to the Exploding Vanishing Gradients Now that we gained intuition about the nature of the vanishing gradi ents problem and how it manifests itself in deep neural networks let us focus on a simple and practical heuristic to solve these problems.']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9995309578238164,0.8571428571428571,0.812047578724999,0.05555555555555555
463,"What is perplexity, and how is it used in evaluating language models?","Perplexity is a metric used to evaluate language models in natural language processing. It is the inverse probability of a test set normalized by the number of words in the set. A lower perplexity indicates that a language model is a better predictor of the words in the test set. Perplexity is closely related to the information theoretic notion of entropy and can be used to compare different n-gram models. However, it's important to note that perplexity alone does not guarantee an improvement in the performance of language processing tasks like speech recognition or machine translation. It is commonly used as a quick check on an algorithm's performance. In practice, perplexity is calculated without any prior knowledge of the test set or its vocabulary to ensure fair comparisons between language models. Source: N_gram_Language_Models.txt","['3.2.1 Perplexity In practice we don t use raw probability as our metric for evaluating language mod els but a variant called perplexity. The perplexity sometimes called PPL for short perplexity of a language model on a test set is the inverse probability of the test set normalized by the number of words. For a test set W w1w2 .wN perplexity W P w1w2 .', 'The exact choice of UNK has an effect on metrics like perplexity. A language model can achieve low perplexity by choosing a small vocabulary and assigning the unknown word a high probability. Thus perplexities can only be compared across language models with the same vocabularies Buck et al. 2014 . 3.', 'We leave this exact calculation as exercise 3.12. We see in Section 3.8 that perplexity is also closely related to the information theoretic notion of entropy. We mentioned above that perplexity is a function of both the text and the lan guage model given a text W different language models will have different perplex ities. Because of this perplexity can be used to compare different n gram models.', 'An intrinsic improvement in perplexity does not guarantee an extrinsic im provement in the performance of a language processing task like speech recognition10 CHAPTER 3 N GRAM LANGUAGE MODELS or machine translation. Nonetheless because perplexity often correlates with such improvements it is commonly used as a quick check on an algorithm.', 'And the higher the probability the lower the perplexity since as Eq. 3.15 showed perplexity is related inversely to the likelihood of the test sequence according to the model . So a lower perplexity can tell us that a language model is a better predictor of the words in the test set.', 'Note that in computing perplexities the n gram model P must be constructed without any knowledge of the test set or any prior knowledge of the vocabulary of the test set. Any kind of knowledge of the test set can cause the perplexity to be artiÔ¨Åcially low. The perplexity of two language models is only comparable if they use identical vocabularies.', 'A better n gram model is one that assigns a higher probability to the test data and perplexity is a normalized version of the probability of the test set. The perplexity measure actually arises from the information theoretic concept of cross entropy which explains otherwise mysterious properties of perplexity why the inverse probability for example and its relationship to entropy. Entropy is a Entropy measure of information.', 'The perplexity of a test set according to a language model is the geometric mean of the inverse test set probability computed by the model. Smoothing algorithms provide a more sophisticated way to estimate the prob ability of n grams. Commonly used smoothing algorithms for n grams rely on lower order n gram counts through backoff or interpolation. Both backoff and interpolation require discounting to create a probability dis tribution.']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9978147557084348,1.0,0.954780746573739,0.5185185185185185
464,How does LSTM solve the vanishing gradient problem?,"LSTM (Long Short-Term Memory) solves the vanishing gradient problem by introducing a special set of units that have a linear structure and a recurrent connection to itself which is fixed to 1. This architecture makes it easier for an RNN to preserve information over many timesteps, thus addressing the vanishing gradient issue. Additionally, LSTM units have mechanisms like forget gates and input gates that help in controlling the flow of information and gradients, further mitigating the vanishing gradient problem.","['Sutskever et al. 2011 use the Hessian Free opti mizer in conjunction with structural damping a spe ciÔ¨Åc damping strategy of the Hessian. This approach seems to deal very well with the vanishing gradient though more detailed analysis is still missing.', 'The algorithm is motivated by the assumption that when gradients explode the curvature and higher or der derivatives explode as well and we are faced with a speciÔ¨Åc pattern in the error surface namely a val ley with a single steep wall. In order to deal with the vanishing gradient problem we use a regulariza tion term that forces the error signal not to vanish as it travels back in time.', 'Below we discuss the math ematical reasoning behind the vanishing gradient problem. Consider Equations 5 and 6 at a time step t to compute the RNN error dE dW we sum the error at each time step. That is dEt dW for every time step t is computed and accumulated.', 'When the gradient value goes to zero however it can go undetected while dras tically reducing the learning quality of the model for far away words in the corpus this issue is called the Vanishing Gradient Problem. Due to vanishing gradients we don t know whether there is no dependency between steps t and t n in the data or we just cannot capture the true dependency due to this issue.', 'In Hochreiter and Schmidhuber 1997 Graves et al. 2009 a solution is proposed for the vanishing gra dients problem where the structure of the model is changed. SpeciÔ¨Åcally it introduces a special set of units called LSTM units which are linear and have a recurrent connection to itself which is Ô¨Åxed to 1.', '3.3. Vanishing gradient regularization We opt to address the vanishing gradients problem us ing a regularization term that represents a preference for parameter values such that back propagated gra dients neither increase or decrease too much in mag nitude.', 'To gain practical intuition about the vanishing gradient problem you may visit the following example website. 2.4 Solution to the Exploding Vanishing Gradients Now that we gained intuition about the nature of the vanishing gradi ents problem and how it manifests itself in deep neural networks let us focus on a simple and practical heuristic to solve these problems.', 'github.io posts 2015 08 Understanding LSTMs 65How does LSTM solve vanishing gradients 66 The LSTM architecture makes it much easier for an RNN to preserve information over many timesteps e.g. if the forget gate is set to 1 for a cell dimension and the input gate set to 0 then the information of that cell is preserved indefinitely. In contrast it s harder for a vanilla RNN to']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.0000000000000009,1.0,0.9457284189524471,0.058823529411764705
465,"What is the impact of bidirectional RNNs, and where are they applicable?","Bidirectional RNNs provide a contextual representation that includes both left and right context, making them useful for tasks where understanding both past and future information is important. However, bidirectional RNNs are only applicable when you have access to the entire input sequence, and they are not suitable for tasks like Language Modeling where only left context is available.","['Input audio START conditioning 77Bidirectional RNNs was terribly exciting the movie Forward RNN Backward RNN Concatenated hidden states This contextual representation of terribly has both left and right context 78Bidirectional RNNs 79 Note bidirectional RNNs are only applicable if you have access to the entire input sequence They are not applicable to Language Modeling because in LM you only have left context available.', 'The Problem of Long Term Dependencies One of the appeals of RNNs is the idea that they might be able to connect previous information to the present task such as using previous video frames might inform the understanding of the present frame. If RNNs could do this they d be extremely useful. But can they It depends. Sometimes we only need to look at recent information to perform the present task.', 'Perplexity 2J 9 2.2 Advantages Disadvantages and Applications of RNNs RNNs have several advantages 1. They can process input sequences of any length 2. The model size does not increase for longer input sequence lengths 3. Computation for step t can in theory use information from many steps back. 4.', 'Unfortunately as that gap grows RNNs become unable to learn to connect the information. In theory RNNs are absolutely capable of handling such long term dependencies. A human could carefully pick parameters for them to solve toy problems of this form. Sadly in practice RNNs don t seem to be able to learn them.', 'Linear interaction distance. A related issue with RNNs is the diffi culty with which distant tokens in a sequence can interact with each other. By interact we mean that the presence of one token already observed in the past gainfully affects the processing of another token.', 'A simple form of RNN is as follows ht œÉ W ht 1 Uxt 4 Dependence on the sequence index where ht Rd U Rd d and W Rd d. By 2017 the intuition was that there were twofold issues with the recurrent neural net work form and they both had to do with the the depenence on the sequence index often called the dependence on time highlighted in Equation 4.', 'h h h Figure 8 A bi directional RNN model h t f W xt V h t 1 b 17 h t f W xt V h t 1 b 18 ÀÜ yt g Uht c g U h t h t c 19 RNNs can also be multi layered. Figure 9 shows a multi layer bi directional RNN where each lower layer feeds the next layer.', 'input context Same weights applied on every timestep so there is symmetry in how inputs are processed. RNN Disadvantages Recurrent computation is slow In practice difficult to access information from many steps back More on these later 30Training an RNN Language Model Get a big corpus of text which is a sequence of words Feed into RNN LM compute output distribution for every step t. i.e. predict probability dist of every word given words so far']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.922093222854596,1.0,0.971402854089245,0.38095238095238093
466,Describe the concept of multi-layer or stacked RNNs and their benefit.,"Multi-layer or stacked RNNs consist of multiple RNN layers where the hidden states from one layer serve as inputs to the next layer. This allows the network to compute more complex representations by having lower RNNs compute lower level features and higher RNNs compute higher level features. Stacked RNNs enable the network to capture more intricate patterns and dependencies in the data, leading to improved performance compared to using just a single layer of high-dimensional encodings. Additionally, research has shown that having multiple layers in RNNs, typically 2 to 4 layers for tasks like Neural Machine Translation, can be beneficial for achieving better results. To train deeper RNNs effectively, skip connections or dense connections are often necessary. 

Source: Lecture03_RNNs_and_LLMs","['This allows the network to compute more complex representations The lower RNNs should compute lower level features and the higher RNNs should compute higher level features. Multi layer RNNs are also called stacked RNNs. 80Multi layer RNNs was terribly exciting the movie RNN layer 1 RNN layer 2 81 RNN layer 3 The hidden states from RNN layer i are the inputs to RNN layer i', 'layer i are the inputs to RNN layer i 1Multi layer RNNs in practice 39 Multi layer or stacked RNNs allow a network to compute more complex representations they work better than just have one layer of high dimensional encodings The lower RNNs should compute lower level features and the higher RNNs should compute higher level features.', 'h h h Figure 8 A bi directional RNN model h t f W xt V h t 1 b 17 h t f W xt V h t 1 b 18 ÀÜ yt g Uht c g U h t h t c 19 RNNs can also be multi layered. Figure 9 shows a multi layer bi directional RNN where each lower layer feeds the next layer.', 'RNNs should compute higher level features. High performing RNNs are usually multi layer but aren t as deep as convolutional or feed forward networks For example In a 2017 paper Britz et al.', 'org pdf 1703.03906.pdfTerminology By the end of the course You will understand phrases like stacked bidirectional LSTMs with residual connections and self attention The RNN described in this lecture simple vanilla Elman RNN You learned about other RNN flavors like LSTM and GRU and multi layer RNNs 83', 'To maintain two hidden layers at any time this network consumes twice as much memory space for its weight and bias parameters. The Ô¨Ånal classiÔ¨Åcation result ÀÜ yt is generated through combining the score results produced by both RNN hidden layers. Fig ure 8 shows the bi directional network architecture and Equations 17 and 18 show the mathematical formulation behind setting up the bi directional RNN hidden layer.', 'Perplexity 2J 9 2.2 Advantages Disadvantages and Applications of RNNs RNNs have several advantages 1. They can process input sequences of any length 2. The model size does not increase for longer input sequence lengths 3. Computation for step t can in theory use information from many steps back. 4.', 'find that for Neural Machine Translation 2 to 4 layers is best for the encoder RNN and 4 layers is best for the decoder RNN Often 2 layers is a lot better than 1 and 3 might be a little better than 2 Usually skip connections dense connections are needed to train deeper RNNs e.g. 8 layers Transformer based networks e.g.']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9516766997768183,0.875,0.960457801953072,0.05555555555555555
467,How does Dropout work during training and testing?,"During training, dropout works by randomly dropping neurons in the forward pass by setting them to zero with a probability \( p \). This helps prevent overfitting by training multiple smaller networks simultaneously and averaging their predictions. During testing, the network uses all neurons in the network to compute the forward pass, ensuring that the expected output of a neuron during testing is similar to that during training to maintain network behavior. 

Source: Lecture02_notes_Backpropagation","['However a key sub tlety is that in order for dropout to work effectively the expected output of a neuron during testing should be approximately the same as it was during training else the magnitude of the outputs could be radically different and the behavior of the network is no longer well deÔ¨Åned.', 'Image credits to Srivastava et al. In practice the way we introduce dropout is that we take the out put h of each layer of neurons and keep each neuron with prob ability p and else set it to 0. Then during back propagation we only pass gradients through neurons that were kept alive during the forward pass. Finally during testing we compute the forward pass using all of the neurons in the network.', 'The result is that the network typically learns more meaningful informa tion from the data is less likely to overÔ¨Åt and usually obtains higher performance overall on the task at hand. One intuitive reason why this technique should be so effective is that what dropout is doing is essentially doing is training exponentially many smaller networks at once and averaging over their predictions. Dropout applied to an artiÔ¨Åcial neural network.', 'In the next section we discuss dropout which effectively acts as another form of regularization by randomly dropping i.e. setting to zero neurons in the forward pass. 2.3 Dropout Dropout is a powerful technique for regularization Ô¨Årst introduced by Srivastava et al. in Dropout A Simple Way to Prevent Neural Net works from OverÔ¨Åtting.', 'Training on the test set introduces a bias that makes the probabilities all look too high and causes huge inaccuracies in perplexity the probability based metric we introduce below. Sometimes we use a particular test set so often that we implicitly tune to its characteristics. We then need a fresh test set that is truly unseen. In such cases we call the initial test set the development test set or devset.', 'Lastly all the training examples are sorted by the length of paragraph and divided into mini batches of 32 examples each. We use Adamax for optimization as described in Kingma and Ba 5We use the Wikipedia Search API https www. mediawiki.org wiki API Search. 2014 . Dropout with p 0.3 is applied to word embeddings and all the hidden units of LSTMs.', '5 except p 0.15 for input layer via dropout mask During testing Multiply all weights by 1 ùëù No other dropout ùë•2 ùë•3 0 ùë§1 ùë§2 ùë§3 ùë§4 ùëè 1 ùë¶ ùë§1ùë•1 ùë§2ùë•2 ùë§3ùë•3 ùë•', 'While GPT 3 takes a step towards test time sample efÔ¨Åciency closer to that of humans one shot or zero shot it still sees much more text during pre training than a human sees in the their lifetime Lin20 . Improving pre training sample efÔ¨Åciency is an important direction for future work and might come from grounding in the physical world to provide additional information or from algorithmic improvements.']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9915064809453206,1.0,0.9184956625233034,0.045454545454545456
468,What is the role of parameter initialization in neural networks?,"Parameter initialization in neural networks plays a crucial role in achieving optimal performance. Proper initialization of weights and biases can lead to faster convergence and lower error rates during training. It is recommended to initialize weights to small random numbers normally distributed around 0 to ensure the network starts learning effectively. Different initialization schemes can impact training dynamics and the overall performance of the neural network. Additionally, initializing parameters in a reasonable way helps maintain activation variances and backpropagated gradient variances across layers, contributing to the stability and efficiency of the training process.","['s 6 n l n l 1 s 6 n l n l 1 Where n l is the number of input units to W fan in and n l 1 is the number of output units from W fan out . In this parameter initialization scheme bias units are initialized to 0. This approach attempts to maintain activation variances as well as backpropagated gradient variances across layers.', 'However in Understanding the dif ficulty of training deep feedforward neural networks 2010 Xavier et al study the effect of different weight and bias initialization schemes on training dynamics. The empirical Ô¨Åndings suggest that for sigmoid and tanh activation units faster convergence and lower error rates are achieved when the weights of a matrix W Rn l 1 n l are initialized randomly with a uniform distribution as follows W U s 6 n l n l 1 s', '2.6 Parameter Initialization A key step towards achieving superlative performance with a neu ral network is initializing the parameters in a reasonable way. A good starting strategy is to initialize the weights to small random numbers normally distributed around 0 and in practice this often words acceptably well.', '2015 . Our proposed layer normalization method however is not a re parameterization of the original neural network. The layer normalized model thus has different invariance properties than the other methods that we will study in the following section. 5 Analysis In this section we investigate the invariance properties of different normalization schemes. 5.1 Invariance under weights and data transformations The proposed layer normalization is related to batch normalization and weight normalization.', 'Similar to the re centering of the weight matrix in layer normalization we can also show that batch normalization is invariant to re centering of the dataset. 5.2 Geometry of parameter space during learning We have investigated the invariance of the model s prediction under re centering and re scaling of the parameters. Learning however can behave very differently under different parameterizations even though the models express the same underlying function.', 'In weight normalization instead of the variance the L2 norm of the incoming weights is used to normalize the summed inputs to a neuron. Applying either weight normalization or batch normal ization using expected statistics is equivalent to have a different parameterization of the original feed forward neural network. Re parameterization in the ReLU network was studied in the Path normalized SGD Neyshabur et al. 2015 .', 'For Ô¨Åne tuning the BERT model is Ô¨Årst initialized with the pre trained parameters and all of the param eters are Ô¨Åne tuned using labeled data from the downstream tasks. Each downstream task has sep arate Ô¨Åne tuned models even though they are ini tialized with the same pre trained parameters.', 'The summed inputs are computed through a linear projection with the weight matrix W l and the bottom up inputs hl given as follows al i wl i hl hl 1 i f al i bl i 1 where f is an element wise non linear function and wl i is the incoming weights to the ith hidden units and bl i is the scalar bias parameter. The parameters in the neural network are learnt using gradient based optimization algorithms with the gradients being computed by back propagation.']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9872076333770005,1.0,0.9325463385044754,0.045454545454545456
469,"What are the differences between Vanilla RNNs, LSTMs, and GRUs?","Vanilla RNNs have a simple architecture where the hidden state is constantly being rewritten, making it difficult to retain information over long sequences. LSTMs (Long Short-Term Memory) address the vanishing gradient problem by introducing gates like input, forget, and output gates to control the flow of information in the network, allowing for better long-term memory retention. GRUs (Gated Recurrent Units) are a variation of LSTMs that combine the forget and input gates into a single update gate, simplifying the architecture while still being effective in capturing long-range dependencies. 

Source: lecture03_RNNs_and_LLMs.txt","['The motivation for using these is similar to those for GRUs however the architecture of such units does differ. Let us Ô¨Årst take a look at the mathematical formulation of LSTM units before diving into the intuition behind this design it œÉ W i xt U i ht 1 Input gate ft œÉ W f xt U f ht 1 Forget gate ot œÉ W o xt U o ht 1 Output Exposure gate ct tanh W c xt U c ht', 'In a vanilla RNN the hidden state is constantly being rewritten First off next time How about an RNN with separate memory which is added to LSTMs And then Creating more direct and linear pass through connections in model Attention residual connections etc.', 'github.io posts 2015 08 Understanding LSTMs 65How does LSTM solve vanishing gradients 66 The LSTM architecture makes it much easier for an RNN to preserve information over many timesteps e.g. if the forget gate is set to 1 for a cell dimension and the input gate set to 0 then the information of that cell is preserved indefinitely. In contrast it s harder for a vanilla RNN to', 'org pdf 1703.03906.pdfTerminology By the end of the course You will understand phrases like stacked bidirectional LSTMs with residual connections and self attention The RNN described in this lecture simple vanilla Elman RNN You learned about other RNN flavors like LSTM and GRU and multi layer RNNs 83', 'There are lots of others like Depth Gated RNNs by Yao et al. 2015 http arxiv.org pdf 1508.03790v2.pdf . There s also some completely different approach to tackling long term dependencies like Clockwork RNNs by Koutnik et al. 2014 http arxiv.org pdf 1402.3511v1.pdf .', '60Long Short Term Memory RNNs LSTMs A type of RNN proposed by Hochreiter and Schmidhuber in 1997 as a solution to the problem of vanishing gradients Everyone cites that paper but really a crucial part of the modern LSTM is from Gers et al. 2000 Only started to be recognized as promising through the work of S s student Alex Graves c.', 'Jozefowicz et al. 2015 http jmlr.org proceedings papers v37 jozefowicz15.pdf tested more than ten thousand RNN architectures finding some that worked better than LSTMs on certain tasks. Conclusion Earlier I mentioned the remarkable results people are achieving with RNNs. Essentially all of these are achieved using LSTMs.', 'A new family of neural networks Recurrent Neural Networks RNNs This is the most important concept in the class It leads to GPT 3 and ChatGPT Important and used in Ass1 but not the only way to build LMs0 model power Training error Test error Classic view Regularization works to prevent overfitting when we have a lot of features or later a very powerful deep model etc.']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9616244390478762,0.6,0.9500524552398928,0.07692307692307693
470,What is the difference between traditional language models and conditioned language models?,"Traditional language models typically consider only a finite window of previous words for conditioning the model, while conditioned language models, such as Recurrent Neural Networks (RNN), are capable of conditioning the model on all previous words in the corpus. This allows conditioned models to have a more comprehensive understanding of the context in which words appear, potentially leading to improved performance in tasks that require understanding long-range dependencies in language.","['2 Recurrent Neural Networks RNN Unlike the conventional translation models where only a Ô¨Ånite win dow of previous words would be considered for conditioning the language model Recurrent Neural Networks RNN are capable of conditioning the model on all previous words in the corpus. xt 1 xt xt 1 ht 1 ht ht 1 W W yt 1 yt yt 1 Figure 3 A Recurrent Neural Network RNN .', '3.4. While they both model English like sentences there is clearly no overlap in generated sentences and little overlap even in small phrases. Statistical models are likely to be pretty use less as predictors if the training sets and the test sets are as different as Shakespeare and WSJ.', 'As a result our experiments do not include any bidirectional architectures or other training objectives such as denoising. This is a noticeable difference from much of the recent literature which has documented improved Ô¨Åne tuning performance when using these approaches over standard language models RSR 19 . Thus our design decision comes at the cost of potentially worse performance on tasks which empirically beneÔ¨Åt from bidirectionality.', 'file name: lecture04 Conditioned Generation. Hamidreza Mahyar mahyarh@mcmaster.ca Computational Natural Language Processing Conditioned GenerationLanguage Models Language models are generative models of text s P x The Malfoys said Hermione. Harry was watching him. He looked like Madame Maxime. When she strode up the wrong staircase to visit himself.', 'For many purposes a pretrained multilingual model is more practical than a monolingual model since it avoids the need to build many 100 separate monolin gual models. And multilingual models can improve performance on low resourced languages by leveraging linguistic information from a similar language in the train ing data that happens to have more resources.', 'Our approach of stufÔ¨Ång the model s context with previous examples is most structurally similar to RL2 and also resembles HYC01 in that an inner loop of adaptation takes place through computation in the model s activations across timesteps without updating the weights while an outer loop in this case just language model pre training updates the weights and implicitly learns the ability to adapt to or at least recognize tasks deÔ¨Åned at inference time.', 'We documented roughly predictable trends of scaling in performance without using Ô¨Åne tuning. We also discussed the social impacts of this class of model. Despite many limitations and weaknesses these results suggest that very large language models may be an important ingredient in the development of adaptable general language systems. Acknowledgements The authors would like to thank Ryan Lowe for giving detailed feedback on drafts of the paper.', 'While n gram models are much simpler than state of the art neural language models based on the RNNs and trans formers we will introduce in Chapter 9 they are an important foundational tool for understanding the fundamental concepts of language modeling.']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9999989821844766,1.0,0.9011931366608699,0.047619047619047616
471,How can LSTM networks be used in language modeling?,"LSTM networks can be used in language modeling by modeling the temporal interactions between words. They can capture long-term dependencies in text, making them suitable for tasks where understanding context over a large span is crucial. Additionally, LSTM networks can be placed in both directions and their outputs can be concatenated to enhance the model's ability to understand and generate language effectively.","['of any length Apply the same weights on each step Can optionally produce output on each step Recurrent Neural Network Language Model We ve shown that RNNs are a great way to build a LM despite some problems RNNs are also useful for much moreWhy should we care about Language Modeling 70 Language Modeling is a benchmark task that helps us measure our progress on predicting language use Language Modeling is a subcomponent of many NLP tasks especially those involving generating', '3. Contextual Embedding Layer. We use a Long Short Term Memory Network LSTM Hochreiter Schmidhuber 1997 on top of the embeddings provided by the previous layers to model the temporal interactions between words. We place an LSTM in both directions and concatenate the outputs of the two LSTMs.', 'Your model should include an embedding layer at least one RNN layer and a fully connected layer for output. Refer to the Recurrent Neural Networks RNN section of the lectures for guidance on the architecture. Use the Long Short Term Memory RNNs LSTMs section as a reference to enhance your model with LSTM cells to improve its ability to capture long term dependencies in text. 2.', 'LSTM neural networks for language modeling. In Proc. of Interspeech 2012. 14Peter D. Turney and Patrick Pantel. From frequency to meaning Vector space models of semantics. Journal of ArtiÔ¨Åcial Intelligence Research 37 1 141 188 2010.', 'We report with LMn performance of the networks trained from the language model embeddings Table 7 . Gen eralization performance is reported in per word accuracy PWA for POS and F1 score for other tasks. language models from the relatively fast training of the supervised networks. Once the language models are trained we can perform multiple experiments on the supervised networks in a rela tively short time.', '2.2 Recurrent Neural Net Language Model RNNLM Recurrent neural network based language model has been proposed to overcome certain limitations of the feedforward NNLM such as the need to specify the context length the order of the model N and because theoretically RNNs can efÔ¨Åciently represent more complex patterns than the shallow neural networks 15 2 .', 'All the networks were trained separately on each task using the sentence level likelihood SLL . The word embeddings were initialized to LM2 embeddings and then Ô¨Åne tuned for each task. We summarize features used by our implementation in Table 14 and we report performance achieved on each task in Table 15.', 'Hopefully walking through them step by step in this essay has made them a bit more approachable.1 24 24 3 23 PM Understanding LSTM Networks colah s blog 7 7 Attention and Augmented Recurrent Neural Networks On Distill Conv Nets A Modular Perspective Neural Networks Manifolds and Topology Deep Learning NLP and Representations LSTMs were a big step in what we can accomplish with RNNs.']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.0000000000000007,1.0,0.9564290923600679,0.21739130434782608
472,What are the methods of generation in conditioned language modeling?,"The methods of generation in conditioned language modeling include NL Generation, Translation, Summarization, Response Generation, Image Captioning, and Speech Recognition.","['file name: lecture04 Conditioned Generation. Hamidreza Mahyar mahyarh@mcmaster.ca Computational Natural Language Processing Conditioned GenerationLanguage Models Language models are generative models of text s P x The Malfoys said Hermione. Harry was watching him. He looked like Madame Maxime. When she strode up the wrong staircase to visit himself.', 'generate text according to some specification Input X Structured Data English Document Utterance Image Speech Output Y Text NL Description Japanese Short Description Response Text Transcript Task NL Generation Translation Summarization Response Generation Image Captioning Speech RecognitionFormulation and ModelingCalculating the Probability of a SentenceConditional Language ModelsOne Type of Language Model LSTM LSTM LSTM LSTM predict hate predict this predict movie predict s LSTM Mikolov et al.', 'Future directions. The methods in this paper could be applied to any task where humans can compare samples including dialogue machine translation question answering speech synthesis and music generation. We expect this method to be particularly important for generating long samples where the distributional shift and degeneracy of maximum likelihood samples can be problematic. It may be possible to improve sample efÔ¨Åciency by training to predict feedback across many tasks 42 .', '7871 7880 2020. URL https aclanthology.org 2020.acl main.703. Xiang Lisa Li and Percy Liang. PreÔ¨Åx tuning Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing Volume 1 Long Papers pp.', 'The generative model is trained on one billion words. These approaches should be seen as a linear counterpart of our work. Using multilayer models vastly expands the parameter sharing opportunities see Section 5 . Our approach simply consists of initializing the word lookup tables of the supervised networks with the embeddings computed by the language models. Supervised training is then performed as in Section 3.5.', '3.9.4 News Article Generation Previous work on generative language models qualitatively tested their ability to generate synthetic news articles by conditional sampling from the model given a human written prompt consisting of a plausible Ô¨Årst sentence for a news story RWC 19 .', '2 Recurrent Neural Networks RNN Unlike the conventional translation models where only a Ô¨Ånite win dow of previous words would be considered for conditioning the language model Recurrent Neural Networks RNN are capable of conditioning the model on all previous words in the corpus. xt 1 xt xt 1 ht 1 ht ht 1 W W yt 1 yt yt 1 Figure 3 A Recurrent Neural Network RNN .', 'We report with LMn performance of the networks trained from the language model embeddings Table 7 . Gen eralization performance is reported in per word accuracy PWA for POS and F1 score for other tasks. language models from the relatively fast training of the supervised networks. Once the language models are trained we can perform multiple experiments on the supervised networks in a rela tively short time.']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9898872166401128,0.6666666666666666,0.8704792720507712,0.037037037037037035
473,What is beam search and how is it used in language generation?,"Beam search is a technique used in language generation where, on each step of the decoder, the algorithm keeps track of the k most probable partial translations, referred to as hypotheses. The value of k, known as the beam size, is typically set around 5 to 10 in Neural Machine Translation (NMT) tasks. Each hypothesis has a score, which is its log probability. The scores are all negative, and a higher score indicates a better hypothesis. Beam search is more efficient than exhaustive search but is not guaranteed to find the optimal solution. It helps in searching for high-scoring hypotheses by tracking the top k on each step. By maintaining several paths instead of picking just one high probability word, beam search allows for a more diverse and potentially better generation of sequences in language models.

Source: lecture05_Seq2Seq","['Core idea On each step of decoder keep track of the k most probable partial translations which we call hypotheses k is the beam size in practice around 5 to 10 in NMT A hypothesis has a score which is its log probability Scores are all negative and higher score is better We search for high scoring hypotheses tracking top k on each step Beam search is not guaranteed to find optimal solution But much more efficient than exhaustive search 18Beam search decoding example Beam size k', 'On tasks with free form completion we use beam search with the same parameters as RSR 19 a beam width of 4 and a length penalty of Œ± 0.6. We score the model using F1 similarity score BLEU or exact match depending on what is standard for the dataset at hand. Final results are reported on the test set when publicly available for each model size and learning setting zero one and few shot .', 'Argmax Try to generate the sentence with the highest probability.Ancestral Sampling Randomly generate words one by one. An exact method for sampling from P X no further work needed.Greedy Search One by one pick the single highest probability word Not exact real problems Will often generate the easy words first Will prefer multiple common words to one rare wordBeam Search Instead of picking one high probability word maintain several paths Some in reading materials more in a later classModel', '2020 which were optimized for GPT 3 without performing any prompt engineering to optimize them on our model. For simplicity we use greedy search for all generative tasks compared with beam search used in Brown et al. 2020 .', 'Quality can often be improved signiÔ¨Åcantly by non uniform sampling strategies such as beam search 51 but these can lead to repetition and other undesirable artifacts 69 23 . Optimizing for quality may be a principled approach to overcoming these problems. Our goal in this paper is to advance methods for training language models on objectives that more closely capture the behavior we care about.', 'search 18Beam search decoding example Beam size k 2. Blue numbers START Calculate prob dist of next word 19Beam search decoding example Beam size k 2. Blue numbers START he I 0.7 log PLM he START 0.9 log PLM I START Take top k words and compute scores 20Beam search decoding example Beam size k 2. Blue numbers hit struck was got START he I 10 0.', 'Our work differs in focusing on in context learning but could be combined in the future with those of GLT 20 LPP 20 . Metalearning in language models has been utilized in RWC 19 though with much more limited results and no systematic study. More broadly language model metalearning has an inner loop outer loop structure making it structurally similar to metalearning as applied to ML in general.', '6 For each of the k hypotheses find top k next words and calculate scoresBeam search decoding example Beam size k 2.']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9218209045611295,1.0,0.9126744863936759,0.05263157894736842
474,Explain the concept of model ensembling in language models.,"Model ensembling in language models involves using multiple models together to improve performance. One common approach is parameter averaging, where several models are trained independently and then their parameters are averaged to make predictions. This technique can help mitigate overfitting and improve the overall robustness of the model. Another method is ensemble distillation, where the knowledge from multiple models is combined into a single model to achieve better performance. By leveraging the strengths of different models, ensembling can enhance the predictive power and generalization of language models.","['agree use when you want to restrict possible answersParameter Averaging Problem Ensembling means we have to use M models at test time increasing our time memory complexity Parameter averaging is a cheap way to get some good effects of ensembling Basically write out models several times near the end of training and take the average of parametersEnsemble Distillation e.g. Kim et al.', 'Surprisingly grammatical but incoherent. We need to consider more than three words at a time if we want to model language well. But increasing n worsens sparsity problem and increases model sizeHow to build a neural language model Recall the Language Modeling task Input sequence of words Output prob. dist. of the next word How about a window based neural model We saw this applied to Named Entity Recognition in Lecture 2 LOCATION', 'We documented roughly predictable trends of scaling in performance without using Ô¨Åne tuning. We also discussed the social impacts of this class of model. Despite many limitations and weaknesses these results suggest that very large language models may be an important ingredient in the development of adaptable general language systems. Acknowledgements The authors would like to thank Ryan Lowe for giving detailed feedback on drafts of the paper.', 'But a model s improvement in perplexity should always be conÔ¨Årmed by an end to end evaluation of a real task before concluding the evaluation of the model. 3.3 Sampling sentences from a language model One important way to visualize what kind of knowledge a language model embodies is to sample from it. Sampling from a distribution means to choose random points sampling according to their likelihood.', 'Our approach of stufÔ¨Ång the model s context with previous examples is most structurally similar to RL2 and also resembles HYC01 in that an inner loop of adaptation takes place through computation in the model s activations across timesteps without updating the weights while an outer loop in this case just language model pre training updates the weights and implicitly learns the ability to adapt to or at least recognize tasks deÔ¨Åned at inference time.', 'applied to Named Entity Recognition in Lecture 2 LOCATION in Paris are amazing museums 24A fixed window neural Language Model their as the proctor started the clock discard the students opened fixed window 25A fixed window neural Language Model the students opened their books laptops concatenated word embeddings words one hot vectors hidden layer a zoo output distribution 26A fixed window neural Language Model the students opened their books laptops a zoo Improvements over n gram LM No sparsity problem', 'The literature on this is also extensive QMZH19 HZJ 19 so we offer only a few brief comments on future directions speciÔ¨Åc to large language models. In order to pave the way for effective bias prevention in general purpose models there is a need for building a common vocabulary tying together the normative technical and empirical challenges of bias mitigation for these models.', 'We also provide an in depth ablation study of our model on the SQuAD development set vi sualize the intermediate feature spaces in our model and analyse its performance as compared to a more traditional language model for machine comprehension Rajpurkar et al. 2016 . 2 MODEL Our machine comprehension model is a hierarchical multi stage process and consists of six layers Figure 1 1.']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9687264226750371,0.6,0.9083530092897878,0.05555555555555555
475,How does ensemble distillation differ from parameter averaging?,"Ensemble distillation differs from parameter averaging in that ensemble distillation trains a model to copy the ensemble by matching the description over predicted words, aiming to make the same mistakes as an ensemble to increase accuracy notably. On the other hand, parameter averaging involves writing out models several times near the end of training and taking the average of parameters as a cheap way to get some good effects of ensembling. 

Source: Lecture04_Conditioned_Generation.","['e.g. Kim et al. 2016 Problem parameter averaging only works for models within the same run Knowledge distillation trains a model to copy the ensemble Specifically it tries to match the description over predicted words Why We want the model to make the same mistakes as an ensemble Shown to increase accuracy notablyStacking What if we have two very different models where prediction of outputs is done in very different ways e.g.', 'agree use when you want to restrict possible answersParameter Averaging Problem Ensembling means we have to use M models at test time increasing our time memory complexity Parameter averaging is a cheap way to get some good effects of ensembling Basically write out models several times near the end of training and take the average of parametersEnsemble Distillation e.g. Kim et al.', 'Yet it is often difÔ¨Åcult to Ô¨Ågure out which ideas are most responsible for the state of the art performance of a large ensemble. Because neural networks are nonconvex training runs with different initial parameters usually give different solutions. Table 11 reports results obtained for the CHUNK and NER task after ten training runs with random initial parameters.', 'Training Strategy Distillation Objective Design a loss function that effectively combines the traditio nal task specific loss and a distillation loss that measures the discrepancy between the teacher s and student s hidden states. Dataset Selection Utilize task specific datasets like the Stanford Sentiment Treebank SST for sentiment analysis CoNLL 2003 for NER and WMT datasets for machine translation.', 'Some in reading materials more in a later classModel EnsemblingEnsembling Why Multiple models make somewhat uncorrelated errors Models tend to be more uncertain when they are about to make errors Smooths over idiosyncrasies of the model LSTM1 predict1 I LSTM2 predict2 Combine predictions from multiple models s sLinear Interpolation Take a weighted average of the M model probabilities Second term often set to uniform distribution 1 MLog linear Interpolation Weighted', 'The parameters of the combining layers were then trained on the existing training set while keeping the ten networks Ô¨Åxed joined ensemble . This approach did not improve on simple voting. These ensembles come of course at the expense of a ten fold increase of the running time. On the other hand multiple training times could be improved using smart sampling strategies Neal 1996 .', 'The learning rate for each parameter depends on the history of gradient updates of that parameter in a way such that parameters with a scarce history of updates are updated faster using a larger learning rate. In other words parameters that have not been updated much in the past are likelier to have higher learning rates now.', 'Table 8 The added value versus the risk of setting each hyperparameter. The Ô¨Ågures show the differences in performance between the best achievable conÔ¨Ågurations when restricting a hyperparameter to different values. This difference indicates the potential gain of tuning a given hyperparameter as well as the risks of decreased performance when not tuning it.']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9957593478996193,1.0,0.9666675356990622,0.058823529411764705
476,What are some common evaluation metrics for conditioned language models?,"Some common evaluation metrics for conditioned language models include BLEU scores for generative tasks like machine translation, accuracy for classification tasks such as sentiment analysis and NER, and metrics based on neural models like BertScore. These metrics are used to measure the effectiveness and performance of conditioned language models in various NLP tasks. 

Source: SEP775_FinalProjects_2.txt","['Indeed existing automatic metrics for evaluating summary quality such as ROUGE 39 have received criticism for poor correlation with human judgments 55 45 6 33 . We follow the works of 3 73 who Ô¨Åne tune language models from human feedback using reward learning 35 . We Ô¨Årst collect a dataset of human preferences between pairs of summaries then train a reward model RM via supervised learning to predict the human preferred summary.', 'out this review paperHow do we EvaluateBasic Evaluation Paradigm Use parallel test set Use system to generate translations Compare target translations w referenceHuman Evaluation Ask a human to do evaluation Final goal but slow expensive and sometimes inconsistentBLEU Works by comparing n gram overlap w reference Pros Easy to use good for measuring system improvement Cons Often doesn t match human eval bad for comparing very different systemsEmbedding based Metrics Recently many metrics based on neural models BertScore Find similarity between', 'Training and Distillation Fine tune the student model on the selected dataset while also learning from the teacher model s hidden state outputs to achieve similar perfor mance. McMaster University 4McMaster University SEP 775 Evaluation Metrics Performance Metrics Use accuracy for classification tasks sentiment analysis NER and BLEU scores for generative tasks machine translation to measure the student mo del s effectiveness.', 'Thus for speech recognition we can compare the performance of two language models by running the speech recognizer twice once with each language model and seeing which gives the more accurate transcription. Unfortunately running big NLP systems end to end is often very expensive. In stead it would be nice to have a metric that can be used to quickly evaluate potential improvements in a language model.', '8 Conclusion We presented a 175 billion parameter language model which shows strong performance on many NLP tasks and benchmarks in the zero shot one shot and few shot settings in some cases nearly matching the performance of 40state of the art Ô¨Åne tuned systems as well as generating high quality samples and strong qualitative performance at tasks deÔ¨Åned on the Ô¨Çy.', 'Thus our MT evaluation system requires two ingredients 1. a numerical translation closeness metric 2. a corpus of good quality human reference trans lations We fashion our closeness metric after the highly suc cessful word error rate metric used by the speech recognition community appropriately modiÔ¨Åed for multiple reference translations and allowing for le gitimate differences in word choice and word or der.', 'As a result our experiments do not include any bidirectional architectures or other training objectives such as denoising. This is a noticeable difference from much of the recent literature which has documented improved Ô¨Åne tuning performance when using these approaches over standard language models RSR 19 . Thus our design decision comes at the cost of potentially worse performance on tasks which empirically beneÔ¨Åt from bidirectionality.', 'We show that instruction tuning Ô¨Ånetuning language models on a collection of datasets described via instructions substantially improves zero shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction tune it on over 60 NLP datasets verbalized via natural language instruction templates. We evaluate this instruction tuned model which we call FLAN on unseen task types.']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",1.0000000000000002,0.75,0.9142770809464977,0.21052631578947367
477,What is the significance of stacking in model ensembling?,"Stacking in model ensembling is significant when we have two very different models that predict outputs in different ways. It allows us to combine predictions from multiple models by considering their outputs and leveraging their diverse approaches to improve overall performance. By using stacking, we can benefit from the strengths of each individual model and potentially mitigate their weaknesses, leading to more robust and accurate predictions. Additionally, stacking helps in situations where models make somewhat uncorrelated errors, as it smooths over idiosyncrasies of the models and can increase accuracy notably.","['agree use when you want to restrict possible answersParameter Averaging Problem Ensembling means we have to use M models at test time increasing our time memory complexity Parameter averaging is a cheap way to get some good effects of ensembling Basically write out models several times near the end of training and take the average of parametersEnsemble Distillation e.g. Kim et al.', 'a phrase based translation model and a neural MT model Niehues et al. 2017 Stacking uses the output of one system in calculating features for another systemCase Studies in Conditional Language ModelingFrom Images e.g. Karpathy et al. 2015 Input is image features output is text Use standard image encoders e.g. CNN Often pre trained on large databases such as ImageNetCheck out this review paperHow do we EvaluateBasic', 'Some in reading materials more in a later classModel EnsemblingEnsembling Why Multiple models make somewhat uncorrelated errors Models tend to be more uncertain when they are about to make errors Smooths over idiosyncrasies of the model LSTM1 predict1 I LSTM2 predict2 Combine predictions from multiple models s sLinear Interpolation Take a weighted average of the M model probabilities Second term often set to uniform distribution 1 MLog linear Interpolation Weighted', 'Figure 8 Ablation study result using mod els with instructions removed from Ô¨Ånetun ing FT . In a Ô¨Ånal ablation study we explore the role of in structions during Ô¨Ånetuning as one possibility is that performance gains come entirely from multi task Ô¨Åne tuning and the model could perform just as well without instructions. We hence consider two Ô¨Ånetuning setups without instructions.', 'e.g. Kim et al. 2016 Problem parameter averaging only works for models within the same run Knowledge distillation trains a model to copy the ensemble Specifically it tries to match the description over predicted words Why We want the model to make the same mistakes as an ensemble Shown to increase accuracy notablyStacking What if we have two very different models where prediction of outputs is done in very different ways e.g.', 'The small effect of templates is striking given our original motivation that composing ten templates per task would mitigate overÔ¨Åtting to any particular template. This results serves to underscore however the unpredictability of Ô¨Ånetuning large language models as one hypothesis is that models at such scale do not easily overÔ¨Åt to a Ô¨Ånetuning single task.', 'Figure 7 shows these results. We see that for the two models on the order of 100B pa rameters instruction tuning substantially im proves performance on held out tasks as is expected given the prior results in our pa per. The behavior on held out tasks for the 8B and smaller models however is thought provoking instruction tuning actually hurts performance on held out tasks.', 'See Appendix B for the details of each variation. In this appendix section we experimentally demonstrate how different choices of the similarity function Œ± Equation 1 and the fusion function Œ≤ Equation 2 impact the performance of our model. Each variation is deÔ¨Åned as following Eqn. 1 dot product. Dot product Œ± is deÔ¨Åned as Œ± h u h u 6 where indicates matrix transpose.']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9579770805575657,1.0,0.9157696380352998,0.058823529411764705
478,How are images used as input for conditioned language models?,"Images can be used as input for conditioned language models by representing the image classes as learned dimensional embeddings per class and adding the respective embedding to the input representation of every input position together with positional encodings. This allows for conditioning on image class in models like the Image Transformer, which can achieve similar log likelihoods as in unconditioned generation when trained on datasets like CIFAR 10. (Source: lecture04_Conditioned_Generation.txt)","['We further hope to have provided additional evidence that even in the light of generative adversarial networks likelihood based models of images is very much a promising area for further research as is using network architectures such as the Image Transformer in GANs. In future work we would like to explore a broader variety of conditioning information including free form text as previously proposed Mansimov et al. 2015 and tasks combining modalities such as language driven editing of images.', 'As a result our experiments do not include any bidirectional architectures or other training objectives such as denoising. This is a noticeable difference from much of the recent literature which has documented improved Ô¨Åne tuning performance when using these approaches over standard language models RSR 19 . Thus our design decision comes at the cost of potentially worse performance on tasks which empirically beneÔ¨Åt from bidirectionality.', 'a phrase based translation model and a neural MT model Niehues et al. 2017 Stacking uses the output of one system in calculating features for another systemCase Studies in Conditional Language ModelingFrom Images e.g. Karpathy et al. 2015 Input is image features output is text Use standard image encoders e.g. CNN Often pre trained on large databases such as ImageNetCheck out this review paperHow do we EvaluateBasic', 'For DMOL we only evaluated learned coordinate embeddings. 5.2. Conditioning on Image Class We represent the image classes as learned d dimensional embeddings per class and simply add the respective em bedding to the input representation of every input position together with the positional encodings. We trained the class conditioned Image Transformer on CIFAR 10 achieving very similar log likelihoods as in un conditioned generation.', 'in the embedding parameters LCG 19 . Many of these techniques provide signiÔ¨Åcant gains on downstream tasks. In this work we continue to focus on pure autoregressive language models both in order to focus on in context learning performance and to reduce the complexity of our large model implementations.', 'We show that instruction tuning Ô¨Ånetuning language models on a collection of datasets described via instructions substantially improves zero shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction tune it on over 60 NLP datasets verbalized via natural language instruction templates. We evaluate this instruction tuned model which we call FLAN on unseen task types.', '6. Conclusion In this work we demonstrate that models based on self attention can operate effectively on modalities other than text and through local self attention scale to signiÔ¨Åcantly larger structures than sentences. With fewer layers its larger receptive Ô¨Åelds allow the Image Transformer to signiÔ¨Åcantly improve over the state of the art in unconditional probabilis tic image modeling of comparatively complex images from ImageNet as well as super resolution.', 'Instruction tuning is a simple method that as depicted in Figure 2 combines appealing aspects of both the pretrain Ô¨Ånetune and prompting paradigms by using supervision via Ô¨Ånetuning to improve language model s responses to inference time text interactions. Our empirical results demonstrate promising abilities of language models to perform tasks described purely via instructions.']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9918168868462133,1.0,0.898176770415307,0.05
479,What challenges exist in evaluating conditioned language models?,"One of the challenges in evaluating conditioned language models is the need to address biases found in large-scale generative models, which requires continuous research and the development of different methodological approaches to characterize biases effectively. Additionally, the social impacts of these models need to be considered despite their limitations and weaknesses. Another challenge is the potential trade-off in performance on tasks that benefit from bidirectionality when excluding bidirectional architectures or other training objectives like denoising, as observed in recent literature. Furthermore, improving sample efficiency during pre-training is a shared limitation among language models, indicating a need for advancements in this area. These challenges highlight the complexity and ongoing research required to evaluate and enhance conditioned language models effectively.

Source: Language_Models_are_Few_Shot_Learners.txt","['In this paper we test this hypothesis by training a 175 billion parameter autoregressive language model which we call GPT 3 and measuring its in context learning abilities. SpeciÔ¨Åcally we evaluate GPT 3 on over two dozen NLP datasets as well as several novel tasks designed to test rapid adaptation to tasks unlikely to be directly contained in the training set.', '386.2.4 Future Bias and Fairness Challenges We have presented this preliminary analysis to share some of the biases we found in order to motivate further research and to highlight the inherent difÔ¨Åculties in characterizing biases in large scale generative models we expect this to be an area of continuous research for us and are excited to discuss different methodological approaches with the community.', 'We documented roughly predictable trends of scaling in performance without using Ô¨Åne tuning. We also discussed the social impacts of this class of model. Despite many limitations and weaknesses these results suggest that very large language models may be an important ingredient in the development of adaptable general language systems. Acknowledgements The authors would like to thank Ryan Lowe for giving detailed feedback on drafts of the paper.', 'The literature on this is also extensive QMZH19 HZJ 19 so we offer only a few brief comments on future directions speciÔ¨Åc to large language models. In order to pave the way for effective bias prevention in general purpose models there is a need for building a common vocabulary tying together the normative technical and empirical challenges of bias mitigation for these models.', 'As a result our experiments do not include any bidirectional architectures or other training objectives such as denoising. This is a noticeable difference from much of the recent literature which has documented improved Ô¨Åne tuning performance when using these approaches over standard language models RSR 19 . Thus our design decision comes at the cost of potentially worse performance on tasks which empirically beneÔ¨Åt from bidirectionality.', 'Promising future directions in this vein might include learning the objective function from humans ZSW 19a Ô¨Åne tuning with reinforcement learning or adding additional modalities such as images to provide grounding and a better model of the world CLY 19 . Another limitation broadly shared by language models is poor sample efÔ¨Åciency during pre training.', 'By presenting a broad characterization of GPT 3 s strengths and weaknesses including these limitations we hope to stimulate study of few shot learning in language models and draw attention to where progress is most needed. A heuristic sense of the overall results can be seen in Figure 1.3 which aggregates the various tasks though it should not be seen as a rigorous or meaningful benchmark in itself .', 'As Ô¨Åne tuned language models have neared human performance on many standard benchmark tasks considerable effort has been devoted to constructing more difÔ¨Åcult or open ended tasks including question answering KPR 19 IBGC 14 CCE 18 MCKS18 reading comprehension CHI 18 RCM19 and adversarially constructed datasets designed to be difÔ¨Åcult for existing language models SBBC19 NWD 19 .']",BAAI/bge-small-en-v1.5,128,10,8,"
You are an AI Teaching Assistant for the SEP 775 course. 
Your job is to answer students' questions about course materials according to the instructions and context provided.
If you found the answer based on the context provided, you should provide the answer first, then at the end, beginning a new sentence with the words ""Source:"", followed by the name of the lecture, or assignment, or paper if possible.
",0.9932392273138442,1.0,0.8493122167628413,0.058823529411764705
